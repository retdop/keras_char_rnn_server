    MovedAttribute("filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"),
    MovedAttribute("input", "__builtin__", "builtins", "raw_input", "input"),
    MovedAttribute("map", "itertools", "builtins", "imap", "map"),
    MovedAttribute("range", "__builtin__", "builtins", "xrange", "range"),
    MovedAttribute("reload_module", "__builtin__", "imp", "reload"),
    MovedAttribute("reduce", "__builtin__", "functools"),
    MovedAttribute("StringIO", "StringIO", "io"),
    MovedAttribute("UserString", "UserString", "collections"),
    MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),
    MovedAttribute("zip", "itertools", "builtins", "izip", "zip"),
    MovedAttribute("zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"),
    MovedModule("builtins", "__builtin__"),
    MovedModule("configparser", "ConfigParser"),
    MovedModule("copyreg", "copy_reg"),
    MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
    MovedModule("http_cookies", "Cookie", "http.cookies"),
    MovedModule("html_entities", "htmlentitydefs", "html.entities"),
    MovedModule("html_parser", "HTMLParser", "html.parser"),
    MovedModule("http_client", "httplib", "http.client"),
    MovedModule("email_mime_multipart", "email.MIMEMultipart", "email.mime.multipart"),
    MovedModule("email_mime_text", "email.MIMEText", "email.mime.text"),
    MovedModule("email_mime_base", "email.MIMEBase", "email.mime.base"),
    MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
    MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
    MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
    MovedModule("cPickle", "cPickle", "pickle"),
    MovedModule("queue", "Queue"),
    MovedModule("reprlib", "repr"),
    MovedModule("socketserver", "SocketServer"),
    MovedModule("tkinter", "Tkinter"),
    MovedModule("tkinter_dialog", "Dialog", "tkinter.dialog"),
    MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
    MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
    MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
    MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
    MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
    MovedModule("tkinter_colorchooser", "tkColorChooser",
                "tkinter.colorchooser"),
    MovedModule("tkinter_commondialog", "tkCommonDialog",
                "tkinter.commondialog"),
    MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_font", "tkFont", "tkinter.font"),
    MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog",
                "tkinter.simpledialog"),
    MovedModule("urllib_parse", __name__ + ".moves.urllib_parse", "urllib.parse"),
    MovedModule("urllib_error", __name__ + ".moves.urllib_error", "urllib.error"),
    MovedModule("urllib", __name__ + ".moves.urllib", __name__ + ".moves.urllib"),
    MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
    MovedModule("winreg", "_winreg"),
]
for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
del attr
moves = sys.modules[__name__ + ".moves"] = _MovedItems(__name__ + ".moves")

class Module_six_moves_urllib_parse(types.ModuleType):
    
_urllib_parse_moved_attributes = [
    MovedAttribute("ParseResult", "urlparse", "urllib.parse"),
    MovedAttribute("parse_qs", "urlparse", "urllib.parse"),
    MovedAttribute("parse_qsl", "urlparse", "urllib.parse"),
    MovedAttribute("urldefrag", "urlparse", "urllib.parse"),
    MovedAttribute("urljoin", "urlparse", "urllib.parse"),
    MovedAttribute("urlparse", "urlparse", "urllib.parse"),
    MovedAttribute("urlsplit", "urlparse", "urllib.parse"),
    MovedAttribute("urlunparse", "urlparse", "urllib.parse"),
    MovedAttribute("urlunsplit", "urlparse", "urllib.parse"),
    MovedAttribute("quote", "urllib", "urllib.parse"),
    MovedAttribute("quote_plus", "urllib", "urllib.parse"),
    MovedAttribute("unquote", "urllib", "urllib.parse"),
    MovedAttribute("unquote_plus", "urllib", "urllib.parse"),
    MovedAttribute("urlencode", "urllib", "urllib.parse"),
]
for attr in _urllib_parse_moved_attributes:
    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr
sys.modules[__name__ + ".moves.urllib_parse"] = Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse")
sys.modules[__name__ + ".moves.urllib.parse"] = Module_six_moves_urllib_parse(__name__ + ".moves.urllib.parse")

class Module_six_moves_urllib_error(types.ModuleType):
    
_urllib_response_moved_attributes = [
    MovedAttribute("addbase", "urllib", "urllib.response"),
    MovedAttribute("addclosehook", "urllib", "urllib.response"),
    MovedAttribute("addinfo", "urllib", "urllib.response"),
    MovedAttribute("addinfourl", "urllib", "urllib.response"),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr
sys.modules[__name__ + ".moves.urllib_response"] = Module_six_moves_urllib_response(__name__ + ".moves.urllib_response")
sys.modules[__name__ + ".moves.urllib.response"] = Module_six_moves_urllib_response(__name__ + ".moves.urllib.response")

class Module_six_moves_urllib_robotparser(types.ModuleType):
        setattr(_MovedItems, move.name, move)

def remove_move(name):
        try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError("no such move, %r" % (name,))

if PY3:
    _meth_func = "__func__"
    _meth_self = "__self__"
    _func_closure = "__closure__"
    _func_code = "__code__"
    _func_defaults = "__defaults__"
    _func_globals = "__globals__"
    _iterkeys = "keys"
    _itervalues = "values"
    _iteritems = "items"
    _iterlists = "lists"
else:
    _meth_func = "im_func"
    _meth_self = "im_self"
    _func_closure = "func_closure"
    _func_code = "func_code"
    _func_defaults = "func_defaults"
    _func_globals = "func_globals"
    _iterkeys = "iterkeys"
    _itervalues = "itervalues"
    _iteritems = "iteritems"
    _iterlists = "iterlists"

try:
    advance_iterator = next
except NameError:
    def advance_iterator(it):
        return it.next()
next = advance_iterator

try:
    callable = callable
except NameError:
    def callable(obj):
        return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)

if PY3:
    def get_unbound_function(unbound):
        return unbound
    create_bound_method = types.MethodType
    Iterator = object
else:
    def get_unbound_function(unbound):
        return unbound.im_func
    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)
    class Iterator(object):
        def next(self):
            return type(self).__next__(self)
    callable = callable
_add_doc(get_unbound_function,
             return iter(getattr(d, _iterkeys)(**kw))
def itervalues(d, **kw):
        return iter(getattr(d, _itervalues)(**kw))
def iteritems(d, **kw):
        return iter(getattr(d, _iteritems)(**kw))
def iterlists(d, **kw):
        return iter(getattr(d, _iterlists)(**kw))

if PY3:
    def b(s):
        return s.encode("latin-1")
    def u(s):
        return s
    unichr = chr
    if sys.version_info[1] <= 1:
        def int2byte(i):
            return bytes((i,))
    else:
                int2byte = operator.methodcaller("to_bytes", 1, "big")
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
else:
    def b(s):
        return s
    def u(s):
        return unicode(s, "unicode_escape")
    unichr = unichr
    int2byte = chr
    def byte2int(bs):
        return ord(bs[0])
    def indexbytes(buf, i):
        return ord(buf[i])
    def iterbytes(buf):
        return (ord(byte) for byte in buf)
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
_add_doc(b,         if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec(        fp = kwargs.pop("file", sys.stdout)
        if fp is None:
            return
        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop("sep", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError("sep must be None or a string")
        end = kwargs.pop("end", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError("end must be None or a string")
        if kwargs:
            raise TypeError("invalid keyword arguments to print()")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode("\n")
            space = unicode(" ")
        else:
            newline = "\n"
            space = " "
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)
_add_doc(reraise,     return meta("NewBase", bases, {})
def add_metaclass(metaclass):
        def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        for slots_var in orig_vars.get('__slots__', ()):
            orig_vars.pop(slots_var)
        return metaclass(cls.__name__, cls.__bases__, orig_vars)
    return wrapper
import os
import time
import ctypes
import sys
from distutils.version import LooseVersion
try:
    import numpy as np
    def make_memmap(filename, dtype='uint8', mode='r+', offset=0,
                    shape=None, order='C'):
                mm = np.memmap(filename, dtype=dtype, mode=mode, offset=offset,
                       shape=shape, order=order)
        if LooseVersion(np.__version__) < '1.13':
            mm.offset = offset
        return mm
except ImportError:
    def make_memmap(filename, dtype='uint8', mode='r+', offset=0,
                    shape=None, order='C'):
        raise NotImplementedError(
            "'joblib.backports.make_memmap' should not be used "
            'if numpy is not installed.')

if os.name == 'nt':
    error_access_denied = 5
    try:
        from os import replace
    except ImportError:
                def replace(src, dst):
            if not isinstance(src, unicode):                  src = unicode(src, sys.getfilesystemencoding())              if not isinstance(dst, unicode):                  dst = unicode(dst, sys.getfilesystemencoding())  
            movefile_replace_existing = 0x1
            return_value = ctypes.windll.kernel32.MoveFileExW(
                src, dst, movefile_replace_existing)
            if return_value == 0:
                raise ctypes.WinError()
    def concurrency_safe_rename(src, dst):
                max_sleep_time = 1
        total_sleep_time = 0
        sleep_time = 0.001
        while total_sleep_time < max_sleep_time:
            try:
                replace(src, dst)
                break
            except Exception as exc:
                if getattr(exc, 'winerror', None) == error_access_denied:
                    time.sleep(sleep_time)
                    total_sleep_time += sleep_time
                    sleep_time *= 2
                else:
                    raise
        else:
            raise
else:
    try:
        from os import replace as concurrency_safe_rename
    except ImportError:
        from os import rename as concurrency_safe_rename  

import errno
import os
import shutil
import sys
import time

def disk_used(path):
        size = 0
    for file in os.listdir(path) + ['.']:
        stat = os.stat(os.path.join(path, file))
        if hasattr(stat, 'st_blocks'):
            size += stat.st_blocks * 512
        else:
                                    size += (stat.st_size // 512 + 1) * 512
            return int(size / 1024.)

def memstr_to_bytes(text):
        kilo = 1024
    units = dict(K=kilo, M=kilo ** 2, G=kilo ** 3)
    try:
        size = int(units[text[-1]] * float(text[:-1]))
    except (KeyError, ValueError):
        raise ValueError(
            "Invalid literal for size give: %s (type %s) should be "
            "alike '10G', '500M', '50K'." % (text, type(text)))
    return size

def mkdirp(d):
        try:
        os.makedirs(d)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise

RM_SUBDIRS_RETRY_TIME = 0.1

def rm_subdirs(path, onerror=None):
    
        
    names = []
    try:
        names = os.listdir(path)
    except os.error as err:
        if onerror is not None:
            onerror(os.listdir, path, sys.exc_info())
        else:
            raise
    for name in names:
        fullname = os.path.join(path, name)
        if os.path.isdir(fullname):
            if onerror is not None:
                shutil.rmtree(fullname, False, onerror)
            else:
                                                err_count = 0
                while True:
                    try:
                        shutil.rmtree(fullname, False, None)
                        break
                    except os.error:
                        if err_count > 0:
                            raise
                        err_count += 1
                        time.sleep(RM_SUBDIRS_RETRY_TIME)

import inspect
import keyword
import linecache
import os
import pydoc
import sys
import time
import tokenize
import traceback
try:                               generate_tokens = tokenize.generate_tokens
except AttributeError:             generate_tokens = tokenize.tokenize
INDENT = ' ' * 8

def safe_repr(value):
            try:
        return pydoc.text.repr(value)
    except KeyboardInterrupt:
        raise
    except:
        try:
            return repr(value)
        except KeyboardInterrupt:
            raise
        except:
            try:
                                                name = getattr(value, '__name__', None)
                if name:
                                        return safe_repr(name)
                klass = getattr(value, '__class__', None)
                if klass:
                    return '%s instance' % safe_repr(klass)
            except KeyboardInterrupt:
                raise
            except:
                return 'UNRECOVERABLE REPR FAILURE'

def eq_repr(value, repr=safe_repr):
    return '=%s' % repr(value)

def uniq_stable(elems):
        unique = []
    unique_set = set()
    for nn in elems:
        if nn not in unique_set:
            unique.append(nn)
            unique_set.add(nn)
    return unique

def fix_frame_records_filenames(records):
        fixed_records = []
    for frame, filename, line_no, func_name, lines, index in records:
                        better_fn = frame.f_globals.get('__file__', None)
        if isinstance(better_fn, str):
                                                filename = better_fn
        fixed_records.append((frame, filename, line_no, func_name, lines,
                              index))
    return fixed_records

def _fixed_getframes(etb, context=1, tb_offset=0):
    LNUM_POS, LINES_POS, INDEX_POS = 2, 4, 5
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
                rec_check = records[tb_offset:]
    try:
        rname = rec_check[0][1]
        if rname == '<ipython console>' or rname.endswith('<string>'):
            return rec_check
    except IndexError:
        pass
    aux = traceback.extract_tb(etb)
    assert len(records) == len(aux)
    for i, (file, lnum, _, _) in enumerate(aux):
        maybe_start = lnum - 1 - context // 2
        start = max(maybe_start, 0)
        end = start + context
        lines = linecache.getlines(file)[start:end]
        buf = list(records[i])
        buf[LNUM_POS] = lnum
        buf[INDEX_POS] = lnum - 1 - start
        buf[LINES_POS] = lines
        records[i] = tuple(buf)
    return records[tb_offset:]

def _format_traceback_lines(lnum, index, lines, lvals=None):
    numbers_width = 7
    res = []
    i = lnum - index
    for line in lines:
        if i == lnum:
                        pad = numbers_width - len(str(i))
            if pad >= 3:
                marker = '-' * (pad - 3) + '-> '
            elif pad == 2:
                marker = '> '
            elif pad == 1:
                marker = '>'
            else:
                marker = ''
            num = marker + str(i)
        else:
            num = '%*s' % (numbers_width, i)
        line = '%s %s' % (num, line)
        res.append(line)
        if lvals and i == lnum:
            res.append(lvals + '\n')
        i = i + 1
    return res

def format_records(records):           frames = []
    abspath = os.path.abspath
    for frame, file, lnum, func, lines, index in records:
        try:
            file = file and abspath(file) or '?'
        except OSError:
                                                pass
        if file.endswith('.pyc'):
            file = file[:-4] + '.py'
        link = file
        args, varargs, varkw, locals = inspect.getargvalues(frame)
        if func == '?':
            call = ''
        else:
                        try:
                call = 'in %s%s' % (func, inspect.formatargvalues(args,
                                            varargs, varkw, locals,
                                            formatvalue=eq_repr))
            except KeyError:
                                                                                                                                print("\nJoblib's exception reporting continues...\n")
                call = 'in %s(***failed resolving arguments***)' % func
                        names = []
        def tokeneater(token_type, token, start, end, line):
            
                        if token == '.':
                try:
                    names[-1] += '.'
                                        tokeneater.name_cont = True
                    return
                except IndexError:
                    pass
            if token_type == tokenize.NAME and token not in keyword.kwlist:
                if tokeneater.name_cont:
                                        names[-1] += token
                    tokeneater.name_cont = False
                else:
                                                                                                                                            names.append(token)
            elif token_type == tokenize.NEWLINE:
                raise IndexError
                        tokeneater.name_cont = False
        def linereader(file=file, lnum=[lnum], getline=linecache.getline):
            line = getline(file, lnum[0])
            lnum[0] += 1
            return line
                        try:
                                    for token in generate_tokens(linereader):
                tokeneater(*token)
        except (IndexError, UnicodeDecodeError, SyntaxError):
                                                pass
        except tokenize.TokenError as msg:
            _m = ("An unexpected error occurred while tokenizing input file %s\n"
                  "The following traceback may be corrupted or invalid\n"
                  "The error message is: %s\n" % (file, msg))
            print(_m)
                unique_names = uniq_stable(names)
                lvals = []
        for name_full in unique_names:
            name_base = name_full.split('.', 1)[0]
            if name_base in frame.f_code.co_varnames:
                if name_base in locals.keys():
                    try:
                        value = safe_repr(eval(name_full, locals))
                    except:
                        value = "undefined"
                else:
                    value = "undefined"
                name = name_full
                lvals.append('%s = %s' % (name, value))
                                                                                                                                if lvals:
            lvals = '%s%s' % (INDENT, ('\n%s' % INDENT).join(lvals))
        else:
            lvals = ''
        level = '%s\n%s %s\n' % (75 * '.', link, call)
        if index is None:
            frames.append(level)
        else:
            frames.append('%s%s' % (level, ''.join(
                _format_traceback_lines(lnum, index, lines, lvals))))
    return frames

def format_exc(etype, evalue, etb, context=5, tb_offset=0):
            try:
        etype = etype.__name__
    except AttributeError:
        pass
        pyver = 'Python ' + sys.version.split()[0] + ': ' + sys.executable
    date = time.ctime(time.time())
    pid = 'PID: %i' % os.getpid()
    head = '%s%s%s\n%s%s%s' % (
        etype, ' ' * (75 - len(str(etype)) - len(date)),
        date, pid, ' ' * (75 - len(str(pid)) - len(pyver)),
        pyver)
        records = _fixed_getframes(etb, context, tb_offset)
        try:
        etype_str, evalue_str = map(str, (etype, evalue))
    except:
                etype, evalue = str, sys.exc_info()[:2]
        etype_str, evalue_str = map(str, (etype, evalue))
        exception = ['%s: %s' % (etype_str, evalue_str)]
    frames = format_records(records)
    return '%s\n%s\n%s' % (head, '\n'.join(frames), ''.join(exception[0]))

def format_outer_frames(context=5, stack_start=None, stack_end=None,
                        ignore_ipython=True):
    LNUM_POS, LINES_POS, INDEX_POS = 2, 4, 5
    records = inspect.getouterframes(inspect.currentframe())
    output = list()
    for i, (frame, filename, line_no, func_name, lines, index) \
                                                in enumerate(records):
                        better_fn = frame.f_globals.get('__file__', None)
        if isinstance(better_fn, str):
                                                filename = better_fn
            if filename.endswith('.pyc'):
                filename = filename[:-4] + '.py'
        if ignore_ipython:
                        if (os.path.basename(filename) in ('iplib.py', 'py3compat.py')
                        and func_name in ('execfile', 'safe_execfile', 'runcode')):
                break
        maybe_start = line_no - 1 - context // 2
        start = max(maybe_start, 0)
        end = start + context
        lines = linecache.getlines(filename)[start:end]
        buf = list(records[i])
        buf[LNUM_POS] = line_no
        buf[INDEX_POS] = line_no - 1 - start
        buf[LINES_POS] = lines
        output.append(tuple(buf))
    return '\n'.join(format_records(output[stack_end:stack_start:-1]))

from itertools import islice
import inspect
import warnings
import re
import os
from ._compat import _basestring
from .logger import pformat
from ._memory_helpers import open_py_source
from ._compat import PY3_OR_LATER

def get_func_code(func):
        source_file = None
    try:
        code = func.__code__
        source_file = code.co_filename
        if not os.path.exists(source_file):
                                    source_code = ''.join(inspect.getsourcelines(func)[0])
            line_no = 1
            if source_file.startswith('<doctest '):
                source_file, line_no = re.match(
                    '\<doctest (.*\.rst)\[(.*)\]\>', source_file).groups()
                line_no = int(line_no)
                source_file = '<doctest %s>' % source_file
            return source_code, source_file, line_no
                with open_py_source(source_file) as source_file_obj:
            first_line = code.co_firstlineno
                        source_lines = list(islice(source_file_obj, first_line - 1, None))
        return ''.join(inspect.getblock(source_lines)), source_file, first_line
    except:
                        if hasattr(func, '__code__'):
                        return str(func.__code__.__hash__()), source_file, -1
        else:
                                                            return repr(func), source_file, -1

def _clean_win_chars(string):
        import urllib
    if hasattr(urllib, 'quote'):
        quote = urllib.quote
    else:
                import urllib.parse
        quote = urllib.parse.quote
    for char in ('<', '>', '!', ':', '\\'):
        string = string.replace(char, quote(char))
    return string

def get_func_name(func, resolv_alias=True, win_characters=True):
        if hasattr(func, '__module__'):
        module = func.__module__
    else:
        try:
            module = inspect.getmodule(func)
        except TypeError:
            if hasattr(func, '__class__'):
                module = func.__class__.__module__
            else:
                module = 'unknown'
    if module is None:
                module = ''
    if module == '__main__':
        try:
            filename = os.path.abspath(inspect.getsourcefile(func))
        except:
            filename = None
        if filename is not None:
                        parts = filename.split(os.sep)
            if parts[-1].startswith('<ipython-input'):
                                                                parts[-1] = '__ipython-input__'
            filename = '-'.join(parts)
            if filename.endswith('.py'):
                filename = filename[:-3]
            module = module + '-' + filename
    module = module.split('.')
    if hasattr(func, 'func_name'):
        name = func.func_name
    elif hasattr(func, '__name__'):
        name = func.__name__
    else:
        name = 'unknown'
        if resolv_alias:
                if hasattr(func, 'func_globals') and name in func.func_globals:
            if not func.func_globals[name] is func:
                name = '%s-alias' % name
    if inspect.ismethod(func):
                if hasattr(func, 'im_class'):
            klass = func.im_class
            module.append(klass.__name__)
    if os.name == 'nt' and win_characters:
                name = _clean_win_chars(name)
        module = [_clean_win_chars(s) for s in module]
    return module, name

def getfullargspec(func):
        try:
        return inspect.getfullargspec(func)
    except AttributeError:
        arg_spec = inspect.getargspec(func)
        import collections
        tuple_fields = ('args varargs varkw defaults kwonlyargs '
                        'kwonlydefaults annotations')
        tuple_type = collections.namedtuple('FullArgSpec', tuple_fields)
        return tuple_type(args=arg_spec.args,
                          varargs=arg_spec.varargs,
                          varkw=arg_spec.keywords,
                          defaults=arg_spec.defaults,
                          kwonlyargs=[],
                          kwonlydefaults=None,
                          annotations={})

def _signature_str(function_name, arg_spec):
                arg_spec_for_format = arg_spec[:7 if PY3_OR_LATER else 4]
    arg_spec_str = inspect.formatargspec(*arg_spec_for_format)
    return '{}{}'.format(function_name, arg_spec_str)

def _function_called_str(function_name, args, kwargs):
        template_str = '{0}({1}, {2})'
    args_str = repr(args)[1:-1]
    kwargs_str = ', '.join('%s=%s' % (k, v)
                           for k, v in kwargs.items())
    return template_str.format(function_name, args_str,
                               kwargs_str)

def filter_args(func, ignore_lst, args=(), kwargs=dict()):
        args = list(args)
    if isinstance(ignore_lst, _basestring):
                raise ValueError(
            'ignore_lst must be a list of parameters to ignore '
            '%s (type %s) was given' % (ignore_lst, type(ignore_lst)))
        if (not inspect.ismethod(func) and not inspect.isfunction(func)):
        if ignore_lst:
            warnings.warn('Cannot inspect object %s, ignore list will '
                          'not work.' % func, stacklevel=2)
        return {'*': args, '**': kwargs}
    arg_spec = getfullargspec(func)
    arg_names = arg_spec.args + arg_spec.kwonlyargs
    arg_defaults = arg_spec.defaults or ()
    arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]
                                        for k in arg_spec.kwonlyargs)
    arg_varargs = arg_spec.varargs
    arg_varkw = arg_spec.varkw
    if inspect.ismethod(func):
                        args = [func.__self__, ] + args
        
    _, name = get_func_name(func, resolv_alias=False)
    arg_dict = dict()
    arg_position = -1
    for arg_position, arg_name in enumerate(arg_names):
        if arg_position < len(args):
                        if arg_name not in arg_spec.kwonlyargs:
                arg_dict[arg_name] = args[arg_position]
            else:
                raise ValueError(
                    "Keyword-only parameter '%s' was passed as "
                    'positional parameter for %s:\n'
                    '     %s was called.'
                    % (arg_name,
                       _signature_str(name, arg_spec),
                       _function_called_str(name, args, kwargs))
                )
        else:
            position = arg_position - len(arg_names)
            if arg_name in kwargs:
                arg_dict[arg_name] = kwargs.pop(arg_name)
            else:
                try:
                    arg_dict[arg_name] = arg_defaults[position]
                except (IndexError, KeyError):
                                        raise ValueError(
                        'Wrong number of arguments for %s:\n'
                        '     %s was called.'
                        % (_signature_str(name, arg_spec),
                           _function_called_str(name, args, kwargs))
                    )
    varkwargs = dict()
    for arg_name, arg_value in sorted(kwargs.items()):
        if arg_name in arg_dict:
            arg_dict[arg_name] = arg_value
        elif arg_varkw is not None:
            varkwargs[arg_name] = arg_value
        else:
            raise TypeError("Ignore list for %s() contains an unexpected "
                            "keyword argument '%s'" % (name, arg_name))
    if arg_varkw is not None:
        arg_dict['**'] = varkwargs
    if arg_varargs is not None:
        varargs = args[arg_position + 1:]
        arg_dict['*'] = varargs
        for item in ignore_lst:
        if item in arg_dict:
            arg_dict.pop(item)
        else:
            raise ValueError("Ignore list: argument '%s' is not defined for "
                             "function %s"
                             % (item,
                                _signature_str(name, arg_spec))
                             )
        return arg_dict

def _format_arg(arg):
    formatted_arg = pformat(arg, indent=2)
    if len(formatted_arg) > 1500:
        formatted_arg = '%s...' % formatted_arg[:700]
    return formatted_arg

def format_signature(func, *args, **kwargs):
        module, name = get_func_name(func)
    module = [m for m in module if m]
    if module:
        module.append(name)
        module_path = '.'.join(module)
    else:
        module_path = name
    arg_str = list()
    previous_length = 0
    for arg in args:
        formatted_arg = _format_arg(arg)
        if previous_length > 80:
            formatted_arg = '\n%s' % formatted_arg
        previous_length = len(formatted_arg)
        arg_str.append(formatted_arg)
    arg_str.extend(['%s=%s' % (v, _format_arg(i)) for v, i in kwargs.items()])
    arg_str = ', '.join(arg_str)
    signature = '%s(%s)' % (name, arg_str)
    return module_path, signature

def format_call(func, args, kwargs, object_name="Memory"):
        path, signature = format_signature(func, *args, **kwargs)
    msg = '%s\n[%s] Calling %s...\n%s' % (80 * '_', object_name,
                                          path, signature)
    return msg
        
import pickle
import hashlib
import sys
import types
import struct
import io
import decimal
from ._compat import _bytes_or_unicode, PY3_OR_LATER

if PY3_OR_LATER:
    Pickler = pickle._Pickler
else:
    Pickler = pickle.Pickler

class _ConsistentSet(object):
        def __init__(self, set_sequence):
                try:
                                                            self._sequence = sorted(set_sequence)
        except (TypeError, decimal.InvalidOperation):
                                    self._sequence = sorted((hash(e) for e in set_sequence))

class _MyHash(object):
    
    def __init__(self, *args):
        self.args = args

class Hasher(Pickler):
    
    def __init__(self, hash_name='md5'):
        self.stream = io.BytesIO()
                        protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER
                    else pickle.HIGHEST_PROTOCOL)
        Pickler.__init__(self, self.stream, protocol=protocol)
                self._hash = hashlib.new(hash_name)
    def hash(self, obj, return_digest=True):
        try:
            self.dump(obj)
        except pickle.PicklingError as e:
            e.args += ('PicklingError while hashing %r: %r' % (obj, e),)
            raise
        dumps = self.stream.getvalue()
        self._hash.update(dumps)
        if return_digest:
            return self._hash.hexdigest()
    def save(self, obj):
        if isinstance(obj, (types.MethodType, type({}.pop))):
                                    if hasattr(obj, '__func__'):
                func_name = obj.__func__.__name__
            else:
                func_name = obj.__name__
            inst = obj.__self__
            if type(inst) == type(pickle):
                obj = _MyHash(func_name, inst.__name__)
            elif inst is None:
                                obj = _MyHash(func_name, inst)
            else:
                cls = obj.__self__.__class__
                obj = _MyHash(func_name, inst, cls)
        Pickler.save(self, obj)
    def memoize(self, obj):
                                        if isinstance(obj, _bytes_or_unicode):
            return
        Pickler.memoize(self, obj)
            def save_global(self, obj, name=None, pack=struct.pack):
                                kwargs = dict(name=name, pack=pack)
        if sys.version_info >= (3, 4):
            del kwargs['pack']
        try:
            Pickler.save_global(self, obj, **kwargs)
        except pickle.PicklingError:
            Pickler.save_global(self, obj, **kwargs)
            module = getattr(obj, "__module__", None)
            if module == '__main__':
                my_name = name
                if my_name is None:
                    my_name = obj.__name__
                mod = sys.modules[module]
                if not hasattr(mod, my_name):
                                                            setattr(mod, my_name, obj)
    dispatch = Pickler.dispatch.copy()
        dispatch[type(len)] = save_global
        dispatch[type(object)] = save_global
        dispatch[type(Pickler)] = save_global
        dispatch[type(pickle.dump)] = save_global
    def _batch_setitems(self, items):
                try:
                                                            Pickler._batch_setitems(self, iter(sorted(items)))
        except TypeError:
                                    Pickler._batch_setitems(self, iter(sorted((hash(k), v)
                                                      for k, v in items)))
    def save_set(self, set_items):
                Pickler.save(self, _ConsistentSet(set_items))
    dispatch[type(set())] = save_set

class NumpyHasher(Hasher):
    
    def __init__(self, hash_name='md5', coerce_mmap=False):
                self.coerce_mmap = coerce_mmap
        Hasher.__init__(self, hash_name=hash_name)
                import numpy as np
        self.np = np
        if hasattr(np, 'getbuffer'):
            self._getbuffer = np.getbuffer
        else:
            self._getbuffer = memoryview
    def save(self, obj):
                if isinstance(obj, self.np.ndarray) and not obj.dtype.hasobject:
                                    if obj.shape == ():
                                                obj_c_contiguous = obj.flatten()
            elif obj.flags.c_contiguous:
                obj_c_contiguous = obj
            elif obj.flags.f_contiguous:
                obj_c_contiguous = obj.T
            else:
                                                                obj_c_contiguous = obj.flatten()
                                                            self._hash.update(
                self._getbuffer(obj_c_contiguous.view(self.np.uint8)))
                                                if self.coerce_mmap and isinstance(obj, self.np.memmap):
                                                                klass = self.np.ndarray
            else:
                klass = obj.__class__
                        
                        obj = (klass, ('HASHED', obj.dtype, obj.shape, obj.strides))
        elif isinstance(obj, self.np.dtype):
                                                                                                                                                klass = obj.__class__
            obj = (klass, ('HASHED', obj.descr))
        Hasher.save(self, obj)

def hash(obj, hash_name='md5', coerce_mmap=False):
        if 'numpy' in sys.modules:
        hasher = NumpyHasher(hash_name=hash_name, coerce_mmap=coerce_mmap)
    else:
        hasher = Hasher(hash_name=hash_name)
    return hasher.hash(obj)

from __future__ import print_function
import time
import sys
import os
import shutil
import logging
import pprint
from .disk import mkdirp

def _squeeze_time(t):
        if sys.platform.startswith('win'):
        return max(0, t - .1)
    else:
        return t

def format_time(t):
    t = _squeeze_time(t)
    return "%.1fs, %.1fmin" % (t, t / 60.)

def short_format_time(t):
    t = _squeeze_time(t)
    if t > 60:
        return "%4.1fmin" % (t / 60.)
    else:
        return " %5.1fs" % (t)

def pformat(obj, indent=0, depth=3):
    if 'numpy' in sys.modules:
        import numpy as np
        print_options = np.get_printoptions()
        np.set_printoptions(precision=6, threshold=64, edgeitems=1)
    else:
        print_options = None
    out = pprint.pformat(obj, depth=depth, indent=indent)
    if print_options:
        np.set_printoptions(**print_options)
    return out

class Logger(object):
    
    def __init__(self, depth=3):
                self.depth = depth
    def warn(self, msg):
        logging.warning("[%s]: %s" % (self, msg))
    def debug(self, msg):
                logging.debug("[%s]: %s" % (self, msg))
    def format(self, obj, indent=0):
                return pformat(obj, indent=indent, depth=self.depth)

class PrintTime(object):
    
    def __init__(self, logfile=None, logdir=None):
        if logfile is not None and logdir is not None:
            raise ValueError('Cannot specify both logfile and logdir')
                self.last_time = time.time()
        self.start_time = self.last_time
        if logdir is not None:
            logfile = os.path.join(logdir, 'joblib.log')
        self.logfile = logfile
        if logfile is not None:
            mkdirp(os.path.dirname(logfile))
            if os.path.exists(logfile):
                                for i in range(1, 9):
                    try:
                        shutil.move(logfile + '.%i' % i,
                                    logfile + '.%i' % (i + 1))
                    except:
                        "No reason failing here"
                                                try:
                    shutil.copy(logfile, logfile + '.1')
                except:
                    "No reason failing here"
            try:
                with open(logfile, 'w') as logfile:
                    logfile.write('\nLogging joblib python script\n')
                    logfile.write('\n---%s---\n' % time.ctime(self.last_time))
            except:
                                                
    def __call__(self, msg='', total=False):
                if not total:
            time_lapse = time.time() - self.last_time
            full_msg = "%s: %s" % (msg, format_time(time_lapse))
        else:
                        time_lapse = time.time() - self.start_time
            full_msg = "%s: %.2fs, %.1f min" % (msg, time_lapse,
                                                time_lapse / 60)
        print(full_msg, file=sys.stderr)
        if self.logfile is not None:
            try:
                with open(self.logfile, 'a') as f:
                    print(full_msg, file=f)
            except:
                                                        self.last_time = time.time()

from __future__ import with_statement
import os
import shutil
import time
import pydoc
import re
import functools
import traceback
import warnings
import inspect
import json
import weakref
import io
import operator
import collections
import datetime
import threading
from . import hashing
from .func_inspect import get_func_code, get_func_name, filter_args
from .func_inspect import format_call
from .func_inspect import format_signature
from ._memory_helpers import open_py_source
from .logger import Logger, format_time, pformat
from . import numpy_pickle
from .disk import mkdirp, rm_subdirs, memstr_to_bytes
from ._compat import _basestring, PY3_OR_LATER
from .backports import concurrency_safe_rename
FIRST_LINE_TEXT = "
CacheItemInfo = collections.namedtuple('CacheItemInfo',
                                       'path size last_access')


def extract_first_line(func_code):
        if func_code.startswith(FIRST_LINE_TEXT):
        func_code = func_code.split('\n')
        first_line = int(func_code[0][len(FIRST_LINE_TEXT):])
        func_code = '\n'.join(func_code[1:])
    else:
        first_line = -1
    return func_code, first_line

class JobLibCollisionWarning(UserWarning):
    
def _get_func_fullname(func):
        modules, funcname = get_func_name(func)
    modules.append(funcname)
    return os.path.join(*modules)

def _cache_key_to_dir(cachedir, func, argument_hash):
        parts = [cachedir]
    if isinstance(func, _basestring):
        parts.append(func)
    else:
        parts.append(_get_func_fullname(func))
    if argument_hash is not None:
        parts.append(argument_hash)
    return os.path.join(*parts)

def _load_output(output_dir, func_name, timestamp=None, metadata=None,
                 mmap_mode=None, verbose=0):
        if verbose > 1:
        signature = ""
        try:
            if metadata is not None:
                args = ", ".join(['%s=%s' % (name, value)
                                  for name, value
                                  in metadata['input_args'].items()])
                signature = "%s(%s)" % (os.path.basename(func_name),
                                             args)
            else:
                signature = os.path.basename(func_name)
        except KeyError:
            pass
        if timestamp is not None:
            t = "% 16s" % format_time(time.time() - timestamp)
        else:
            t = ""
        if verbose < 10:
            print('[Memory]%s: Loading %s...' % (t, str(signature)))
        else:
            print('[Memory]%s: Loading %s from %s' % (
                    t, str(signature), output_dir))
    filename = os.path.join(output_dir, 'output.pkl')
    if not os.path.isfile(filename):
        raise KeyError(
            "Non-existing cache value (may have been cleared).\n"
            "File %s does not exist" % filename)
    result = numpy_pickle.load(filename, mmap_mode=mmap_mode)
    return result

def _get_cache_items(root_path):
        cache_items = []
    for dirpath, dirnames, filenames in os.walk(root_path):
        is_cache_hash_dir = re.match('[a-f0-9]{32}', os.path.basename(dirpath))
        if is_cache_hash_dir:
            output_filename = os.path.join(dirpath, 'output.pkl')
            try:
                last_access = os.path.getatime(output_filename)
            except OSError:
                try:
                    last_access = os.path.getatime(dirpath)
                except OSError:
                                        continue
            last_access = datetime.datetime.fromtimestamp(last_access)
            try:
                full_filenames = [os.path.join(dirpath, fn)
                                  for fn in filenames]
                dirsize = sum(os.path.getsize(fn)
                              for fn in full_filenames)
            except OSError:
                                                                continue
            cache_items.append(CacheItemInfo(dirpath, dirsize, last_access))
    return cache_items

def _get_cache_items_to_delete(root_path, bytes_limit):
        if isinstance(bytes_limit, _basestring):
        bytes_limit = memstr_to_bytes(bytes_limit)
    cache_items = _get_cache_items(root_path)
    cache_size = sum(item.size for item in cache_items)
    to_delete_size = cache_size - bytes_limit
    if to_delete_size < 0:
        return []
            cache_items.sort(key=operator.attrgetter('last_access'))
    cache_items_to_delete = []
    size_so_far = 0
    for item in cache_items:
        if size_so_far > to_delete_size:
            break
        cache_items_to_delete.append(item)
        size_so_far += item.size
    return cache_items_to_delete

def concurrency_safe_write(to_write, filename, write_func):
        thread_id = id(threading.current_thread())
    temporary_filename = '{}.thread-{}-pid-{}'.format(
        filename, thread_id, os.getpid())
    write_func(to_write, temporary_filename)
    concurrency_safe_rename(temporary_filename, filename)

_FUNCTION_HASHES = weakref.WeakKeyDictionary()

class MemorizedResult(Logger):
        def __init__(self, cachedir, func, argument_hash,
                 mmap_mode=None, verbose=0, timestamp=None, metadata=None):
        Logger.__init__(self)
        if isinstance(func, _basestring):
            self.func = func
        else:
            self.func = _get_func_fullname(func)
        self.argument_hash = argument_hash
        self.cachedir = cachedir
        self.mmap_mode = mmap_mode
        self._output_dir = _cache_key_to_dir(cachedir, self.func,
                                             argument_hash)
        if metadata is not None:
            self.metadata = metadata
        else:
            self.metadata = {}
                        try:
                with open(os.path.join(self._output_dir, 'metadata.json'),
                          'rb') as f:
                    self.metadata = json.load(f)
            except:
                pass
        self.duration = self.metadata.get('duration', None)
        self.verbose = verbose
        self.timestamp = timestamp
    def get(self):
                return _load_output(self._output_dir, _get_func_fullname(self.func),
                            timestamp=self.timestamp,
                            metadata=self.metadata, mmap_mode=self.mmap_mode,
                            verbose=self.verbose)
    def clear(self):
                shutil.rmtree(self._output_dir, ignore_errors=True)
    def __repr__(self):
        return ('{class_name}(cachedir="{cachedir}", func="{func}", '
                'argument_hash="{argument_hash}")'.format(
                    class_name=self.__class__.__name__,
                    cachedir=self.cachedir,
                    func=self.func,
                    argument_hash=self.argument_hash
                    ))
    def __reduce__(self):
        return (self.__class__, (self.cachedir, self.func, self.argument_hash),
                {'mmap_mode': self.mmap_mode})

class NotMemorizedResult(object):
        __slots__ = ('value', 'valid')
    def __init__(self, value):
        self.value = value
        self.valid = True
    def get(self):
        if self.valid:
            return self.value
        else:
            raise KeyError("No value stored.")
    def clear(self):
        self.valid = False
        self.value = None
    def __repr__(self):
        if self.valid:
            return '{class_name}({value})'.format(
                class_name=self.__class__.__name__,
                value=pformat(self.value)
                )
        else:
            return self.__class__.__name__ + ' with no value'
        def __getstate__(self):
        return {"valid": self.valid, "value": self.value}
    def __setstate__(self, state):
        self.valid = state["valid"]
        self.value = state["value"]

class NotMemorizedFunc(object):
            def __init__(self, func):
        self.func = func
    def __call__(self, *args, **kwargs):
        return self.func(*args, **kwargs)
    def call_and_shelve(self, *args, **kwargs):
        return NotMemorizedResult(self.func(*args, **kwargs))
    def __reduce__(self):
        return (self.__class__, (self.func,))
    def __repr__(self):
        return '%s(func=%s)' % (
                    self.__class__.__name__,
                    self.func
            )
    def clear(self, warn=True):
                pass

class MemorizedFunc(Logger):
                
    def __init__(self, func, cachedir, ignore=None, mmap_mode=None,
                 compress=False, verbose=1, timestamp=None):
                Logger.__init__(self)
        self.mmap_mode = mmap_mode
        self.func = func
        if ignore is None:
            ignore = []
        self.ignore = ignore
        self._verbose = verbose
        self.cachedir = cachedir
        self.compress = compress
        if compress and self.mmap_mode is not None:
            warnings.warn('Compressed results cannot be memmapped',
                          stacklevel=2)
        if timestamp is None:
            timestamp = time.time()
        self.timestamp = timestamp
        mkdirp(self.cachedir)
        try:
            functools.update_wrapper(self, func)
        except:
            " Objects like ufunc don't like that "
        if inspect.isfunction(func):
            doc = pydoc.TextDoc().document(func)
                        doc = doc.replace('\n', '\n\n', 1)
                        doc = re.sub('\x08.', '', doc)
        else:
                        doc = func.__doc__
        self.__doc__ = 'Memoized version of %s' % doc
    def _cached_call(self, args, kwargs):
                                output_dir, argument_hash = self._get_output_dir(*args, **kwargs)
        metadata = None
        output_pickle_path = os.path.join(output_dir, 'output.pkl')
                if not (self._check_previous_func_code(stacklevel=4) and
                os.path.isfile(output_pickle_path)):
            if self._verbose > 10:
                _, name = get_func_name(self.func)
                self.warn('Computing func %s, argument hash %s in '
                          'directory %s'
                        % (name, argument_hash, output_dir))
            out, metadata = self.call(*args, **kwargs)
            if self.mmap_mode is not None:
                                                out = _load_output(output_dir, _get_func_fullname(self.func),
                                   timestamp=self.timestamp,
                                   mmap_mode=self.mmap_mode,
                                   verbose=self._verbose)
        else:
            try:
                t0 = time.time()
                out = _load_output(output_dir, _get_func_fullname(self.func),
                                   timestamp=self.timestamp,
                                   metadata=metadata, mmap_mode=self.mmap_mode,
                                   verbose=self._verbose)
                if self._verbose > 4:
                    t = time.time() - t0
                    _, name = get_func_name(self.func)
                    msg = '%s cache loaded - %s' % (name, format_time(t))
                    print(max(0, (80 - len(msg))) * '_' + msg)
            except Exception:
                                _, signature = format_signature(self.func, *args, **kwargs)
                self.warn('Exception while loading results for '
                          '{}\n {}'.format(
                              signature, traceback.format_exc()))
                out, metadata = self.call(*args, **kwargs)
                argument_hash = None
        return (out, argument_hash, metadata)
    def call_and_shelve(self, *args, **kwargs):
                _, argument_hash, metadata = self._cached_call(args, kwargs)
        return MemorizedResult(self.cachedir, self.func, argument_hash,
            metadata=metadata, verbose=self._verbose - 1,
            timestamp=self.timestamp)
    def __call__(self, *args, **kwargs):
        return self._cached_call(args, kwargs)[0]
    def __reduce__(self):
                return (self.__class__, (self.func, self.cachedir, self.ignore,
                self.mmap_mode, self.compress, self._verbose))
            
    def _get_argument_hash(self, *args, **kwargs):
        return hashing.hash(filter_args(self.func, self.ignore,
                                         args, kwargs),
                             coerce_mmap=(self.mmap_mode is not None))
    def _get_output_dir(self, *args, **kwargs):
                argument_hash = self._get_argument_hash(*args, **kwargs)
        output_dir = os.path.join(self._get_func_dir(self.func),
                                  argument_hash)
        return output_dir, argument_hash
    get_output_dir = _get_output_dir  
    def _get_func_dir(self, mkdir=True):
                func_dir = _cache_key_to_dir(self.cachedir, self.func, None)
        if mkdir:
            mkdirp(func_dir)
        return func_dir
    def _hash_func(self):
                func_code_h = hash(getattr(self.func, '__code__', None))
        return id(self.func), hash(self.func), func_code_h
    def _write_func_code(self, filename, func_code, first_line):
                                                        func_code = u'%s %i\n%s' % (FIRST_LINE_TEXT, first_line, func_code)
        with io.open(filename, 'w', encoding="UTF-8") as out:
            out.write(func_code)
                is_named_callable = False
        if PY3_OR_LATER:
            is_named_callable = (hasattr(self.func, '__name__')
                                 and self.func.__name__ != '<lambda>')
        else:
            is_named_callable = (hasattr(self.func, 'func_name')
                                 and self.func.func_name != '<lambda>')
        if is_named_callable:
                                    func_hash = self._hash_func()
            try:
                _FUNCTION_HASHES[self.func] = func_hash
            except TypeError:
                                pass
    def _check_previous_func_code(self, stacklevel=2):
                                                try:
            if self.func in _FUNCTION_HASHES:
                                                                func_hash = self._hash_func()
                if func_hash == _FUNCTION_HASHES[self.func]:
                    return True
        except TypeError:
                        pass
                                func_code, source_file, first_line = get_func_code(self.func)
        func_dir = self._get_func_dir()
        func_code_file = os.path.join(func_dir, 'func_code.py')
        try:
            with io.open(func_code_file, encoding="UTF-8") as infile:
                old_func_code, old_first_line = \
                            extract_first_line(infile.read())
        except IOError:
                self._write_func_code(func_code_file, func_code, first_line)
                return False
        if old_func_code == func_code:
            return True
                        
        _, func_name = get_func_name(self.func, resolv_alias=False,
                                     win_characters=False)
        if old_first_line == first_line == -1 or func_name == '<lambda>':
            if not first_line == -1:
                func_description = '%s (%s:%i)' % (func_name,
                                                source_file, first_line)
            else:
                func_description = func_name
            warnings.warn(JobLibCollisionWarning(
                "Cannot detect name collisions for function '%s'"
                        % func_description), stacklevel=stacklevel)
                                        if not old_first_line == first_line and source_file is not None:
            possible_collision = False
            if os.path.exists(source_file):
                _, func_name = get_func_name(self.func, resolv_alias=False)
                num_lines = len(func_code.split('\n'))
                with open_py_source(source_file) as f:
                    on_disk_func_code = f.readlines()[
                        old_first_line - 1:old_first_line - 1 + num_lines - 1]
                on_disk_func_code = ''.join(on_disk_func_code)
                possible_collision = (on_disk_func_code.rstrip()
                                      == old_func_code.rstrip())
            else:
                possible_collision = source_file.startswith('<doctest ')
            if possible_collision:
                warnings.warn(JobLibCollisionWarning(
                        'Possible name collisions between functions '
                        "'%s' (%s:%i) and '%s' (%s:%i)" %
                        (func_name, source_file, old_first_line,
                        func_name, source_file, first_line)),
                                stacklevel=stacklevel)
                        if self._verbose > 10:
            _, func_name = get_func_name(self.func, resolv_alias=False)
            self.warn("Function %s (stored in %s) has changed." %
                        (func_name, func_dir))
        self.clear(warn=True)
        return False
    def clear(self, warn=True):
                func_dir = self._get_func_dir(mkdir=False)
        if self._verbose > 0 and warn:
            self.warn("Clearing cache %s" % func_dir)
        if os.path.exists(func_dir):
            shutil.rmtree(func_dir, ignore_errors=True)
        mkdirp(func_dir)
        func_code, _, first_line = get_func_code(self.func)
        func_code_file = os.path.join(func_dir, 'func_code.py')
        self._write_func_code(func_code_file, func_code, first_line)
    def call(self, *args, **kwargs):
                start_time = time.time()
        output_dir, _ = self._get_output_dir(*args, **kwargs)
        if self._verbose > 0:
            print(format_call(self.func, args, kwargs))
        output = self.func(*args, **kwargs)
        self._persist_output(output, output_dir)
        duration = time.time() - start_time
        metadata = self._persist_input(output_dir, duration, args, kwargs)
        if self._verbose > 0:
            _, name = get_func_name(self.func)
            msg = '%s - %s' % (name, format_time(duration))
            print(max(0, (80 - len(msg))) * '_' + msg)
        return output, metadata
        def _persist_output(self, output, dir):
                try:
            filename = os.path.join(dir, 'output.pkl')
            mkdirp(dir)
            write_func = functools.partial(numpy_pickle.dump,
                                           compress=self.compress)
            concurrency_safe_write(output, filename, write_func)
            if self._verbose > 10:
                print('Persisting in %s' % dir)
        except OSError:
            " Race condition in the creation of the directory "
    def _persist_input(self, output_dir, duration, args, kwargs,
                       this_duration_limit=0.5):
                start_time = time.time()
        argument_dict = filter_args(self.func, self.ignore,
                                    args, kwargs)
        input_repr = dict((k, repr(v)) for k, v in argument_dict.items())
                        metadata = {"duration": duration, "input_args": input_repr}
        try:
            mkdirp(output_dir)
            filename = os.path.join(output_dir, 'metadata.json')
            def write_func(output, dest_filename):
                with open(dest_filename, 'w') as f:
                    json.dump(output, f)
            concurrency_safe_write(metadata, filename, write_func)
        except Exception:
            pass
        this_duration = time.time() - start_time
        if this_duration > this_duration_limit:
                                                                                    warnings.warn("Persisting input arguments took %.2fs to run.\n"
                          "If this happens often in your code, it can cause "
                          "performance problems \n"
                          "(results will be correct in all cases). \n"
                          "The reason for this is probably some large input "
                          "arguments for a wrapped\n"
                          " function (e.g. large strings).\n"
                          "THIS IS A JOBLIB ISSUE. If you can, kindly provide "
                          "the joblib's team with an\n"
                          " example so that they can fix the problem."
                          % this_duration, stacklevel=5)
        return metadata
    
            
    def __repr__(self):
        return '%s(func=%s, cachedir=%s)' % (
                    self.__class__.__name__,
                    self.func,
                    repr(self.cachedir),
                    )

class Memory(Logger):
                
    def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1,
                 bytes_limit=None):
                        Logger.__init__(self)
        self._verbose = verbose
        self.mmap_mode = mmap_mode
        self.timestamp = time.time()
        self.compress = compress
        self.bytes_limit = bytes_limit
        if compress and mmap_mode is not None:
            warnings.warn('Compressed results cannot be memmapped',
                          stacklevel=2)
        if cachedir is None:
            self.cachedir = None
        else:
            self.cachedir = os.path.join(cachedir, 'joblib')
            mkdirp(self.cachedir)
    def cache(self, func=None, ignore=None, verbose=None,
                        mmap_mode=False):
                if func is None:
                                    return functools.partial(self.cache, ignore=ignore,
                                     verbose=verbose, mmap_mode=mmap_mode)
        if self.cachedir is None:
            return NotMemorizedFunc(func)
        if verbose is None:
            verbose = self._verbose
        if mmap_mode is False:
            mmap_mode = self.mmap_mode
        if isinstance(func, MemorizedFunc):
            func = func.func
        return MemorizedFunc(func, cachedir=self.cachedir,
                                   mmap_mode=mmap_mode,
                                   ignore=ignore,
                                   compress=self.compress,
                                   verbose=verbose,
                                   timestamp=self.timestamp)
    def clear(self, warn=True):
                if warn:
            self.warn('Flushing completely the cache')
        if self.cachedir is not None:
            rm_subdirs(self.cachedir)
    def reduce_size(self):
                if self.cachedir is not None and self.bytes_limit is not None:
            cache_items_to_delete = _get_cache_items_to_delete(
                self.cachedir, self.bytes_limit)
            for cache_item in cache_items_to_delete:
                if self._verbose > 10:
                    print('Deleting cache item {}'.format(cache_item))
                try:
                    shutil.rmtree(cache_item.path, ignore_errors=True)
                except OSError:
                                                                                                    pass
    def eval(self, func, *args, **kwargs):
                if self.cachedir is None:
            return func(*args, **kwargs)
        return self.cache(func)(*args, **kwargs)
            
    def __repr__(self):
        return '%s(cachedir=%s)' % (
                    self.__class__.__name__,
                    repr(self.cachedir),
                    )
    def __reduce__(self):
                        cachedir = self.cachedir[:-7] if self.cachedir is not None else None
        return (self.__class__, (cachedir,
                self.mmap_mode, self.compress, self._verbose))
import sys
from ._compat import PY3_OR_LATER
class JoblibException(Exception):
        def __init__(self, *args):
                                                                                                Exception.__init__(self, *args)
    def __repr__(self):
        if hasattr(self, 'args') and len(self.args) > 0:
            message = self.args[0]
        else:
            message = ''
        name = self.__class__.__name__
        return '%s\n%s\n%s\n%s' % (name, 75 * '_', message, 75 * '_')
    __str__ = __repr__

class TransportableException(JoblibException):
    
    def __init__(self, message, etype):
                        JoblibException.__init__(self, message, etype)
        self.message = message
        self.etype = etype

class WorkerInterrupt(Exception):
        pass

_exception_mapping = dict()

def _mk_exception(exception, name=None):
            if name is None:
        name = exception.__name__
    this_name = 'Joblib%s' % name
    if this_name in _exception_mapping:
                this_exception = _exception_mapping[this_name]
    else:
        if exception is Exception:
                                    return JoblibException, this_name
        try:
            this_exception = type(
                this_name, (JoblibException, exception), {})
            _exception_mapping[this_name] = this_exception
        except TypeError:
                                                            this_exception = JoblibException
    return this_exception, this_name

def _mk_common_exceptions():
    namespace = dict()
    if PY3_OR_LATER:
        import builtins as _builtin_exceptions
        common_exceptions = filter(
            lambda x: x.endswith('Error'),
            dir(_builtin_exceptions))
    else:
        import exceptions as _builtin_exceptions
        common_exceptions = dir(_builtin_exceptions)
    for name in common_exceptions:
        obj = getattr(_builtin_exceptions, name)
        if isinstance(obj, type) and issubclass(obj, BaseException):
            this_obj, this_name = _mk_exception(obj, name=name)
            namespace[this_name] = this_obj
    return namespace

locals().update(_mk_common_exceptions())

import pickle
import os
import sys
import warnings
try:
    from pathlib import Path
except ImportError:
    Path = None
from .numpy_pickle_utils import _COMPRESSORS
from .numpy_pickle_utils import BinaryZlibFile
from .numpy_pickle_utils import Unpickler, Pickler
from .numpy_pickle_utils import _read_fileobject, _write_fileobject
from .numpy_pickle_utils import _read_bytes, BUFFER_SIZE
from .numpy_pickle_compat import load_compatibility
from .numpy_pickle_compat import NDArrayWrapper
from .numpy_pickle_compat import ZNDArrayWrapper  from ._compat import _basestring, PY3_OR_LATER
from .backports import make_memmap

class NumpyArrayWrapper(object):
    
    def __init__(self, subclass, shape, order, dtype, allow_mmap=False):
                self.subclass = subclass
        self.shape = shape
        self.order = order
        self.dtype = dtype
        self.allow_mmap = allow_mmap
    def write_array(self, array, pickler):
                        buffersize = max(16 * 1024 ** 2 // array.itemsize, 1)
        if array.dtype.hasobject:
                                                pickle.dump(array, pickler.file_handle, protocol=2)
        else:
            for chunk in pickler.np.nditer(array,
                                           flags=['external_loop',
                                                  'buffered',
                                                  'zerosize_ok'],
                                           buffersize=buffersize,
                                           order=self.order):
                pickler.file_handle.write(chunk.tostring('C'))
    def read_array(self, unpickler):
                if len(self.shape) == 0:
            count = 1
        else:
            count = unpickler.np.multiply.reduce(self.shape)
                if self.dtype.hasobject:
                        array = pickle.load(unpickler.file_handle)
        else:
            if (not PY3_OR_LATER and
                    unpickler.np.compat.isfileobj(unpickler.file_handle)):
                                                                                                array = unpickler.np.fromfile(unpickler.file_handle,
                                              dtype=self.dtype, count=count)
            else:
                                                                                                                                max_read_count = BUFFER_SIZE // min(BUFFER_SIZE,
                                                    self.dtype.itemsize)
                array = unpickler.np.empty(count, dtype=self.dtype)
                for i in range(0, count, max_read_count):
                    read_count = min(max_read_count, count - i)
                    read_size = int(read_count * self.dtype.itemsize)
                    data = _read_bytes(unpickler.file_handle,
                                       read_size, "array data")
                    array[i:i + read_count] = \
                        unpickler.np.frombuffer(data, dtype=self.dtype,
                                                count=read_count)
                    del data
            if self.order == 'F':
                array.shape = self.shape[::-1]
                array = array.transpose()
            else:
                array.shape = self.shape
        return array
    def read_mmap(self, unpickler):
                offset = unpickler.file_handle.tell()
        if unpickler.mmap_mode == 'w+':
            unpickler.mmap_mode = 'r+'
        marray = make_memmap(unpickler.filename,
                             dtype=self.dtype,
                             shape=self.shape,
                             order=self.order,
                             mode=unpickler.mmap_mode,
                             offset=offset)
                unpickler.file_handle.seek(offset + marray.nbytes)
        return marray
    def read(self, unpickler):
                        if unpickler.mmap_mode is not None and self.allow_mmap:
            array = self.read_mmap(unpickler)
        else:
            array = self.read_array(unpickler)
                if (hasattr(array, '__array_prepare__') and
            self.subclass not in (unpickler.np.ndarray,
                                  unpickler.np.memmap)):
                        new_array = unpickler.np.core.multiarray._reconstruct(
                self.subclass, (0,), 'b')
            return new_array.__array_prepare__(array)
        else:
            return array

class NumpyPickler(Pickler):
    
    dispatch = Pickler.dispatch.copy()
    def __init__(self, fp, protocol=None):
        self.file_handle = fp
        self.buffered = isinstance(self.file_handle, BinaryZlibFile)
                        if protocol is None:
            protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER
                        else pickle.HIGHEST_PROTOCOL)
        Pickler.__init__(self, self.file_handle, protocol=protocol)
                try:
            import numpy as np
        except ImportError:
            np = None
        self.np = np
    def _create_array_wrapper(self, array):
                order = 'F' if (array.flags.f_contiguous and
                        not array.flags.c_contiguous) else 'C'
        allow_mmap = not self.buffered and not array.dtype.hasobject
        wrapper = NumpyArrayWrapper(type(array),
                                    array.shape, order, array.dtype,
                                    allow_mmap=allow_mmap)
        return wrapper
    def save(self, obj):
                if self.np is not None and type(obj) in (self.np.ndarray,
                                                 self.np.matrix,
                                                 self.np.memmap):
            if type(obj) is self.np.memmap:
                                obj = self.np.asanyarray(obj)
                        wrapper = self._create_array_wrapper(obj)
            Pickler.save(self, wrapper)
                                                                        if self.proto >= 4:
                self.framer.commit_frame(force=True)
                        wrapper.write_array(obj, self)
            return
        return Pickler.save(self, obj)

class NumpyUnpickler(Unpickler):
    
    dispatch = Unpickler.dispatch.copy()
    def __init__(self, filename, file_handle, mmap_mode=None):
                        self._dirname = os.path.dirname(filename)
        self.mmap_mode = mmap_mode
        self.file_handle = file_handle
                self.filename = filename
        self.compat_mode = False
        Unpickler.__init__(self, self.file_handle)
        try:
            import numpy as np
        except ImportError:
            np = None
        self.np = np
    def load_build(self):
                Unpickler.load_build(self)
                if isinstance(self.stack[-1], (NDArrayWrapper, NumpyArrayWrapper)):
            if self.np is None:
                raise ImportError("Trying to unpickle an ndarray, "
                                  "but numpy didn't import correctly")
            array_wrapper = self.stack.pop()
                                                if isinstance(array_wrapper, NDArrayWrapper):
                self.compat_mode = True
            self.stack.append(array_wrapper.read(self))
        if PY3_OR_LATER:
        dispatch[pickle.BUILD[0]] = load_build
    else:
        dispatch[pickle.BUILD] = load_build

def dump(value, filename, compress=0, protocol=None, cache_size=None):
    
    if Path is not None and isinstance(filename, Path):
        filename = str(filename)
    is_filename = isinstance(filename, _basestring)
    is_fileobj = hasattr(filename, "write")
    compress_method = 'zlib'      if compress is True:
                compress_level = 3
    elif isinstance(compress, tuple):
                if len(compress) != 2:
            raise ValueError(
                'Compress argument tuple should contain exactly 2 elements: '
                '(compress method, compress level), you passed {}'
                .format(compress))
        compress_method, compress_level = compress
    else:
        compress_level = compress
    if compress_level is not False and compress_level not in range(10):
                raise ValueError(
            'Non valid compress level given: "{}". Possible values are '
            '{}.'.format(compress_level, list(range(10))))
    if compress_method not in _COMPRESSORS:
                raise ValueError(
            'Non valid compression method given: "{}". Possible values are '
            '{}.'.format(compress_method, _COMPRESSORS))
    if not is_filename and not is_fileobj:
                        raise ValueError(
            'Second argument should be a filename or a file-like object, '
            '%s (type %s) was given.'
            % (filename, type(filename))
        )
    if is_filename and not isinstance(compress, tuple):
                                if filename.endswith('.z'):
            compress_method = 'zlib'
        elif filename.endswith('.gz'):
            compress_method = 'gzip'
        elif filename.endswith('.bz2'):
            compress_method = 'bz2'
        elif filename.endswith('.lzma'):
            compress_method = 'lzma'
        elif filename.endswith('.xz'):
            compress_method = 'xz'
        else:
                                    compress_method = None
        if compress_method in _COMPRESSORS and compress_level == 0:
                                    compress_level = 3
    if not PY3_OR_LATER and compress_method in ('lzma', 'xz'):
        raise NotImplementedError("{} compression is only available for "
                                  "python version >= 3.3. You are using "
                                  "{}.{}".format(compress_method,
                                                 sys.version_info[0],
                                                 sys.version_info[1]))
    if cache_size is not None:
                warnings.warn("Please do not set 'cache_size' in joblib.dump, "
                      "this parameter has no effect and will be removed. "
                      "You used 'cache_size={}'".format(cache_size),
                      DeprecationWarning, stacklevel=2)
    if compress_level != 0:
        with _write_fileobject(filename, compress=(compress_method,
                                                   compress_level)) as f:
            NumpyPickler(f, protocol=protocol).dump(value)
    elif is_filename:
        with open(filename, 'wb') as f:
            NumpyPickler(f, protocol=protocol).dump(value)
    else:
        NumpyPickler(filename, protocol=protocol).dump(value)
        if is_fileobj:
        return
            return [filename]

def _unpickle(fobj, filename="", mmap_mode=None):
                                unpickler = NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode)
    obj = None
    try:
        obj = unpickler.load()
        if unpickler.compat_mode:
            warnings.warn("The file '%s' has been generated with a "
                          "joblib version less than 0.10. "
                          "Please regenerate this pickle file."
                          % filename,
                          DeprecationWarning, stacklevel=3)
    except UnicodeDecodeError as exc:
                if PY3_OR_LATER:
            new_exc = ValueError(
                'You may be trying to read with '
                'python 3 a joblib pickle generated with python 2. '
                'This feature is not supported by joblib.')
            new_exc.__cause__ = exc
            raise new_exc
                raise
    return obj

def load(filename, mmap_mode=None):
        if Path is not None and isinstance(filename, Path):
        filename = str(filename)
    if hasattr(filename, "read"):
        fobj = filename
        filename = getattr(fobj, 'name', '')
        with _read_fileobject(fobj, filename, mmap_mode) as fobj:
            obj = _unpickle(fobj)
    else:
        with open(filename, 'rb') as f:
            with _read_fileobject(f, filename, mmap_mode) as fobj:
                if isinstance(fobj, _basestring):
                                                                                return load_compatibility(fobj)
                obj = _unpickle(fobj, filename, mmap_mode)
    return obj
import pickle
import os
import zlib
from io import BytesIO
from ._compat import PY3_OR_LATER
from .numpy_pickle_utils import _ZFILE_PREFIX
from .numpy_pickle_utils import Unpickler

def hex_str(an_int):
        return '{:
if PY3_OR_LATER:
    def asbytes(s):
        if isinstance(s, bytes):
            return s
        return s.encode('latin1')
else:
    asbytes = str
_MAX_LEN = len(hex_str(2 ** 64))
_CHUNK_SIZE = 64 * 1024

def read_zfile(file_handle):
        file_handle.seek(0)
    header_length = len(_ZFILE_PREFIX) + _MAX_LEN
    length = file_handle.read(header_length)
    length = length[len(_ZFILE_PREFIX):]
    length = int(length, 16)
                        next_byte = file_handle.read(1)
    if next_byte != b' ':
                        file_handle.seek(header_length)
            data = zlib.decompress(file_handle.read(), 15, length)
    assert len(data) == length, (
        "Incorrect data length while decompressing %s."
        "The file could be corrupted." % file_handle)
    return data

def write_zfile(file_handle, data, compress=1):
        file_handle.write(_ZFILE_PREFIX)
    length = hex_str(len(data))
        file_handle.write(asbytes(length.ljust(_MAX_LEN)))
    file_handle.write(zlib.compress(asbytes(data), compress))

class NDArrayWrapper(object):
    
    def __init__(self, filename, subclass, allow_mmap=True):
                self.filename = filename
        self.subclass = subclass
        self.allow_mmap = allow_mmap
    def read(self, unpickler):
                filename = os.path.join(unpickler._dirname, self.filename)
                                allow_mmap = getattr(self, 'allow_mmap', True)
        memmap_kwargs = ({} if not allow_mmap
                         else {'mmap_mode': unpickler.mmap_mode})
        array = unpickler.np.load(filename, **memmap_kwargs)
                        if (hasattr(array, '__array_prepare__') and
            self.subclass not in (unpickler.np.ndarray,
                                  unpickler.np.memmap)):
                        new_array = unpickler.np.core.multiarray._reconstruct(
                self.subclass, (0,), 'b')
            return new_array.__array_prepare__(array)
        else:
            return array

class ZNDArrayWrapper(NDArrayWrapper):
    
    def __init__(self, filename, init_args, state):
                self.filename = filename
        self.state = state
        self.init_args = init_args
    def read(self, unpickler):
                                filename = os.path.join(unpickler._dirname, self.filename)
        array = unpickler.np.core.multiarray._reconstruct(*self.init_args)
        with open(filename, 'rb') as f:
            data = read_zfile(f)
        state = self.state + (data,)
        array.__setstate__(state)
        return array

class ZipNumpyUnpickler(Unpickler):
    
    dispatch = Unpickler.dispatch.copy()
    def __init__(self, filename, file_handle, mmap_mode=None):
                self._filename = os.path.basename(filename)
        self._dirname = os.path.dirname(filename)
        self.mmap_mode = mmap_mode
        self.file_handle = self._open_pickle(file_handle)
        Unpickler.__init__(self, self.file_handle)
        try:
            import numpy as np
        except ImportError:
            np = None
        self.np = np
    def _open_pickle(self, file_handle):
        return BytesIO(read_zfile(file_handle))
    def load_build(self):
                Unpickler.load_build(self)
        if isinstance(self.stack[-1], NDArrayWrapper):
            if self.np is None:
                raise ImportError("Trying to unpickle an ndarray, "
                                  "but numpy didn't import correctly")
            nd_array_wrapper = self.stack.pop()
            array = nd_array_wrapper.read(self)
            self.stack.append(array)
        if PY3_OR_LATER:
        dispatch[pickle.BUILD[0]] = load_build
    else:
        dispatch[pickle.BUILD] = load_build

def load_compatibility(filename):
        with open(filename, 'rb') as file_handle:
                                        unpickler = ZipNumpyUnpickler(filename, file_handle=file_handle)
        try:
            obj = unpickler.load()
        except UnicodeDecodeError as exc:
                        if PY3_OR_LATER:
                new_exc = ValueError(
                    'You may be trying to read with '
                    'python 3 a joblib pickle generated with python 2. '
                    'This feature is not supported by joblib.')
                new_exc.__cause__ = exc
                raise new_exc
        finally:
            if hasattr(unpickler, 'file_handle'):
                unpickler.file_handle.close()
        return obj

import pickle
import sys
import io
import zlib
import gzip
import warnings
import contextlib
from contextlib import closing
from ._compat import PY3_OR_LATER, PY27, _basestring
try:
    from threading import RLock
except ImportError:
    from dummy_threading import RLock
if PY3_OR_LATER:
    Unpickler = pickle._Unpickler
    Pickler = pickle._Pickler
    xrange = range
else:
    Unpickler = pickle.Unpickler
    Pickler = pickle.Pickler
try:
    import numpy as np
except ImportError:
    np = None
try:
    import lzma
except ImportError:
    lzma = None

try:
                    import bz2
except ImportError:
    bz2 = None

_ZFILE_PREFIX = b'ZF'  _ZLIB_PREFIX = b'\x78'
_GZIP_PREFIX = b'\x1f\x8b'
_BZ2_PREFIX = b'BZ'
_XZ_PREFIX = b'\xfd\x37\x7a\x58\x5a'
_LZMA_PREFIX = b'\x5d\x00'
_COMPRESSORS = ('zlib', 'bz2', 'lzma', 'xz', 'gzip')
_COMPRESSOR_CLASSES = [gzip.GzipFile]
if bz2 is not None:
    _COMPRESSOR_CLASSES.append(bz2.BZ2File)
if lzma is not None:
    _COMPRESSOR_CLASSES.append(lzma.LZMAFile)
_MAX_PREFIX_LEN = max(len(prefix)
                      for prefix in (_ZFILE_PREFIX, _GZIP_PREFIX, _BZ2_PREFIX,
                                     _XZ_PREFIX, _LZMA_PREFIX))
_IO_BUFFER_SIZE = 1024 ** 2

def _is_raw_file(fileobj):
        if PY3_OR_LATER:
        fileobj = getattr(fileobj, 'raw', fileobj)
        return isinstance(fileobj, io.FileIO)
    else:
        return isinstance(fileobj, file)  
def _detect_compressor(fileobj):
            if hasattr(fileobj, 'peek'):
                        first_bytes = fileobj.peek(_MAX_PREFIX_LEN)
    else:
                first_bytes = fileobj.read(_MAX_PREFIX_LEN)
        fileobj.seek(0)
    if first_bytes.startswith(_ZLIB_PREFIX):
        return "zlib"
    elif first_bytes.startswith(_GZIP_PREFIX):
        return "gzip"
    elif first_bytes.startswith(_BZ2_PREFIX):
        return "bz2"
    elif first_bytes.startswith(_LZMA_PREFIX):
        return "lzma"
    elif first_bytes.startswith(_XZ_PREFIX):
        return "xz"
    elif first_bytes.startswith(_ZFILE_PREFIX):
        return "compat"
    return "not-compressed"

def _buffered_read_file(fobj):
        if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
                        return fobj
    else:
        return io.BufferedReader(fobj, buffer_size=_IO_BUFFER_SIZE)

def _buffered_write_file(fobj):
        if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):
                                        return closing(fobj)
    else:
        return io.BufferedWriter(fobj, buffer_size=_IO_BUFFER_SIZE)

@contextlib.contextmanager
def _read_fileobject(fileobj, filename, mmap_mode=None):
            compressor = _detect_compressor(fileobj)
    if compressor == 'compat':
                                warnings.warn("The file '%s' has been generated with a joblib "
                      "version less than 0.10. "
                      "Please regenerate this pickle file." % filename,
                      DeprecationWarning, stacklevel=2)
        yield filename
    else:
                        if compressor == 'zlib':
            fileobj = _buffered_read_file(BinaryZlibFile(fileobj, 'rb'))
        elif compressor == 'gzip':
            fileobj = _buffered_read_file(BinaryGzipFile(fileobj, 'rb'))
        elif compressor == 'bz2' and bz2 is not None:
            if PY3_OR_LATER:
                fileobj = _buffered_read_file(bz2.BZ2File(fileobj, 'rb'))
            else:
                                                fileobj = _buffered_read_file(bz2.BZ2File(fileobj.name, 'rb'))
        elif (compressor == 'lzma' or compressor == 'xz'):
            if PY3_OR_LATER and lzma is not None:
                                                                                                fileobj = _buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))
            else:
                raise NotImplementedError("Lzma decompression is not "
                                          "supported for this version of "
                                          "python ({}.{})"
                                          .format(sys.version_info[0],
                                                  sys.version_info[1]))
                                if mmap_mode is not None:
            if isinstance(fileobj, io.BytesIO):
                warnings.warn('In memory persistence is not compatible with '
                              'mmap_mode "%(mmap_mode)s" flag passed. '
                              'mmap_mode option will be ignored.'
                              % locals(), stacklevel=2)
            elif compressor != 'not-compressed':
                warnings.warn('mmap_mode "%(mmap_mode)s" is not compatible '
                              'with compressed file %(filename)s. '
                              '"%(mmap_mode)s" flag will be ignored.'
                              % locals(), stacklevel=2)
            elif not _is_raw_file(fileobj):
                warnings.warn('"%(fileobj)r" is not a raw file, mmap_mode '
                              '"%(mmap_mode)s" flag will be ignored.'
                              % locals(), stacklevel=2)
        yield fileobj

def _write_fileobject(filename, compress=("zlib", 3)):
        compressmethod = compress[0]
    compresslevel = compress[1]
    if compressmethod == "gzip":
        return _buffered_write_file(BinaryGzipFile(filename, 'wb',
                                    compresslevel=compresslevel))
    elif compressmethod == "bz2" and bz2 is not None:
        return _buffered_write_file(bz2.BZ2File(filename, 'wb',
                                                compresslevel=compresslevel))
    elif lzma is not None and compressmethod == "xz":
        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',
                                                  check=lzma.CHECK_NONE,
                                                  preset=compresslevel))
    elif lzma is not None and compressmethod == "lzma":
        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',
                                                  preset=compresslevel,
                                                  format=lzma.FORMAT_ALONE))
    else:
        return _buffered_write_file(BinaryZlibFile(filename, 'wb',
                                    compresslevel=compresslevel))

_MODE_CLOSED = 0
_MODE_READ = 1
_MODE_READ_EOF = 2
_MODE_WRITE = 3
_BUFFER_SIZE = 8192

class BinaryZlibFile(io.BufferedIOBase):
    
    wbits = zlib.MAX_WBITS
    def __init__(self, filename, mode="rb", compresslevel=9):
                        self._lock = RLock()
        self._fp = None
        self._closefp = False
        self._mode = _MODE_CLOSED
        self._pos = 0
        self._size = -1
        if not isinstance(compresslevel, int) or not (1 <= compresslevel <= 9):
            raise ValueError("'compresslevel' must be an integer "
                             "between 1 and 9. You provided 'compresslevel={}'"
                             .format(compresslevel))
        if mode == "rb":
            mode_code = _MODE_READ
            self._decompressor = zlib.decompressobj(self.wbits)
            self._buffer = b""
            self._buffer_offset = 0
        elif mode == "wb":
            mode_code = _MODE_WRITE
            self._compressor = zlib.compressobj(compresslevel,
                                                zlib.DEFLATED,
                                                self.wbits,
                                                zlib.DEF_MEM_LEVEL,
                                                0)
        else:
            raise ValueError("Invalid mode: %r" % (mode,))
        if isinstance(filename, _basestring):
            self._fp = io.open(filename, mode)
            self._closefp = True
            self._mode = mode_code
        elif hasattr(filename, "read") or hasattr(filename, "write"):
            self._fp = filename
            self._mode = mode_code
        else:
            raise TypeError("filename must be a str or bytes object, "
                            "or a file")
    def close(self):
                with self._lock:
            if self._mode == _MODE_CLOSED:
                return
            try:
                if self._mode in (_MODE_READ, _MODE_READ_EOF):
                    self._decompressor = None
                elif self._mode == _MODE_WRITE:
                    self._fp.write(self._compressor.flush())
                    self._compressor = None
            finally:
                try:
                    if self._closefp:
                        self._fp.close()
                finally:
                    self._fp = None
                    self._closefp = False
                    self._mode = _MODE_CLOSED
                    self._buffer = b""
                    self._buffer_offset = 0
    @property
    def closed(self):
                return self._mode == _MODE_CLOSED
    def fileno(self):
                self._check_not_closed()
        return self._fp.fileno()
    def seekable(self):
                return self.readable() and self._fp.seekable()
    def readable(self):
                self._check_not_closed()
        return self._mode in (_MODE_READ, _MODE_READ_EOF)
    def writable(self):
                self._check_not_closed()
        return self._mode == _MODE_WRITE
    
    def _check_not_closed(self):
        if self.closed:
            fname = getattr(self._fp, 'name', None)
            msg = "I/O operation on closed file"
            if fname is not None:
                msg += " {}".format(fname)
            msg += "."
            raise ValueError(msg)
    def _check_can_read(self):
        if self._mode not in (_MODE_READ, _MODE_READ_EOF):
            self._check_not_closed()
            raise io.UnsupportedOperation("File not open for reading")
    def _check_can_write(self):
        if self._mode != _MODE_WRITE:
            self._check_not_closed()
            raise io.UnsupportedOperation("File not open for writing")
    def _check_can_seek(self):
        if self._mode not in (_MODE_READ, _MODE_READ_EOF):
            self._check_not_closed()
            raise io.UnsupportedOperation("Seeking is only supported "
                                          "on files open for reading")
        if not self._fp.seekable():
            raise io.UnsupportedOperation("The underlying file object "
                                          "does not support seeking")
        def _fill_buffer(self):
        if self._mode == _MODE_READ_EOF:
            return False
                        while self._buffer_offset == len(self._buffer):
            try:
                rawblock = (self._decompressor.unused_data or
                            self._fp.read(_BUFFER_SIZE))
                if not rawblock:
                    raise EOFError
            except EOFError:
                                self._mode = _MODE_READ_EOF
                self._size = self._pos
                return False
            else:
                self._buffer = self._decompressor.decompress(rawblock)
            self._buffer_offset = 0
        return True
            def _read_all(self, return_data=True):
                self._buffer = self._buffer[self._buffer_offset:]
        self._buffer_offset = 0
        blocks = []
        while self._fill_buffer():
            if return_data:
                blocks.append(self._buffer)
            self._pos += len(self._buffer)
            self._buffer = b""
        if return_data:
            return b"".join(blocks)
            def _read_block(self, n_bytes, return_data=True):
                end = self._buffer_offset + n_bytes
        if end <= len(self._buffer):
            data = self._buffer[self._buffer_offset: end]
            self._buffer_offset = end
            self._pos += len(data)
            return data if return_data else None
                self._buffer = self._buffer[self._buffer_offset:]
        self._buffer_offset = 0
        blocks = []
        while n_bytes > 0 and self._fill_buffer():
            if n_bytes < len(self._buffer):
                data = self._buffer[:n_bytes]
                self._buffer_offset = n_bytes
            else:
                data = self._buffer
                self._buffer = b""
            if return_data:
                blocks.append(data)
            self._pos += len(data)
            n_bytes -= len(data)
        if return_data:
            return b"".join(blocks)
    def read(self, size=-1):
                with self._lock:
            self._check_can_read()
            if size == 0:
                return b""
            elif size < 0:
                return self._read_all()
            else:
                return self._read_block(size)
    def readinto(self, b):
                with self._lock:
            return io.BufferedIOBase.readinto(self, b)
    def write(self, data):
                with self._lock:
            self._check_can_write()
                        if isinstance(data, memoryview):
                data = data.tobytes()
            compressed = self._compressor.compress(data)
            self._fp.write(compressed)
            self._pos += len(data)
            return len(data)
        def _rewind(self):
        self._fp.seek(0, 0)
        self._mode = _MODE_READ
        self._pos = 0
        self._decompressor = zlib.decompressobj(self.wbits)
        self._buffer = b""
        self._buffer_offset = 0
    def seek(self, offset, whence=0):
                with self._lock:
            self._check_can_seek()
                        if whence == 0:
                pass
            elif whence == 1:
                offset = self._pos + offset
            elif whence == 2:
                                if self._size < 0:
                    self._read_all(return_data=False)
                offset = self._size + offset
            else:
                raise ValueError("Invalid value for whence: %s" % (whence,))
                        if offset < self._pos:
                self._rewind()
            else:
                offset -= self._pos
                        self._read_block(offset, return_data=False)
            return self._pos
    def tell(self):
                with self._lock:
            self._check_not_closed()
            return self._pos

class BinaryGzipFile(BinaryZlibFile):
    
    wbits = 31  
BUFFER_SIZE = 2 ** 18  
def _read_bytes(fp, size, error_template="ran out of data"):
        data = bytes()
    while True:
                                try:
            r = fp.read(size - len(data))
            data += r
            if len(r) == 0 or len(data) == size:
                break
        except io.BlockingIOError:
            pass
    if len(data) != size:
        msg = "EOF: reading %s, expected %d bytes got %d"
        raise ValueError(msg % (error_template, size, len(data)))
    else:
        return data
from __future__ import division
import os
import sys
from math import sqrt
import functools
import time
import threading
import itertools
from numbers import Integral
from contextlib import contextmanager
import warnings
try:
    import cPickle as pickle
except ImportError:
    import pickle
from ._multiprocessing_helpers import mp
from .format_stack import format_outer_frames
from .logger import Logger, short_format_time
from .my_exceptions import TransportableException, _mk_exception
from .disk import memstr_to_bytes
from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,
                                 ThreadingBackend, SequentialBackend)
from ._compat import _basestring
from ._parallel_backends import AutoBatchingMixin  from ._parallel_backends import ParallelBackendBase  
BACKENDS = {
    'multiprocessing': MultiprocessingBackend,
    'threading': ThreadingBackend,
    'sequential': SequentialBackend,
}
DEFAULT_BACKEND = 'multiprocessing'
DEFAULT_N_JOBS = 1
_backend = threading.local()

def get_active_backend():
        active_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
    if active_backend_and_jobs is not None:
        return active_backend_and_jobs
            active_backend = BACKENDS[DEFAULT_BACKEND]()
    return active_backend, DEFAULT_N_JOBS

@contextmanager
def parallel_backend(backend, n_jobs=-1, **backend_params):
        if isinstance(backend, _basestring):
        backend = BACKENDS[backend](**backend_params)
    old_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)
    try:
        _backend.backend_and_jobs = (backend, n_jobs)
                yield backend, n_jobs
    finally:
        if old_backend_and_jobs is None:
            if getattr(_backend, 'backend_and_jobs', None) is not None:
                del _backend.backend_and_jobs
        else:
            _backend.backend_and_jobs = old_backend_and_jobs

if hasattr(mp, 'get_context'):
    method = os.environ.get('JOBLIB_START_METHOD', '').strip() or None
    DEFAULT_MP_CONTEXT = mp.get_context(method=method)
else:
    DEFAULT_MP_CONTEXT = None

class BatchedCalls(object):
    
    def __init__(self, iterator_slice):
        self.items = list(iterator_slice)
        self._size = len(self.items)
    def __call__(self):
        return [func(*args, **kwargs) for func, args, kwargs in self.items]
    def __len__(self):
        return self._size

def cpu_count():
        if mp is None:
        return 1
    return mp.cpu_count()

def _verbosity_filter(index, verbose):
        if not verbose:
        return True
    elif verbose > 10:
        return False
    if index == 0:
        return False
    verbose = .5 * (11 - verbose) ** 2
    scale = sqrt(index / verbose)
    next_scale = sqrt((index + 1) / verbose)
    return (int(next_scale) == int(scale))

def delayed(function, check_pickle=True):
                if check_pickle:
        pickle.dumps(function)
    def delayed_function(*args, **kwargs):
        return function, args, kwargs
    try:
        delayed_function = functools.wraps(function)(delayed_function)
    except AttributeError:
        " functools.wraps fails on some callable objects "
    return delayed_function

class BatchCompletionCallBack(object):
        def __init__(self, dispatch_timestamp, batch_size, parallel):
        self.dispatch_timestamp = dispatch_timestamp
        self.batch_size = batch_size
        self.parallel = parallel
    def __call__(self, out):
        self.parallel.n_completed_tasks += self.batch_size
        this_batch_duration = time.time() - self.dispatch_timestamp
        self.parallel._backend.batch_completed(self.batch_size,
                                               this_batch_duration)
        self.parallel.print_progress()
        if self.parallel._original_iterator is not None:
            self.parallel.dispatch_next()

def register_parallel_backend(name, factory, make_default=False):
        BACKENDS[name] = factory
    if make_default:
        global DEFAULT_BACKEND
        DEFAULT_BACKEND = name

def effective_n_jobs(n_jobs=-1):
        backend, _ = get_active_backend()
    return backend.effective_n_jobs(n_jobs=n_jobs)

class Parallel(Logger):
    ''' Helper class for readable parallel mapping.
        Parameters
        -----------
        n_jobs: int, default: 1
            The maximum number of concurrently running jobs, such as the number
            of Python worker processes when backend="multiprocessing"
            or the size of the thread-pool when backend="threading".
            If -1 all CPUs are used. If 1 is given, no parallel computing code
            is used at all, which is useful for debugging. For n_jobs below -1,
            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all
            CPUs but one are used.
        backend: str, ParallelBackendBase instance or None, \
                default: 'multiprocessing'
            Specify the parallelization backend implementation.
            Supported backends are:
            - "multiprocessing" used by default, can induce some
              communication and memory overhead when exchanging input and
              output data with the worker Python processes.
            - "threading" is a very low-overhead backend but it suffers
              from the Python Global Interpreter Lock if the called function
              relies a lot on Python objects. "threading" is mostly useful
              when the execution bottleneck is a compiled extension that
              explicitly releases the GIL (for instance a Cython loop wrapped
              in a "with nogil" block or an expensive call to a library such
              as NumPy).
            - finally, you can register backends by calling
              register_parallel_backend. This will allow you to implement
              a backend of your liking.
        verbose: int, optional
            The verbosity level: if non zero, progress messages are
            printed. Above 50, the output is sent to stdout.
            The frequency of the messages increases with the verbosity level.
            If it more than 10, all iterations are reported.
        timeout: float, optional
            Timeout limit for each task to complete.  If any task takes longer
            a TimeOutError will be raised. Only applied when n_jobs != 1
        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}
            The number of batches (of tasks) to be pre-dispatched.
            Default is '2*n_jobs'. When batch_size="auto" this is reasonable
            default and the multiprocessing workers should never starve.
        batch_size: int or 'auto', default: 'auto'
            The number of atomic tasks to dispatch at once to each
            worker. When individual evaluations are very fast, multiprocessing
            can be slower than sequential computation because of the overhead.
            Batching fast computations together can mitigate this.
            The ``'auto'`` strategy keeps track of the time it takes for a batch
            to complete, and dynamically adjusts the batch size to keep the time
            on the order of half a second, using a heuristic. The initial batch
            size is 1.
            ``batch_size="auto"`` with ``backend="threading"`` will dispatch
            batches of a single task at a time as the threading backend has
            very little overhead and using larger batch size has not proved to
            bring any gain in that case.
        temp_folder: str, optional
            Folder to be used by the pool for memmaping large arrays
            for sharing memory with worker processes. If None, this will try in
            order:
            - a folder pointed by the JOBLIB_TEMP_FOLDER environment
              variable,
            - /dev/shm if the folder exists and is writable: this is a
              RAMdisk filesystem available by default on modern Linux
              distributions,
            - the default system temporary folder that can be
              overridden with TMP, TMPDIR or TEMP environment
              variables, typically /tmp under Unix operating systems.
            Only active when backend="multiprocessing".
        max_nbytes int, str, or None, optional, 1M by default
            Threshold on the size of arrays passed to the workers that
            triggers automated memory mapping in temp_folder. Can be an int
            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.
            Use None to disable memmaping of large arrays.
            Only active when backend="multiprocessing".
        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}
            Memmapping mode for numpy arrays passed to workers.
            See 'max_nbytes' parameter documentation for more details.
        Notes
        -----
        This object uses the multiprocessing module to compute in
        parallel the application of a function to many different
        arguments. The main functionality it brings in addition to
        using the raw multiprocessing API are (see examples for details):
        * More readable code, in particular since it avoids
          constructing list of arguments.
        * Easier debugging:
            - informative tracebacks even when the error happens on
              the client side
            - using 'n_jobs=1' enables to turn off parallel computing
              for debugging without changing the codepath
            - early capture of pickling errors
        * An optional progress meter.
        * Interruption of multiprocesses jobs with 'Ctrl-C'
        * Flexible pickling control for the communication to and from
          the worker processes.
        * Ability to use shared memory efficiently with worker
          processes for large numpy-based datastructures.
        Examples
        --------
        A simple example:
        >>> from math import sqrt
        >>> from sklearn.externals.joblib import Parallel, delayed
        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))
        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
        Reshaping the output when the function has several return
        values:
        >>> from math import modf
        >>> from sklearn.externals.joblib import Parallel, delayed
        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))
        >>> res, i = zip(*r)
        >>> res
        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)
        >>> i
        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)
        The progress meter: the higher the value of `verbose`, the more
        messages:
        >>> from time import sleep
        >>> from sklearn.externals.joblib import Parallel, delayed
        >>> r = Parallel(n_jobs=2, verbose=5)(delayed(sleep)(.1) for _ in range(10))         [Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s
        [Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s
        [Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s
        [Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s
        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished
        Traceback example, note how the line of the error is indicated
        as well as the values of the parameter passed to the function that
        triggered the exception, even though the traceback happens in the
        child process:
        >>> from heapq import nlargest
        >>> from sklearn.externals.joblib import Parallel, delayed
        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3))                 ---------------------------------------------------------------------------
        Sub-process traceback:
        ---------------------------------------------------------------------------
        TypeError                                          Mon Nov 12 11:37:46 2012
        PID: 12934                                    Python 2.7.3: /usr/bin/python
        ...........................................................................
        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)
            419         if n >= size:
            420             return sorted(iterable, key=key, reverse=True)[:n]
            421
            422                 423     if key is None:
        --> 424         it = izip(iterable, count(0,-1))                                425         result = _nlargest(n, it)
            426         return map(itemgetter(0), result)                               427
            428              TypeError: izip argument         ___________________________________________________________________________

        Using pre_dispatch in a producer/consumer situation, where the
        data is generated on the fly. Note how the producer is first
        called 3 times before the parallel loop is initiated, and then
        called to generate new data on the fly. In this case the total
        number of iterations cannot be reported in the progress messages:
        >>> from math import sqrt
        >>> from sklearn.externals.joblib import Parallel, delayed
        >>> def producer():
        ...     for i in range(6):
        ...         print('Produced %s' % i)
        ...         yield i
        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(
        ...                delayed(sqrt)(i) for i in producer())         Produced 0
        Produced 1
        Produced 2
        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s
        Produced 3
        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s
        Produced 4
        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s
        Produced 5
        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s
        [Parallel(n_jobs=2)]: Done 5 out of 6 | elapsed:  0.0s remaining: 0.0s
        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished
    '''
    def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,
                 pre_dispatch='2 * n_jobs', batch_size='auto',
                 temp_folder=None, max_nbytes='1M', mmap_mode='r'):
        active_backend, default_n_jobs = get_active_backend()
        if backend is None and n_jobs == 1:
                                    n_jobs = default_n_jobs
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.timeout = timeout
        self.pre_dispatch = pre_dispatch
        if isinstance(max_nbytes, _basestring):
            max_nbytes = memstr_to_bytes(max_nbytes)
        self._backend_args = dict(
            max_nbytes=max_nbytes,
            mmap_mode=mmap_mode,
            temp_folder=temp_folder,
            verbose=max(0, self.verbose - 50),
        )
        if DEFAULT_MP_CONTEXT is not None:
            self._backend_args['context'] = DEFAULT_MP_CONTEXT
        if backend is None:
            backend = active_backend
        elif isinstance(backend, ParallelBackendBase):
                        pass
        elif hasattr(backend, 'Pool') and hasattr(backend, 'Lock'):
                                                self._backend_args['context'] = backend
            backend = MultiprocessingBackend()
        else:
            try:
                backend_factory = BACKENDS[backend]
            except KeyError:
                raise ValueError("Invalid backend: %s, expected one of %r"
                                 % (backend, sorted(BACKENDS.keys())))
            backend = backend_factory()
        if (batch_size == 'auto' or isinstance(batch_size, Integral) and
                batch_size > 0):
            self.batch_size = batch_size
        else:
            raise ValueError(
                "batch_size must be 'auto' or a positive integer, got: %r"
                % batch_size)
        self._backend = backend
        self._output = None
        self._jobs = list()
        self._managed_backend = False
                        self._lock = threading.Lock()
    def __enter__(self):
        self._managed_backend = True
        self._initialize_backend()
        return self
    def __exit__(self, exc_type, exc_value, traceback):
        self._terminate_backend()
        self._managed_backend = False
    def _initialize_backend(self):
                try:
            n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,
                                             **self._backend_args)
            if self.timeout is not None and not self._backend.supports_timeout:
                warnings.warn(
                    'The backend class {!r} does not support timeout. '
                    "You have set 'timeout={}' in Parallel but "
                    "the 'timeout' parameter will not be used.".format(
                        self._backend.__class__.__name__,
                        self.timeout))
        except FallbackToBackend as e:
                        self._backend = e.backend
            n_jobs = self._initialize_backend()
        return n_jobs
    def _effective_n_jobs(self):
        if self._backend:
            return self._backend.effective_n_jobs(self.n_jobs)
        return 1
    def _terminate_backend(self):
        if self._backend is not None:
            self._backend.terminate()
    def _dispatch(self, batch):
                        if self._aborting:
            return
        self.n_dispatched_tasks += len(batch)
        self.n_dispatched_batches += 1
        dispatch_timestamp = time.time()
        cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
        job = self._backend.apply_async(batch, callback=cb)
        self._jobs.append(job)
    def dispatch_next(self):
                if not self.dispatch_one_batch(self._original_iterator):
            self._iterating = False
            self._original_iterator = None
    def dispatch_one_batch(self, iterator):
                if self.batch_size == 'auto':
            batch_size = self._backend.compute_batch_size()
        else:
                        batch_size = self.batch_size
        with self._lock:
            tasks = BatchedCalls(itertools.islice(iterator, batch_size))
            if len(tasks) == 0:
                                return False
            else:
                self._dispatch(tasks)
                return True
    def _print(self, msg, msg_args):
                                if not self.verbose:
            return
        if self.verbose < 50:
            writer = sys.stderr.write
        else:
            writer = sys.stdout.write
        msg = msg % msg_args
        writer('[%s]: %s\n' % (self, msg))
    def print_progress(self):
                if not self.verbose:
            return
        elapsed_time = time.time() - self._start_time
                                                if self._original_iterator is not None:
            if _verbosity_filter(self.n_dispatched_batches, self.verbose):
                return
            self._print('Done %3i tasks      | elapsed: %s',
                        (self.n_completed_tasks,
                         short_format_time(elapsed_time), ))
        else:
            index = self.n_completed_tasks
                        total_tasks = self.n_dispatched_tasks
                        if not index == 0:
                                                cursor = (total_tasks - index + 1 -
                          self._pre_dispatch_amount)
                frequency = (total_tasks // self.verbose) + 1
                is_last_item = (index + 1 == total_tasks)
                if (is_last_item or cursor % frequency):
                    return
            remaining_time = (elapsed_time / index) * \
                             (self.n_dispatched_tasks - index * 1.0)
                        self._print('Done %3i out of %3i | elapsed: %s remaining: %s',
                        (index,
                         total_tasks,
                         short_format_time(elapsed_time),
                         short_format_time(remaining_time),
                         ))
    def retrieve(self):
        self._output = list()
        while self._iterating or len(self._jobs) > 0:
            if len(self._jobs) == 0:
                                time.sleep(0.01)
                continue
                                                with self._lock:
                job = self._jobs.pop(0)
            try:
                if getattr(self._backend, 'supports_timeout', False):
                    self._output.extend(job.get(timeout=self.timeout))
                else:
                    self._output.extend(job.get())
            except BaseException as exception:
                                
                                self._aborting = True
                                                                                backend = self._backend
                if (backend is not None and
                        hasattr(backend, 'abort_everything')):
                                                                                ensure_ready = self._managed_backend
                    backend.abort_everything(ensure_ready=ensure_ready)
                if not isinstance(exception, TransportableException):
                    raise
                else:
                                                            this_report = format_outer_frames(context=10,
                                                      stack_start=1)
                    report = 
from mmap import mmap
import errno
import os
import stat
import sys
import threading
import atexit
import tempfile
import shutil
import warnings
from time import sleep
try:
    WindowsError
except NameError:
    WindowsError = type(None)
from pickle import whichmodule
try:
        from cPickle import loads
    from cPickle import dumps
except ImportError:
    from pickle import loads
    from pickle import dumps
    import copyreg
from pickle import Pickler
from pickle import HIGHEST_PROTOCOL
from io import BytesIO
from ._multiprocessing_helpers import mp, assert_spawning
from multiprocessing.pool import Pool
try:
    import numpy as np
    from numpy.lib.stride_tricks import as_strided
except ImportError:
    np = None
from .numpy_pickle import load
from .numpy_pickle import dump
from .hashing import hash
from .backports import make_memmap
SYSTEM_SHARED_MEM_FS = '/dev/shm'
FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR
FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR

def _get_backing_memmap(a):
        b = getattr(a, 'base', None)
    if b is None:
                        return None
    elif isinstance(b, mmap):
                return a
    else:
                return _get_backing_memmap(b)

def has_shareable_memory(a):
        return _get_backing_memmap(a) is not None

def _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,
                         total_buffer_len):
        if mode == 'w+':
                mode = 'r+'
    if strides is None:
                return make_memmap(filename, dtype=dtype, shape=shape, mode=mode,
                           offset=offset, order=order)
    else:
                        base = make_memmap(filename, dtype=dtype, shape=total_buffer_len,
                           mode=mode, offset=offset, order=order)
        return as_strided(base, shape=shape, strides=strides)

def _reduce_memmap_backed(a, m):
            a_start, a_end = np.byte_bounds(a)
    m_start = np.byte_bounds(m)[0]
    offset = a_start - m_start
        offset += m.offset
    if m.flags['F_CONTIGUOUS']:
        order = 'F'
    else:
                        order = 'C'
    if a.flags['F_CONTIGUOUS'] or a.flags['C_CONTIGUOUS']:
                strides = None
        total_buffer_len = None
    else:
                        strides = a.strides
        total_buffer_len = (a_end - a_start) // a.itemsize
    return (_strided_from_memmap,
            (m.filename, a.dtype, m.mode, offset, order, a.shape, strides,
             total_buffer_len))

def reduce_memmap(a):
        m = _get_backing_memmap(a)
    if m is not None:
                        return _reduce_memmap_backed(a, m)
    else:
                                return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))

class ArrayMemmapReducer(object):
    
    def __init__(self, max_nbytes, temp_folder, mmap_mode, verbose=0,
                 context_id=None, prewarm=True):
        self._max_nbytes = max_nbytes
        self._temp_folder = temp_folder
        self._mmap_mode = mmap_mode
        self.verbose = int(verbose)
        self._prewarm = prewarm
        if context_id is not None:
            warnings.warn('context_id is deprecated and ignored in joblib'
                          ' 0.9.4 and will be removed in 0.11',
                          DeprecationWarning)
    def __call__(self, a):
        m = _get_backing_memmap(a)
        if m is not None:
                        return _reduce_memmap_backed(a, m)
        if (not a.dtype.hasobject
                and self._max_nbytes is not None
                and a.nbytes > self._max_nbytes):
                                    try:
                os.makedirs(self._temp_folder)
                os.chmod(self._temp_folder, FOLDER_PERMISSIONS)
            except OSError as e:
                if e.errno != errno.EEXIST:
                    raise e
                                    basename = "%d-%d-%s.pkl" % (
                os.getpid(), id(threading.current_thread()), hash(a))
            filename = os.path.join(self._temp_folder, basename)
                        
                                                if not os.path.exists(filename):
                if self.verbose > 0:
                    print("Memmaping (shape=%r, dtype=%s) to new file %s" % (
                        a.shape, a.dtype, filename))
                for dumped_filename in dump(a, filename):
                    os.chmod(dumped_filename, FILE_PERMISSIONS)
                if self._prewarm:
                                                            load(filename, mmap_mode=self._mmap_mode).max()
            elif self.verbose > 1:
                print("Memmaping (shape=%s, dtype=%s) to old file %s" % (
                    a.shape, a.dtype, filename))
                        return (load, (filename, self._mmap_mode))
        else:
                                    if self.verbose > 1:
                print("Pickling array (shape=%r, dtype=%s)." % (
                    a.shape, a.dtype))
            return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))

class CustomizablePickler(Pickler):
    
                    
    def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):
        Pickler.__init__(self, writer, protocol=protocol)
        if reducers is None:
            reducers = {}
        if hasattr(Pickler, 'dispatch'):
                                    self.dispatch = Pickler.dispatch.copy()
        else:
                                    self.dispatch_table = copyreg.dispatch_table.copy()
        for type, reduce_func in reducers.items():
            self.register(type, reduce_func)
    def register(self, type, reduce_func):
                if hasattr(Pickler, 'dispatch'):
                                    def dispatcher(self, obj):
                reduced = reduce_func(obj)
                self.save_reduce(obj=obj, *reduced)
            self.dispatch[type] = dispatcher
        else:
            self.dispatch_table[type] = reduce_func

class CustomizablePicklingQueue(object):
    
    def __init__(self, context, reducers=None):
        self._reducers = reducers
        self._reader, self._writer = context.Pipe(duplex=False)
        self._rlock = context.Lock()
        if sys.platform == 'win32':
            self._wlock = None
        else:
            self._wlock = context.Lock()
        self._make_methods()
    def __getstate__(self):
        assert_spawning(self)
        return (self._reader, self._writer, self._rlock, self._wlock,
                self._reducers)
    def __setstate__(self, state):
        (self._reader, self._writer, self._rlock, self._wlock,
         self._reducers) = state
        self._make_methods()
    def empty(self):
        return not self._reader.poll()
    def _make_methods(self):
        self._recv = recv = self._reader.recv
        racquire, rrelease = self._rlock.acquire, self._rlock.release
        def get():
            racquire()
            try:
                return recv()
            finally:
                rrelease()
        self.get = get
        if self._reducers:
            def send(obj):
                buffer = BytesIO()
                CustomizablePickler(buffer, self._reducers).dump(obj)
                self._writer.send_bytes(buffer.getvalue())
            self._send = send
        else:
            self._send = send = self._writer.send
        if self._wlock is None:
                        self.put = send
        else:
            wlock_acquire, wlock_release = (
                self._wlock.acquire, self._wlock.release)
            def put(obj):
                wlock_acquire()
                try:
                    return send(obj)
                finally:
                    wlock_release()
            self.put = put

class PicklingPool(Pool):
    
    def __init__(self, processes=None, forward_reducers=None,
                 backward_reducers=None, **kwargs):
        if forward_reducers is None:
            forward_reducers = dict()
        if backward_reducers is None:
            backward_reducers = dict()
        self._forward_reducers = forward_reducers
        self._backward_reducers = backward_reducers
        poolargs = dict(processes=processes)
        poolargs.update(kwargs)
        super(PicklingPool, self).__init__(**poolargs)
    def _setup_queues(self):
        context = getattr(self, '_ctx', mp)
        self._inqueue = CustomizablePicklingQueue(context,
                                                  self._forward_reducers)
        self._outqueue = CustomizablePicklingQueue(context,
                                                   self._backward_reducers)
        self._quick_put = self._inqueue._send
        self._quick_get = self._outqueue._recv

def delete_folder(folder_path):
        try:
        if os.path.exists(folder_path):
            shutil.rmtree(folder_path)
    except WindowsError:
        warnings.warn("Failed to clean temporary folder: %s" % folder_path)

class MemmapingPool(PicklingPool):
    
    def __init__(self, processes=None, temp_folder=None, max_nbytes=1e6,
                 mmap_mode='r', forward_reducers=None, backward_reducers=None,
                 verbose=0, context_id=None, prewarm=False, **kwargs):
        if forward_reducers is None:
            forward_reducers = dict()
        if backward_reducers is None:
            backward_reducers = dict()
        if context_id is not None:
            warnings.warn('context_id is deprecated and ignored in joblib'
                          ' 0.9.4 and will be removed in 0.11',
                          DeprecationWarning)
                                use_shared_mem = False
        pool_folder_name = "joblib_memmaping_pool_%d_%d" % (
            os.getpid(), id(self))
        if temp_folder is None:
            temp_folder = os.environ.get('JOBLIB_TEMP_FOLDER', None)
        if temp_folder is None:
            if os.path.exists(SYSTEM_SHARED_MEM_FS):
                try:
                    temp_folder = SYSTEM_SHARED_MEM_FS
                    pool_folder = os.path.join(temp_folder, pool_folder_name)
                    if not os.path.exists(pool_folder):
                        os.makedirs(pool_folder)
                    use_shared_mem = True
                except IOError:
                                                            temp_folder = None
        if temp_folder is None:
                        temp_folder = tempfile.gettempdir()
        temp_folder = os.path.abspath(os.path.expanduser(temp_folder))
        pool_folder = os.path.join(temp_folder, pool_folder_name)
        self._temp_folder = pool_folder
                                                pool_module_name = whichmodule(delete_folder, 'delete_folder')
        def _cleanup():
                                                                                                            delete_folder = __import__(
                pool_module_name, fromlist=['delete_folder']).delete_folder
            delete_folder(pool_folder)
        atexit.register(_cleanup)
        if np is not None:
                                                if prewarm == "auto":
                prewarm = not use_shared_mem
            forward_reduce_ndarray = ArrayMemmapReducer(
                max_nbytes, pool_folder, mmap_mode, verbose,
                prewarm=prewarm)
            forward_reducers[np.ndarray] = forward_reduce_ndarray
            forward_reducers[np.memmap] = reduce_memmap
                                                            backward_reduce_ndarray = ArrayMemmapReducer(
                None, pool_folder, mmap_mode, verbose)
            backward_reducers[np.ndarray] = backward_reduce_ndarray
            backward_reducers[np.memmap] = reduce_memmap
        poolargs = dict(
            processes=processes,
            forward_reducers=forward_reducers,
            backward_reducers=backward_reducers)
        poolargs.update(kwargs)
        super(MemmapingPool, self).__init__(**poolargs)
    def terminate(self):
        n_retries = 10
        for i in range(n_retries):
            try:
                super(MemmapingPool, self).terminate()
                break
            except OSError as e:
                if isinstance(e, WindowsError):
                                                            sleep(0.1)
                    if i + 1 == n_retries:
                        warnings.warn("Failed to terminate worker processes in"
                                      " multiprocessing pool: %r" % e)
        delete_folder(self._temp_folder)
import sys
PY3_OR_LATER = sys.version_info[0] >= 3
PY27 = sys.version_info[:2] == (2, 7)
try:
    _basestring = basestring
    _bytes_or_unicode = (str, unicode)
except NameError:
    _basestring = str
    _bytes_or_unicode = (bytes, str)

def with_metaclass(meta, *bases):
        return meta("NewBase", bases, {})
try:
        from tokenize import open as open_py_source
except ImportError:
        from codecs import lookup, BOM_UTF8
    import re
    from io import TextIOWrapper, open
    cookie_re = re.compile("coding[:=]\s*([-\w.]+)")
    def _get_normal_name(orig_enc):
                        enc = orig_enc[:12].lower().replace("_", "-")
        if enc == "utf-8" or enc.startswith("utf-8-"):
            return "utf-8"
        if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
           enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
            return "iso-8859-1"
        return orig_enc
    def _detect_encoding(readline):
                bom_found = False
        encoding = None
        default = 'utf-8'
        def read_or_stop():
            try:
                return readline()
            except StopIteration:
                return b''
        def find_cookie(line):
            try:
                line_string = line.decode('ascii')
            except UnicodeDecodeError:
                return None
            matches = cookie_re.findall(line_string)
            if not matches:
                return None
            encoding = _get_normal_name(matches[0])
            try:
                codec = lookup(encoding)
            except LookupError:
                                raise SyntaxError("unknown encoding: " + encoding)
            if bom_found:
                if codec.name != 'utf-8':
                                        raise SyntaxError('encoding problem: utf-8')
                encoding += '-sig'
            return encoding
        first = read_or_stop()
        if first.startswith(BOM_UTF8):
            bom_found = True
            first = first[3:]
            default = 'utf-8-sig'
        if not first:
            return default, []
        encoding = find_cookie(first)
        if encoding:
            return encoding, [first]
        second = read_or_stop()
        if not second:
            return default, [first]
        encoding = find_cookie(second)
        if encoding:
            return encoding, [first, second]
        return default, [first, second]
    def open_py_source(filename):
                buffer = open(filename, 'rb')
        encoding, lines = _detect_encoding(buffer.readline)
        buffer.seek(0)
        text = TextIOWrapper(buffer, encoding, line_buffering=True)
        text.mode = 'r'
        return text
import os
import warnings

mp = int(os.environ.get('JOBLIB_MULTIPROCESSING', 1)) or None
if mp:
    try:
        import multiprocessing as mp
    except ImportError:
        mp = None
if mp is not None:
    try:
        _sem = mp.Semaphore()
        del _sem      except (ImportError, OSError) as e:
        mp = None
        warnings.warn('%s.  joblib will operate in serial mode' % (e,))

if mp is not None:
    try:
                from multiprocessing.context import assert_spawning
    except ImportError:
        from multiprocessing.forking import assert_spawning
else:
    assert_spawning = None
import gc
import os
import sys
import warnings
import threading
from abc import ABCMeta, abstractmethod
from .format_stack import format_exc
from .my_exceptions import WorkerInterrupt, TransportableException
from ._multiprocessing_helpers import mp
from ._compat import with_metaclass
if mp is not None:
    from .pool import MemmapingPool
    from multiprocessing.pool import ThreadPool

class ParallelBackendBase(with_metaclass(ABCMeta)):
    
    supports_timeout = False
    @abstractmethod
    def effective_n_jobs(self, n_jobs):
        
    @abstractmethod
    def apply_async(self, func, callback=None):
        
    def configure(self, n_jobs=1, parallel=None, **backend_args):
                self.parallel = parallel
        return self.effective_n_jobs(n_jobs)
    def terminate(self):
        
    def compute_batch_size(self):
                return 1
    def batch_completed(self, batch_size, duration):
        
    def get_exceptions(self):
                return []
    def abort_everything(self, ensure_ready=True):
                                pass

class SequentialBackend(ParallelBackendBase):
    
    def effective_n_jobs(self, n_jobs):
                if n_jobs == 0:
            raise ValueError('n_jobs == 0 in Parallel has no meaning')
        return 1
    def apply_async(self, func, callback=None):
                result = ImmediateResult(func)
        if callback:
            callback(result)
        return result

class PoolManagerMixin(object):
    
    def effective_n_jobs(self, n_jobs):
                if n_jobs == 0:
            raise ValueError('n_jobs == 0 in Parallel has no meaning')
        elif mp is None or n_jobs is None:
                                    return 1
        elif n_jobs < 0:
            n_jobs = max(mp.cpu_count() + 1 + n_jobs, 1)
        return n_jobs
    def terminate(self):
                if self._pool is not None:
            self._pool.close()
            self._pool.terminate()              self._pool = None
    def apply_async(self, func, callback=None):
                return self._pool.apply_async(SafeFunction(func), callback=callback)
    def abort_everything(self, ensure_ready=True):
                self.terminate()
        if ensure_ready:
            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,
                           **self.parallel._backend_args)

class AutoBatchingMixin(object):
    
                    MIN_IDEAL_BATCH_DURATION = .2
            MAX_IDEAL_BATCH_DURATION = 2
        _effective_batch_size = 1
    _smoothed_batch_duration = 0.0
    def compute_batch_size(self):
                old_batch_size = self._effective_batch_size
        batch_duration = self._smoothed_batch_duration
        if (batch_duration > 0 and
                batch_duration < self.MIN_IDEAL_BATCH_DURATION):
                                                ideal_batch_size = int(old_batch_size *
                                   self.MIN_IDEAL_BATCH_DURATION /
                                   batch_duration)
                        batch_size = max(2 * ideal_batch_size, 1)
            self._effective_batch_size = batch_size
            if self.parallel.verbose >= 10:
                self.parallel._print(
                    "Batch computation too fast (%.4fs.) "
                    "Setting batch_size=%d.", (batch_duration, batch_size))
        elif (batch_duration > self.MAX_IDEAL_BATCH_DURATION and
              old_batch_size >= 2):
                                                                        batch_size = old_batch_size // 2
            self._effective_batch_size = batch_size
            if self.parallel.verbose >= 10:
                self.parallel._print(
                    "Batch computation too slow (%.4fs.) "
                    "Setting batch_size=%d.", (batch_duration, batch_size))
        else:
                        batch_size = old_batch_size
        if batch_size != old_batch_size:
                                                                        self._smoothed_batch_duration = 0
        return batch_size
    def batch_completed(self, batch_size, duration):
                if batch_size == self._effective_batch_size:
                                    old_duration = self._smoothed_batch_duration
            if old_duration == 0:
                                                new_duration = duration
            else:
                                                new_duration = 0.8 * old_duration + 0.2 * duration
            self._smoothed_batch_duration = new_duration

class ThreadingBackend(PoolManagerMixin, ParallelBackendBase):
    
    supports_timeout = True
    def configure(self, n_jobs=1, parallel=None, **backend_args):
                n_jobs = self.effective_n_jobs(n_jobs)
        if n_jobs == 1:
                        raise FallbackToBackend(SequentialBackend())
        self.parallel = parallel
        self._pool = ThreadPool(n_jobs)
        return n_jobs

class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,
                             ParallelBackendBase):
    
        JOBLIB_SPAWNED_PROCESS = "__JOBLIB_SPAWNED_PARALLEL__"
    supports_timeout = True
    def effective_n_jobs(self, n_jobs):
                if mp is None:
            return 1
        if mp.current_process().daemon:
                        if n_jobs != 1:
                warnings.warn(
                    'Multiprocessing-backed parallel loops cannot be nested,'
                    ' setting n_jobs=1',
                    stacklevel=3)
            return 1
        if not isinstance(threading.current_thread(), threading._MainThread):
                        warnings.warn(
                'Multiprocessing-backed parallel loops cannot be nested'
                ' below threads, setting n_jobs=1',
                stacklevel=3)
            return 1
        return super(MultiprocessingBackend, self).effective_n_jobs(n_jobs)
    def configure(self, n_jobs=1, parallel=None, **backend_args):
                n_jobs = self.effective_n_jobs(n_jobs)
        if n_jobs == 1:
            raise FallbackToBackend(SequentialBackend())
        already_forked = int(os.environ.get(self.JOBLIB_SPAWNED_PROCESS, 0))
        if already_forked:
            raise ImportError(
                '[joblib] Attempting to do parallel computing '
                'without protecting your import on a system that does '
                'not support forking. To use parallel-computing in a '
                'script, you must protect your main loop using "if '
                "__name__ == '__main__'"
                '". Please see the joblib documentation on Parallel '
                'for more information')
                os.environ[self.JOBLIB_SPAWNED_PROCESS] = '1'
                gc.collect()
        self._pool = MemmapingPool(n_jobs, **backend_args)
        self.parallel = parallel
        return n_jobs
    def terminate(self):
                super(MultiprocessingBackend, self).terminate()
        if self.JOBLIB_SPAWNED_PROCESS in os.environ:
            del os.environ[self.JOBLIB_SPAWNED_PROCESS]

class ImmediateResult(object):
    def __init__(self, batch):
                        self.results = batch()
    def get(self):
        return self.results

class SafeFunction(object):
        def __init__(self, func):
        self.func = func
    def __call__(self, *args, **kwargs):
        try:
            return self.func(*args, **kwargs)
        except KeyboardInterrupt:
                                                raise WorkerInterrupt()
        except:
            e_type, e_value, e_tb = sys.exc_info()
            text = format_exc(e_type, e_value, e_tb, context=10, tb_offset=1)
            raise TransportableException(text, e_type)

class FallbackToBackend(Exception):
    
    def __init__(self, backend):
        self.backend = backend
__version__ = '0.11'

from .memory import Memory, MemorizedResult
from .logger import PrintTime
from .logger import Logger
from .hashing import hash
from .numpy_pickle import dump
from .numpy_pickle import load
from .parallel import Parallel
from .parallel import delayed
from .parallel import cpu_count
from .parallel import register_parallel_backend
from .parallel import parallel_backend
from .parallel import effective_n_jobs

__all__ = ['Memory', 'MemorizedResult', 'PrintTime', 'Logger', 'hash', 'dump',
           'load', 'Parallel', 'delayed', 'cpu_count', 'effective_n_jobs',
           'register_parallel_backend', 'parallel_backend']
from array import array
from collections import Mapping
from operator import itemgetter
import numpy as np
import scipy.sparse as sp
from ..base import BaseEstimator, TransformerMixin
from ..externals import six
from ..externals.six.moves import xrange
from ..utils import check_array, tosequence
from ..utils.fixes import frombuffer_empty

def _tosequence(X):
        if isinstance(X, Mapping):          return [X]
    else:
        return tosequence(X)

class DictVectorizer(BaseEstimator, TransformerMixin):
    
    def __init__(self, dtype=np.float64, separator="=", sparse=True,
                 sort=True):
        self.dtype = dtype
        self.separator = separator
        self.sparse = sparse
        self.sort = sort
    def fit(self, X, y=None):
                feature_names = []
        vocab = {}
        for x in X:
            for f, v in six.iteritems(x):
                if isinstance(v, six.string_types):
                    f = "%s%s%s" % (f, self.separator, v)
                if f not in vocab:
                    feature_names.append(f)
                    vocab[f] = len(vocab)
        if self.sort:
            feature_names.sort()
            vocab = dict((f, i) for i, f in enumerate(feature_names))
        self.feature_names_ = feature_names
        self.vocabulary_ = vocab
        return self
    def _transform(self, X, fitting):
                                        assert array("i").itemsize == 4, (
            "sizeof(int) != 4 on your platform; please report this at"
            " https://github.com/scikit-learn/scikit-learn/issues and"
            " include the output from platform.platform() in your bug report")
        dtype = self.dtype
        if fitting:
            feature_names = []
            vocab = {}
        else:
            feature_names = self.feature_names_
            vocab = self.vocabulary_
                X = [X] if isinstance(X, Mapping) else X
        indices = array("i")
        indptr = array("i", [0])
                        values = []
                        for x in X:
            for f, v in six.iteritems(x):
                if isinstance(v, six.string_types):
                    f = "%s%s%s" % (f, self.separator, v)
                    v = 1
                if f in vocab:
                    indices.append(vocab[f])
                    values.append(dtype(v))
                else:
                    if fitting:
                        feature_names.append(f)
                        vocab[f] = len(vocab)
                        indices.append(vocab[f])
                        values.append(dtype(v))
            indptr.append(len(indices))
        if len(indptr) == 1:
            raise ValueError("Sample sequence X is empty.")
        indices = frombuffer_empty(indices, dtype=np.intc)
        indptr = np.frombuffer(indptr, dtype=np.intc)
        shape = (len(indptr) - 1, len(vocab))
        result_matrix = sp.csr_matrix((values, indices, indptr),
                                      shape=shape, dtype=dtype)
                if fitting and self.sort:
            feature_names.sort()
            map_index = np.empty(len(feature_names), dtype=np.int32)
            for new_val, f in enumerate(feature_names):
                map_index[new_val] = vocab[f]
                vocab[f] = new_val
            result_matrix = result_matrix[:, map_index]
        if self.sparse:
            result_matrix.sort_indices()
        else:
            result_matrix = result_matrix.toarray()
        if fitting:
            self.feature_names_ = feature_names
            self.vocabulary_ = vocab
        return result_matrix
    def fit_transform(self, X, y=None):
                return self._transform(X, fitting=True)
    def inverse_transform(self, X, dict_type=dict):
                        X = check_array(X, accept_sparse=['csr', 'csc'])
        n_samples = X.shape[0]
        names = self.feature_names_
        dicts = [dict_type() for _ in xrange(n_samples)]
        if sp.issparse(X):
            for i, j in zip(*X.nonzero()):
                dicts[i][names[j]] = X[i, j]
        else:
            for i, d in enumerate(dicts):
                for j, v in enumerate(X[i, :]):
                    if v != 0:
                        d[names[j]] = X[i, j]
        return dicts
    def transform(self, X, y=None):
                if self.sparse:
            return self._transform(X, fitting=False)
        else:
            dtype = self.dtype
            vocab = self.vocabulary_
            X = _tosequence(X)
            Xa = np.zeros((len(X), len(vocab)), dtype=dtype)
            for i, x in enumerate(X):
                for f, v in six.iteritems(x):
                    if isinstance(v, six.string_types):
                        f = "%s%s%s" % (f, self.separator, v)
                        v = 1
                    try:
                        Xa[i, vocab[f]] = dtype(v)
                    except KeyError:
                        pass
            return Xa
    def get_feature_names(self):
                return self.feature_names_
    def restrict(self, support, indices=False):
                if not indices:
            support = np.where(support)[0]
        names = self.feature_names_
        new_vocab = {}
        for i in support:
            new_vocab[names[i]] = len(new_vocab)
        self.vocabulary_ = new_vocab
        self.feature_names_ = [f for f, i in sorted(six.iteritems(new_vocab),
                                                    key=itemgetter(1))]
        return self
import numbers
import numpy as np
import scipy.sparse as sp
from . import _hashing
from ..base import BaseEstimator, TransformerMixin

def _iteritems(d):
        return d.iteritems() if hasattr(d, "iteritems") else d.items()

class FeatureHasher(BaseEstimator, TransformerMixin):
    
    def __init__(self, n_features=(2 ** 20), input_type="dict",
                 dtype=np.float64, non_negative=False):
        self._validate_params(n_features, input_type)
        self.dtype = dtype
        self.input_type = input_type
        self.n_features = n_features
        self.non_negative = non_negative
    @staticmethod
    def _validate_params(n_features, input_type):
                        if not isinstance(n_features, (numbers.Integral, np.integer)):
            raise TypeError("n_features must be integral, got %r (%s)."
                            % (n_features, type(n_features)))
        elif n_features < 1 or n_features >= 2 ** 31:
            raise ValueError("Invalid number of features (%d)." % n_features)
        if input_type not in ("dict", "pair", "string"):
            raise ValueError("input_type must be 'dict', 'pair' or 'string',"
                             " got %r." % input_type)
    def fit(self, X=None, y=None):
                        self._validate_params(self.n_features, self.input_type)
        return self
    def transform(self, raw_X, y=None):
                raw_X = iter(raw_X)
        if self.input_type == "dict":
            raw_X = (_iteritems(d) for d in raw_X)
        elif self.input_type == "string":
            raw_X = (((f, 1) for f in x) for x in raw_X)
        indices, indptr, values = \
            _hashing.transform(raw_X, self.n_features, self.dtype)
        n_samples = indptr.shape[0] - 1
        if n_samples == 0:
            raise ValueError("Cannot vectorize empty sequence.")
        X = sp.csr_matrix((values, indices, indptr), dtype=self.dtype,
                          shape=(n_samples, self.n_features))
        X.sum_duplicates()          if self.non_negative:
            np.abs(X.data, X.data)
        return X

from itertools import product
import numbers
import numpy as np
from scipy import sparse
from numpy.lib.stride_tricks import as_strided
from ..utils import check_array, check_random_state
from ..utils.fixes import astype
from ..base import BaseEstimator
__all__ = ['PatchExtractor',
           'extract_patches_2d',
           'grid_to_graph',
           'img_to_graph',
           'reconstruct_from_patches_2d']

def _make_edges_3d(n_x, n_y, n_z=1):
        vertices = np.arange(n_x * n_y * n_z).reshape((n_x, n_y, n_z))
    edges_deep = np.vstack((vertices[:, :, :-1].ravel(),
                            vertices[:, :, 1:].ravel()))
    edges_right = np.vstack((vertices[:, :-1].ravel(),
                             vertices[:, 1:].ravel()))
    edges_down = np.vstack((vertices[:-1].ravel(), vertices[1:].ravel()))
    edges = np.hstack((edges_deep, edges_right, edges_down))
    return edges

def _compute_gradient_3d(edges, img):
    n_x, n_y, n_z = img.shape
    gradient = np.abs(img[edges[0] // (n_y * n_z),
                      (edges[0] % (n_y * n_z)) // n_z,
                      (edges[0] % (n_y * n_z)) % n_z] -
                      img[edges[1] // (n_y * n_z),
                      (edges[1] % (n_y * n_z)) // n_z,
                      (edges[1] % (n_y * n_z)) % n_z])
    return gradient

def _mask_edges_weights(mask, edges, weights=None):
        inds = np.arange(mask.size)
    inds = inds[mask.ravel()]
    ind_mask = np.logical_and(np.in1d(edges[0], inds),
                              np.in1d(edges[1], inds))
    edges = edges[:, ind_mask]
    if weights is not None:
        weights = weights[ind_mask]
    if len(edges.ravel()):
        maxval = edges.max()
    else:
        maxval = 0
    order = np.searchsorted(np.unique(edges.ravel()), np.arange(maxval + 1))
    edges = order[edges]
    if weights is None:
        return edges
    else:
        return edges, weights

def _to_graph(n_x, n_y, n_z, mask=None, img=None,
              return_as=sparse.coo_matrix, dtype=None):
        edges = _make_edges_3d(n_x, n_y, n_z)
    if dtype is None:
        if img is None:
            dtype = np.int
        else:
            dtype = img.dtype
    if img is not None:
        img = np.atleast_3d(img)
        weights = _compute_gradient_3d(edges, img)
        if mask is not None:
            edges, weights = _mask_edges_weights(mask, edges, weights)
            diag = img.squeeze()[mask]
        else:
            diag = img.ravel()
        n_voxels = diag.size
    else:
        if mask is not None:
            mask = astype(mask, dtype=np.bool, copy=False)
            mask = np.asarray(mask, dtype=np.bool)
            edges = _mask_edges_weights(mask, edges)
            n_voxels = np.sum(mask)
        else:
            n_voxels = n_x * n_y * n_z
        weights = np.ones(edges.shape[1], dtype=dtype)
        diag = np.ones(n_voxels, dtype=dtype)
    diag_idx = np.arange(n_voxels)
    i_idx = np.hstack((edges[0], edges[1]))
    j_idx = np.hstack((edges[1], edges[0]))
    graph = sparse.coo_matrix((np.hstack((weights, weights, diag)),
                              (np.hstack((i_idx, diag_idx)),
                               np.hstack((j_idx, diag_idx)))),
                              (n_voxels, n_voxels),
                              dtype=dtype)
    if return_as is np.ndarray:
        return graph.toarray()
    return return_as(graph)

def img_to_graph(img, mask=None, return_as=sparse.coo_matrix, dtype=None):
        img = np.atleast_3d(img)
    n_x, n_y, n_z = img.shape
    return _to_graph(n_x, n_y, n_z, mask, img, return_as, dtype)

def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,
                  dtype=np.int):
        return _to_graph(n_x, n_y, n_z, mask=mask, return_as=return_as,
                     dtype=dtype)

def _compute_n_patches(i_h, i_w, p_h, p_w, max_patches=None):
        n_h = i_h - p_h + 1
    n_w = i_w - p_w + 1
    all_patches = n_h * n_w
    if max_patches:
        if (isinstance(max_patches, (numbers.Integral))
                and max_patches < all_patches):
            return max_patches
        elif (isinstance(max_patches, (numbers.Real))
                and 0 < max_patches < 1):
            return int(max_patches * all_patches)
        else:
            raise ValueError("Invalid value for max_patches: %r" % max_patches)
    else:
        return all_patches

def extract_patches(arr, patch_shape=8, extraction_step=1):
    
    arr_ndim = arr.ndim
    if isinstance(patch_shape, numbers.Number):
        patch_shape = tuple([patch_shape] * arr_ndim)
    if isinstance(extraction_step, numbers.Number):
        extraction_step = tuple([extraction_step] * arr_ndim)
    patch_strides = arr.strides
    slices = [slice(None, None, st) for st in extraction_step]
    indexing_strides = arr[slices].strides
    patch_indices_shape = ((np.array(arr.shape) - np.array(patch_shape)) //
                           np.array(extraction_step)) + 1
    shape = tuple(list(patch_indices_shape) + list(patch_shape))
    strides = tuple(list(indexing_strides) + list(patch_strides))
    patches = as_strided(arr, shape=shape, strides=strides)
    return patches

def extract_patches_2d(image, patch_size, max_patches=None, random_state=None):
        i_h, i_w = image.shape[:2]
    p_h, p_w = patch_size
    if p_h > i_h:
        raise ValueError("Height of the patch should be less than the height"
                         " of the image.")
    if p_w > i_w:
        raise ValueError("Width of the patch should be less than the width"
                         " of the image.")
    image = check_array(image, allow_nd=True)
    image = image.reshape((i_h, i_w, -1))
    n_colors = image.shape[-1]
    extracted_patches = extract_patches(image,
                                        patch_shape=(p_h, p_w, n_colors),
                                        extraction_step=1)
    n_patches = _compute_n_patches(i_h, i_w, p_h, p_w, max_patches)
    if max_patches:
        rng = check_random_state(random_state)
        i_s = rng.randint(i_h - p_h + 1, size=n_patches)
        j_s = rng.randint(i_w - p_w + 1, size=n_patches)
        patches = extracted_patches[i_s, j_s, 0]
    else:
        patches = extracted_patches
    patches = patches.reshape(-1, p_h, p_w, n_colors)
        if patches.shape[-1] == 1:
        return patches.reshape((n_patches, p_h, p_w))
    else:
        return patches

def reconstruct_from_patches_2d(patches, image_size):
        i_h, i_w = image_size[:2]
    p_h, p_w = patches.shape[1:3]
    img = np.zeros(image_size)
        n_h = i_h - p_h + 1
    n_w = i_w - p_w + 1
    for p, (i, j) in zip(patches, product(range(n_h), range(n_w))):
        img[i:i + p_h, j:j + p_w] += p
    for i in range(i_h):
        for j in range(i_w):
                                    img[i, j] /= float(min(i + 1, p_h, i_h - i) *
                               min(j + 1, p_w, i_w - j))
    return img

class PatchExtractor(BaseEstimator):
        def __init__(self, patch_size=None, max_patches=None, random_state=None):
        self.patch_size = patch_size
        self.max_patches = max_patches
        self.random_state = random_state
    def fit(self, X, y=None):
                return self
    def transform(self, X):
                self.random_state = check_random_state(self.random_state)
        n_images, i_h, i_w = X.shape[:3]
        X = np.reshape(X, (n_images, i_h, i_w, -1))
        n_channels = X.shape[-1]
        if self.patch_size is None:
            patch_size = i_h // 10, i_w // 10
        else:
            patch_size = self.patch_size
                p_h, p_w = patch_size
        n_patches = _compute_n_patches(i_h, i_w, p_h, p_w, self.max_patches)
        patches_shape = (n_images * n_patches,) + patch_size
        if n_channels > 1:
            patches_shape += (n_channels,)
                patches = np.empty(patches_shape)
        for ii, image in enumerate(X):
            patches[ii * n_patches:(ii + 1) * n_patches] = extract_patches_2d(
                image, patch_size, self.max_patches, self.random_state)
        return patches
import os

def configuration(parent_package='', top_path=None):
    import numpy
    from numpy.distutils.misc_util import Configuration
    config = Configuration('feature_extraction', parent_package, top_path)
    libraries = []
    if os.name == 'posix':
        libraries.append('m')
    config.add_extension('_hashing',
                         sources=['_hashing.pyx'],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_subpackage("tests")
    return config
ENGLISH_STOP_WORDS = frozenset([
    "a", "about", "above", "across", "after", "afterwards", "again", "against",
    "all", "almost", "alone", "along", "already", "also", "although", "always",
    "am", "among", "amongst", "amoungst", "amount", "an", "and", "another",
    "any", "anyhow", "anyone", "anything", "anyway", "anywhere", "are",
    "around", "as", "at", "back", "be", "became", "because", "become",
    "becomes", "becoming", "been", "before", "beforehand", "behind", "being",
    "below", "beside", "besides", "between", "beyond", "bill", "both",
    "bottom", "but", "by", "call", "can", "cannot", "cant", "co", "con",
    "could", "couldnt", "cry", "de", "describe", "detail", "do", "done",
    "down", "due", "during", "each", "eg", "eight", "either", "eleven", "else",
    "elsewhere", "empty", "enough", "etc", "even", "ever", "every", "everyone",
    "everything", "everywhere", "except", "few", "fifteen", "fifty", "fill",
    "find", "fire", "first", "five", "for", "former", "formerly", "forty",
    "found", "four", "from", "front", "full", "further", "get", "give", "go",
    "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter",
    "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his",
    "how", "however", "hundred", "i", "ie", "if", "in", "inc", "indeed",
    "interest", "into", "is", "it", "its", "itself", "keep", "last", "latter",
    "latterly", "least", "less", "ltd", "made", "many", "may", "me",
    "meanwhile", "might", "mill", "mine", "more", "moreover", "most", "mostly",
    "move", "much", "must", "my", "myself", "name", "namely", "neither",
    "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone",
    "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on",
    "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our",
    "ours", "ourselves", "out", "over", "own", "part", "per", "perhaps",
    "please", "put", "rather", "re", "same", "see", "seem", "seemed",
    "seeming", "seems", "serious", "several", "she", "should", "show", "side",
    "since", "sincere", "six", "sixty", "so", "some", "somehow", "someone",
    "something", "sometime", "sometimes", "somewhere", "still", "such",
    "system", "take", "ten", "than", "that", "the", "their", "them",
    "themselves", "then", "thence", "there", "thereafter", "thereby",
    "therefore", "therein", "thereupon", "these", "they", "thick", "thin",
    "third", "this", "those", "though", "three", "through", "throughout",
    "thru", "thus", "to", "together", "too", "top", "toward", "towards",
    "twelve", "twenty", "two", "un", "under", "until", "up", "upon", "us",
    "very", "via", "was", "we", "well", "were", "what", "whatever", "when",
    "whence", "whenever", "where", "whereafter", "whereas", "whereby",
    "wherein", "whereupon", "wherever", "whether", "which", "while", "whither",
    "who", "whoever", "whole", "whom", "whose", "why", "will", "with",
    "within", "without", "would", "yet", "you", "your", "yours", "yourself",
    "yourselves"])
from __future__ import unicode_literals
import array
from collections import Mapping, defaultdict
import numbers
from operator import itemgetter
import re
import unicodedata
import numpy as np
import scipy.sparse as sp
from ..base import BaseEstimator, TransformerMixin
from ..externals import six
from ..externals.six.moves import xrange
from ..preprocessing import normalize
from .hashing import FeatureHasher
from .stop_words import ENGLISH_STOP_WORDS
from ..utils import deprecated
from ..utils.fixes import frombuffer_empty, bincount
from ..utils.validation import check_is_fitted
__all__ = ['CountVectorizer',
           'ENGLISH_STOP_WORDS',
           'TfidfTransformer',
           'TfidfVectorizer',
           'strip_accents_ascii',
           'strip_accents_unicode',
           'strip_tags']

def strip_accents_unicode(s):
        normalized = unicodedata.normalize('NFKD', s)
    if normalized == s:
        return s
    else:
        return ''.join([c for c in normalized if not unicodedata.combining(c)])

def strip_accents_ascii(s):
        nkfd_form = unicodedata.normalize('NFKD', s)
    return nkfd_form.encode('ASCII', 'ignore').decode('ASCII')

def strip_tags(s):
        return re.compile(r"<([^>]+)>", flags=re.UNICODE).sub(" ", s)

def _check_stop_list(stop):
    if stop == "english":
        return ENGLISH_STOP_WORDS
    elif isinstance(stop, six.string_types):
        raise ValueError("not a built-in stop list: %s" % stop)
    elif stop is None:
        return None
    else:                       return frozenset(stop)

class VectorizerMixin(object):
    
    _white_spaces = re.compile(r"\s\s+")
    def decode(self, doc):
                if self.input == 'filename':
            with open(doc, 'rb') as fh:
                doc = fh.read()
        elif self.input == 'file':
            doc = doc.read()
        if isinstance(doc, bytes):
            doc = doc.decode(self.encoding, self.decode_error)
        if doc is np.nan:
            raise ValueError("np.nan is an invalid document, expected byte or "
                             "unicode string.")
        return doc
    def _word_ngrams(self, tokens, stop_words=None):
                        if stop_words is not None:
            tokens = [w for w in tokens if w not in stop_words]
                min_n, max_n = self.ngram_range
        if max_n != 1:
            original_tokens = tokens
            tokens = []
            n_original_tokens = len(original_tokens)
            for n in xrange(min_n,
                            min(max_n + 1, n_original_tokens + 1)):
                for i in xrange(n_original_tokens - n + 1):
                    tokens.append(" ".join(original_tokens[i: i + n]))
        return tokens
    def _char_ngrams(self, text_document):
                        text_document = self._white_spaces.sub(" ", text_document)
        text_len = len(text_document)
        ngrams = []
        min_n, max_n = self.ngram_range
        for n in xrange(min_n, min(max_n + 1, text_len + 1)):
            for i in xrange(text_len - n + 1):
                ngrams.append(text_document[i: i + n])
        return ngrams
    def _char_wb_ngrams(self, text_document):
                        text_document = self._white_spaces.sub(" ", text_document)
        min_n, max_n = self.ngram_range
        ngrams = []
        for w in text_document.split():
            w = ' ' + w + ' '
            w_len = len(w)
            for n in xrange(min_n, max_n + 1):
                offset = 0
                ngrams.append(w[offset:offset + n])
                while offset + n < w_len:
                    offset += 1
                    ngrams.append(w[offset:offset + n])
                if offset == 0:                       break
        return ngrams
    def build_preprocessor(self):
                if self.preprocessor is not None:
            return self.preprocessor
                                                noop = lambda x: x
                if not self.strip_accents:
            strip_accents = noop
        elif callable(self.strip_accents):
            strip_accents = self.strip_accents
        elif self.strip_accents == 'ascii':
            strip_accents = strip_accents_ascii
        elif self.strip_accents == 'unicode':
            strip_accents = strip_accents_unicode
        else:
            raise ValueError('Invalid value for "strip_accents": %s' %
                             self.strip_accents)
        if self.lowercase:
            return lambda x: strip_accents(x.lower())
        else:
            return strip_accents
    def build_tokenizer(self):
                if self.tokenizer is not None:
            return self.tokenizer
        token_pattern = re.compile(self.token_pattern)
        return lambda doc: token_pattern.findall(doc)
    def get_stop_words(self):
                return _check_stop_list(self.stop_words)
    def build_analyzer(self):
                if callable(self.analyzer):
            return self.analyzer
        preprocess = self.build_preprocessor()
        if self.analyzer == 'char':
            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))
        elif self.analyzer == 'char_wb':
            return lambda doc: self._char_wb_ngrams(
                preprocess(self.decode(doc)))
        elif self.analyzer == 'word':
            stop_words = self.get_stop_words()
            tokenize = self.build_tokenizer()
            return lambda doc: self._word_ngrams(
                tokenize(preprocess(self.decode(doc))), stop_words)
        else:
            raise ValueError('%s is not a valid tokenization scheme/analyzer' %
                             self.analyzer)
    def _validate_vocabulary(self):
        vocabulary = self.vocabulary
        if vocabulary is not None:
            if isinstance(vocabulary, set):
                vocabulary = sorted(vocabulary)
            if not isinstance(vocabulary, Mapping):
                vocab = {}
                for i, t in enumerate(vocabulary):
                    if vocab.setdefault(t, i) != i:
                        msg = "Duplicate term in vocabulary: %r" % t
                        raise ValueError(msg)
                vocabulary = vocab
            else:
                indices = set(six.itervalues(vocabulary))
                if len(indices) != len(vocabulary):
                    raise ValueError("Vocabulary contains repeated indices.")
                for i in xrange(len(vocabulary)):
                    if i not in indices:
                        msg = ("Vocabulary of size %d doesn't contain index "
                               "%d." % (len(vocabulary), i))
                        raise ValueError(msg)
            if not vocabulary:
                raise ValueError("empty vocabulary passed to fit")
            self.fixed_vocabulary_ = True
            self.vocabulary_ = dict(vocabulary)
        else:
            self.fixed_vocabulary_ = False
    def _check_vocabulary(self):
                msg = "%(name)s - Vocabulary wasn't fitted."
        check_is_fitted(self, 'vocabulary_', msg=msg),
        if len(self.vocabulary_) == 0:
            raise ValueError("Vocabulary is empty")

class HashingVectorizer(BaseEstimator, VectorizerMixin):
        def __init__(self, input='content', encoding='utf-8',
                 decode_error='strict', strip_accents=None,
                 lowercase=True, preprocessor=None, tokenizer=None,
                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
                 ngram_range=(1, 1), analyzer='word', n_features=(2 ** 20),
                 binary=False, norm='l2', non_negative=False,
                 dtype=np.float64):
        self.input = input
        self.encoding = encoding
        self.decode_error = decode_error
        self.strip_accents = strip_accents
        self.preprocessor = preprocessor
        self.tokenizer = tokenizer
        self.analyzer = analyzer
        self.lowercase = lowercase
        self.token_pattern = token_pattern
        self.stop_words = stop_words
        self.n_features = n_features
        self.ngram_range = ngram_range
        self.binary = binary
        self.norm = norm
        self.non_negative = non_negative
        self.dtype = dtype
    def partial_fit(self, X, y=None):
                return self
    def fit(self, X, y=None):
                        if isinstance(X, six.string_types):
            raise ValueError(
                "Iterable over raw text documents expected, "
                "string object received.")
        self._get_hasher().fit(X, y=y)
        return self
    def transform(self, X, y=None):
                if isinstance(X, six.string_types):
            raise ValueError(
                "Iterable over raw text documents expected, "
                "string object received.")
        analyzer = self.build_analyzer()
        X = self._get_hasher().transform(analyzer(doc) for doc in X)
        if self.binary:
            X.data.fill(1)
        if self.norm is not None:
            X = normalize(X, norm=self.norm, copy=False)
        return X
        fit_transform = transform
    def _get_hasher(self):
        return FeatureHasher(n_features=self.n_features,
                             input_type='string', dtype=self.dtype,
                             non_negative=self.non_negative)

def _document_frequency(X):
        if sp.isspmatrix_csr(X):
        return bincount(X.indices, minlength=X.shape[1])
    else:
        return np.diff(sp.csc_matrix(X, copy=False).indptr)

class CountVectorizer(BaseEstimator, VectorizerMixin):
    
    def __init__(self, input='content', encoding='utf-8',
                 decode_error='strict', strip_accents=None,
                 lowercase=True, preprocessor=None, tokenizer=None,
                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
                 ngram_range=(1, 1), analyzer='word',
                 max_df=1.0, min_df=1, max_features=None,
                 vocabulary=None, binary=False, dtype=np.int64):
        self.input = input
        self.encoding = encoding
        self.decode_error = decode_error
        self.strip_accents = strip_accents
        self.preprocessor = preprocessor
        self.tokenizer = tokenizer
        self.analyzer = analyzer
        self.lowercase = lowercase
        self.token_pattern = token_pattern
        self.stop_words = stop_words
        self.max_df = max_df
        self.min_df = min_df
        if max_df < 0 or min_df < 0:
            raise ValueError("negative value for max_df or min_df")
        self.max_features = max_features
        if max_features is not None:
            if (not isinstance(max_features, numbers.Integral) or
                    max_features <= 0):
                raise ValueError(
                    "max_features=%r, neither a positive integer nor None"
                    % max_features)
        self.ngram_range = ngram_range
        self.vocabulary = vocabulary
        self.binary = binary
        self.dtype = dtype
    def _sort_features(self, X, vocabulary):
                sorted_features = sorted(six.iteritems(vocabulary))
        map_index = np.empty(len(sorted_features), dtype=np.int32)
        for new_val, (term, old_val) in enumerate(sorted_features):
            vocabulary[term] = new_val
            map_index[old_val] = new_val
        X.indices = map_index.take(X.indices, mode='clip')
        return X
    def _limit_features(self, X, vocabulary, high=None, low=None,
                        limit=None):
                if high is None and low is None and limit is None:
            return X, set()
                dfs = _document_frequency(X)
        tfs = np.asarray(X.sum(axis=0)).ravel()
        mask = np.ones(len(dfs), dtype=bool)
        if high is not None:
            mask &= dfs <= high
        if low is not None:
            mask &= dfs >= low
        if limit is not None and mask.sum() > limit:
            mask_inds = (-tfs[mask]).argsort()[:limit]
            new_mask = np.zeros(len(dfs), dtype=bool)
            new_mask[np.where(mask)[0][mask_inds]] = True
            mask = new_mask
        new_indices = np.cumsum(mask) - 1          removed_terms = set()
        for term, old_index in list(six.iteritems(vocabulary)):
            if mask[old_index]:
                vocabulary[term] = new_indices[old_index]
            else:
                del vocabulary[term]
                removed_terms.add(term)
        kept_indices = np.where(mask)[0]
        if len(kept_indices) == 0:
            raise ValueError("After pruning, no terms remain. Try a lower"
                             " min_df or a higher max_df.")
        return X[:, kept_indices], removed_terms
    def _count_vocab(self, raw_documents, fixed_vocab):
                if fixed_vocab:
            vocabulary = self.vocabulary_
        else:
                        vocabulary = defaultdict()
            vocabulary.default_factory = vocabulary.__len__
        analyze = self.build_analyzer()
        j_indices = []
        indptr = _make_int_array()
        values = _make_int_array()
        indptr.append(0)
        for doc in raw_documents:
            feature_counter = {}
            for feature in analyze(doc):
                try:
                    feature_idx = vocabulary[feature]
                    if feature_idx not in feature_counter:
                        feature_counter[feature_idx] = 1
                    else:
                        feature_counter[feature_idx] += 1
                except KeyError:
                                        continue
            j_indices.extend(feature_counter.keys())
            values.extend(feature_counter.values())
            indptr.append(len(j_indices))
        if not fixed_vocab:
                        vocabulary = dict(vocabulary)
            if not vocabulary:
                raise ValueError("empty vocabulary; perhaps the documents only"
                                 " contain stop words")
        j_indices = np.asarray(j_indices, dtype=np.intc)
        indptr = np.frombuffer(indptr, dtype=np.intc)
        values = frombuffer_empty(values, dtype=np.intc)
        X = sp.csr_matrix((values, j_indices, indptr),
                          shape=(len(indptr) - 1, len(vocabulary)),
                          dtype=self.dtype)
        X.sort_indices()
        return vocabulary, X
    def fit(self, raw_documents, y=None):
                self.fit_transform(raw_documents)
        return self
    def fit_transform(self, raw_documents, y=None):
                                        if isinstance(raw_documents, six.string_types):
            raise ValueError(
                "Iterable over raw text documents expected, "
                "string object received.")
        self._validate_vocabulary()
        max_df = self.max_df
        min_df = self.min_df
        max_features = self.max_features
        vocabulary, X = self._count_vocab(raw_documents,
                                          self.fixed_vocabulary_)
        if self.binary:
            X.data.fill(1)
        if not self.fixed_vocabulary_:
            X = self._sort_features(X, vocabulary)
            n_doc = X.shape[0]
            max_doc_count = (max_df
                             if isinstance(max_df, numbers.Integral)
                             else max_df * n_doc)
            min_doc_count = (min_df
                             if isinstance(min_df, numbers.Integral)
                             else min_df * n_doc)
            if max_doc_count < min_doc_count:
                raise ValueError(
                    "max_df corresponds to < documents than min_df")
            X, self.stop_words_ = self._limit_features(X, vocabulary,
                                                       max_doc_count,
                                                       min_doc_count,
                                                       max_features)
            self.vocabulary_ = vocabulary
        return X
    def transform(self, raw_documents):
                if isinstance(raw_documents, six.string_types):
            raise ValueError(
                "Iterable over raw text documents expected, "
                "string object received.")
        if not hasattr(self, 'vocabulary_'):
            self._validate_vocabulary()
        self._check_vocabulary()
                _, X = self._count_vocab(raw_documents, fixed_vocab=True)
        if self.binary:
            X.data.fill(1)
        return X
    def inverse_transform(self, X):
                self._check_vocabulary()
        if sp.issparse(X):
                        X = X.tocsr()
        else:
                                    X = np.asmatrix(X)
        n_samples = X.shape[0]
        terms = np.array(list(self.vocabulary_.keys()))
        indices = np.array(list(self.vocabulary_.values()))
        inverse_vocabulary = terms[np.argsort(indices)]
        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
                for i in range(n_samples)]
    def get_feature_names(self):
                self._check_vocabulary()
        return [t for t, i in sorted(six.iteritems(self.vocabulary_),
                                     key=itemgetter(1))]

def _make_int_array():
        return array.array(str("i"))

class TfidfTransformer(BaseEstimator, TransformerMixin):
    
    def __init__(self, norm='l2', use_idf=True, smooth_idf=True,
                 sublinear_tf=False):
        self.norm = norm
        self.use_idf = use_idf
        self.smooth_idf = smooth_idf
        self.sublinear_tf = sublinear_tf
    def fit(self, X, y=None):
                if not sp.issparse(X):
            X = sp.csc_matrix(X)
        if self.use_idf:
            n_samples, n_features = X.shape
            df = _document_frequency(X)
                        df += int(self.smooth_idf)
            n_samples += int(self.smooth_idf)
                                    idf = np.log(float(n_samples) / df) + 1.0
            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, 
                                        n=n_features, format='csr')
        return self
    def transform(self, X, copy=True):
                if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):
                        X = sp.csr_matrix(X, copy=copy)
        else:
                        X = sp.csr_matrix(X, dtype=np.float64, copy=copy)
        n_samples, n_features = X.shape
        if self.sublinear_tf:
            np.log(X.data, X.data)
            X.data += 1
        if self.use_idf:
            check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')
            expected_n_features = self._idf_diag.shape[0]
            if n_features != expected_n_features:
                raise ValueError("Input has n_features=%d while the model"
                                 " has been trained with n_features=%d" % (
                                     n_features, expected_n_features))
                        X = X * self._idf_diag
        if self.norm:
            X = normalize(X, norm=self.norm, copy=False)
        return X
    @property
    def idf_(self):
        if hasattr(self, "_idf_diag"):
            return np.ravel(self._idf_diag.sum(axis=0))
        else:
            return None

class TfidfVectorizer(CountVectorizer):
    
    def __init__(self, input='content', encoding='utf-8',
                 decode_error='strict', strip_accents=None, lowercase=True,
                 preprocessor=None, tokenizer=None, analyzer='word',
                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
                 ngram_range=(1, 1), max_df=1.0, min_df=1,
                 max_features=None, vocabulary=None, binary=False,
                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,
                 sublinear_tf=False):
        super(TfidfVectorizer, self).__init__(
            input=input, encoding=encoding, decode_error=decode_error,
            strip_accents=strip_accents, lowercase=lowercase,
            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,
            stop_words=stop_words, token_pattern=token_pattern,
            ngram_range=ngram_range, max_df=max_df, min_df=min_df,
            max_features=max_features, vocabulary=vocabulary, binary=binary,
            dtype=dtype)
        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,
                                       smooth_idf=smooth_idf,
                                       sublinear_tf=sublinear_tf)
        
    @property
    def norm(self):
        return self._tfidf.norm
    @norm.setter
    def norm(self, value):
        self._tfidf.norm = value
    @property
    def use_idf(self):
        return self._tfidf.use_idf
    @use_idf.setter
    def use_idf(self, value):
        self._tfidf.use_idf = value
    @property
    def smooth_idf(self):
        return self._tfidf.smooth_idf
    @smooth_idf.setter
    def smooth_idf(self, value):
        self._tfidf.smooth_idf = value
    @property
    def sublinear_tf(self):
        return self._tfidf.sublinear_tf
    @sublinear_tf.setter
    def sublinear_tf(self, value):
        self._tfidf.sublinear_tf = value
    @property
    def idf_(self):
        return self._tfidf.idf_
    def fit(self, raw_documents, y=None):
                X = super(TfidfVectorizer, self).fit_transform(raw_documents)
        self._tfidf.fit(X)
        return self
    def fit_transform(self, raw_documents, y=None):
                X = super(TfidfVectorizer, self).fit_transform(raw_documents)
        self._tfidf.fit(X)
                        return self._tfidf.transform(X, copy=False)
    def transform(self, raw_documents, copy=True):
                check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')
        X = super(TfidfVectorizer, self).transform(raw_documents)
        return self._tfidf.transform(X, copy=False)
import array
from cpython cimport array
cimport cython
from libc.stdlib cimport abs
cimport numpy as np
import numpy as np
from sklearn.utils.murmurhash cimport murmurhash3_bytes_s32
np.import_array()

@cython.boundscheck(False)
@cython.cdivision(True)
def transform(raw_X, Py_ssize_t n_features, dtype):
        assert n_features > 0
    cdef np.int32_t h
    cdef double value
    cdef array.array indices
    cdef array.array indptr
    indices = array.array("i")
    indptr = array.array("i", [0])
            cdef Py_ssize_t capacity = 8192         cdef np.int32_t size = 0
    cdef np.ndarray values = np.empty(capacity, dtype=dtype)
    for x in raw_X:
        for f, v in x:
            if isinstance(v, (str, unicode)):
                f = "%s%s%s" % (f, '=', v)
                value = 1
            else:
                value = v
            if value == 0:
                continue
            if isinstance(f, unicode):
                f = (<unicode>f).encode("utf-8")
                                    elif not isinstance(f, bytes):
                raise TypeError("feature names must be strings")
            h = murmurhash3_bytes_s32(<bytes>f, 0)
            array.resize_smart(indices, len(indices) + 1)
            indices[len(indices) - 1] = abs(h) % n_features
            value *= (h >= 0) * 2 - 1
            values[size] = value
            size += 1
            if size == capacity:
                capacity *= 2
                                                values = np.resize(values, capacity)
        array.resize_smart(indptr, len(indptr) + 1)
        indptr[len(indptr) - 1] = size
    if len(indices):
        indices_a = np.frombuffer(indices, dtype=np.int32)
    else:               indices_a = np.empty(0, dtype=np.int32)
    return (indices_a, np.frombuffer(indptr, dtype=np.int32), values[:size])
from .dict_vectorizer import DictVectorizer
from .hashing import FeatureHasher
from .image import img_to_graph, grid_to_graph
from . import text
__all__ = ['DictVectorizer', 'image', 'img_to_graph', 'grid_to_graph', 'text',
           'FeatureHasher']

from abc import ABCMeta, abstractmethod
from warnings import warn
import numpy as np
from scipy.sparse import issparse, csc_matrix
from ..base import TransformerMixin
from ..utils import check_array, safe_mask
from ..externals import six

class SelectorMixin(six.with_metaclass(ABCMeta, TransformerMixin)):
    
    def get_support(self, indices=False):
                mask = self._get_support_mask()
        return mask if not indices else np.where(mask)[0]
    @abstractmethod
    def _get_support_mask(self):
        
    def transform(self, X):
                X = check_array(X, accept_sparse='csr')
        mask = self.get_support()
        if not mask.any():
            warn("No features were selected: either the data is"
                 " too noisy or the selection test too strict.",
                 UserWarning)
            return np.empty(0).reshape((X.shape[0], 0))
        if len(mask) != X.shape[1]:
            raise ValueError("X has a different shape than during fitting.")
        return X[:, safe_mask(X, mask)]
    def inverse_transform(self, X):
                if issparse(X):
            X = X.tocsc()
                                                it = self.inverse_transform(np.diff(X.indptr).reshape(1, -1))
            col_nonzeros = it.ravel()
            indptr = np.concatenate([[0], np.cumsum(col_nonzeros)])
            Xt = csc_matrix((X.data, X.indices, indptr),
                            shape=(X.shape[0], len(indptr) - 1), dtype=X.dtype)
            return Xt
        support = self.get_support()
        X = check_array(X)
        if support.sum() != X.shape[1]:
            raise ValueError("X has a different shape than during fitting.")
        if X.ndim == 1:
            X = X[None, :]
        Xt = np.zeros((X.shape[0], support.size), dtype=X.dtype)
        Xt[:, support] = X
        return Xt
import numpy as np
from .base import SelectorMixin
from ..base import BaseEstimator, clone
from ..externals import six
from ..exceptions import NotFittedError
from ..utils.fixes import norm

def _get_feature_importances(estimator, norm_order=1):
    
    def __init__(self, estimator, threshold=None, prefit=False, norm_order=1):
        self.estimator = estimator
        self.threshold = threshold
        self.prefit = prefit
        self.norm_order = norm_order
    def _get_support_mask(self):
                if self.prefit:
            estimator = self.estimator
        elif hasattr(self, 'estimator_'):
            estimator = self.estimator_
        else:
            raise ValueError(
                'Either fit the model before transform or set "prefit=True"'
                ' while passing the fitted estimator to the constructor.')
        scores = _get_feature_importances(estimator, self.norm_order)
        self.threshold_ = _calculate_threshold(estimator, scores,
                                               self.threshold)
        return scores >= self.threshold_
    def fit(self, X, y=None, **fit_params):
                if self.prefit:
            raise NotFittedError(
                "Since 'prefit=True', call transform directly")
        self.estimator_ = clone(self.estimator)
        self.estimator_.fit(X, y, **fit_params)
        return self
    def partial_fit(self, X, y=None, **fit_params):
                if self.prefit:
            raise NotFittedError(
                "Since 'prefit=True', call transform directly")
        if not hasattr(self, "estimator_"):
            self.estimator_ = clone(self.estimator)
        self.estimator_.partial_fit(X, y, **fit_params)
        return self
from __future__ import division
import numpy as np
from scipy.sparse import issparse
from scipy.special import digamma
from ..externals.six import moves
from ..metrics.cluster.supervised import mutual_info_score
from ..neighbors import NearestNeighbors
from ..preprocessing import scale
from ..utils import check_random_state
from ..utils.validation import check_X_y
from ..utils.multiclass import check_classification_targets

def _compute_mi_cc(x, y, n_neighbors):
        n_samples = x.size
    x = x.reshape((-1, 1))
    y = y.reshape((-1, 1))
    xy = np.hstack((x, y))
        nn = NearestNeighbors(metric='chebyshev', n_neighbors=n_neighbors)
    nn.fit(xy)
    radius = nn.kneighbors()[0]
    radius = np.nextafter(radius[:, -1], 0)
            nn.set_params(algorithm='kd_tree')
    nn.fit(x)
    ind = nn.radius_neighbors(radius=radius, return_distance=False)
    nx = np.array([i.size for i in ind])
    nn.fit(y)
    ind = nn.radius_neighbors(radius=radius, return_distance=False)
    ny = np.array([i.size for i in ind])
    mi = (digamma(n_samples) + digamma(n_neighbors) -
          np.mean(digamma(nx + 1)) - np.mean(digamma(ny + 1)))
    return max(0, mi)

def _compute_mi_cd(c, d, n_neighbors):
        n_samples = c.shape[0]
    c = c.reshape((-1, 1))
    radius = np.empty(n_samples)
    label_counts = np.empty(n_samples)
    k_all = np.empty(n_samples)
    nn = NearestNeighbors()
    for label in np.unique(d):
        mask = d == label
        count = np.sum(mask)
        if count > 1:
            k = min(n_neighbors, count - 1)
            nn.set_params(n_neighbors=k)
            nn.fit(c[mask])
            r = nn.kneighbors()[0]
            radius[mask] = np.nextafter(r[:, -1], 0)
            k_all[mask] = k
        label_counts[mask] = count
        mask = label_counts > 1
    n_samples = np.sum(mask)
    label_counts = label_counts[mask]
    k_all = k_all[mask]
    c = c[mask]
    radius = radius[mask]
    nn.set_params(algorithm='kd_tree')
    nn.fit(c)
    ind = nn.radius_neighbors(radius=radius, return_distance=False)
    m_all = np.array([i.size for i in ind])
    mi = (digamma(n_samples) + np.mean(digamma(k_all)) -
          np.mean(digamma(label_counts)) -
          np.mean(digamma(m_all + 1)))
    return max(0, mi)

def _compute_mi(x, y, x_discrete, y_discrete, n_neighbors=3):
        if x_discrete and y_discrete:
        return mutual_info_score(x, y)
    elif x_discrete and not y_discrete:
        return _compute_mi_cd(y, x, n_neighbors)
    elif not x_discrete and y_discrete:
        return _compute_mi_cd(x, y, n_neighbors)
    else:
        return _compute_mi_cc(x, y, n_neighbors)

def _iterate_columns(X, columns=None):
        if columns is None:
        columns = range(X.shape[1])
    if issparse(X):
        for i in columns:
            x = np.zeros(X.shape[0])
            start_ptr, end_ptr = X.indptr[i], X.indptr[i + 1]
            x[X.indices[start_ptr:end_ptr]] = X.data[start_ptr:end_ptr]
            yield x
    else:
        for i in columns:
            yield X[:, i]

def _estimate_mi(X, y, discrete_features='auto', discrete_target=False,
                 n_neighbors=3, copy=True, random_state=None):
        X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)
    n_samples, n_features = X.shape
    if discrete_features == 'auto':
        discrete_features = issparse(X)
    if isinstance(discrete_features, bool):
        discrete_mask = np.empty(n_features, dtype=bool)
        discrete_mask.fill(discrete_features)
    else:
        discrete_features = np.asarray(discrete_features)
        if discrete_features.dtype != 'bool':
            discrete_mask = np.zeros(n_features, dtype=bool)
            discrete_mask[discrete_features] = True
        else:
            discrete_mask = discrete_features
    continuous_mask = ~discrete_mask
    if np.any(continuous_mask) and issparse(X):
        raise ValueError("Sparse matrix `X` can't have continuous features.")
    rng = check_random_state(random_state)
    if np.any(continuous_mask):
        if copy:
            X = X.copy()
        if not discrete_target:
            X[:, continuous_mask] = scale(X[:, continuous_mask],
                                          with_mean=False, copy=False)
                X = X.astype(float)
        means = np.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))
        X[:, continuous_mask] += 1e-10 * means * rng.randn(
                n_samples, np.sum(continuous_mask))
    if not discrete_target:
        y = scale(y, with_mean=False)
        y += 1e-10 * np.maximum(1, np.mean(np.abs(y))) * rng.randn(n_samples)
    mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for
          x, discrete_feature in moves.zip(_iterate_columns(X), discrete_mask)]
    return np.array(mi)

def mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3,
                           copy=True, random_state=None):
        return _estimate_mi(X, y, discrete_features, False, n_neighbors,
                        copy, random_state)

def mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3,
                        copy=True, random_state=None):
        check_classification_targets(y)
    return _estimate_mi(X, y, discrete_features, True, n_neighbors,
                        copy, random_state)

import numpy as np
from ..utils import check_X_y, safe_sqr
from ..utils.metaestimators import if_delegate_has_method
from ..base import BaseEstimator
from ..base import MetaEstimatorMixin
from ..base import clone
from ..base import is_classifier
from ..externals.joblib import Parallel, delayed
from ..model_selection import check_cv
from ..model_selection._validation import _safe_split, _score
from ..metrics.scorer import check_scoring
from .base import SelectorMixin

def _rfe_single_fit(rfe, estimator, X, y, train, test, scorer):
        X_train, y_train = _safe_split(estimator, X, y, train)
    X_test, y_test = _safe_split(estimator, X, y, test, train)
    return rfe._fit(
        X_train, y_train, lambda estimator, features:
        _score(estimator, X_test[:, features], y_test, scorer)).scores_
class RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin):
        def __init__(self, estimator, n_features_to_select=None, step=1,
                 verbose=0):
        self.estimator = estimator
        self.n_features_to_select = n_features_to_select
        self.step = step
        self.verbose = verbose
    @property
    def _estimator_type(self):
        return self.estimator._estimator_type
    def fit(self, X, y):
                return self._fit(X, y)
    def _fit(self, X, y, step_score=None):
        X, y = check_X_y(X, y, "csc")
                n_features = X.shape[1]
        if self.n_features_to_select is None:
            n_features_to_select = n_features // 2
        else:
            n_features_to_select = self.n_features_to_select
        if 0.0 < self.step < 1.0:
            step = int(max(1, self.step * n_features))
        else:
            step = int(self.step)
        if step <= 0:
            raise ValueError("Step must be >0")
        support_ = np.ones(n_features, dtype=np.bool)
        ranking_ = np.ones(n_features, dtype=np.int)
        if step_score:
            self.scores_ = []
                while np.sum(support_) > n_features_to_select:
                        features = np.arange(n_features)[support_]
                        estimator = clone(self.estimator)
            if self.verbose > 0:
                print("Fitting estimator with %d features." % np.sum(support_))
            estimator.fit(X[:, features], y)
                        if hasattr(estimator, 'coef_'):
                coefs = estimator.coef_
            else:
                coefs = getattr(estimator, 'feature_importances_', None)
            if coefs is None:
                raise RuntimeError('The classifier does not expose '
                                   '"coef_" or "feature_importances_" '
                                   'attributes')
                        if coefs.ndim > 1:
                ranks = np.argsort(safe_sqr(coefs).sum(axis=0))
            else:
                ranks = np.argsort(safe_sqr(coefs))
                        ranks = np.ravel(ranks)
                        threshold = min(step, np.sum(support_) - n_features_to_select)
                                                if step_score:
                self.scores_.append(step_score(estimator, features))
            support_[features[ranks][:threshold]] = False
            ranking_[np.logical_not(support_)] += 1
                features = np.arange(n_features)[support_]
        self.estimator_ = clone(self.estimator)
        self.estimator_.fit(X[:, features], y)
                if step_score:
            self.scores_.append(step_score(self.estimator_, features))
        self.n_features_ = support_.sum()
        self.support_ = support_
        self.ranking_ = ranking_
        return self
    @if_delegate_has_method(delegate='estimator')
    def predict(self, X):
                return self.estimator_.predict(self.transform(X))
    @if_delegate_has_method(delegate='estimator')
    def score(self, X, y):
                return self.estimator_.score(self.transform(X), y)
    def _get_support_mask(self):
        return self.support_
    @if_delegate_has_method(delegate='estimator')
    def decision_function(self, X):
        return self.estimator_.decision_function(self.transform(X))
    @if_delegate_has_method(delegate='estimator')
    def predict_proba(self, X):
        return self.estimator_.predict_proba(self.transform(X))
    @if_delegate_has_method(delegate='estimator')
    def predict_log_proba(self, X):
        return self.estimator_.predict_log_proba(self.transform(X))

class RFECV(RFE, MetaEstimatorMixin):
        def __init__(self, estimator, step=1, cv=None, scoring=None, verbose=0,
                 n_jobs=1):
        self.estimator = estimator
        self.step = step
        self.cv = cv
        self.scoring = scoring
        self.verbose = verbose
        self.n_jobs = n_jobs
    def fit(self, X, y):
                X, y = check_X_y(X, y, "csr")
                cv = check_cv(self.cv, y, is_classifier(self.estimator))
        scorer = check_scoring(self.estimator, scoring=self.scoring)
        n_features = X.shape[1]
        n_features_to_select = 1
        if 0.0 < self.step < 1.0:
            step = int(max(1, self.step * n_features))
        else:
            step = int(self.step)
        if step <= 0:
            raise ValueError("Step must be >0")
        rfe = RFE(estimator=self.estimator,
                  n_features_to_select=n_features_to_select,
                  step=self.step, verbose=self.verbose)
                        
                                                        
        if self.n_jobs == 1:
            parallel, func = list, _rfe_single_fit
        else:
            parallel, func, = Parallel(n_jobs=self.n_jobs), delayed(_rfe_single_fit)
        scores = parallel(
            func(rfe, self.estimator, X, y, train, test, scorer)
            for train, test in cv.split(X, y))
        scores = np.sum(scores, axis=0)
        n_features_to_select = max(
            n_features - (np.argmax(scores) * step),
            n_features_to_select)
                rfe = RFE(estimator=self.estimator,
                  n_features_to_select=n_features_to_select, step=self.step)
        rfe.fit(X, y)
                self.support_ = rfe.support_
        self.n_features_ = rfe.n_features_
        self.ranking_ = rfe.ranking_
        self.estimator_ = clone(self.estimator)
        self.estimator_.fit(self.transform(X), y)
                        self.grid_scores_ = scores[::-1] / cv.get_n_splits(X, y)
        return self

import numpy as np
import warnings
from scipy import special, stats
from scipy.sparse import issparse
from ..base import BaseEstimator
from ..preprocessing import LabelBinarizer
from ..utils import (as_float_array, check_array, check_X_y, safe_sqr,
                     safe_mask)
from ..utils.extmath import norm, safe_sparse_dot, row_norms
from ..utils.validation import check_is_fitted
from .base import SelectorMixin

def _clean_nans(scores):
                scores = as_float_array(scores, copy=True)
    scores[np.isnan(scores)] = np.finfo(scores.dtype).min
    return scores


def f_oneway(*args):
        n_classes = len(args)
    args = [as_float_array(a) for a in args]
    n_samples_per_class = np.array([a.shape[0] for a in args])
    n_samples = np.sum(n_samples_per_class)
    ss_alldata = sum(safe_sqr(a).sum(axis=0) for a in args)
    sums_args = [np.asarray(a.sum(axis=0)) for a in args]
    square_of_sums_alldata = sum(sums_args) ** 2
    square_of_sums_args = [s ** 2 for s in sums_args]
    sstot = ss_alldata - square_of_sums_alldata / float(n_samples)
    ssbn = 0.
    for k, _ in enumerate(args):
        ssbn += square_of_sums_args[k] / n_samples_per_class[k]
    ssbn -= square_of_sums_alldata / float(n_samples)
    sswn = sstot - ssbn
    dfbn = n_classes - 1
    dfwn = n_samples - n_classes
    msb = ssbn / float(dfbn)
    msw = sswn / float(dfwn)
    constant_features_idx = np.where(msw == 0.)[0]
    if (np.nonzero(msb)[0].size != msb.size and constant_features_idx.size):
        warnings.warn("Features %s are constant." % constant_features_idx,
                      UserWarning)
    f = msb / msw
        f = np.asarray(f).ravel()
    prob = special.fdtrc(dfbn, dfwn, f)
    return f, prob

def f_classif(X, y):
        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'])
    args = [X[safe_mask(X, y == k)] for k in np.unique(y)]
    return f_oneway(*args)

def _chisquare(f_obs, f_exp):
        f_obs = np.asarray(f_obs, dtype=np.float64)
    k = len(f_obs)
        chisq = f_obs
    chisq -= f_exp
    chisq **= 2
    with np.errstate(invalid="ignore"):
        chisq /= f_exp
    chisq = chisq.sum(axis=0)
    return chisq, special.chdtrc(k - 1, chisq)

def chi2(X, y):
    
            X = check_array(X, accept_sparse='csr')
    if np.any((X.data if issparse(X) else X) < 0):
        raise ValueError("Input X must be non-negative.")
    Y = LabelBinarizer().fit_transform(y)
    if Y.shape[1] == 1:
        Y = np.append(1 - Y, Y, axis=1)
    observed = safe_sparse_dot(Y.T, X)          
    feature_count = X.sum(axis=0).reshape(1, -1)
    class_prob = Y.mean(axis=0).reshape(1, -1)
    expected = np.dot(class_prob.T, feature_count)
    return _chisquare(observed, expected)

def f_regression(X, y, center=True):
        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64)
    n_samples = X.shape[0]
                if center:
        y = y - np.mean(y)
        if issparse(X):
            X_means = X.mean(axis=0).getA1()
        else:
            X_means = X.mean(axis=0)
                X_norms = np.sqrt(row_norms(X.T, squared=True) -
                          n_samples * X_means ** 2)
    else:
        X_norms = row_norms(X.T)
        corr = safe_sparse_dot(y, X)
    corr /= X_norms
    corr /= norm(y)
        degrees_of_freedom = y.size - (2 if center else 1)
    F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom
    pv = stats.f.sf(F, 1, degrees_of_freedom)
    return F, pv

class _BaseFilter(BaseEstimator, SelectorMixin):
    
    def __init__(self, score_func):
        self.score_func = score_func
    def fit(self, X, y):
                X, y = check_X_y(X, y, ['csr', 'csc'], multi_output=True)
        if not callable(self.score_func):
            raise TypeError("The score function should be a callable, %s (%s) "
                            "was passed."
                            % (self.score_func, type(self.score_func)))
        self._check_params(X, y)
        score_func_ret = self.score_func(X, y)
        if isinstance(score_func_ret, (list, tuple)):
            self.scores_, self.pvalues_ = score_func_ret
            self.pvalues_ = np.asarray(self.pvalues_)
        else:
            self.scores_ = score_func_ret
            self.pvalues_ = None
        self.scores_ = np.asarray(self.scores_)
        return self
    def _check_params(self, X, y):
        pass

class SelectPercentile(_BaseFilter):
    
    def __init__(self, score_func=f_classif, percentile=10):
        super(SelectPercentile, self).__init__(score_func)
        self.percentile = percentile
    def _check_params(self, X, y):
        if not 0 <= self.percentile <= 100:
            raise ValueError("percentile should be >=0, <=100; got %r"
                             % self.percentile)
    def _get_support_mask(self):
        check_is_fitted(self, 'scores_')
                if self.percentile == 100:
            return np.ones(len(self.scores_), dtype=np.bool)
        elif self.percentile == 0:
            return np.zeros(len(self.scores_), dtype=np.bool)
        scores = _clean_nans(self.scores_)
        treshold = stats.scoreatpercentile(scores,
                                           100 - self.percentile)
        mask = scores > treshold
        ties = np.where(scores == treshold)[0]
        if len(ties):
            max_feats = int(len(scores) * self.percentile / 100)
            kept_ties = ties[:max_feats - mask.sum()]
            mask[kept_ties] = True
        return mask

class SelectKBest(_BaseFilter):
    
    def __init__(self, score_func=f_classif, k=10):
        super(SelectKBest, self).__init__(score_func)
        self.k = k
    def _check_params(self, X, y):
        if not (self.k == "all" or 0 <= self.k <= X.shape[1]):
            raise ValueError("k should be >=0, <= n_features; got %r."
                             "Use k='all' to return all features."
                             % self.k)
    def _get_support_mask(self):
        check_is_fitted(self, 'scores_')
        if self.k == 'all':
            return np.ones(self.scores_.shape, dtype=bool)
        elif self.k == 0:
            return np.zeros(self.scores_.shape, dtype=bool)
        else:
            scores = _clean_nans(self.scores_)
            mask = np.zeros(scores.shape, dtype=bool)
                                    mask[np.argsort(scores, kind="mergesort")[-self.k:]] = 1
            return mask

class SelectFpr(_BaseFilter):
    
    def __init__(self, score_func=f_classif, alpha=5e-2):
        super(SelectFpr, self).__init__(score_func)
        self.alpha = alpha
    def _get_support_mask(self):
        check_is_fitted(self, 'scores_')
        return self.pvalues_ < self.alpha

class SelectFdr(_BaseFilter):
    
    def __init__(self, score_func=f_classif, alpha=5e-2):
        super(SelectFdr, self).__init__(score_func)
        self.alpha = alpha
    def _get_support_mask(self):
        check_is_fitted(self, 'scores_')
        n_features = len(self.pvalues_)
        sv = np.sort(self.pvalues_)
        selected = sv[sv <= float(self.alpha) / n_features *
                      np.arange(1, n_features + 1)]
        if selected.size == 0:
            return np.zeros_like(self.pvalues_, dtype=bool)
        return self.pvalues_ <= selected.max()

class SelectFwe(_BaseFilter):
    
    def __init__(self, score_func=f_classif, alpha=5e-2):
        super(SelectFwe, self).__init__(score_func)
        self.alpha = alpha
    def _get_support_mask(self):
        check_is_fitted(self, 'scores_')
        return (self.pvalues_ < self.alpha / len(self.pvalues_))

class GenericUnivariateSelect(_BaseFilter):
    
    _selection_modes = {'percentile': SelectPercentile,
                        'k_best': SelectKBest,
                        'fpr': SelectFpr,
                        'fdr': SelectFdr,
                        'fwe': SelectFwe}
    def __init__(self, score_func=f_classif, mode='percentile', param=1e-5):
        super(GenericUnivariateSelect, self).__init__(score_func)
        self.mode = mode
        self.param = param
    def _make_selector(self):
        selector = self._selection_modes[self.mode](score_func=self.score_func)
                        possible_params = selector._get_param_names()
        possible_params.remove('score_func')
        selector.set_params(**{possible_params[0]: self.param})
        return selector
    def _check_params(self, X, y):
        if self.mode not in self._selection_modes:
            raise ValueError("The mode passed should be one of %s, %r,"
                             " (type %s) was passed."
                             % (self._selection_modes.keys(), self.mode,
                                type(self.mode)))
        self._make_selector()._check_params(X, y)
    def _get_support_mask(self):
        check_is_fitted(self, 'scores_')
        selector = self._make_selector()
        selector.pvalues_ = self.pvalues_
        selector.scores_ = self.scores_
        return selector._get_support_mask()
import numpy as np
from ..base import BaseEstimator
from .base import SelectorMixin
from ..utils import check_array
from ..utils.sparsefuncs import mean_variance_axis
from ..utils.validation import check_is_fitted

class VarianceThreshold(BaseEstimator, SelectorMixin):
    
    def __init__(self, threshold=0.):
        self.threshold = threshold
    def fit(self, X, y=None):
                X = check_array(X, ('csr', 'csc'), dtype=np.float64)
        if hasattr(X, "toarray"):               _, self.variances_ = mean_variance_axis(X, axis=0)
        else:
            self.variances_ = np.var(X, axis=0)
        if np.all(self.variances_ <= self.threshold):
            msg = "No feature in X meets the variance threshold {0:.5f}"
            if X.shape[0] == 1:
                msg += " (X contains only one sample)"
            raise ValueError(msg.format(self.threshold))
        return self
    def _get_support_mask(self):
        check_is_fitted(self, 'variances_')
        return self.variances_ > self.threshold
from .univariate_selection import chi2
from .univariate_selection import f_classif
from .univariate_selection import f_oneway
from .univariate_selection import f_regression
from .univariate_selection import SelectPercentile
from .univariate_selection import SelectKBest
from .univariate_selection import SelectFpr
from .univariate_selection import SelectFdr
from .univariate_selection import SelectFwe
from .univariate_selection import GenericUnivariateSelect
from .variance_threshold import VarianceThreshold
from .rfe import RFE
from .rfe import RFECV
from .from_model import SelectFromModel
from .mutual_info_ import mutual_info_regression, mutual_info_classif

__all__ = ['GenericUnivariateSelect',
           'RFE',
           'RFECV',
           'SelectFdr',
           'SelectFpr',
           'SelectFwe',
           'SelectKBest',
           'SelectFromModel',
           'SelectPercentile',
           'VarianceThreshold',
           'chi2',
           'f_classif',
           'f_oneway',
           'f_regression',
           'mutual_info_classif',
           'mutual_info_regression']


import numpy as np

def absolute_exponential(theta, d):
        theta = np.asarray(theta, dtype=np.float64)
    d = np.abs(np.asarray(d, dtype=np.float64))
    if d.ndim > 1:
        n_features = d.shape[1]
    else:
        n_features = 1
    if theta.size == 1:
        return np.exp(- theta[0] * np.sum(d, axis=1))
    elif theta.size != n_features:
        raise ValueError("Length of theta must be 1 or %s" % n_features)
    else:
        return np.exp(- np.sum(theta.reshape(1, n_features) * d, axis=1))

def squared_exponential(theta, d):
    
    theta = np.asarray(theta, dtype=np.float64)
    d = np.asarray(d, dtype=np.float64)
    if d.ndim > 1:
        n_features = d.shape[1]
    else:
        n_features = 1
    if theta.size == 1:
        return np.exp(-theta[0] * np.sum(d ** 2, axis=1))
    elif theta.size != n_features:
        raise ValueError("Length of theta must be 1 or %s" % n_features)
    else:
        return np.exp(-np.sum(theta.reshape(1, n_features) * d ** 2, axis=1))

def generalized_exponential(theta, d):
    
    theta = np.asarray(theta, dtype=np.float64)
    d = np.asarray(d, dtype=np.float64)
    if d.ndim > 1:
        n_features = d.shape[1]
    else:
        n_features = 1
    lth = theta.size
    if n_features > 1 and lth == 2:
        theta = np.hstack([np.repeat(theta[0], n_features), theta[1]])
    elif lth != n_features + 1:
        raise Exception("Length of theta must be 2 or %s" % (n_features + 1))
    else:
        theta = theta.reshape(1, lth)
    td = theta[:, 0:-1].reshape(1, n_features) * np.abs(d) ** theta[:, -1]
    r = np.exp(- np.sum(td, 1))
    return r

def pure_nugget(theta, d):
    
    theta = np.asarray(theta, dtype=np.float64)
    d = np.asarray(d, dtype=np.float64)
    n_eval = d.shape[0]
    r = np.zeros(n_eval)
    r[np.all(d == 0., axis=1)] = 1.
    return r

def cubic(theta, d):
    
    theta = np.asarray(theta, dtype=np.float64)
    d = np.asarray(d, dtype=np.float64)
    if d.ndim > 1:
        n_features = d.shape[1]
    else:
        n_features = 1
    lth = theta.size
    if lth == 1:
        td = np.abs(d) * theta
    elif lth != n_features:
        raise Exception("Length of theta must be 1 or " + str(n_features))
    else:
        td = np.abs(d) * theta.reshape(1, n_features)
    td[td > 1.] = 1.
    ss = 1. - td ** 2. * (3. - 2. * td)
    r = np.prod(ss, 1)
    return r

def linear(theta, d):
    
    theta = np.asarray(theta, dtype=np.float64)
    d = np.asarray(d, dtype=np.float64)
    if d.ndim > 1:
        n_features = d.shape[1]
    else:
        n_features = 1
    lth = theta.size
    if lth == 1:
        td = np.abs(d) * theta
    elif lth != n_features:
        raise Exception("Length of theta must be 1 or %s" % n_features)
    else:
        td = np.abs(d) * theta.reshape(1, n_features)
    td[td > 1.] = 1.
    ss = 1. - td
    r = np.prod(ss, 1)
    return r

from __future__ import print_function
import numpy as np
from scipy import linalg, optimize
from ..base import BaseEstimator, RegressorMixin
from ..metrics.pairwise import manhattan_distances
from ..utils import check_random_state, check_array, check_X_y
from ..utils.validation import check_is_fitted
from . import regression_models as regression
from . import correlation_models as correlation
from ..utils import deprecated
MACHINE_EPSILON = np.finfo(np.double).eps

@deprecated("l1_cross_distances was deprecated in version 0.18 "
            "and will be removed in 0.20.")
def l1_cross_distances(X):
        X = check_array(X)
    n_samples, n_features = X.shape
    n_nonzero_cross_dist = n_samples * (n_samples - 1) // 2
    ij = np.zeros((n_nonzero_cross_dist, 2), dtype=np.int)
    D = np.zeros((n_nonzero_cross_dist, n_features))
    ll_1 = 0
    for k in range(n_samples - 1):
        ll_0 = ll_1
        ll_1 = ll_0 + n_samples - k - 1
        ij[ll_0:ll_1, 0] = k
        ij[ll_0:ll_1, 1] = np.arange(k + 1, n_samples)
        D[ll_0:ll_1] = np.abs(X[k] - X[(k + 1):n_samples])
    return D, ij

@deprecated("GaussianProcess was deprecated in version 0.18 and will be "
            "removed in 0.20. Use the GaussianProcessRegressor instead.")
class GaussianProcess(BaseEstimator, RegressorMixin):
    
    _regression_types = {
        'constant': regression.constant,
        'linear': regression.linear,
        'quadratic': regression.quadratic}
    _correlation_types = {
        'absolute_exponential': correlation.absolute_exponential,
        'squared_exponential': correlation.squared_exponential,
        'generalized_exponential': correlation.generalized_exponential,
        'cubic': correlation.cubic,
        'linear': correlation.linear}
    _optimizer_types = [
        'fmin_cobyla',
        'Welch']
    def __init__(self, regr='constant', corr='squared_exponential', beta0=None,
                 storage_mode='full', verbose=False, theta0=1e-1,
                 thetaL=None, thetaU=None, optimizer='fmin_cobyla',
                 random_start=1, normalize=True,
                 nugget=10. * MACHINE_EPSILON, random_state=None):
        self.regr = regr
        self.corr = corr
        self.beta0 = beta0
        self.storage_mode = storage_mode
        self.verbose = verbose
        self.theta0 = theta0
        self.thetaL = thetaL
        self.thetaU = thetaU
        self.normalize = normalize
        self.nugget = nugget
        self.optimizer = optimizer
        self.random_start = random_start
        self.random_state = random_state
    def fit(self, X, y):
                        self._check_params()
        self.random_state = check_random_state(self.random_state)
                X, y = check_X_y(X, y, multi_output=True, y_numeric=True)
        self.y_ndim_ = y.ndim
        if y.ndim == 1:
            y = y[:, np.newaxis]
                n_samples, n_features = X.shape
        _, n_targets = y.shape
                self._check_params(n_samples)
                if self.normalize:
            X_mean = np.mean(X, axis=0)
            X_std = np.std(X, axis=0)
            y_mean = np.mean(y, axis=0)
            y_std = np.std(y, axis=0)
            X_std[X_std == 0.] = 1.
            y_std[y_std == 0.] = 1.
                        X = (X - X_mean) / X_std
            y = (y - y_mean) / y_std
        else:
            X_mean = np.zeros(1)
            X_std = np.ones(1)
            y_mean = np.zeros(1)
            y_std = np.ones(1)
                D, ij = l1_cross_distances(X)
        if (np.min(np.sum(D, axis=1)) == 0.
                and self.corr != correlation.pure_nugget):
            raise Exception("Multiple input features cannot have the same"
                            " target value.")
                F = self.regr(X)
        n_samples_F = F.shape[0]
        if F.ndim > 1:
            p = F.shape[1]
        else:
            p = 1
        if n_samples_F != n_samples:
            raise Exception("Number of rows in F and X do not match. Most "
                            "likely something is going wrong with the "
                            "regression model.")
        if p > n_samples_F:
            raise Exception(("Ordinary least squares problem is undetermined "
                             "n_samples=%d must be greater than the "
                             "regression model size p=%d.") % (n_samples, p))
        if self.beta0 is not None:
            if self.beta0.shape[0] != p:
                raise Exception("Shapes of beta0 and F do not match.")
                self.X = X
        self.y = y
        self.D = D
        self.ij = ij
        self.F = F
        self.X_mean, self.X_std = X_mean, X_std
        self.y_mean, self.y_std = y_mean, y_std
                if self.thetaL is not None and self.thetaU is not None:
                        if self.verbose:
                print("Performing Maximum Likelihood Estimation of the "
                      "autocorrelation parameters...")
            self.theta_, self.reduced_likelihood_function_value_, par = \
                self._arg_max_reduced_likelihood_function()
            if np.isinf(self.reduced_likelihood_function_value_):
                raise Exception("Bad parameter region. "
                                "Try increasing upper bound")
        else:
                        if self.verbose:
                print("Given autocorrelation parameters. "
                      "Computing Gaussian Process model parameters...")
            self.theta_ = self.theta0
            self.reduced_likelihood_function_value_, par = \
                self.reduced_likelihood_function()
            if np.isinf(self.reduced_likelihood_function_value_):
                raise Exception("Bad point. Try increasing theta0.")
        self.beta = par['beta']
        self.gamma = par['gamma']
        self.sigma2 = par['sigma2']
        self.C = par['C']
        self.Ft = par['Ft']
        self.G = par['G']
        if self.storage_mode == 'light':
                                    if self.verbose:
                print("Light storage mode specified. "
                      "Flushing autocorrelation matrix...")
            self.D = None
            self.ij = None
            self.F = None
            self.C = None
            self.Ft = None
            self.G = None
        return self
    def predict(self, X, eval_MSE=False, batch_size=None):
                check_is_fitted(self, "X")
                X = check_array(X)
        n_eval, _ = X.shape
        n_samples, n_features = self.X.shape
        n_samples_y, n_targets = self.y.shape
                self._check_params(n_samples)
        if X.shape[1] != n_features:
            raise ValueError(("The number of features in X (X.shape[1] = %d) "
                              "should match the number of features used "
                              "for fit() "
                              "which is %d.") % (X.shape[1], n_features))
        if batch_size is None:
                        
                        X = (X - self.X_mean) / self.X_std
                        y = np.zeros(n_eval)
            if eval_MSE:
                MSE = np.zeros(n_eval)
                        dx = manhattan_distances(X, Y=self.X, sum_over_features=False)
                        f = self.regr(X)
            r = self.corr(self.theta_, dx).reshape(n_eval, n_samples)
                        y_ = np.dot(f, self.beta) + np.dot(r, self.gamma)
                        y = (self.y_mean + self.y_std * y_).reshape(n_eval, n_targets)
            if self.y_ndim_ == 1:
                y = y.ravel()
                        if eval_MSE:
                C = self.C
                if C is None:
                                        if self.verbose:
                        print("This GaussianProcess used 'light' storage mode "
                              "at instantiation. Need to recompute "
                              "autocorrelation matrix...")
                    reduced_likelihood_function_value, par = \
                        self.reduced_likelihood_function()
                    self.C = par['C']
                    self.Ft = par['Ft']
                    self.G = par['G']
                rt = linalg.solve_triangular(self.C, r.T, lower=True)
                if self.beta0 is None:
                                        u = linalg.solve_triangular(self.G.T,
                                                np.dot(self.Ft.T, rt) - f.T,
                                                lower=True)
                else:
                                        u = np.zeros((n_targets, n_eval))
                MSE = np.dot(self.sigma2.reshape(n_targets, 1),
                             (1. - (rt ** 2.).sum(axis=0)
                              + (u ** 2.).sum(axis=0))[np.newaxis, :])
                MSE = np.sqrt((MSE ** 2.).sum(axis=0) / n_targets)
                                                MSE[MSE < 0.] = 0.
                if self.y_ndim_ == 1:
                    MSE = MSE.ravel()
                return y, MSE
            else:
                return y
        else:
            
            if type(batch_size) is not int or batch_size <= 0:
                raise Exception("batch_size must be a positive integer")
            if eval_MSE:
                y, MSE = np.zeros(n_eval), np.zeros(n_eval)
                for k in range(max(1, int(n_eval / batch_size))):
                    batch_from = k * batch_size
                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])
                    y[batch_from:batch_to], MSE[batch_from:batch_to] = \
                        self.predict(X[batch_from:batch_to],
                                     eval_MSE=eval_MSE, batch_size=None)
                return y, MSE
            else:
                y = np.zeros(n_eval)
                for k in range(max(1, int(n_eval / batch_size))):
                    batch_from = k * batch_size
                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])
                    y[batch_from:batch_to] = \
                        self.predict(X[batch_from:batch_to],
                                     eval_MSE=eval_MSE, batch_size=None)
                return y
    def reduced_likelihood_function(self, theta=None):
                check_is_fitted(self, "X")
        if theta is None:
                        theta = self.theta_
                reduced_likelihood_function_value = - np.inf
        par = {}
                n_samples = self.X.shape[0]
        D = self.D
        ij = self.ij
        F = self.F
        if D is None:
                        D, ij = l1_cross_distances(self.X)
            if (np.min(np.sum(D, axis=1)) == 0.
                    and self.corr != correlation.pure_nugget):
                raise Exception("Multiple X are not allowed")
            F = self.regr(self.X)
                r = self.corr(theta, D)
        R = np.eye(n_samples) * (1. + self.nugget)
        R[ij[:, 0], ij[:, 1]] = r
        R[ij[:, 1], ij[:, 0]] = r
                try:
            C = linalg.cholesky(R, lower=True)
        except linalg.LinAlgError:
            return reduced_likelihood_function_value, par
                Ft = linalg.solve_triangular(C, F, lower=True)
        Q, G = linalg.qr(Ft, mode='economic')
        sv = linalg.svd(G, compute_uv=False)
        rcondG = sv[-1] / sv[0]
        if rcondG < 1e-10:
                        sv = linalg.svd(F, compute_uv=False)
            condF = sv[0] / sv[-1]
            if condF > 1e15:
                raise Exception("F is too ill conditioned. Poor combination "
                                "of regression model and observations.")
            else:
                                return reduced_likelihood_function_value, par
        Yt = linalg.solve_triangular(C, self.y, lower=True)
        if self.beta0 is None:
                        beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))
        else:
                        beta = np.array(self.beta0)
        rho = Yt - np.dot(Ft, beta)
        sigma2 = (rho ** 2.).sum(axis=0) / n_samples
                        detR = (np.diag(C) ** (2. / n_samples)).prod()
                reduced_likelihood_function_value = - sigma2.sum() * detR
        par['sigma2'] = sigma2 * self.y_std ** 2.
        par['beta'] = beta
        par['gamma'] = linalg.solve_triangular(C.T, rho)
        par['C'] = C
        par['Ft'] = Ft
        par['G'] = G
        return reduced_likelihood_function_value, par
    def _arg_max_reduced_likelihood_function(self):
        
                best_optimal_theta = []
        best_optimal_rlf_value = []
        best_optimal_par = []
        if self.verbose:
            print("The chosen optimizer is: " + str(self.optimizer))
            if self.random_start > 1:
                print(str(self.random_start) + " random starts are required.")
        percent_completed = 0.
                if self.optimizer == 'Welch' and self.theta0.size == 1:
            self.optimizer = 'fmin_cobyla'
        if self.optimizer == 'fmin_cobyla':
            def minus_reduced_likelihood_function(log10t):
                return - self.reduced_likelihood_function(
                    theta=10. ** log10t)[0]
            constraints = []
            for i in range(self.theta0.size):
                constraints.append(lambda log10t, i=i:
                                   log10t[i] - np.log10(self.thetaL[0, i]))
                constraints.append(lambda log10t, i=i:
                                   np.log10(self.thetaU[0, i]) - log10t[i])
            for k in range(self.random_start):
                if k == 0:
                                        theta0 = self.theta0
                else:
                                                            log10theta0 = (np.log10(self.thetaL)
                                   + self.random_state.rand(*self.theta0.shape)
                                   * np.log10(self.thetaU / self.thetaL))
                    theta0 = 10. ** log10theta0
                                try:
                    log10_optimal_theta = \
                        optimize.fmin_cobyla(minus_reduced_likelihood_function,
                                             np.log10(theta0).ravel(), constraints,
                                             iprint=0)
                except ValueError as ve:
                    print("Optimization failed. Try increasing the ``nugget``")
                    raise ve
                optimal_theta = 10. ** log10_optimal_theta
                optimal_rlf_value, optimal_par = \
                    self.reduced_likelihood_function(theta=optimal_theta)
                                if k > 0:
                    if optimal_rlf_value > best_optimal_rlf_value:
                        best_optimal_rlf_value = optimal_rlf_value
                        best_optimal_par = optimal_par
                        best_optimal_theta = optimal_theta
                else:
                    best_optimal_rlf_value = optimal_rlf_value
                    best_optimal_par = optimal_par
                    best_optimal_theta = optimal_theta
                if self.verbose and self.random_start > 1:
                    if (20 * k) / self.random_start > percent_completed:
                        percent_completed = (20 * k) / self.random_start
                        print("%s completed" % (5 * percent_completed))
            optimal_rlf_value = best_optimal_rlf_value
            optimal_par = best_optimal_par
            optimal_theta = best_optimal_theta
        elif self.optimizer == 'Welch':
                        theta0, thetaL, thetaU = self.theta0, self.thetaL, self.thetaU
            corr = self.corr
            verbose = self.verbose
                        self.optimizer = 'fmin_cobyla'
            self.verbose = False
                        if verbose:
                print("Initialize under isotropy assumption...")
            self.theta0 = check_array(self.theta0.min())
            self.thetaL = check_array(self.thetaL.min())
            self.thetaU = check_array(self.thetaU.max())
            theta_iso, optimal_rlf_value_iso, par_iso = \
                self._arg_max_reduced_likelihood_function()
            optimal_theta = theta_iso + np.zeros(theta0.shape)
                        if verbose:
                print("Now improving allowing for anisotropy...")
            for i in self.random_state.permutation(theta0.size):
                if verbose:
                    print("Proceeding along dimension %d..." % (i + 1))
                self.theta0 = check_array(theta_iso)
                self.thetaL = check_array(thetaL[0, i])
                self.thetaU = check_array(thetaU[0, i])
                def corr_cut(t, d):
                    return corr(check_array(np.hstack([optimal_theta[0][0:i],
                                                       t[0],
                                                       optimal_theta[0][(i +
                                                                         1)::]])),
                                d)
                self.corr = corr_cut
                optimal_theta[0, i], optimal_rlf_value, optimal_par = \
                    self._arg_max_reduced_likelihood_function()
                        self.theta0, self.thetaL, self.thetaU = theta0, thetaL, thetaU
            self.corr = corr
            self.optimizer = 'Welch'
            self.verbose = verbose
        else:
            raise NotImplementedError("This optimizer ('%s') is not "
                                      "implemented yet. Please contribute!"
                                      % self.optimizer)
        return optimal_theta, optimal_rlf_value, optimal_par
    def _check_params(self, n_samples=None):
                if not callable(self.regr):
            if self.regr in self._regression_types:
                self.regr = self._regression_types[self.regr]
            else:
                raise ValueError("regr should be one of %s or callable, "
                                 "%s was given."
                                 % (self._regression_types.keys(), self.regr))
                if self.beta0 is not None:
            self.beta0 = np.atleast_2d(self.beta0)
            if self.beta0.shape[1] != 1:
                                self.beta0 = self.beta0.T
                if not callable(self.corr):
            if self.corr in self._correlation_types:
                self.corr = self._correlation_types[self.corr]
            else:
                raise ValueError("corr should be one of %s or callable, "
                                 "%s was given."
                                 % (self._correlation_types.keys(), self.corr))
                if self.storage_mode != 'full' and self.storage_mode != 'light':
            raise ValueError("Storage mode should either be 'full' or "
                             "'light', %s was given." % self.storage_mode)
                self.theta0 = np.atleast_2d(self.theta0)
        lth = self.theta0.size
        if self.thetaL is not None and self.thetaU is not None:
            self.thetaL = np.atleast_2d(self.thetaL)
            self.thetaU = np.atleast_2d(self.thetaU)
            if self.thetaL.size != lth or self.thetaU.size != lth:
                raise ValueError("theta0, thetaL and thetaU must have the "
                                 "same length.")
            if np.any(self.thetaL <= 0) or np.any(self.thetaU < self.thetaL):
                raise ValueError("The bounds must satisfy O < thetaL <= "
                                 "thetaU.")
        elif self.thetaL is None and self.thetaU is None:
            if np.any(self.theta0 <= 0):
                raise ValueError("theta0 must be strictly positive.")
        elif self.thetaL is None or self.thetaU is None:
            raise ValueError("thetaL and thetaU should either be both or "
                             "neither specified.")
                self.verbose = bool(self.verbose)
                self.normalize = bool(self.normalize)
                self.nugget = np.asarray(self.nugget)
        if np.any(self.nugget) < 0.:
            raise ValueError("nugget must be positive or zero.")
        if (n_samples is not None
                and self.nugget.shape not in [(), (n_samples,)]):
            raise ValueError("nugget must be either a scalar "
                             "or array of length n_samples.")
                if self.optimizer not in self._optimizer_types:
            raise ValueError("optimizer should be one of %s"
                             % self._optimizer_types)
                self.random_start = int(self.random_start)

import warnings
from operator import itemgetter
import numpy as np
from scipy.linalg import cholesky, cho_solve, solve
from scipy.optimize import fmin_l_bfgs_b
from scipy.special import erf
from sklearn.base import BaseEstimator, ClassifierMixin, clone
from sklearn.gaussian_process.kernels \
    import RBF, CompoundKernel, ConstantKernel as C
from sklearn.utils.validation import check_X_y, check_is_fitted, check_array
from sklearn.utils import check_random_state
from sklearn.preprocessing import LabelEncoder
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier

LAMBDAS = np.array([0.41, 0.4, 0.37, 0.44, 0.39])[:, np.newaxis]
COEFS = np.array([-1854.8214151, 3516.89893646, 221.29346712,
                  128.12323805, -2010.49422654])[:, np.newaxis]

class _BinaryGaussianProcessClassifierLaplace(BaseEstimator):
        def __init__(self, kernel=None, optimizer="fmin_l_bfgs_b",
                 n_restarts_optimizer=0, max_iter_predict=100,
                 warm_start=False, copy_X_train=True, random_state=None):
        self.kernel = kernel
        self.optimizer = optimizer
        self.n_restarts_optimizer = n_restarts_optimizer
        self.max_iter_predict = max_iter_predict
        self.warm_start = warm_start
        self.copy_X_train = copy_X_train
        self.random_state = random_state
    def fit(self, X, y):
                if self.kernel is None:              self.kernel_ = C(1.0, constant_value_bounds="fixed") \
                * RBF(1.0, length_scale_bounds="fixed")
        else:
            self.kernel_ = clone(self.kernel)
        self.rng = check_random_state(self.random_state)
        self.X_train_ = np.copy(X) if self.copy_X_train else X
                        label_encoder = LabelEncoder()
        self.y_train_ = label_encoder.fit_transform(y)
        self.classes_ = label_encoder.classes_
        if self.classes_.size > 2:
            raise ValueError("%s supports only binary classification. "
                             "y contains classes %s"
                             % (self.__class__.__name__, self.classes_))
        elif self.classes_.size == 1:
            raise ValueError("{0:s} requires 2 classes.".format(
                self.__class__.__name__))
        if self.optimizer is not None and self.kernel_.n_dims > 0:
                                    def obj_func(theta, eval_gradient=True):
                if eval_gradient:
                    lml, grad = self.log_marginal_likelihood(
                        theta, eval_gradient=True)
                    return -lml, -grad
                else:
                    return -self.log_marginal_likelihood(theta)
                        optima = [self._constrained_optimization(obj_func,
                                                     self.kernel_.theta,
                                                     self.kernel_.bounds)]
                                    if self.n_restarts_optimizer > 0:
                if not np.isfinite(self.kernel_.bounds).all():
                    raise ValueError(
                        "Multiple optimizer restarts (n_restarts_optimizer>0) "
                        "requires that all bounds are finite.")
                bounds = self.kernel_.bounds
                for iteration in range(self.n_restarts_optimizer):
                    theta_initial = np.exp(self.rng.uniform(bounds[:, 0],
                                                            bounds[:, 1]))
                    optima.append(
                        self._constrained_optimization(obj_func, theta_initial,
                                                       bounds))
                                    lml_values = list(map(itemgetter(1), optima))
            self.kernel_.theta = optima[np.argmin(lml_values)][0]
            self.log_marginal_likelihood_value_ = -np.min(lml_values)
        else:
            self.log_marginal_likelihood_value_ = \
                self.log_marginal_likelihood(self.kernel_.theta)
                        K = self.kernel_(self.X_train_)
        _, (self.pi_, self.W_sr_, self.L_, _, _) = \
            self._posterior_mode(K, return_temporaries=True)
        return self
    def predict(self, X):
                check_is_fitted(self, ["X_train_", "y_train_", "pi_", "W_sr_", "L_"])
                                K_star = self.kernel_(self.X_train_, X)          f_star = K_star.T.dot(self.y_train_ - self.pi_)  
        return np.where(f_star > 0, self.classes_[1], self.classes_[0])
    def predict_proba(self, X):
                check_is_fitted(self, ["X_train_", "y_train_", "pi_", "W_sr_", "L_"])
                K_star = self.kernel_(self.X_train_, X)          f_star = K_star.T.dot(self.y_train_ - self.pi_)          v = solve(self.L_, self.W_sr_[:, np.newaxis] * K_star)                  var_f_star = self.kernel_.diag(X) - np.einsum("ij,ij->j", v, v)
                                                                alpha = 1 / (2 * var_f_star)
        gamma = LAMBDAS * f_star
        integrals = np.sqrt(np.pi / alpha) \
            * erf(gamma * np.sqrt(alpha / (alpha + LAMBDAS**2))) \
            / (2 * np.sqrt(var_f_star * 2 * np.pi))
        pi_star = (COEFS * integrals).sum(axis=0) + .5 * COEFS.sum()
        return np.vstack((1 - pi_star, pi_star)).T
    def log_marginal_likelihood(self, theta=None, eval_gradient=False):
                if theta is None:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated for theta!=None")
            return self.log_marginal_likelihood_value_
        kernel = self.kernel_.clone_with_theta(theta)
        if eval_gradient:
            K, K_gradient = kernel(self.X_train_, eval_gradient=True)
        else:
            K = kernel(self.X_train_)
                        Z, (pi, W_sr, L, b, a) = \
            self._posterior_mode(K, return_temporaries=True)
        if not eval_gradient:
            return Z
                d_Z = np.empty(theta.shape[0])
                R = W_sr[:, np.newaxis] * cho_solve((L, True), np.diag(W_sr))          C = solve(L, W_sr[:, np.newaxis] * K)                  s_2 = -0.5 * (np.diag(K) - np.einsum('ij, ij -> j', C, C)) \
            * (pi * (1 - pi) * (1 - 2 * pi))  
        for j in range(d_Z.shape[0]):
            C = K_gradient[:, :, j]                           s_1 = .5 * a.T.dot(C).dot(a) - .5 * R.T.ravel().dot(C.ravel())
            b = C.dot(self.y_train_ - pi)              s_3 = b - K.dot(R.dot(b))  
            d_Z[j] = s_1 + s_2.T.dot(s_3)  
        return Z, d_Z
    def _posterior_mode(self, K, return_temporaries=False):
                
                        if self.warm_start and hasattr(self, "f_cached") \
           and self.f_cached.shape == self.y_train_.shape:
            f = self.f_cached
        else:
            f = np.zeros_like(self.y_train_, dtype=np.float64)
                log_marginal_likelihood = -np.inf
        for _ in range(self.max_iter_predict):
                        pi = 1 / (1 + np.exp(-f))
            W = pi * (1 - pi)
                        W_sr = np.sqrt(W)
            W_sr_K = W_sr[:, np.newaxis] * K
            B = np.eye(W.shape[0]) + W_sr_K * W_sr
            L = cholesky(B, lower=True)
                        b = W * f + (self.y_train_ - pi)
                        a = b - W_sr * cho_solve((L, True), W_sr_K.dot(b))
                        f = K.dot(a)
                                    lml = -0.5 * a.T.dot(f) \
                - np.log(1 + np.exp(-(self.y_train_ * 2 - 1) * f)).sum() \
                - np.log(np.diag(L)).sum()
                                                if lml - log_marginal_likelihood < 1e-10:
                break
            log_marginal_likelihood = lml
        self.f_cached = f          if return_temporaries:
            return log_marginal_likelihood, (pi, W_sr, L, b, a)
        else:
            return log_marginal_likelihood
    def _constrained_optimization(self, obj_func, initial_theta, bounds):
        if self.optimizer == "fmin_l_bfgs_b":
            theta_opt, func_min, convergence_dict = \
                fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)
            if convergence_dict["warnflag"] != 0:
                warnings.warn("fmin_l_bfgs_b terminated abnormally with the "
                              " state: %s" % convergence_dict)
        elif callable(self.optimizer):
            theta_opt, func_min = \
                self.optimizer(obj_func, initial_theta, bounds=bounds)
        else:
            raise ValueError("Unknown optimizer %s." % self.optimizer)
        return theta_opt, func_min

class GaussianProcessClassifier(BaseEstimator, ClassifierMixin):
        def __init__(self, kernel=None, optimizer="fmin_l_bfgs_b",
                 n_restarts_optimizer=0, max_iter_predict=100,
                 warm_start=False, copy_X_train=True, random_state=None,
                 multi_class="one_vs_rest", n_jobs=1):
        self.kernel = kernel
        self.optimizer = optimizer
        self.n_restarts_optimizer = n_restarts_optimizer
        self.max_iter_predict = max_iter_predict
        self.warm_start = warm_start
        self.copy_X_train = copy_X_train
        self.random_state = random_state
        self.multi_class = multi_class
        self.n_jobs = n_jobs
    def fit(self, X, y):
                X, y = check_X_y(X, y, multi_output=False)
        self.base_estimator_ = _BinaryGaussianProcessClassifierLaplace(
            self.kernel, self.optimizer, self.n_restarts_optimizer,
            self.max_iter_predict, self.warm_start, self.copy_X_train,
            self.random_state)
        self.classes_ = np.unique(y)
        self.n_classes_ = self.classes_.size
        if self.n_classes_ == 1:
            raise ValueError("GaussianProcessClassifier requires 2 or more "
                             "distinct classes. Only class %s present."
                             % self.classes_[0])
        if self.n_classes_ > 2:
            if self.multi_class == "one_vs_rest":
                self.base_estimator_ = \
                    OneVsRestClassifier(self.base_estimator_,
                                        n_jobs=self.n_jobs)
            elif self.multi_class == "one_vs_one":
                self.base_estimator_ = \
                    OneVsOneClassifier(self.base_estimator_,
                                       n_jobs=self.n_jobs)
            else:
                raise ValueError("Unknown multi-class mode %s"
                                 % self.multi_class)
        self.base_estimator_.fit(X, y)
        if self.n_classes_ > 2:
            self.log_marginal_likelihood_value_ = np.mean(
                [estimator.log_marginal_likelihood()
                 for estimator in self.base_estimator_.estimators_])
        else:
            self.log_marginal_likelihood_value_ = \
                self.base_estimator_.log_marginal_likelihood()
        return self
    def predict(self, X):
                check_is_fitted(self, ["classes_", "n_classes_"])
        X = check_array(X)
        return self.base_estimator_.predict(X)
    def predict_proba(self, X):
                check_is_fitted(self, ["classes_", "n_classes_"])
        if self.n_classes_ > 2 and self.multi_class == "one_vs_one":
            raise ValueError("one_vs_one multi-class mode does not support "
                             "predicting probability estimates. Use "
                             "one_vs_rest mode instead.")
        X = check_array(X)
        return self.base_estimator_.predict_proba(X)
    @property
    def kernel_(self):
        if self.n_classes_ == 2:
            return self.base_estimator_.kernel_
        else:
            return CompoundKernel(
                [estimator.kernel_
                 for estimator in self.base_estimator_.estimators_])
    def log_marginal_likelihood(self, theta=None, eval_gradient=False):
                check_is_fitted(self, ["classes_", "n_classes_"])
        if theta is None:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated for theta!=None")
            return self.log_marginal_likelihood_value_
        theta = np.asarray(theta)
        if self.n_classes_ == 2:
            return self.base_estimator_.log_marginal_likelihood(
                theta, eval_gradient)
        else:
            if eval_gradient:
                raise NotImplementedError(
                    "Gradient of log-marginal-likelihood not implemented for "
                    "multi-class GPC.")
            estimators = self.base_estimator_.estimators_
            n_dims = estimators[0].kernel_.n_dims
            if theta.shape[0] == n_dims:                  return np.mean(
                    [estimator.log_marginal_likelihood(theta)
                     for i, estimator in enumerate(estimators)])
            elif theta.shape[0] == n_dims * self.classes_.shape[0]:
                                return np.mean(
                    [estimator.log_marginal_likelihood(
                        theta[n_dims * i:n_dims * (i + 1)])
                     for i, estimator in enumerate(estimators)])
            else:
                raise ValueError("Shape of theta must be either %d or %d. "
                                 "Obtained theta with shape %d."
                                 % (n_dims, n_dims * self.classes_.shape[0],
                                    theta.shape[0]))

import warnings
from operator import itemgetter
import numpy as np
from scipy.linalg import cholesky, cho_solve, solve_triangular
from scipy.optimize import fmin_l_bfgs_b
from sklearn.base import BaseEstimator, RegressorMixin, clone
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.utils import check_random_state
from sklearn.utils.validation import check_X_y, check_array
from sklearn.utils.deprecation import deprecated

class GaussianProcessRegressor(BaseEstimator, RegressorMixin):
        def __init__(self, kernel=None, alpha=1e-10,
                 optimizer="fmin_l_bfgs_b", n_restarts_optimizer=0,
                 normalize_y=False, copy_X_train=True, random_state=None):
        self.kernel = kernel
        self.alpha = alpha
        self.optimizer = optimizer
        self.n_restarts_optimizer = n_restarts_optimizer
        self.normalize_y = normalize_y
        self.copy_X_train = copy_X_train
        self.random_state = random_state
    @property
    @deprecated("Attribute rng was deprecated in version 0.19 and "
                "will be removed in 0.21.")
    def rng(self):
        return self._rng
    @property
    @deprecated("Attribute y_train_mean was deprecated in version 0.19 and "
                "will be removed in 0.21.")
    def y_train_mean(self):
        return self._y_train_mean
    def fit(self, X, y):
                if self.kernel is None:              self.kernel_ = C(1.0, constant_value_bounds="fixed") \
                * RBF(1.0, length_scale_bounds="fixed")
        else:
            self.kernel_ = clone(self.kernel)
        self._rng = check_random_state(self.random_state)
        X, y = check_X_y(X, y, multi_output=True, y_numeric=True)
                if self.normalize_y:
            self._y_train_mean = np.mean(y, axis=0)
                        y = y - self._y_train_mean
        else:
            self._y_train_mean = np.zeros(1)
        if np.iterable(self.alpha) \
           and self.alpha.shape[0] != y.shape[0]:
            if self.alpha.shape[0] == 1:
                self.alpha = self.alpha[0]
            else:
                raise ValueError("alpha must be a scalar or an array"
                                 " with same number of entries as y.(%d != %d)"
                                 % (self.alpha.shape[0], y.shape[0]))
        self.X_train_ = np.copy(X) if self.copy_X_train else X
        self.y_train_ = np.copy(y) if self.copy_X_train else y
        if self.optimizer is not None and self.kernel_.n_dims > 0:
                                    def obj_func(theta, eval_gradient=True):
                if eval_gradient:
                    lml, grad = self.log_marginal_likelihood(
                        theta, eval_gradient=True)
                    return -lml, -grad
                else:
                    return -self.log_marginal_likelihood(theta)
                        optima = [(self._constrained_optimization(obj_func,
                                                      self.kernel_.theta,
                                                      self.kernel_.bounds))]
                                    if self.n_restarts_optimizer > 0:
                if not np.isfinite(self.kernel_.bounds).all():
                    raise ValueError(
                        "Multiple optimizer restarts (n_restarts_optimizer>0) "
                        "requires that all bounds are finite.")
                bounds = self.kernel_.bounds
                for iteration in range(self.n_restarts_optimizer):
                    theta_initial = \
                        self._rng.uniform(bounds[:, 0], bounds[:, 1])
                    optima.append(
                        self._constrained_optimization(obj_func, theta_initial,
                                                       bounds))
                                    lml_values = list(map(itemgetter(1), optima))
            self.kernel_.theta = optima[np.argmin(lml_values)][0]
            self.log_marginal_likelihood_value_ = -np.min(lml_values)
        else:
            self.log_marginal_likelihood_value_ = \
                self.log_marginal_likelihood(self.kernel_.theta)
                        K = self.kernel_(self.X_train_)
        K[np.diag_indices_from(K)] += self.alpha
        self.L_ = cholesky(K, lower=True)          self.alpha_ = cho_solve((self.L_, True), self.y_train_)  
        return self
    def predict(self, X, return_std=False, return_cov=False):
                if return_std and return_cov:
            raise RuntimeError(
                "Not returning standard deviation of predictions when "
                "returning full covariance.")
        X = check_array(X)
        if not hasattr(self, "X_train_"):              y_mean = np.zeros(X.shape[0])
            if return_cov:
                y_cov = self.kernel(X)
                return y_mean, y_cov
            elif return_std:
                y_var = self.kernel.diag(X)
                return y_mean, np.sqrt(y_var)
            else:
                return y_mean
        else:              K_trans = self.kernel_(X, self.X_train_)
            y_mean = K_trans.dot(self.alpha_)              y_mean = self._y_train_mean + y_mean              if return_cov:
                v = cho_solve((self.L_, True), K_trans.T)                  y_cov = self.kernel_(X) - K_trans.dot(v)                  return y_mean, y_cov
            elif return_std:
                                                L_inv = solve_triangular(self.L_.T, np.eye(self.L_.shape[0]))
                K_inv = L_inv.dot(L_inv.T)
                                y_var = self.kernel_.diag(X)
                y_var -= np.einsum("ij,ij->i", np.dot(K_trans, K_inv), K_trans)
                                                y_var_negative = y_var < 0
                if np.any(y_var_negative):
                    warnings.warn("Predicted variances smaller than 0. "
                                  "Setting those variances to 0.")
                    y_var[y_var_negative] = 0.0
                return y_mean, np.sqrt(y_var)
            else:
                return y_mean
    def sample_y(self, X, n_samples=1, random_state=0):
                rng = check_random_state(random_state)
        y_mean, y_cov = self.predict(X, return_cov=True)
        if y_mean.ndim == 1:
            y_samples = rng.multivariate_normal(y_mean, y_cov, n_samples).T
        else:
            y_samples = \
                [rng.multivariate_normal(y_mean[:, i], y_cov,
                                         n_samples).T[:, np.newaxis]
                 for i in range(y_mean.shape[1])]
            y_samples = np.hstack(y_samples)
        return y_samples
    def log_marginal_likelihood(self, theta=None, eval_gradient=False):
                if theta is None:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated for theta!=None")
            return self.log_marginal_likelihood_value_
        kernel = self.kernel_.clone_with_theta(theta)
        if eval_gradient:
            K, K_gradient = kernel(self.X_train_, eval_gradient=True)
        else:
            K = kernel(self.X_train_)
        K[np.diag_indices_from(K)] += self.alpha
        try:
            L = cholesky(K, lower=True)          except np.linalg.LinAlgError:
            return (-np.inf, np.zeros_like(theta)) \
                if eval_gradient else -np.inf
                y_train = self.y_train_
        if y_train.ndim == 1:
            y_train = y_train[:, np.newaxis]
        alpha = cho_solve((L, True), y_train)  
                log_likelihood_dims = -0.5 * np.einsum("ik,ik->k", y_train, alpha)
        log_likelihood_dims -= np.log(np.diag(L)).sum()
        log_likelihood_dims -= K.shape[0] / 2 * np.log(2 * np.pi)
        log_likelihood = log_likelihood_dims.sum(-1)  
        if eval_gradient:              tmp = np.einsum("ik,jk->ijk", alpha, alpha)              tmp -= cho_solve((L, True), np.eye(K.shape[0]))[:, :, np.newaxis]
                                                log_likelihood_gradient_dims = \
                0.5 * np.einsum("ijl,ijk->kl", tmp, K_gradient)
            log_likelihood_gradient = log_likelihood_gradient_dims.sum(-1)
        if eval_gradient:
            return log_likelihood, log_likelihood_gradient
        else:
            return log_likelihood
    def _constrained_optimization(self, obj_func, initial_theta, bounds):
        if self.optimizer == "fmin_l_bfgs_b":
            theta_opt, func_min, convergence_dict = \
                fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)
            if convergence_dict["warnflag"] != 0:
                warnings.warn("fmin_l_bfgs_b terminated abnormally with the "
                              " state: %s" % convergence_dict)
        elif callable(self.optimizer):
            theta_opt, func_min = \
                self.optimizer(obj_func, initial_theta, bounds=bounds)
        else:
            raise ValueError("Unknown optimizer %s." % self.optimizer)
        return theta_opt, func_min

from abc import ABCMeta, abstractmethod
from collections import namedtuple
import math
import numpy as np
from scipy.special import kv, gamma
from scipy.spatial.distance import pdist, cdist, squareform
from ..metrics.pairwise import pairwise_kernels
from ..externals import six
from ..base import clone
from sklearn.externals.funcsigs import signature

def _check_length_scale(X, length_scale):
    length_scale = np.squeeze(length_scale).astype(float)
    if np.ndim(length_scale) > 1:
        raise ValueError("length_scale cannot be of dimension greater than 1")
    if np.ndim(length_scale) == 1 and X.shape[1] != length_scale.shape[0]:
        raise ValueError("Anisotropic kernel must have the same number of "
                         "dimensions as data (%d!=%d)"
                         % (length_scale.shape[0], X.shape[1]))
    return length_scale

class Hyperparameter(namedtuple('Hyperparameter',
                                ('name', 'value_type', 'bounds',
                                 'n_elements', 'fixed'))):
                                        __slots__ = ()
    def __new__(cls, name, value_type, bounds, n_elements=1, fixed=None):
        if not isinstance(bounds, six.string_types) or bounds != "fixed":
            bounds = np.atleast_2d(bounds)
            if n_elements > 1:                  if bounds.shape[0] == 1:
                    bounds = np.repeat(bounds, n_elements, 0)
                elif bounds.shape[0] != n_elements:
                    raise ValueError("Bounds on %s should have either 1 or "
                                     "%d dimensions. Given are %d"
                                     % (name, n_elements, bounds.shape[0]))
        if fixed is None:
            fixed = isinstance(bounds, six.string_types) and bounds == "fixed"
        return super(Hyperparameter, cls).__new__(
            cls, name, value_type, bounds, n_elements, fixed)
            def __eq__(self, other):
        return (self.name == other.name and
                self.value_type == other.value_type and
                np.all(self.bounds == other.bounds) and
                self.n_elements == other.n_elements and
                self.fixed == other.fixed)

class Kernel(six.with_metaclass(ABCMeta)):
    
    def get_params(self, deep=True):
                params = dict()
                        cls = self.__class__
        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
        init_sign = signature(init)
        args, varargs = [], []
        for parameter in init_sign.parameters.values():
            if (parameter.kind != parameter.VAR_KEYWORD and
                    parameter.name != 'self'):
                args.append(parameter.name)
            if parameter.kind == parameter.VAR_POSITIONAL:
                varargs.append(parameter.name)
        if len(varargs) != 0:
            raise RuntimeError("scikit-learn kernels should always "
                               "specify their parameters in the signature"
                               " of their __init__ (no varargs)."
                               " %s doesn't follow this convention."
                               % (cls, ))
        for arg in args:
            params[arg] = getattr(self, arg, None)
        return params
    def set_params(self, **params):
                if not params:
                        return self
        valid_params = self.get_params(deep=True)
        for key, value in six.iteritems(params):
            split = key.split('__', 1)
            if len(split) > 1:
                                name, sub_name = split
                if name not in valid_params:
                    raise ValueError('Invalid parameter %s for kernel %s. '
                                     'Check the list of available parameters '
                                     'with `kernel.get_params().keys()`.' %
                                     (name, self))
                sub_object = valid_params[name]
                sub_object.set_params(**{sub_name: value})
            else:
                                if key not in valid_params:
                    raise ValueError('Invalid parameter %s for kernel %s. '
                                     'Check the list of available parameters '
                                     'with `kernel.get_params().keys()`.' %
                                     (key, self.__class__.__name__))
                setattr(self, key, value)
        return self
    def clone_with_theta(self, theta):
                cloned = clone(self)
        cloned.theta = theta
        return cloned
    @property
    def n_dims(self):
                return self.theta.shape[0]
    @property
    def hyperparameters(self):
                r = []
        for attr in dir(self):
            if attr.startswith("hyperparameter_"):
                r.append(getattr(self, attr))
        return r
    @property
    def theta(self):
                theta = []
        params = self.get_params()
        for hyperparameter in self.hyperparameters:
            if not hyperparameter.fixed:
                theta.append(params[hyperparameter.name])
        if len(theta) > 0:
            return np.log(np.hstack(theta))
        else:
            return np.array([])
    @theta.setter
    def theta(self, theta):
                params = self.get_params()
        i = 0
        for hyperparameter in self.hyperparameters:
            if hyperparameter.fixed:
                continue
            if hyperparameter.n_elements > 1:
                                params[hyperparameter.name] = np.exp(
                    theta[i:i + hyperparameter.n_elements])
                i += hyperparameter.n_elements
            else:
                params[hyperparameter.name] = np.exp(theta[i])
                i += 1
        if i != len(theta):
            raise ValueError("theta has not the correct number of entries."
                             " Should be %d; given are %d"
                             % (i, len(theta)))
        self.set_params(**params)
    @property
    def bounds(self):
                bounds = []
        for hyperparameter in self.hyperparameters:
            if not hyperparameter.fixed:
                bounds.append(hyperparameter.bounds)
        if len(bounds) > 0:
            return np.log(np.vstack(bounds))
        else:
            return np.array([])
    def __add__(self, b):
        if not isinstance(b, Kernel):
            return Sum(self, ConstantKernel(b))
        return Sum(self, b)
    def __radd__(self, b):
        if not isinstance(b, Kernel):
            return Sum(ConstantKernel(b), self)
        return Sum(b, self)
    def __mul__(self, b):
        if not isinstance(b, Kernel):
            return Product(self, ConstantKernel(b))
        return Product(self, b)
    def __rmul__(self, b):
        if not isinstance(b, Kernel):
            return Product(ConstantKernel(b), self)
        return Product(b, self)
    def __pow__(self, b):
        return Exponentiation(self, b)
    def __eq__(self, b):
        if type(self) != type(b):
            return False
        params_a = self.get_params()
        params_b = b.get_params()
        for key in set(list(params_a.keys()) + list(params_b.keys())):
            if np.any(params_a.get(key, None) != params_b.get(key, None)):
                return False
        return True
    def __repr__(self):
        return "{0}({1})".format(self.__class__.__name__,
                                 ", ".join(map("{0:.3g}".format, self.theta)))
    @abstractmethod
    def __call__(self, X, Y=None, eval_gradient=False):
        
    @abstractmethod
    def diag(self, X):
        
    @abstractmethod
    def is_stationary(self):
        
class NormalizedKernelMixin(object):
    
    def diag(self, X):
                return np.ones(X.shape[0])

class StationaryKernelMixin(object):
    
    def is_stationary(self):
                return True

class CompoundKernel(Kernel):
    
    def __init__(self, kernels):
        self.kernels = kernels
    def get_params(self, deep=True):
                return dict(kernels=self.kernels)
    @property
    def theta(self):
                return np.hstack([kernel.theta for kernel in self.kernels])
    @theta.setter
    def theta(self, theta):
                k_dims = self.k1.n_dims
        for i, kernel in enumerate(self.kernels):
            kernel.theta = theta[i * k_dims:(i + 1) * k_dims]
    @property
    def bounds(self):
                return np.vstack([kernel.bounds for kernel in self.kernels])
    def __call__(self, X, Y=None, eval_gradient=False):
                if eval_gradient:
            K = []
            K_grad = []
            for kernel in self.kernels:
                K_single, K_grad_single = kernel(X, Y, eval_gradient)
                K.append(K_single)
                K_grad.append(K_grad_single[..., np.newaxis])
            return np.dstack(K), np.concatenate(K_grad, 3)
        else:
            return np.dstack([kernel(X, Y, eval_gradient)
                              for kernel in self.kernels])
    def __eq__(self, b):
        if type(self) != type(b) or len(self.kernels) != len(b.kernels):
            return False
        return np.all([self.kernels[i] == b.kernels[i]
                       for i in range(len(self.kernels))])
    def is_stationary(self):
                return np.all([kernel.is_stationary() for kernel in self.kernels])
    def diag(self, X):
                return np.vstack([kernel.diag(X) for kernel in self.kernels]).T

class KernelOperator(Kernel):
    
    def __init__(self, k1, k2):
        self.k1 = k1
        self.k2 = k2
    def get_params(self, deep=True):
                params = dict(k1=self.k1, k2=self.k2)
        if deep:
            deep_items = self.k1.get_params().items()
            params.update(('k1__' + k, val) for k, val in deep_items)
            deep_items = self.k2.get_params().items()
            params.update(('k2__' + k, val) for k, val in deep_items)
        return params
    @property
    def hyperparameters(self):
                r = []
        for hyperparameter in self.k1.hyperparameters:
            r.append(Hyperparameter("k1__" + hyperparameter.name,
                                    hyperparameter.value_type,
                                    hyperparameter.bounds,
                                    hyperparameter.n_elements))
        for hyperparameter in self.k2.hyperparameters:
            r.append(Hyperparameter("k2__" + hyperparameter.name,
                                    hyperparameter.value_type,
                                    hyperparameter.bounds,
                                    hyperparameter.n_elements))
        return r
    @property
    def theta(self):
                return np.append(self.k1.theta, self.k2.theta)
    @theta.setter
    def theta(self, theta):
                k1_dims = self.k1.n_dims
        self.k1.theta = theta[:k1_dims]
        self.k2.theta = theta[k1_dims:]
    @property
    def bounds(self):
                if self.k1.bounds.size == 0:
            return self.k2.bounds
        if self.k2.bounds.size == 0:
            return self.k1.bounds
        return np.vstack((self.k1.bounds, self.k2.bounds))
    def __eq__(self, b):
        if type(self) != type(b):
            return False
        return (self.k1 == b.k1 and self.k2 == b.k2) \
            or (self.k1 == b.k2 and self.k2 == b.k1)
    def is_stationary(self):
                return self.k1.is_stationary() and self.k2.is_stationary()

class Sum(KernelOperator):
    
    def __call__(self, X, Y=None, eval_gradient=False):
                if eval_gradient:
            K1, K1_gradient = self.k1(X, Y, eval_gradient=True)
            K2, K2_gradient = self.k2(X, Y, eval_gradient=True)
            return K1 + K2, np.dstack((K1_gradient, K2_gradient))
        else:
            return self.k1(X, Y) + self.k2(X, Y)
    def diag(self, X):
                return self.k1.diag(X) + self.k2.diag(X)
    def __repr__(self):
        return "{0} + {1}".format(self.k1, self.k2)

class Product(KernelOperator):
    
    def __call__(self, X, Y=None, eval_gradient=False):
                if eval_gradient:
            K1, K1_gradient = self.k1(X, Y, eval_gradient=True)
            K2, K2_gradient = self.k2(X, Y, eval_gradient=True)
            return K1 * K2, np.dstack((K1_gradient * K2[:, :, np.newaxis],
                                       K2_gradient * K1[:, :, np.newaxis]))
        else:
            return self.k1(X, Y) * self.k2(X, Y)
    def diag(self, X):
                return self.k1.diag(X) * self.k2.diag(X)
    def __repr__(self):
        return "{0} * {1}".format(self.k1, self.k2)

class Exponentiation(Kernel):
        def __init__(self, kernel, exponent):
        self.kernel = kernel
        self.exponent = exponent
    def get_params(self, deep=True):
                params = dict(kernel=self.kernel, exponent=self.exponent)
        if deep:
            deep_items = self.kernel.get_params().items()
            params.update(('kernel__' + k, val) for k, val in deep_items)
        return params
    @property
    def hyperparameters(self):
                r = []
        for hyperparameter in self.kernel.hyperparameters:
            r.append(Hyperparameter("kernel__" + hyperparameter.name,
                                    hyperparameter.value_type,
                                    hyperparameter.bounds,
                                    hyperparameter.n_elements))
        return r
    @property
    def theta(self):
                return self.kernel.theta
    @theta.setter
    def theta(self, theta):
                self.kernel.theta = theta
    @property
    def bounds(self):
                return self.kernel.bounds
    def __eq__(self, b):
        if type(self) != type(b):
            return False
        return (self.kernel == b.kernel and self.exponent == b.exponent)
    def __call__(self, X, Y=None, eval_gradient=False):
                if eval_gradient:
            K, K_gradient = self.kernel(X, Y, eval_gradient=True)
            K_gradient *= \
                self.exponent * K[:, :, np.newaxis] ** (self.exponent - 1)
            return K ** self.exponent, K_gradient
        else:
            K = self.kernel(X, Y, eval_gradient=False)
            return K ** self.exponent
    def diag(self, X):
                return self.kernel.diag(X) ** self.exponent
    def __repr__(self):
        return "{0} ** {1}".format(self.kernel, self.exponent)
    def is_stationary(self):
                return self.kernel.is_stationary()

class ConstantKernel(StationaryKernelMixin, Kernel):
        def __init__(self, constant_value=1.0, constant_value_bounds=(1e-5, 1e5)):
        self.constant_value = constant_value
        self.constant_value_bounds = constant_value_bounds
    @property
    def hyperparameter_constant_value(self):
        return Hyperparameter(
            "constant_value", "numeric", self.constant_value_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                X = np.atleast_2d(X)
        if Y is None:
            Y = X
        elif eval_gradient:
            raise ValueError("Gradient can only be evaluated when Y is None.")
        K = self.constant_value * np.ones((X.shape[0], Y.shape[0]))
        if eval_gradient:
            if not self.hyperparameter_constant_value.fixed:
                return (K, self.constant_value
                        * np.ones((X.shape[0], X.shape[0], 1)))
            else:
                return K, np.empty((X.shape[0], X.shape[0], 0))
        else:
            return K
    def diag(self, X):
                return self.constant_value * np.ones(X.shape[0])
    def __repr__(self):
        return "{0:.3g}**2".format(np.sqrt(self.constant_value))

class WhiteKernel(StationaryKernelMixin, Kernel):
        def __init__(self, noise_level=1.0, noise_level_bounds=(1e-5, 1e5)):
        self.noise_level = noise_level
        self.noise_level_bounds = noise_level_bounds
    @property
    def hyperparameter_noise_level(self):
        return Hyperparameter(
            "noise_level", "numeric", self.noise_level_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                X = np.atleast_2d(X)
        if Y is not None and eval_gradient:
            raise ValueError("Gradient can only be evaluated when Y is None.")
        if Y is None:
            K = self.noise_level * np.eye(X.shape[0])
            if eval_gradient:
                if not self.hyperparameter_noise_level.fixed:
                    return (K, self.noise_level
                            * np.eye(X.shape[0])[:, :, np.newaxis])
                else:
                    return K, np.empty((X.shape[0], X.shape[0], 0))
            else:
                return K
        else:
            return np.zeros((X.shape[0], Y.shape[0]))
    def diag(self, X):
                return self.noise_level * np.ones(X.shape[0])
    def __repr__(self):
        return "{0}(noise_level={1:.3g})".format(self.__class__.__name__,
                                                 self.noise_level)

class RBF(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
        def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):
        self.length_scale = length_scale
        self.length_scale_bounds = length_scale_bounds
    @property
    def anisotropic(self):
        return np.iterable(self.length_scale) and len(self.length_scale) > 1
    @property
    def hyperparameter_length_scale(self):
        if self.anisotropic:
            return Hyperparameter("length_scale", "numeric",
                                  self.length_scale_bounds,
                                  len(self.length_scale))
        return Hyperparameter(
            "length_scale", "numeric", self.length_scale_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                X = np.atleast_2d(X)
        length_scale = _check_length_scale(X, self.length_scale)
        if Y is None:
            dists = pdist(X / length_scale, metric='sqeuclidean')
            K = np.exp(-.5 * dists)
                        K = squareform(K)
            np.fill_diagonal(K, 1)
        else:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated when Y is None.")
            dists = cdist(X / length_scale, Y / length_scale,
                          metric='sqeuclidean')
            K = np.exp(-.5 * dists)
        if eval_gradient:
            if self.hyperparameter_length_scale.fixed:
                                return K, np.empty((X.shape[0], X.shape[0], 0))
            elif not self.anisotropic or length_scale.shape[0] == 1:
                K_gradient = \
                    (K * squareform(dists))[:, :, np.newaxis]
                return K, K_gradient
            elif self.anisotropic:
                                K_gradient = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 \
                    / (length_scale ** 2)
                K_gradient *= K[..., np.newaxis]
                return K, K_gradient
        else:
            return K
    def __repr__(self):
        if self.anisotropic:
            return "{0}(length_scale=[{1}])".format(
                self.__class__.__name__, ", ".join(map("{0:.3g}".format,
                                                   self.length_scale)))
        else:              return "{0}(length_scale={1:.3g})".format(
                self.__class__.__name__, np.ravel(self.length_scale)[0])

class Matern(RBF):
        def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5),
                 nu=1.5):
        super(Matern, self).__init__(length_scale, length_scale_bounds)
        self.nu = nu
    def __call__(self, X, Y=None, eval_gradient=False):
                X = np.atleast_2d(X)
        length_scale = _check_length_scale(X, self.length_scale)
        if Y is None:
            dists = pdist(X / length_scale, metric='euclidean')
        else:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated when Y is None.")
            dists = cdist(X / length_scale, Y / length_scale,
                          metric='euclidean')
        if self.nu == 0.5:
            K = np.exp(-dists)
        elif self.nu == 1.5:
            K = dists * math.sqrt(3)
            K = (1. + K) * np.exp(-K)
        elif self.nu == 2.5:
            K = dists * math.sqrt(5)
            K = (1. + K + K ** 2 / 3.0) * np.exp(-K)
        else:              K = dists
            K[K == 0.0] += np.finfo(float).eps              tmp = (math.sqrt(2 * self.nu) * K)
            K.fill((2 ** (1. - self.nu)) / gamma(self.nu))
            K *= tmp ** self.nu
            K *= kv(self.nu, tmp)
        if Y is None:
                        K = squareform(K)
            np.fill_diagonal(K, 1)
        if eval_gradient:
            if self.hyperparameter_length_scale.fixed:
                                K_gradient = np.empty((X.shape[0], X.shape[0], 0))
                return K, K_gradient
                        if self.anisotropic:
                D = (X[:, np.newaxis, :] - X[np.newaxis, :, :])**2 \
                    / (length_scale ** 2)
            else:
                D = squareform(dists**2)[:, :, np.newaxis]
            if self.nu == 0.5:
                K_gradient = K[..., np.newaxis] * D \
                    / np.sqrt(D.sum(2))[:, :, np.newaxis]
                K_gradient[~np.isfinite(K_gradient)] = 0
            elif self.nu == 1.5:
                K_gradient = \
                    3 * D * np.exp(-np.sqrt(3 * D.sum(-1)))[..., np.newaxis]
            elif self.nu == 2.5:
                tmp = np.sqrt(5 * D.sum(-1))[..., np.newaxis]
                K_gradient = 5.0 / 3.0 * D * (tmp + 1) * np.exp(-tmp)
            else:
                                def f(theta):                      return self.clone_with_theta(theta)(X, Y)
                return K, _approx_fprime(self.theta, f, 1e-10)
            if not self.anisotropic:
                return K, K_gradient[:, :].sum(-1)[:, :, np.newaxis]
            else:
                return K, K_gradient
        else:
            return K
    def __repr__(self):
        if self.anisotropic:
            return "{0}(length_scale=[{1}], nu={2:.3g})".format(
                self.__class__.__name__,
                ", ".join(map("{0:.3g}".format, self.length_scale)),
                self.nu)
        else:
            return "{0}(length_scale={1:.3g}, nu={2:.3g})".format(
                self.__class__.__name__, np.ravel(self.length_scale)[0],
                self.nu)

class RationalQuadratic(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
        def __init__(self, length_scale=1.0, alpha=1.0,
                 length_scale_bounds=(1e-5, 1e5), alpha_bounds=(1e-5, 1e5)):
        self.length_scale = length_scale
        self.alpha = alpha
        self.length_scale_bounds = length_scale_bounds
        self.alpha_bounds = alpha_bounds
    @property
    def hyperparameter_length_scale(self):
        return Hyperparameter(
            "length_scale", "numeric", self.length_scale_bounds)
    @property
    def hyperparameter_alpha(self):
        return Hyperparameter("alpha", "numeric", self.alpha_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                X = np.atleast_2d(X)
        if Y is None:
            dists = squareform(pdist(X, metric='sqeuclidean'))
            tmp = dists / (2 * self.alpha * self.length_scale ** 2)
            base = (1 + tmp)
            K = base ** -self.alpha
            np.fill_diagonal(K, 1)
        else:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated when Y is None.")
            dists = cdist(X, Y, metric='sqeuclidean')
            K = (1 + dists / (2 * self.alpha * self.length_scale ** 2)) \
                ** -self.alpha
        if eval_gradient:
                        if not self.hyperparameter_length_scale.fixed:
                length_scale_gradient = \
                    dists * K / (self.length_scale ** 2 * base)
                length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
            else:                  length_scale_gradient = np.empty((K.shape[0], K.shape[1], 0))
                        if not self.hyperparameter_alpha.fixed:
                alpha_gradient = \
                    K * (-self.alpha * np.log(base)
                         + dists / (2 * self.length_scale ** 2 * base))
                alpha_gradient = alpha_gradient[:, :, np.newaxis]
            else:                  alpha_gradient = np.empty((K.shape[0], K.shape[1], 0))
            return K, np.dstack((alpha_gradient, length_scale_gradient))
        else:
            return K
    def __repr__(self):
        return "{0}(alpha={1:.3g}, length_scale={2:.3g})".format(
            self.__class__.__name__, self.alpha, self.length_scale)

class ExpSineSquared(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
        def __init__(self, length_scale=1.0, periodicity=1.0,
                 length_scale_bounds=(1e-5, 1e5),
                 periodicity_bounds=(1e-5, 1e5)):
        self.length_scale = length_scale
        self.periodicity = periodicity
        self.length_scale_bounds = length_scale_bounds
        self.periodicity_bounds = periodicity_bounds
    @property
    def hyperparameter_length_scale(self):
        return Hyperparameter(
            "length_scale", "numeric", self.length_scale_bounds)
    @property
    def hyperparameter_periodicity(self):
        return Hyperparameter(
            "periodicity", "numeric", self.periodicity_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                X = np.atleast_2d(X)
        if Y is None:
            dists = squareform(pdist(X, metric='euclidean'))
            arg = np.pi * dists / self.periodicity
            sin_of_arg = np.sin(arg)
            K = np.exp(- 2 * (sin_of_arg / self.length_scale) ** 2)
        else:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated when Y is None.")
            dists = cdist(X, Y, metric='euclidean')
            K = np.exp(- 2 * (np.sin(np.pi / self.periodicity * dists)
                              / self.length_scale) ** 2)
        if eval_gradient:
            cos_of_arg = np.cos(arg)
                        if not self.hyperparameter_length_scale.fixed:
                length_scale_gradient = \
                    4 / self.length_scale**2 * sin_of_arg**2 * K
                length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
            else:                  length_scale_gradient = np.empty((K.shape[0], K.shape[1], 0))
                        if not self.hyperparameter_periodicity.fixed:
                periodicity_gradient = \
                    4 * arg / self.length_scale**2 * cos_of_arg \
                    * sin_of_arg * K
                periodicity_gradient = periodicity_gradient[:, :, np.newaxis]
            else:                  periodicity_gradient = np.empty((K.shape[0], K.shape[1], 0))
            return K, np.dstack((length_scale_gradient, periodicity_gradient))
        else:
            return K
    def __repr__(self):
        return "{0}(length_scale={1:.3g}, periodicity={2:.3g})".format(
            self.__class__.__name__, self.length_scale, self.periodicity)

class DotProduct(Kernel):
    
    def __init__(self, sigma_0=1.0, sigma_0_bounds=(1e-5, 1e5)):
        self.sigma_0 = sigma_0
        self.sigma_0_bounds = sigma_0_bounds
    @property
    def hyperparameter_sigma_0(self):
        return Hyperparameter("sigma_0", "numeric", self.sigma_0_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                X = np.atleast_2d(X)
        if Y is None:
            K = np.inner(X, X) + self.sigma_0 ** 2
        else:
            if eval_gradient:
                raise ValueError(
                    "Gradient can only be evaluated when Y is None.")
            K = np.inner(X, Y) + self.sigma_0 ** 2
        if eval_gradient:
            if not self.hyperparameter_sigma_0.fixed:
                K_gradient = np.empty((K.shape[0], K.shape[1], 1))
                K_gradient[..., 0] = 2 * self.sigma_0 ** 2
                return K, K_gradient
            else:
                return K, np.empty((X.shape[0], X.shape[0], 0))
        else:
            return K
    def diag(self, X):
                return np.einsum('ij,ij->i', X, X) + self.sigma_0 ** 2
    def is_stationary(self):
                return False
    def __repr__(self):
        return "{0}(sigma_0={1:.3g})".format(
            self.__class__.__name__, self.sigma_0)

def _approx_fprime(xk, f, epsilon, args=()):
    f0 = f(*((xk,) + args))
    grad = np.zeros((f0.shape[0], f0.shape[1], len(xk)), float)
    ei = np.zeros((len(xk), ), float)
    for k in range(len(xk)):
        ei[k] = 1.0
        d = epsilon * ei
        grad[:, :, k] = (f(*((xk + d,) + args)) - f0) / d[k]
        ei[k] = 0.0
    return grad

class PairwiseKernel(Kernel):
    
    def __init__(self, gamma=1.0, gamma_bounds=(1e-5, 1e5), metric="linear",
                 pairwise_kernels_kwargs=None):
        self.gamma = gamma
        self.gamma_bounds = gamma_bounds
        self.metric = metric
        self.pairwise_kernels_kwargs = pairwise_kernels_kwargs
    @property
    def hyperparameter_gamma(self):
        return Hyperparameter("gamma", "numeric", self.gamma_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                pairwise_kernels_kwargs = self.pairwise_kernels_kwargs
        if self.pairwise_kernels_kwargs is None:
            pairwise_kernels_kwargs = {}
        X = np.atleast_2d(X)
        K = pairwise_kernels(X, Y, metric=self.metric, gamma=self.gamma,
                             filter_params=True,
                             **pairwise_kernels_kwargs)
        if eval_gradient:
            if self.hyperparameter_gamma.fixed:
                return K, np.empty((X.shape[0], X.shape[0], 0))
            else:
                                def f(gamma):                      return pairwise_kernels(
                        X, Y, metric=self.metric, gamma=np.exp(gamma),
                        filter_params=True, **pairwise_kernels_kwargs)
                return K, _approx_fprime(self.theta, f, 1e-10)
        else:
            return K
    def diag(self, X):
                        return np.apply_along_axis(self, 1, X).ravel()
    def is_stationary(self):
                return self.metric in ["rbf"]
    def __repr__(self):
        return "{0}(gamma={1}, metric={2})".format(
            self.__class__.__name__, self.gamma, self.metric)


import numpy as np

def constant(x):
        x = np.asarray(x, dtype=np.float64)
    n_eval = x.shape[0]
    f = np.ones([n_eval, 1])
    return f

def linear(x):
        x = np.asarray(x, dtype=np.float64)
    n_eval = x.shape[0]
    f = np.hstack([np.ones([n_eval, 1]), x])
    return f

def quadratic(x):
    
    x = np.asarray(x, dtype=np.float64)
    n_eval, n_features = x.shape
    f = np.hstack([np.ones([n_eval, 1]), x])
    for k in range(n_features):
        f = np.hstack([f, x[:, k, np.newaxis] * x[:, k:]])
    return f

from .gpr import GaussianProcessRegressor
from .gpc import GaussianProcessClassifier
from . import kernels
from .gaussian_process import GaussianProcess
from . import correlation_models
from . import regression_models
__all__ = ['GaussianProcess', 'correlation_models', 'regression_models',
           'GaussianProcessRegressor', 'GaussianProcessClassifier',
           'kernels']

from __future__ import division
from abc import ABCMeta, abstractmethod
import numbers
import warnings
import numpy as np
import scipy.sparse as sp
from scipy import linalg
from scipy import sparse
from ..externals import six
from ..externals.joblib import Parallel, delayed
from ..base import BaseEstimator, ClassifierMixin, RegressorMixin
from ..utils import check_array, check_X_y, deprecated, as_float_array
from ..utils.validation import FLOAT_DTYPES
from ..utils import check_random_state
from ..utils.extmath import safe_sparse_dot
from ..utils.sparsefuncs import mean_variance_axis, inplace_column_scale
from ..utils.fixes import sparse_lsqr
from ..utils.seq_dataset import ArrayDataset, CSRDataset
from ..utils.validation import check_is_fitted
from ..exceptions import NotFittedError
from ..preprocessing.data import normalize as f_normalize

SPARSE_INTERCEPT_DECAY = 0.01

def make_dataset(X, y, sample_weight, random_state=None):
    
    rng = check_random_state(random_state)
        seed = rng.randint(1, np.iinfo(np.int32).max)
    if sp.issparse(X):
        dataset = CSRDataset(X.data, X.indptr, X.indices, y, sample_weight,
                             seed=seed)
        intercept_decay = SPARSE_INTERCEPT_DECAY
    else:
        dataset = ArrayDataset(X, y, sample_weight, seed=seed)
        intercept_decay = 1.0
    return dataset, intercept_decay

@deprecated("sparse_center_data was deprecated in version 0.18 and will be "
            "removed in 0.20. Use utilities in preprocessing.data instead")
def sparse_center_data(X, y, fit_intercept, normalize=False):
        if fit_intercept:
                                        if sp.isspmatrix(X) and X.getformat() == 'csr':
            X = sp.csr_matrix(X, copy=normalize, dtype=np.float64)
        else:
            X = sp.csc_matrix(X, copy=normalize, dtype=np.float64)
        X_offset, X_var = mean_variance_axis(X, axis=0)
        if normalize:
                        X_var *= X.shape[0]
            X_std = np.sqrt(X_var, X_var)
            del X_var
            X_std[X_std == 0] = 1
            inplace_column_scale(X, 1. / X_std)
        else:
            X_std = np.ones(X.shape[1])
        y_offset = y.mean(axis=0)
        y = y - y_offset
    else:
        X_offset = np.zeros(X.shape[1])
        X_std = np.ones(X.shape[1])
        y_offset = 0. if y.ndim == 1 else np.zeros(y.shape[1], dtype=X.dtype)
    return X, y, X_offset, y_offset, X_std

@deprecated("center_data was deprecated in version 0.18 and will be removed in "
            "0.20. Use utilities in preprocessing.data instead")
def center_data(X, y, fit_intercept, normalize=False, copy=True,
                sample_weight=None):
        X = as_float_array(X, copy)
    if fit_intercept:
        if isinstance(sample_weight, numbers.Number):
            sample_weight = None
        if sp.issparse(X):
            X_offset = np.zeros(X.shape[1])
            X_std = np.ones(X.shape[1])
        else:
            X_offset = np.average(X, axis=0, weights=sample_weight)
            X -= X_offset
                        if normalize:
                X_std = np.sqrt(np.sum(X ** 2, axis=0))
                X_std[X_std == 0] = 1
                X /= X_std
            else:
                X_std = np.ones(X.shape[1])
        y_offset = np.average(y, axis=0, weights=sample_weight)
        y = y - y_offset
    else:
        X_offset = np.zeros(X.shape[1])
        X_std = np.ones(X.shape[1])
        y_offset = 0. if y.ndim == 1 else np.zeros(y.shape[1], dtype=X.dtype)
    return X, y, X_offset, y_offset, X_std

def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,
                     sample_weight=None, return_mean=False):
    
    if isinstance(sample_weight, numbers.Number):
        sample_weight = None
    X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],
                    dtype=FLOAT_DTYPES)
    if fit_intercept:
        if sp.issparse(X):
            X_offset, X_var = mean_variance_axis(X, axis=0)
            if not return_mean:
                X_offset = np.zeros(X.shape[1])
            if normalize:
                                                
                                X_var *= X.shape[0]
                X_scale = np.sqrt(X_var, X_var)
                del X_var
                X_scale[X_scale == 0] = 1
                inplace_column_scale(X, 1. / X_scale)
            else:
                X_scale = np.ones(X.shape[1])
        else:
            X_offset = np.average(X, axis=0, weights=sample_weight)
            X -= X_offset
            if normalize:
                X, X_scale = f_normalize(X, axis=0, copy=False,
                                         return_norm=True)
            else:
                X_scale = np.ones(X.shape[1])
        y_offset = np.average(y, axis=0, weights=sample_weight)
        y = y - y_offset
    else:
        X_offset = np.zeros(X.shape[1])
        X_scale = np.ones(X.shape[1])
        y_offset = 0. if y.ndim == 1 else np.zeros(y.shape[1], dtype=X.dtype)
    return X, y, X_offset, y_offset, X_scale

def _rescale_data(X, y, sample_weight):
        n_samples = X.shape[0]
    sample_weight = sample_weight * np.ones(n_samples)
    sample_weight = np.sqrt(sample_weight)
    sw_matrix = sparse.dia_matrix((sample_weight, 0),
                                  shape=(n_samples, n_samples))
    X = safe_sparse_dot(sw_matrix, X)
    y = safe_sparse_dot(sw_matrix, y)
    return X, y

class LinearModel(six.with_metaclass(ABCMeta, BaseEstimator)):
    
    @abstractmethod
    def fit(self, X, y):
        
    def _decision_function(self, X):
        check_is_fitted(self, "coef_")
        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])
        return safe_sparse_dot(X, self.coef_.T,
                               dense_output=True) + self.intercept_
    def predict(self, X):
                return self._decision_function(X)
    _preprocess_data = staticmethod(_preprocess_data)
    def _set_intercept(self, X_offset, y_offset, X_scale):
                if self.fit_intercept:
            self.coef_ = self.coef_ / X_scale
            self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)
        else:
            self.intercept_ = 0.

class LinearClassifierMixin(ClassifierMixin):
    
    def decision_function(self, X):
                if not hasattr(self, 'coef_') or self.coef_ is None:
            raise NotFittedError("This %(name)s instance is not fitted "
                                 "yet" % {'name': type(self).__name__})
        X = check_array(X, accept_sparse='csr')
        n_features = self.coef_.shape[1]
        if X.shape[1] != n_features:
            raise ValueError("X has %d features per sample; expecting %d"
                             % (X.shape[1], n_features))
        scores = safe_sparse_dot(X, self.coef_.T,
                                 dense_output=True) + self.intercept_
        return scores.ravel() if scores.shape[1] == 1 else scores
    def predict(self, X):
                scores = self.decision_function(X)
        if len(scores.shape) == 1:
            indices = (scores > 0).astype(np.int)
        else:
            indices = scores.argmax(axis=1)
        return self.classes_[indices]
    def _predict_proba_lr(self, X):
                prob = self.decision_function(X)
        prob *= -1
        np.exp(prob, prob)
        prob += 1
        np.reciprocal(prob, prob)
        if prob.ndim == 1:
            return np.vstack([1 - prob, prob]).T
        else:
                        prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))
            return prob

class SparseCoefMixin(object):
    
    def densify(self):
                msg = "Estimator, %(name)s, must be fitted before densifying."
        check_is_fitted(self, "coef_", msg=msg)
        if sp.issparse(self.coef_):
            self.coef_ = self.coef_.toarray()
        return self
    def sparsify(self):
                msg = "Estimator, %(name)s, must be fitted before sparsifying."
        check_is_fitted(self, "coef_", msg=msg)
        self.coef_ = sp.csr_matrix(self.coef_)
        return self

class LinearRegression(LinearModel, RegressorMixin):
    
    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,
                 n_jobs=1):
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.copy_X = copy_X
        self.n_jobs = n_jobs
    def fit(self, X, y, sample_weight=None):
        
        n_jobs_ = self.n_jobs
        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],
                         y_numeric=True, multi_output=True)
        if sample_weight is not None and np.atleast_1d(sample_weight).ndim > 1:
            raise ValueError("Sample weights must be 1D array or scalar")
        X, y, X_offset, y_offset, X_scale = self._preprocess_data(
            X, y, fit_intercept=self.fit_intercept, normalize=self.normalize,
            copy=self.copy_X, sample_weight=sample_weight)
        if sample_weight is not None:
                        X, y = _rescale_data(X, y, sample_weight)
        if sp.issparse(X):
            if y.ndim < 2:
                out = sparse_lsqr(X, y)
                self.coef_ = out[0]
                self._residues = out[3]
            else:
                                outs = Parallel(n_jobs=n_jobs_)(
                    delayed(sparse_lsqr)(X, y[:, j].ravel())
                    for j in range(y.shape[1]))
                self.coef_ = np.vstack(out[0] for out in outs)
                self._residues = np.vstack(out[3] for out in outs)
        else:
            self.coef_, self._residues, self.rank_, self.singular_ = \
                linalg.lstsq(X, y)
            self.coef_ = self.coef_.T
        if y.ndim == 1:
            self.coef_ = np.ravel(self.coef_)
        self._set_intercept(X_offset, y_offset, X_scale)
        return self

def _pre_fit(X, y, Xy, precompute, normalize, fit_intercept, copy):
        n_samples, n_features = X.shape
    if sparse.isspmatrix(X):
        precompute = False
        X, y, X_offset, y_offset, X_scale = _preprocess_data(
            X, y, fit_intercept=fit_intercept, normalize=normalize,
            return_mean=True)
    else:
                X, y, X_offset, y_offset, X_scale = _preprocess_data(
            X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy)
    if hasattr(precompute, '__array__') and (
            fit_intercept and not np.allclose(X_offset, np.zeros(n_features)) or
            normalize and not np.allclose(X_scale, np.ones(n_features))):
        warnings.warn("Gram matrix was provided but X was centered"
                      " to fit intercept, "
                      "or X was normalized : recomputing Gram matrix.",
                      UserWarning)
                precompute = 'auto'
        Xy = None
        if isinstance(precompute, six.string_types) and precompute == 'auto':
        precompute = (n_samples > n_features)
    if precompute is True:
                precompute = np.empty(shape=(n_features, n_features), dtype=X.dtype,
                              order='C')
        np.dot(X.T, X, out=precompute)
    if not hasattr(precompute, '__array__'):
        Xy = None  
    if hasattr(precompute, '__array__') and Xy is None:
        common_dtype = np.find_common_type([X.dtype, y.dtype], [])
        if y.ndim == 1:
                        Xy = np.empty(shape=n_features, dtype=common_dtype, order='C')
            np.dot(X.T, y, out=Xy)
        else:
                                                n_targets = y.shape[1]
            Xy = np.empty(shape=(n_features, n_targets), dtype=common_dtype,
                          order='F')
            np.dot(y.T, X, out=Xy.T)
    return X, y, X_offset, y_offset, X_scale, precompute, Xy
from __future__ import print_function

from math import log
import numpy as np
from scipy import linalg
from .base import LinearModel
from ..base import RegressorMixin
from ..utils.extmath import fast_logdet, pinvh
from ..utils import check_X_y

class BayesianRidge(LinearModel, RegressorMixin):
    
    def __init__(self, n_iter=300, tol=1.e-3, alpha_1=1.e-6, alpha_2=1.e-6,
                 lambda_1=1.e-6, lambda_2=1.e-6, compute_score=False,
                 fit_intercept=True, normalize=False, copy_X=True,
                 verbose=False):
        self.n_iter = n_iter
        self.tol = tol
        self.alpha_1 = alpha_1
        self.alpha_2 = alpha_2
        self.lambda_1 = lambda_1
        self.lambda_2 = lambda_2
        self.compute_score = compute_score
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.copy_X = copy_X
        self.verbose = verbose
    def fit(self, X, y):
                X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)
        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(
            X, y, self.fit_intercept, self.normalize, self.copy_X)
        self.X_offset_ = X_offset_
        self.X_scale_ = X_scale_
        n_samples, n_features = X.shape
                alpha_ = 1. / np.var(y)
        lambda_ = 1.
        verbose = self.verbose
        lambda_1 = self.lambda_1
        lambda_2 = self.lambda_2
        alpha_1 = self.alpha_1
        alpha_2 = self.alpha_2
        self.scores_ = list()
        coef_old_ = None
        XT_y = np.dot(X.T, y)
        U, S, Vh = linalg.svd(X, full_matrices=False)
        eigen_vals_ = S ** 2
                for iter_ in range(self.n_iter):
                                                if n_samples > n_features:
                coef_ = np.dot(Vh.T,
                               Vh / (eigen_vals_ +
                                     lambda_ / alpha_)[:, np.newaxis])
                coef_ = np.dot(coef_, XT_y)
                if self.compute_score:
                    logdet_sigma_ = - np.sum(
                        np.log(lambda_ + alpha_ * eigen_vals_))
            else:
                coef_ = np.dot(X.T, np.dot(
                    U / (eigen_vals_ + lambda_ / alpha_)[None, :], U.T))
                coef_ = np.dot(coef_, y)
                if self.compute_score:
                    logdet_sigma_ = lambda_ * np.ones(n_features)
                    logdet_sigma_[:n_samples] += alpha_ * eigen_vals_
                    logdet_sigma_ = - np.sum(np.log(logdet_sigma_))
                                    self.alpha_ = alpha_
            self.lambda_ = lambda_
                        rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)
            gamma_ = (np.sum((alpha_ * eigen_vals_) /
                      (lambda_ + alpha_ * eigen_vals_)))
            lambda_ = ((gamma_ + 2 * lambda_1) /
                       (np.sum(coef_ ** 2) + 2 * lambda_2))
            alpha_ = ((n_samples - gamma_ + 2 * alpha_1) /
                      (rmse_ + 2 * alpha_2))
                        if self.compute_score:
                s = lambda_1 * log(lambda_) - lambda_2 * lambda_
                s += alpha_1 * log(alpha_) - alpha_2 * alpha_
                s += 0.5 * (n_features * log(lambda_) +
                            n_samples * log(alpha_) -
                            alpha_ * rmse_ -
                            (lambda_ * np.sum(coef_ ** 2)) -
                            logdet_sigma_ -
                            n_samples * log(2 * np.pi))
                self.scores_.append(s)
                        if iter_ != 0 and np.sum(np.abs(coef_old_ - coef_)) < self.tol:
                if verbose:
                    print("Convergence after ", str(iter_), " iterations")
                break
            coef_old_ = np.copy(coef_)
        self.coef_ = coef_
        sigma_ = np.dot(Vh.T,
                        Vh / (eigen_vals_ + lambda_ / alpha_)[:, np.newaxis])
        self.sigma_ = (1. / alpha_) * sigma_
        self._set_intercept(X_offset_, y_offset_, X_scale_)
        return self
    def predict(self, X, return_std=False):
                y_mean = self._decision_function(X)
        if return_std is False:
            return y_mean
        else:
            if self.normalize:
                X = (X - self.X_offset_) / self.X_scale_
            sigmas_squared_data = (np.dot(X, self.sigma_) * X).sum(axis=1)
            y_std = np.sqrt(sigmas_squared_data + (1. / self.alpha_))
            return y_mean, y_std


class ARDRegression(LinearModel, RegressorMixin):
    
    def __init__(self, n_iter=300, tol=1.e-3, alpha_1=1.e-6, alpha_2=1.e-6,
                 lambda_1=1.e-6, lambda_2=1.e-6, compute_score=False,
                 threshold_lambda=1.e+4, fit_intercept=True, normalize=False,
                 copy_X=True, verbose=False):
        self.n_iter = n_iter
        self.tol = tol
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.alpha_1 = alpha_1
        self.alpha_2 = alpha_2
        self.lambda_1 = lambda_1
        self.lambda_2 = lambda_2
        self.compute_score = compute_score
        self.threshold_lambda = threshold_lambda
        self.copy_X = copy_X
        self.verbose = verbose
    def fit(self, X, y):
                X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)
        n_samples, n_features = X.shape
        coef_ = np.zeros(n_features)
        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(
            X, y, self.fit_intercept, self.normalize, self.copy_X)
                keep_lambda = np.ones(n_features, dtype=bool)
        lambda_1 = self.lambda_1
        lambda_2 = self.lambda_2
        alpha_1 = self.alpha_1
        alpha_2 = self.alpha_2
        verbose = self.verbose
                alpha_ = 1. / np.var(y)
        lambda_ = np.ones(n_features)
        self.scores_ = list()
        coef_old_ = None
                for iter_ in range(self.n_iter):
                        sigma_ = pinvh(np.eye(n_samples) / alpha_ +
                           np.dot(X[:, keep_lambda] *
                           np.reshape(1. / lambda_[keep_lambda], [1, -1]),
                           X[:, keep_lambda].T))
            sigma_ = np.dot(sigma_, X[:, keep_lambda] *
                            np.reshape(1. / lambda_[keep_lambda], [1, -1]))
            sigma_ = - np.dot(np.reshape(1. / lambda_[keep_lambda], [-1, 1]) *
                              X[:, keep_lambda].T, sigma_)
            sigma_.flat[::(sigma_.shape[1] + 1)] += 1. / lambda_[keep_lambda]
            coef_[keep_lambda] = alpha_ * np.dot(
                sigma_, np.dot(X[:, keep_lambda].T, y))
                        rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)
            gamma_ = 1. - lambda_[keep_lambda] * np.diag(sigma_)
            lambda_[keep_lambda] = ((gamma_ + 2. * lambda_1) /
                                    ((coef_[keep_lambda]) ** 2 +
                                     2. * lambda_2))
            alpha_ = ((n_samples - gamma_.sum() + 2. * alpha_1) /
                      (rmse_ + 2. * alpha_2))
                        keep_lambda = lambda_ < self.threshold_lambda
            coef_[~keep_lambda] = 0
                        if self.compute_score:
                s = (lambda_1 * np.log(lambda_) - lambda_2 * lambda_).sum()
                s += alpha_1 * log(alpha_) - alpha_2 * alpha_
                s += 0.5 * (fast_logdet(sigma_) + n_samples * log(alpha_) +
                            np.sum(np.log(lambda_)))
                s -= 0.5 * (alpha_ * rmse_ + (lambda_ * coef_ ** 2).sum())
                self.scores_.append(s)
                        if iter_ > 0 and np.sum(np.abs(coef_old_ - coef_)) < self.tol:
                if verbose:
                    print("Converged after %s iterations" % iter_)
                break
            coef_old_ = np.copy(coef_)
        self.coef_ = coef_
        self.alpha_ = alpha_
        self.sigma_ = sigma_
        self.lambda_ = lambda_
        self._set_intercept(X_offset_, y_offset_, X_scale_)
        return self
    def predict(self, X, return_std=False):
                y_mean = self._decision_function(X)
        if return_std is False:
            return y_mean
        else:
            if self.normalize:
                X = (X - self.X_offset_) / self.X_scale_
            X = X[:, self.lambda_ < self.threshold_lambda]
            sigmas_squared_data = (np.dot(X, self.sigma_) * X).sum(axis=1)
            y_std = np.sqrt(sigmas_squared_data + (1. / self.alpha_))
            return y_mean, y_std
from libc.math cimport fabs
cimport numpy as np
import numpy as np
import numpy.linalg as linalg
cimport cython
from cpython cimport bool
from cython cimport floating
import warnings
ctypedef np.float64_t DOUBLE
ctypedef np.uint32_t UINT32_t
ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,
                         int incY) nogil
ctypedef void (*AXPY)(int N, floating alpha, floating *X, int incX,
                      floating *Y, int incY) nogil
ctypedef floating (*ASUM)(int N, floating *X, int incX) nogil
np.import_array()

cdef enum:
                RAND_R_MAX = 0x7FFFFFFF

cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:
    seed[0] ^= <UINT32_t>(seed[0] << 13)
    seed[0] ^= <UINT32_t>(seed[0] >> 17)
    seed[0] ^= <UINT32_t>(seed[0] << 5)
    return seed[0] % (<UINT32_t>RAND_R_MAX + 1)

cdef inline UINT32_t rand_int(UINT32_t end, UINT32_t* random_state) nogil:
        return our_rand_r(random_state) % end

cdef inline floating fmax(floating x, floating y) nogil:
    if x > y:
        return x
    return y

cdef inline floating fsign(floating f) nogil:
    if f == 0:
        return 0
    elif f > 0:
        return 1.0
    else:
        return -1.0

cdef floating abs_max(int n, floating* a) nogil:
        cdef int i
    cdef floating m = fabs(a[0])
    cdef floating d
    for i in range(1, n):
        d = fabs(a[i])
        if d > m:
            m = d
    return m

cdef floating max(int n, floating* a) nogil:
        cdef int i
    cdef floating m = a[0]
    cdef floating d
    for i in range(1, n):
        d = a[i]
        if d > m:
            m = d
    return m

cdef floating diff_abs_max(int n, floating* a, floating* b) nogil:
        cdef int i
    cdef floating m = fabs(a[0] - b[0])
    cdef floating d
    for i in range(1, n):
        d = fabs(a[i] - b[i])
        if d > m:
            m = d
    return m

cdef extern from "cblas.h":
    enum CBLAS_ORDER:
        CblasRowMajor=101
        CblasColMajor=102
    enum CBLAS_TRANSPOSE:
        CblasNoTrans=111
        CblasTrans=112
        CblasConjTrans=113
        AtlasConj=114
    void daxpy "cblas_daxpy"(int N, double alpha, double *X, int incX,
                             double *Y, int incY) nogil
    void saxpy "cblas_saxpy"(int N, float alpha, float *X, int incX,
                             float *Y, int incY) nogil
    double ddot "cblas_ddot"(int N, double *X, int incX, double *Y, int incY
                             ) nogil
    float sdot "cblas_sdot"(int N, float *X, int incX, float *Y,
                            int incY) nogil
    double dasum "cblas_dasum"(int N, double *X, int incX) nogil
    float sasum "cblas_sasum"(int N, float *X, int incX) nogil
    void dger "cblas_dger"(CBLAS_ORDER Order, int M, int N, double alpha,
                           double *X, int incX, double *Y, int incY,
                           double *A, int lda) nogil
    void sger "cblas_sger"(CBLAS_ORDER Order, int M, int N, float alpha,
                           float *X, int incX, float *Y, int incY,
                           float *A, int lda) nogil
    void dgemv "cblas_dgemv"(CBLAS_ORDER Order, CBLAS_TRANSPOSE TransA,
                             int M, int N, double alpha, double *A, int lda,
                             double *X, int incX, double beta,
                             double *Y, int incY) nogil
    void sgemv "cblas_sgemv"(CBLAS_ORDER Order, CBLAS_TRANSPOSE TransA,
                             int M, int N, float alpha, float *A, int lda,
                             float *X, int incX, float beta,
                             float *Y, int incY) nogil
    double dnrm2 "cblas_dnrm2"(int N, double *X, int incX) nogil
    float snrm2 "cblas_snrm2"(int N, float *X, int incX) nogil
    void dcopy "cblas_dcopy"(int N, double *X, int incX, double *Y,
                             int incY) nogil
    void scopy "cblas_scopy"(int N, float *X, int incX, float *Y,
                            int incY) nogil
    void dscal "cblas_dscal"(int N, double alpha, double *X, int incX) nogil
    void sscal "cblas_sscal"(int N, float alpha, float *X, int incX) nogil

@cython.boundscheck(False)
@cython.wraparound(False)
@cython.cdivision(True)
def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,
                            floating alpha, floating beta,
                            np.ndarray[floating, ndim=2, mode='fortran'] X,
                            np.ndarray[floating, ndim=1, mode='c'] y,
                            int max_iter, floating tol,
                            object rng, bint random=0, bint positive=0):
    
        cdef DOT dot
    cdef AXPY axpy
    cdef ASUM asum
    if floating is float:
        dtype = np.float32
        dot = sdot
        axpy = saxpy
        asum = sasum
    else:
        dtype = np.float64
        dot = ddot
        axpy = daxpy
        asum = dasum
        cdef unsigned int n_samples = X.shape[0]
    cdef unsigned int n_features = X.shape[1]
        cdef unsigned int n_tasks = y.strides[0] / sizeof(floating)
        cdef np.ndarray[floating, ndim=1] norm_cols_X = (X**2).sum(axis=0)
        cdef np.ndarray[floating, ndim=1] R = np.empty(n_samples, dtype=dtype)
    cdef np.ndarray[floating, ndim=1] XtA = np.empty(n_features, dtype=dtype)
    cdef floating tmp
    cdef floating w_ii
    cdef floating d_w_max
    cdef floating w_max
    cdef floating d_w_ii
    cdef floating gap = tol + 1.0
    cdef floating d_w_tol = tol
    cdef floating dual_norm_XtA
    cdef floating R_norm2
    cdef floating w_norm2
    cdef floating l1_norm
    cdef floating const
    cdef floating A_norm2
    cdef unsigned int ii
    cdef unsigned int i
    cdef unsigned int n_iter = 0
    cdef unsigned int f_iter
    cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
    cdef UINT32_t* rand_r_state = &rand_r_state_seed
    cdef floating *X_data = <floating*> X.data
    cdef floating *y_data = <floating*> y.data
    cdef floating *w_data = <floating*> w.data
    cdef floating *R_data = <floating*> R.data
    cdef floating *XtA_data = <floating*> XtA.data
    if alpha == 0 and beta == 0:
        warnings.warn("Coordinate descent with no regularization may lead to unexpected"
            " results and is discouraged.")
    with nogil:
                for i in range(n_samples):
            R[i] = y[i] - dot(n_features, &X_data[i], n_samples, w_data, 1)
                tol *= dot(n_samples, y_data, n_tasks, y_data, n_tasks)
        for n_iter in range(max_iter):
            w_max = 0.0
            d_w_max = 0.0
            for f_iter in range(n_features):                  if random:
                    ii = rand_int(n_features, rand_r_state)
                else:
                    ii = f_iter
                if norm_cols_X[ii] == 0.0:
                    continue
                w_ii = w[ii]  
                if w_ii != 0.0:
                                        axpy(n_samples, w_ii, &X_data[ii * n_samples], 1,
                         R_data, 1)
                                tmp = dot(n_samples, &X_data[ii * n_samples], 1, R_data, 1)
                if positive and tmp < 0:
                    w[ii] = 0.0
                else:
                    w[ii] = (fsign(tmp) * fmax(fabs(tmp) - alpha, 0)
                             / (norm_cols_X[ii] + beta))
                if w[ii] != 0.0:
                                        axpy(n_samples, -w[ii], &X_data[ii * n_samples], 1,
                         R_data, 1)
                                d_w_ii = fabs(w[ii] - w_ii)
                if d_w_ii > d_w_max:
                    d_w_max = d_w_ii
                if fabs(w[ii]) > w_max:
                    w_max = fabs(w[ii])
            if (w_max == 0.0 or
                d_w_max / w_max < d_w_tol or
                n_iter == max_iter - 1):
                                                
                                for i in range(n_features):
                    XtA[i] = dot(n_samples, &X_data[i * n_samples],
                                 1, R_data, 1) - beta * w[i]
                if positive:
                    dual_norm_XtA = max(n_features, XtA_data)
                else:
                    dual_norm_XtA = abs_max(n_features, XtA_data)
                                R_norm2 = dot(n_samples, R_data, 1, R_data, 1)
                                w_norm2 = dot(n_features, w_data, 1, w_data, 1)
                if (dual_norm_XtA > alpha):
                    const = alpha / dual_norm_XtA
                    A_norm2 = R_norm2 * (const ** 2)
                    gap = 0.5 * (R_norm2 + A_norm2)
                else:
                    const = 1.0
                    gap = R_norm2
                l1_norm = asum(n_features, w_data, 1)
                                gap += (alpha * l1_norm
                        - const * dot(n_samples, R_data, 1, y_data, n_tasks)
                        + 0.5 * beta * (1 + const ** 2) * (w_norm2))
                if gap < tol:
                                        break
    return w, gap, tol, n_iter + 1

@cython.boundscheck(False)
@cython.wraparound(False)
@cython.cdivision(True)
def sparse_enet_coordinate_descent(floating [:] w,
                            floating alpha, floating beta,
                            np.ndarray[floating, ndim=1, mode='c'] X_data,
                            np.ndarray[int, ndim=1, mode='c'] X_indices,
                            np.ndarray[int, ndim=1, mode='c'] X_indptr,
                            np.ndarray[floating, ndim=1] y,
                            floating[:] X_mean, int max_iter,
                            floating tol, object rng, bint random=0,
                            bint positive=0):
    
        cdef unsigned int n_samples = y.shape[0]
    cdef unsigned int n_features = w.shape[0]
        cdef unsigned int ii
    cdef floating[:] norm_cols_X
    cdef unsigned int startptr = X_indptr[0]
    cdef unsigned int endptr
        cdef unsigned int n_tasks
        cdef floating[:] R = y.copy()
    cdef floating[:] X_T_R
    cdef floating[:] XtA
        cdef DOT dot
    cdef ASUM asum
    if floating is float:
        dtype = np.float32
        n_tasks = y.strides[0] / sizeof(float)
        dot = sdot
        asum = sasum
    else:
        dtype = np.float64
        n_tasks = y.strides[0] / sizeof(DOUBLE)
        dot = ddot
        asum = dasum
    norm_cols_X = np.zeros(n_features, dtype=dtype)
    X_T_R = np.zeros(n_features, dtype=dtype)
    XtA = np.zeros(n_features, dtype=dtype)
    cdef floating tmp
    cdef floating w_ii
    cdef floating d_w_max
    cdef floating w_max
    cdef floating d_w_ii
    cdef floating X_mean_ii
    cdef floating R_sum = 0.0
    cdef floating R_norm2
    cdef floating w_norm2
    cdef floating A_norm2
    cdef floating l1_norm
    cdef floating normalize_sum
    cdef floating gap = tol + 1.0
    cdef floating d_w_tol = tol
    cdef floating dual_norm_XtA
    cdef unsigned int jj
    cdef unsigned int n_iter = 0
    cdef unsigned int f_iter
    cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
    cdef UINT32_t* rand_r_state = &rand_r_state_seed
    cdef bint center = False
    with nogil:
                for ii in range(n_features):
            if X_mean[ii]:
                center = True
                break
        for ii in range(n_features):
            X_mean_ii = X_mean[ii]
            endptr = X_indptr[ii + 1]
            normalize_sum = 0.0
            w_ii = w[ii]
            for jj in range(startptr, endptr):
                normalize_sum += (X_data[jj] - X_mean_ii) ** 2
                R[X_indices[jj]] -= X_data[jj] * w_ii
            norm_cols_X[ii] = normalize_sum + \
                (n_samples - endptr + startptr) * X_mean_ii ** 2
            if center:
                for jj in range(n_samples):
                    R[jj] += X_mean_ii * w_ii
            startptr = endptr
                tol *= dot(n_samples, &y[0], 1, &y[0], 1)
        for n_iter in range(max_iter):
            w_max = 0.0
            d_w_max = 0.0
            for f_iter in range(n_features):                  if random:
                    ii = rand_int(n_features, rand_r_state)
                else:
                    ii = f_iter
                if norm_cols_X[ii] == 0.0:
                    continue
                startptr = X_indptr[ii]
                endptr = X_indptr[ii + 1]
                w_ii = w[ii]                  X_mean_ii = X_mean[ii]
                if w_ii != 0.0:
                                        for jj in range(startptr, endptr):
                        R[X_indices[jj]] += X_data[jj] * w_ii
                    if center:
                        for jj in range(n_samples):
                            R[jj] -= X_mean_ii * w_ii
                                tmp = 0.0
                for jj in range(startptr, endptr):
                    tmp += R[X_indices[jj]] * X_data[jj]
                if center:
                    R_sum = 0.0
                    for jj in range(n_samples):
                        R_sum += R[jj]
                    tmp -= R_sum * X_mean_ii
                if positive and tmp < 0.0:
                    w[ii] = 0.0
                else:
                    w[ii] = fsign(tmp) * fmax(fabs(tmp) - alpha, 0) \
                            / (norm_cols_X[ii] + beta)
                if w[ii] != 0.0:
                                        for jj in range(startptr, endptr):
                        R[X_indices[jj]] -= X_data[jj] * w[ii]
                    if center:
                        for jj in range(n_samples):
                            R[jj] += X_mean_ii * w[ii]
                                d_w_ii = fabs(w[ii] - w_ii)
                if d_w_ii > d_w_max:
                    d_w_max = d_w_ii
                if w[ii] > w_max:
                    w_max = w[ii]
            if w_max == 0.0 or d_w_max / w_max < d_w_tol or n_iter == max_iter - 1:
                                                
                                if center:
                    R_sum = 0.0
                    for jj in range(n_samples):
                        R_sum += R[jj]
                for ii in range(n_features):
                    X_T_R[ii] = 0.0
                    for jj in range(X_indptr[ii], X_indptr[ii + 1]):
                        X_T_R[ii] += X_data[jj] * R[X_indices[jj]]
                    if center:
                        X_T_R[ii] -= X_mean[ii] * R_sum
                    XtA[ii] = X_T_R[ii] - beta * w[ii]
                if positive:
                    dual_norm_XtA = max(n_features, &XtA[0])
                else:
                    dual_norm_XtA = abs_max(n_features, &XtA[0])
                                R_norm2 = dot(n_samples, &R[0], 1, &R[0], 1)
                                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
                if (dual_norm_XtA > alpha):
                    const = alpha / dual_norm_XtA
                    A_norm2 = R_norm2 * const**2
                    gap = 0.5 * (R_norm2 + A_norm2)
                else:
                    const = 1.0
                    gap = R_norm2
                l1_norm = asum(n_features, &w[0], 1)
                gap += (alpha * l1_norm - const * dot(
                            n_samples,
                            &R[0], 1,
                            &y[0], n_tasks
                            )
                        + 0.5 * beta * (1 + const ** 2) * w_norm2)
                if gap < tol:
                                        break
    return w, gap, tol, n_iter + 1

@cython.boundscheck(False)
@cython.wraparound(False)
@cython.cdivision(True)
def enet_coordinate_descent_gram(floating[:] w, floating alpha, floating beta,
                                 np.ndarray[floating, ndim=2, mode='c'] Q,
                                 np.ndarray[floating, ndim=1, mode='c'] q,
                                 np.ndarray[floating, ndim=1] y,
                                 int max_iter, floating tol, object rng,
                                 bint random=0, bint positive=0):
    
        cdef DOT dot
    cdef AXPY axpy
    cdef ASUM asum
    if floating is float:
        dtype = np.float32
        dot = sdot
        axpy = saxpy
        asum = sasum
    else:
        dtype = np.float64
        dot = ddot
        axpy = daxpy
        asum = dasum
        cdef unsigned int n_samples = y.shape[0]
    cdef unsigned int n_features = Q.shape[0]
        cdef floating[:] H = np.dot(Q, w)
    cdef floating[:] XtA = np.zeros(n_features, dtype=dtype)
    cdef floating tmp
    cdef floating w_ii
    cdef floating d_w_max
    cdef floating w_max
    cdef floating d_w_ii
    cdef floating q_dot_w
    cdef floating w_norm2
    cdef floating gap = tol + 1.0
    cdef floating d_w_tol = tol
    cdef floating dual_norm_XtA
    cdef unsigned int ii
    cdef unsigned int n_iter = 0
    cdef unsigned int f_iter
    cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
    cdef UINT32_t* rand_r_state = &rand_r_state_seed
    cdef floating y_norm2 = np.dot(y, y)
    cdef floating* w_ptr = <floating*>&w[0]
    cdef floating* Q_ptr = &Q[0, 0]
    cdef floating* q_ptr = <floating*>q.data
    cdef floating* H_ptr = &H[0]
    cdef floating* XtA_ptr = &XtA[0]
    tol = tol * y_norm2
    if alpha == 0:
        warnings.warn("Coordinate descent with alpha=0 may lead to unexpected"
            " results and is discouraged.")
    with nogil:
        for n_iter in range(max_iter):
            w_max = 0.0
            d_w_max = 0.0
            for f_iter in range(n_features):                  if random:
                    ii = rand_int(n_features, rand_r_state)
                else:
                    ii = f_iter
                if Q[ii, ii] == 0.0:
                    continue
                w_ii = w[ii]  
                if w_ii != 0.0:
                                        axpy(n_features, -w_ii, Q_ptr + ii * n_features, 1,
                         H_ptr, 1)
                tmp = q[ii] - H[ii]
                if positive and tmp < 0:
                    w[ii] = 0.0
                else:
                    w[ii] = fsign(tmp) * fmax(fabs(tmp) - alpha, 0) \
                        / (Q[ii, ii] + beta)
                if w[ii] != 0.0:
                                        axpy(n_features, w[ii], Q_ptr + ii * n_features, 1,
                         H_ptr, 1)
                                d_w_ii = fabs(w[ii] - w_ii)
                if d_w_ii > d_w_max:
                    d_w_max = d_w_ii
                if fabs(w[ii]) > w_max:
                    w_max = fabs(w[ii])
            if w_max == 0.0 or d_w_max / w_max < d_w_tol or n_iter == max_iter - 1:
                                                
                                q_dot_w = dot(n_features, w_ptr, 1, q_ptr, 1)
                for ii in range(n_features):
                    XtA[ii] = q[ii] - H[ii] - beta * w[ii]
                if positive:
                    dual_norm_XtA = max(n_features, XtA_ptr)
                else:
                    dual_norm_XtA = abs_max(n_features, XtA_ptr)
                                tmp = 0.0
                for ii in range(n_features):
                    tmp += w[ii] * H[ii]
                R_norm2 = y_norm2 + tmp - 2.0 * q_dot_w
                                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)
                if (dual_norm_XtA > alpha):
                    const = alpha / dual_norm_XtA
                    A_norm2 = R_norm2 * (const ** 2)
                    gap = 0.5 * (R_norm2 + A_norm2)
                else:
                    const = 1.0
                    gap = R_norm2
                                gap += (alpha * asum(n_features, &w[0], 1) -
                        const * y_norm2 +  const * q_dot_w +
                        0.5 * beta * (1 + const ** 2) * w_norm2)
                if gap < tol:
                                        break
    return np.asarray(w), gap, tol, n_iter + 1

@cython.boundscheck(False)
@cython.wraparound(False)
@cython.cdivision(True)
def enet_coordinate_descent_multi_task(floating[::1, :] W, floating l1_reg,
                                       floating l2_reg,
                                       np.ndarray[floating, ndim=2, mode='fortran'] X,
                                       np.ndarray[floating, ndim=2] Y,
                                       int max_iter, floating tol, object rng,
                                       bint random=0):
            cdef DOT dot
    cdef AXPY axpy
    cdef ASUM asum
    if floating is float:
        dtype = np.float32
        dot = sdot
        nrm2 = snrm2
        asum = sasum
        copy = scopy
        scal = sscal
        ger = sger
        gemv = sgemv
    else:
        dtype = np.float64
        dot = ddot
        nrm2 = dnrm2
        asum = dasum
        copy = dcopy
        scal = dscal
        ger = dger
        gemv = dgemv
        cdef unsigned int n_samples = X.shape[0]
    cdef unsigned int n_features = X.shape[1]
    cdef unsigned int n_tasks = Y.shape[1]
        cdef floating[:, ::1] XtA = np.zeros((n_features, n_tasks), dtype=dtype)
    cdef floating XtA_axis1norm
    cdef floating dual_norm_XtA
        cdef floating[:, ::1] R = np.zeros((n_samples, n_tasks), dtype=dtype)
    cdef floating[:] norm_cols_X = np.zeros(n_features, dtype=dtype)
    cdef floating[::1] tmp = np.zeros(n_tasks, dtype=dtype)
    cdef floating[:] w_ii = np.zeros(n_tasks, dtype=dtype)
    cdef floating d_w_max
    cdef floating w_max
    cdef floating d_w_ii
    cdef floating nn
    cdef floating W_ii_abs_max
    cdef floating gap = tol + 1.0
    cdef floating d_w_tol = tol
    cdef floating R_norm
    cdef floating w_norm
    cdef floating ry_sum
    cdef floating l21_norm
    cdef unsigned int ii
    cdef unsigned int jj
    cdef unsigned int n_iter = 0
    cdef unsigned int f_iter
    cdef UINT32_t rand_r_state_seed = rng.randint(0, RAND_R_MAX)
    cdef UINT32_t* rand_r_state = &rand_r_state_seed
    cdef floating* X_ptr = &X[0, 0]
    cdef floating* W_ptr = &W[0, 0]
    cdef floating* Y_ptr = &Y[0, 0]
    cdef floating* wii_ptr = &w_ii[0]
    if l1_reg == 0:
        warnings.warn("Coordinate descent with l1_reg=0 may lead to unexpected"
            " results and is discouraged.")
    with nogil:
                for ii in range(n_features):
            for jj in range(n_samples):
                norm_cols_X[ii] += X[jj, ii] ** 2
                for ii in range(n_samples):
            for jj in range(n_tasks):
                R[ii, jj] = Y[ii, jj] - (
                    dot(n_features, X_ptr + ii, n_samples, W_ptr + jj, n_tasks)
                    )
                tol = tol * nrm2(n_samples * n_tasks, Y_ptr, 1) ** 2
        for n_iter in range(max_iter):
            w_max = 0.0
            d_w_max = 0.0
            for f_iter in range(n_features):                  if random:
                    ii = rand_int(n_features, rand_r_state)
                else:
                    ii = f_iter
                if norm_cols_X[ii] == 0.0:
                    continue
                                copy(n_tasks, W_ptr + ii * n_tasks, 1, wii_ptr, 1)
                                if nrm2(n_tasks, wii_ptr, 1) != 0.0:
                                        ger(CblasRowMajor, n_samples, n_tasks, 1.0,
                         X_ptr + ii * n_samples, 1,
                         wii_ptr, 1, &R[0, 0], n_tasks)
                                gemv(CblasRowMajor, CblasTrans,
                      n_samples, n_tasks, 1.0, &R[0, 0], n_tasks,
                      X_ptr + ii * n_samples, 1, 0.0, &tmp[0], 1)
                                nn = nrm2(n_tasks, &tmp[0], 1)
                                copy(n_tasks, &tmp[0], 1, W_ptr + ii * n_tasks, 1)
                scal(n_tasks, fmax(1. - l1_reg / nn, 0) / (norm_cols_X[ii] + l2_reg),
                          W_ptr + ii * n_tasks, 1)
                                if nrm2(n_tasks, W_ptr + ii * n_tasks, 1) != 0.0:
                                                            ger(CblasRowMajor, n_samples, n_tasks, -1.0,
                         X_ptr + ii * n_samples, 1, W_ptr + ii * n_tasks, 1,
                         &R[0, 0], n_tasks)
                                d_w_ii = diff_abs_max(n_tasks, W_ptr + ii * n_tasks, wii_ptr)
                if d_w_ii > d_w_max:
                    d_w_max = d_w_ii
                W_ii_abs_max = abs_max(n_tasks, W_ptr + ii * n_tasks)
                if W_ii_abs_max > w_max:
                    w_max = W_ii_abs_max
            if w_max == 0.0 or d_w_max / w_max < d_w_tol or n_iter == max_iter - 1:
                                                
                                for ii in range(n_features):
                    for jj in range(n_tasks):
                        XtA[ii, jj] = dot(
                            n_samples, X_ptr + ii * n_samples, 1,
                            &R[0, 0] + jj, n_tasks
                            ) - l2_reg * W[jj, ii]
                                dual_norm_XtA = 0.0
                for ii in range(n_features):
                                        XtA_axis1norm = nrm2(n_tasks, &XtA[0, 0] + ii * n_tasks, 1)
                    if XtA_axis1norm > dual_norm_XtA:
                        dual_norm_XtA = XtA_axis1norm
                                                                R_norm = nrm2(n_samples * n_tasks, &R[0, 0], 1)
                w_norm = nrm2(n_features * n_tasks, W_ptr, 1)
                if (dual_norm_XtA > l1_reg):
                    const =  l1_reg / dual_norm_XtA
                    A_norm = R_norm * const
                    gap = 0.5 * (R_norm ** 2 + A_norm ** 2)
                else:
                    const = 1.0
                    gap = R_norm ** 2
                                ry_sum = 0.0
                for ii in range(n_samples):
                    for jj in range(n_tasks):
                        ry_sum += R[ii, jj] * Y[ii, jj]
                                l21_norm = 0.0
                for ii in range(n_features):
                                        l21_norm += nrm2(n_tasks, W_ptr + n_tasks * ii, 1)
                gap += l1_reg * l21_norm - const * ry_sum + \
                     0.5 * l2_reg * (1 + const ** 2) * (w_norm ** 2)
                if gap < tol:
                                        break
    return np.asarray(W), gap, tol, n_iter + 1
import sys
import warnings
from abc import ABCMeta, abstractmethod
import numpy as np
from scipy import sparse
from .base import LinearModel, _pre_fit
from ..base import RegressorMixin
from .base import _preprocess_data
from ..utils import check_array, check_X_y
from ..utils.validation import check_random_state
from ..model_selection import check_cv
from ..externals.joblib import Parallel, delayed
from ..externals import six
from ..externals.six.moves import xrange
from ..utils.extmath import safe_sparse_dot
from ..utils.validation import check_is_fitted
from ..utils.validation import column_or_1d
from ..exceptions import ConvergenceWarning
from . import cd_fast

def _alpha_grid(X, y, Xy=None, l1_ratio=1.0, fit_intercept=True,
                eps=1e-3, n_alphas=100, normalize=False, copy_X=True):
        if l1_ratio == 0:
        raise ValueError("Automatic alpha grid generation is not supported for"
                         " l1_ratio=0. Please supply a grid by providing "
                         "your estimator with the appropriate `alphas=` "
                         "argument.")
    n_samples = len(y)
    sparse_center = False
    if Xy is None:
        X_sparse = sparse.isspmatrix(X)
        sparse_center = X_sparse and (fit_intercept or normalize)
        X = check_array(X, 'csc',
                        copy=(copy_X and fit_intercept and not X_sparse))
        if not X_sparse:
                        X, y, _, _, _ = _preprocess_data(X, y, fit_intercept,
                                             normalize, copy=False)
        Xy = safe_sparse_dot(X.T, y, dense_output=True)
        if sparse_center:
                                    _, _, X_offset, _, X_scale = _preprocess_data(X, y, fit_intercept,
                                                      normalize,
                                                      return_mean=True)
            mean_dot = X_offset * np.sum(y)
    if Xy.ndim == 1:
        Xy = Xy[:, np.newaxis]
    if sparse_center:
        if fit_intercept:
            Xy -= mean_dot[:, np.newaxis]
        if normalize:
            Xy /= X_scale[:, np.newaxis]
    alpha_max = (np.sqrt(np.sum(Xy ** 2, axis=1)).max() /
                 (n_samples * l1_ratio))
    if alpha_max <= np.finfo(float).resolution:
        alphas = np.empty(n_alphas)
        alphas.fill(np.finfo(float).resolution)
        return alphas
    return np.logspace(np.log10(alpha_max * eps), np.log10(alpha_max),
                       num=n_alphas)[::-1]

def lasso_path(X, y, eps=1e-3, n_alphas=100, alphas=None,
               precompute='auto', Xy=None, copy_X=True, coef_init=None,
               verbose=False, return_n_iter=False, positive=False, **params):
        return enet_path(X, y, l1_ratio=1., eps=eps, n_alphas=n_alphas,
                     alphas=alphas, precompute=precompute, Xy=Xy,
                     copy_X=copy_X, coef_init=coef_init, verbose=verbose,
                     positive=positive, return_n_iter=return_n_iter, **params)

def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
              precompute='auto', Xy=None, copy_X=True, coef_init=None,
              verbose=False, return_n_iter=False, positive=False,
              check_input=True, **params):
                if check_input:
        X = check_array(X, 'csc', dtype=[np.float64, np.float32],
                        order='F', copy=copy_X)
        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False,
                        ensure_2d=False)
        if Xy is not None:
                        Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False,
                             ensure_2d=False)
    n_samples, n_features = X.shape
    multi_output = False
    if y.ndim != 1:
        multi_output = True
        _, n_outputs = y.shape
        if not multi_output and sparse.isspmatrix(X):
        if 'X_offset' in params:
                                    X_sparse_scaling = params['X_offset'] / params['X_scale']
            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)
        else:
            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)
            if check_input:
        X, y, X_offset, y_offset, X_scale, precompute, Xy = \
            _pre_fit(X, y, Xy, precompute, normalize=False,
                     fit_intercept=False, copy=False)
    if alphas is None:
                        alphas = _alpha_grid(X, y, Xy=Xy, l1_ratio=l1_ratio,
                             fit_intercept=False, eps=eps, n_alphas=n_alphas,
                             normalize=False, copy_X=False)
    else:
        alphas = np.sort(alphas)[::-1]  
    n_alphas = len(alphas)
    tol = params.get('tol', 1e-4)
    max_iter = params.get('max_iter', 1000)
    dual_gaps = np.empty(n_alphas)
    n_iters = []
    rng = check_random_state(params.get('random_state', None))
    selection = params.get('selection', 'cyclic')
    if selection not in ['random', 'cyclic']:
        raise ValueError("selection should be either random or cyclic.")
    random = (selection == 'random')
    if not multi_output:
        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)
    else:
        coefs = np.empty((n_outputs, n_features, n_alphas),
                         dtype=X.dtype)
    if coef_init is None:
        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1], dtype=X.dtype))
    else:
        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)
    for i, alpha in enumerate(alphas):
        l1_reg = alpha * l1_ratio * n_samples
        l2_reg = alpha * (1.0 - l1_ratio) * n_samples
        if not multi_output and sparse.isspmatrix(X):
            model = cd_fast.sparse_enet_coordinate_descent(
                coef_, l1_reg, l2_reg, X.data, X.indices,
                X.indptr, y, X_sparse_scaling,
                max_iter, tol, rng, random, positive)
        elif multi_output:
            model = cd_fast.enet_coordinate_descent_multi_task(
                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random)
        elif isinstance(precompute, np.ndarray):
                                    if check_input:
                precompute = check_array(precompute, dtype=X.dtype.type,
                                         order='C')
            model = cd_fast.enet_coordinate_descent_gram(
                coef_, l1_reg, l2_reg, precompute, Xy, y, max_iter,
                tol, rng, random, positive)
        elif precompute is False:
            model = cd_fast.enet_coordinate_descent(
                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random,
                positive)
        else:
            raise ValueError("Precompute should be one of True, False, "
                             "'auto' or array-like. Got %r" % precompute)
        coef_, dual_gap_, eps_, n_iter_ = model
        coefs[..., i] = coef_
        dual_gaps[i] = dual_gap_
        n_iters.append(n_iter_)
        if dual_gap_ > eps_:
            warnings.warn('Objective did not converge.' +
                          ' You might want' +
                          ' to increase the number of iterations.' +
                          ' Fitting data with very small alpha' +
                          ' may cause precision problems.',
                          ConvergenceWarning)
        if verbose:
            if verbose > 2:
                print(model)
            elif verbose > 1:
                print('Path: %03i out of %03i' % (i, n_alphas))
            else:
                sys.stderr.write('.')
    if return_n_iter:
        return alphas, coefs, dual_gaps, n_iters
    return alphas, coefs, dual_gaps


class ElasticNet(LinearModel, RegressorMixin):
        path = staticmethod(enet_path)
    def __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True,
                 normalize=False, precompute=False, max_iter=1000,
                 copy_X=True, tol=1e-4, warm_start=False, positive=False,
                 random_state=None, selection='cyclic'):
        self.alpha = alpha
        self.l1_ratio = l1_ratio
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.precompute = precompute
        self.max_iter = max_iter
        self.copy_X = copy_X
        self.tol = tol
        self.warm_start = warm_start
        self.positive = positive
        self.random_state = random_state
        self.selection = selection
    def fit(self, X, y, check_input=True):
        
        if self.alpha == 0:
            warnings.warn("With alpha=0, this algorithm does not converge "
                          "well. You are advised to use the LinearRegression "
                          "estimator", stacklevel=2)
        if isinstance(self.precompute, six.string_types):
            raise ValueError('precompute should be one of True, False or'
                             ' array-like. Got %r' % self.precompute)
                        if check_input:
            X, y = check_X_y(X, y, accept_sparse='csc',
                             order='F', dtype=[np.float64, np.float32],
                             copy=self.copy_X and self.fit_intercept,
                             multi_output=True, y_numeric=True)
            y = check_array(y, order='F', copy=False, dtype=X.dtype.type,
                            ensure_2d=False)
        X, y, X_offset, y_offset, X_scale, precompute, Xy = \
            _pre_fit(X, y, None, self.precompute, self.normalize,
                     self.fit_intercept, copy=False)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        if Xy is not None and Xy.ndim == 1:
            Xy = Xy[:, np.newaxis]
        n_samples, n_features = X.shape
        n_targets = y.shape[1]
        if self.selection not in ['cyclic', 'random']:
            raise ValueError("selection should be either random or cyclic.")
        if not self.warm_start or not hasattr(self, "coef_"):
            coef_ = np.zeros((n_targets, n_features), dtype=X.dtype,
                             order='F')
        else:
            coef_ = self.coef_
            if coef_.ndim == 1:
                coef_ = coef_[np.newaxis, :]
        dual_gaps_ = np.zeros(n_targets, dtype=X.dtype)
        self.n_iter_ = []
        for k in xrange(n_targets):
            if Xy is not None:
                this_Xy = Xy[:, k]
            else:
                this_Xy = None
            _, this_coef, this_dual_gap, this_iter = \
                self.path(X, y[:, k],
                          l1_ratio=self.l1_ratio, eps=None,
                          n_alphas=None, alphas=[self.alpha],
                          precompute=precompute, Xy=this_Xy,
                          fit_intercept=False, normalize=False, copy_X=True,
                          verbose=False, tol=self.tol, positive=self.positive,
                          X_offset=X_offset, X_scale=X_scale, return_n_iter=True,
                          coef_init=coef_[k], max_iter=self.max_iter,
                          random_state=self.random_state,
                          selection=self.selection,
                          check_input=False)
            coef_[k] = this_coef[:, 0]
            dual_gaps_[k] = this_dual_gap[0]
            self.n_iter_.append(this_iter[0])
        if n_targets == 1:
            self.n_iter_ = self.n_iter_[0]
        self.coef_, self.dual_gap_ = map(np.squeeze, [coef_, dual_gaps_])
        self._set_intercept(X_offset, y_offset, X_scale)
                self.coef_ = np.asarray(self.coef_, dtype=X.dtype)
                return self
    @property
    def sparse_coef_(self):
                return sparse.csr_matrix(self.coef_)
    def _decision_function(self, X):
                check_is_fitted(self, 'n_iter_')
        if sparse.isspmatrix(X):
            return safe_sparse_dot(X, self.coef_.T,
                                   dense_output=True) + self.intercept_
        else:
            return super(ElasticNet, self)._decision_function(X)

class Lasso(ElasticNet):
        path = staticmethod(enet_path)
    def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,
                 precompute=False, copy_X=True, max_iter=1000,
                 tol=1e-4, warm_start=False, positive=False,
                 random_state=None, selection='cyclic'):
        super(Lasso, self).__init__(
            alpha=alpha, l1_ratio=1.0, fit_intercept=fit_intercept,
            normalize=normalize, precompute=precompute, copy_X=copy_X,
            max_iter=max_iter, tol=tol, warm_start=warm_start,
            positive=positive, random_state=random_state,
            selection=selection)

def _path_residuals(X, y, train, test, path, path_params, alphas=None,
                    l1_ratio=1, X_order=None, dtype=None):
        X_train = X[train]
    y_train = y[train]
    X_test = X[test]
    y_test = y[test]
    fit_intercept = path_params['fit_intercept']
    normalize = path_params['normalize']
    if y.ndim == 1:
        precompute = path_params['precompute']
    else:
                        precompute = False
    X_train, y_train, X_offset, y_offset, X_scale, precompute, Xy = \
        _pre_fit(X_train, y_train, None, precompute, normalize, fit_intercept,
                 copy=False)
    path_params = path_params.copy()
    path_params['Xy'] = Xy
    path_params['X_offset'] = X_offset
    path_params['X_scale'] = X_scale
    path_params['precompute'] = precompute
    path_params['copy_X'] = False
    path_params['alphas'] = alphas
    if 'l1_ratio' in path_params:
        path_params['l1_ratio'] = l1_ratio
            X_train = check_array(X_train, 'csc', dtype=dtype, order=X_order)
    alphas, coefs, _ = path(X_train, y_train, **path_params)
    del X_train, y_train
    if y.ndim == 1:
                coefs = coefs[np.newaxis, :, :]
        y_offset = np.atleast_1d(y_offset)
        y_test = y_test[:, np.newaxis]
    if normalize:
        nonzeros = np.flatnonzero(X_scale)
        coefs[:, nonzeros] /= X_scale[nonzeros][:, np.newaxis]
    intercepts = y_offset[:, np.newaxis] - np.dot(X_offset, coefs)
    if sparse.issparse(X_test):
        n_order, n_features, n_alphas = coefs.shape
                coefs_feature_major = np.rollaxis(coefs, 1)
        feature_2d = np.reshape(coefs_feature_major, (n_features, -1))
        X_test_coefs = safe_sparse_dot(X_test, feature_2d)
        X_test_coefs = X_test_coefs.reshape(X_test.shape[0], n_order, -1)
    else:
        X_test_coefs = safe_sparse_dot(X_test, coefs)
    residues = X_test_coefs - y_test[:, :, np.newaxis]
    residues += intercepts
    this_mses = ((residues ** 2).mean(axis=0)).mean(axis=0)
    return this_mses

class LinearModelCV(six.with_metaclass(ABCMeta, LinearModel)):
    
    @abstractmethod
    def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                 normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
                 copy_X=True, cv=None, verbose=False, n_jobs=1,
                 positive=False, random_state=None, selection='cyclic'):
        self.eps = eps
        self.n_alphas = n_alphas
        self.alphas = alphas
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.precompute = precompute
        self.max_iter = max_iter
        self.tol = tol
        self.copy_X = copy_X
        self.cv = cv
        self.verbose = verbose
        self.n_jobs = n_jobs
        self.positive = positive
        self.random_state = random_state
        self.selection = selection
    def fit(self, X, y):
                y = check_array(y, copy=False, dtype=[np.float64, np.float32],
                        ensure_2d=False)
        if y.shape[0] == 0:
            raise ValueError("y has 0 samples: %r" % y)
        if hasattr(self, 'l1_ratio'):
            model_str = 'ElasticNet'
        else:
            model_str = 'Lasso'
        if isinstance(self, ElasticNetCV) or isinstance(self, LassoCV):
            if model_str == 'ElasticNet':
                model = ElasticNet()
            else:
                model = Lasso()
            if y.ndim > 1 and y.shape[1] > 1:
                raise ValueError("For multi-task outputs, use "
                                 "MultiTask%sCV" % (model_str))
            y = column_or_1d(y, warn=True)
        else:
            if sparse.isspmatrix(X):
                raise TypeError("X should be dense but a sparse matrix was"
                                "passed")
            elif y.ndim == 1:
                raise ValueError("For mono-task outputs, use "
                                 "%sCV" % (model_str))
            if model_str == 'ElasticNet':
                model = MultiTaskElasticNet()
            else:
                model = MultiTaskLasso()
        if self.selection not in ["random", "cyclic"]:
            raise ValueError("selection should be either random or cyclic.")
                                        copy_X = self.copy_X and self.fit_intercept
        if isinstance(X, np.ndarray) or sparse.isspmatrix(X):
                        reference_to_old_X = X
                                                X = check_array(X, 'csc', copy=False)
            if sparse.isspmatrix(X):
                if (hasattr(reference_to_old_X, "data") and
                   not np.may_share_memory(reference_to_old_X.data, X.data)):
                                        copy_X = False
            elif not np.may_share_memory(reference_to_old_X, X):
                                copy_X = False
            del reference_to_old_X
        else:
            X = check_array(X, 'csc', dtype=[np.float64, np.float32],
                            order='F', copy=copy_X)
            copy_X = False
        if X.shape[0] != y.shape[0]:
            raise ValueError("X and y have inconsistent dimensions (%d != %d)"
                             % (X.shape[0], y.shape[0]))
                path_params = self.get_params()
        if 'l1_ratio' in path_params:
            l1_ratios = np.atleast_1d(path_params['l1_ratio'])
                        path_params['l1_ratio'] = l1_ratios[0]
        else:
            l1_ratios = [1, ]
        path_params.pop('cv', None)
        path_params.pop('n_jobs', None)
        alphas = self.alphas
        n_l1_ratio = len(l1_ratios)
        if alphas is None:
            alphas = []
            for l1_ratio in l1_ratios:
                alphas.append(_alpha_grid(
                    X, y, l1_ratio=l1_ratio,
                    fit_intercept=self.fit_intercept,
                    eps=self.eps, n_alphas=self.n_alphas,
                    normalize=self.normalize,
                    copy_X=self.copy_X))
        else:
                        alphas = np.tile(np.sort(alphas)[::-1], (n_l1_ratio, 1))
                n_alphas = len(alphas[0])
        path_params.update({'n_alphas': n_alphas})
        path_params['copy_X'] = copy_X
                        if not (self.n_jobs == 1 or self.n_jobs is None):
            path_params['copy_X'] = False
                cv = check_cv(self.cv)
                folds = list(cv.split(X))
        best_mse = np.inf
                        jobs = (delayed(_path_residuals)(X, y, train, test, self.path,
                                         path_params, alphas=this_alphas,
                                         l1_ratio=this_l1_ratio, X_order='F',
                                         dtype=X.dtype.type)
                for this_l1_ratio, this_alphas in zip(l1_ratios, alphas)
                for train, test in folds)
        mse_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                             backend="threading")(jobs)
        mse_paths = np.reshape(mse_paths, (n_l1_ratio, len(folds), -1))
        mean_mse = np.mean(mse_paths, axis=1)
        self.mse_path_ = np.squeeze(np.rollaxis(mse_paths, 2, 1))
        for l1_ratio, l1_alphas, mse_alphas in zip(l1_ratios, alphas,
                                                   mean_mse):
            i_best_alpha = np.argmin(mse_alphas)
            this_best_mse = mse_alphas[i_best_alpha]
            if this_best_mse < best_mse:
                best_alpha = l1_alphas[i_best_alpha]
                best_l1_ratio = l1_ratio
                best_mse = this_best_mse
        self.l1_ratio_ = best_l1_ratio
        self.alpha_ = best_alpha
        if self.alphas is None:
            self.alphas_ = np.asarray(alphas)
            if n_l1_ratio == 1:
                self.alphas_ = self.alphas_[0]
                else:
            self.alphas_ = np.asarray(alphas[0])
                common_params = dict((name, value)
                             for name, value in self.get_params().items()
                             if name in model.get_params())
        model.set_params(**common_params)
        model.alpha = best_alpha
        model.l1_ratio = best_l1_ratio
        model.copy_X = copy_X
        model.precompute = False
        model.fit(X, y)
        if not hasattr(self, 'l1_ratio'):
            del self.l1_ratio_
        self.coef_ = model.coef_
        self.intercept_ = model.intercept_
        self.dual_gap_ = model.dual_gap_
        self.n_iter_ = model.n_iter_
        return self

class LassoCV(LinearModelCV, RegressorMixin):
        path = staticmethod(lasso_path)
    def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                 normalize=False, precompute='auto', max_iter=1000, tol=1e-4,
                 copy_X=True, cv=None, verbose=False, n_jobs=1,
                 positive=False, random_state=None, selection='cyclic'):
        super(LassoCV, self).__init__(
            eps=eps, n_alphas=n_alphas, alphas=alphas,
            fit_intercept=fit_intercept, normalize=normalize,
            precompute=precompute, max_iter=max_iter, tol=tol, copy_X=copy_X,
            cv=cv, verbose=verbose, n_jobs=n_jobs, positive=positive,
            random_state=random_state, selection=selection)

class ElasticNetCV(LinearModelCV, RegressorMixin):
        path = staticmethod(enet_path)
    def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                 fit_intercept=True, normalize=False, precompute='auto',
                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,
                 verbose=0, n_jobs=1, positive=False, random_state=None,
                 selection='cyclic'):
        self.l1_ratio = l1_ratio
        self.eps = eps
        self.n_alphas = n_alphas
        self.alphas = alphas
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.precompute = precompute
        self.max_iter = max_iter
        self.tol = tol
        self.cv = cv
        self.copy_X = copy_X
        self.verbose = verbose
        self.n_jobs = n_jobs
        self.positive = positive
        self.random_state = random_state
        self.selection = selection


class MultiTaskElasticNet(Lasso):
        def __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True,
                 normalize=False, copy_X=True, max_iter=1000, tol=1e-4,
                 warm_start=False, random_state=None, selection='cyclic'):
        self.l1_ratio = l1_ratio
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.max_iter = max_iter
        self.copy_X = copy_X
        self.tol = tol
        self.warm_start = warm_start
        self.random_state = random_state
        self.selection = selection
    def fit(self, X, y):
                X = check_array(X, dtype=[np.float64, np.float32], order='F',
                        copy=self.copy_X and self.fit_intercept)
        y = check_array(y, dtype=X.dtype.type, ensure_2d=False)
        if hasattr(self, 'l1_ratio'):
            model_str = 'ElasticNet'
        else:
            model_str = 'Lasso'
        if y.ndim == 1:
            raise ValueError("For mono-task outputs, use %s" % model_str)
        n_samples, n_features = X.shape
        _, n_tasks = y.shape
        if n_samples != y.shape[0]:
            raise ValueError("X and y have inconsistent dimensions (%d != %d)"
                             % (n_samples, y.shape[0]))
        X, y, X_offset, y_offset, X_scale = _preprocess_data(
            X, y, self.fit_intercept, self.normalize, copy=False)
        if not self.warm_start or self.coef_ is None:
            self.coef_ = np.zeros((n_tasks, n_features), dtype=X.dtype.type,
                                  order='F')
        l1_reg = self.alpha * self.l1_ratio * n_samples
        l2_reg = self.alpha * (1.0 - self.l1_ratio) * n_samples
        self.coef_ = np.asfortranarray(self.coef_)  
        if self.selection not in ['random', 'cyclic']:
            raise ValueError("selection should be either random or cyclic.")
        random = (self.selection == 'random')
        self.coef_, self.dual_gap_, self.eps_, self.n_iter_ = \
            cd_fast.enet_coordinate_descent_multi_task(
                self.coef_, l1_reg, l2_reg, X, y, self.max_iter, self.tol,
                check_random_state(self.random_state), random)
        self._set_intercept(X_offset, y_offset, X_scale)
        if self.dual_gap_ > self.eps_:
            warnings.warn('Objective did not converge, you might want'
                          ' to increase the number of iterations',
                          ConvergenceWarning)
                return self

class MultiTaskLasso(MultiTaskElasticNet):
        def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,
                 copy_X=True, max_iter=1000, tol=1e-4, warm_start=False,
                 random_state=None, selection='cyclic'):
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.max_iter = max_iter
        self.copy_X = copy_X
        self.tol = tol
        self.warm_start = warm_start
        self.l1_ratio = 1.0
        self.random_state = random_state
        self.selection = selection

class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):
        path = staticmethod(enet_path)
    def __init__(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,
                 fit_intercept=True, normalize=False,
                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,
                 verbose=0, n_jobs=1, random_state=None, selection='cyclic'):
        self.l1_ratio = l1_ratio
        self.eps = eps
        self.n_alphas = n_alphas
        self.alphas = alphas
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.max_iter = max_iter
        self.tol = tol
        self.cv = cv
        self.copy_X = copy_X
        self.verbose = verbose
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.selection = selection

class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
        path = staticmethod(lasso_path)
    def __init__(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,
                 normalize=False, max_iter=1000, tol=1e-4, copy_X=True,
                 cv=None, verbose=False, n_jobs=1, random_state=None,
                 selection='cyclic'):
        super(MultiTaskLassoCV, self).__init__(
            eps=eps, n_alphas=n_alphas, alphas=alphas,
            fit_intercept=fit_intercept, normalize=normalize,
            max_iter=max_iter, tol=tol, copy_X=copy_X,
            cv=cv, verbose=verbose, n_jobs=n_jobs, random_state=random_state,
            selection=selection)
import numpy as np
from scipy import optimize, sparse
from ..base import BaseEstimator, RegressorMixin
from .base import LinearModel
from ..utils import check_X_y
from ..utils import check_consistent_length
from ..utils import axis0_safe_slice
from ..utils.extmath import safe_sparse_dot

def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):
        X_is_sparse = sparse.issparse(X)
    _, n_features = X.shape
    fit_intercept = (n_features + 2 == w.shape[0])
    if fit_intercept:
        intercept = w[-2]
    sigma = w[-1]
    w = w[:n_features]
    n_samples = np.sum(sample_weight)
            linear_loss = y - safe_sparse_dot(X, w)
    if fit_intercept:
        linear_loss -= intercept
    abs_linear_loss = np.abs(linear_loss)
    outliers_mask = abs_linear_loss > epsilon * sigma
            outliers = abs_linear_loss[outliers_mask]
    num_outliers = np.count_nonzero(outliers_mask)
    n_non_outliers = X.shape[0] - num_outliers
            outliers_sw = sample_weight[outliers_mask]
    n_sw_outliers = np.sum(outliers_sw)
    outlier_loss = (2. * epsilon * np.sum(outliers_sw * outliers) -
                    sigma * n_sw_outliers * epsilon ** 2)
            non_outliers = linear_loss[~outliers_mask]
    weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers
    weighted_loss = np.dot(weighted_non_outliers.T, non_outliers)
    squared_loss = weighted_loss / sigma
    if fit_intercept:
        grad = np.zeros(n_features + 2)
    else:
        grad = np.zeros(n_features + 1)
        X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers)
    grad[:n_features] = (
        2. / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers))
        signed_outliers = np.ones_like(outliers)
    signed_outliers_mask = linear_loss[outliers_mask] < 0
    signed_outliers[signed_outliers_mask] = -1.0
    X_outliers = axis0_safe_slice(X, outliers_mask, num_outliers)
    sw_outliers = sample_weight[outliers_mask] * signed_outliers
    grad[:n_features] -= 2. * epsilon * (
        safe_sparse_dot(sw_outliers, X_outliers))
        grad[:n_features] += alpha * 2. * w
        grad[-1] = n_samples
    grad[-1] -= n_sw_outliers * epsilon ** 2
    grad[-1] -= squared_loss / sigma
        if fit_intercept:
        grad[-2] = -2. * np.sum(weighted_non_outliers) / sigma
        grad[-2] -= 2. * epsilon * np.sum(sw_outliers)
    loss = n_samples * sigma + squared_loss + outlier_loss
    loss += alpha * np.dot(w, w)
    return loss, grad

class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):
    
    def __init__(self, epsilon=1.35, max_iter=100, alpha=0.0001,
                 warm_start=False, fit_intercept=True, tol=1e-05):
        self.epsilon = epsilon
        self.max_iter = max_iter
        self.alpha = alpha
        self.warm_start = warm_start
        self.fit_intercept = fit_intercept
        self.tol = tol
    def fit(self, X, y, sample_weight=None):
                X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
                                    parameters[-1] = 1
                                bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
                        try:
            parameters, f, dict_ = optimize.fmin_l_bfgs_b(
                _huber_loss_and_gradient, parameters,
                args=(X, y, self.epsilon, self.alpha, sample_weight),
                maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
                iprint=0)
        except TypeError:
            parameters, f, dict_ = optimize.fmin_l_bfgs_b(
                _huber_loss_and_gradient, parameters,
                args=(X, y, self.epsilon, self.alpha, sample_weight),
                bounds=bounds)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
                             % dict_['task'].decode('ascii'))
        self.n_iter_ = dict_.get('nit', None)
        self.scale_ = parameters[-1]
        if self.fit_intercept:
            self.intercept_ = parameters[-2]
        else:
            self.intercept_ = 0.0
        self.coef_ = parameters[:X.shape[1]]
        residual = np.abs(
            y - safe_sparse_dot(X, self.coef_) - self.intercept_)
        self.outliers_ = residual > self.scale_ * self.epsilon
        return self
from __future__ import print_function

from math import log
import sys
import warnings
from distutils.version import LooseVersion
import numpy as np
from scipy import linalg, interpolate
from scipy.linalg.lapack import get_lapack_funcs
from .base import LinearModel
from ..base import RegressorMixin
from ..utils import arrayfuncs, as_float_array, check_X_y, deprecated
from ..model_selection import check_cv
from ..exceptions import ConvergenceWarning
from ..externals.joblib import Parallel, delayed
from ..externals.six.moves import xrange
from ..externals.six import string_types
import scipy
solve_triangular_args = {}
if LooseVersion(scipy.__version__) >= LooseVersion('0.12'):
    solve_triangular_args = {'check_finite': False}

def lars_path(X, y, Xy=None, Gram=None, max_iter=500,
              alpha_min=0, method='lar', copy_X=True,
              eps=np.finfo(np.float).eps,
              copy_Gram=True, verbose=0, return_path=True,
              return_n_iter=False, positive=False):
    
    n_features = X.shape[1]
    n_samples = y.size
    max_features = min(max_iter, n_features)
    if return_path:
        coefs = np.zeros((max_features + 1, n_features))
        alphas = np.zeros(max_features + 1)
    else:
        coef, prev_coef = np.zeros(n_features), np.zeros(n_features)
        alpha, prev_alpha = np.array([0.]), np.array([0.])  
    n_iter, n_active = 0, 0
    active, indices = list(), np.arange(n_features)
        sign_active = np.empty(max_features, dtype=np.int8)
    drop = False
                                    L = np.zeros((max_features, max_features), dtype=X.dtype)
    swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))
    solve_cholesky, = get_lapack_funcs(('potrs',), (X,))
    if Gram is None:
        if copy_X:
                                                X = X.copy('F')
    elif isinstance(Gram, string_types) and Gram == 'auto':
        Gram = None
        if X.shape[0] > X.shape[1]:
            Gram = np.dot(X.T, X)
    elif copy_Gram:
        Gram = Gram.copy()
    if Xy is None:
        Cov = np.dot(X.T, y)
    else:
        Cov = Xy.copy()
    if verbose:
        if verbose > 1:
            print("Step\t\tAdded\t\tDropped\t\tActive set size\t\tC")
        else:
            sys.stdout.write('.')
            sys.stdout.flush()
    tiny = np.finfo(np.float).tiny      tiny32 = np.finfo(np.float32).tiny      equality_tolerance = np.finfo(np.float32).eps
    while True:
        if Cov.size:
            if positive:
                C_idx = np.argmax(Cov)
            else:
                C_idx = np.argmax(np.abs(Cov))
            C_ = Cov[C_idx]
            if positive:
                C = C_
            else:
                C = np.fabs(C_)
        else:
            C = 0.
        if return_path:
            alpha = alphas[n_iter, np.newaxis]
            coef = coefs[n_iter]
            prev_alpha = alphas[n_iter - 1, np.newaxis]
            prev_coef = coefs[n_iter - 1]
        alpha[0] = C / n_samples
        if alpha[0] <= alpha_min + equality_tolerance:              if abs(alpha[0] - alpha_min) > equality_tolerance:
                                if n_iter > 0:
                                                            ss = ((prev_alpha[0] - alpha_min) /
                          (prev_alpha[0] - alpha[0]))
                    coef[:] = prev_coef + ss * (coef - prev_coef)
                alpha[0] = alpha_min
            if return_path:
                coefs[n_iter] = coef
            break
        if n_iter >= max_iter or n_active >= n_features:
            break
        if not drop:
                                                                                                
            if positive:
                sign_active[n_active] = np.ones_like(C_)
            else:
                sign_active[n_active] = np.sign(C_)
            m, n = n_active, C_idx + n_active
            Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])
            indices[n], indices[m] = indices[m], indices[n]
            Cov_not_shortened = Cov
            Cov = Cov[1:]  
            if Gram is None:
                X.T[n], X.T[m] = swap(X.T[n], X.T[m])
                c = nrm2(X.T[n_active]) ** 2
                L[n_active, :n_active] = \
                    np.dot(X.T[n_active], X.T[:n_active].T)
            else:
                                                Gram[m], Gram[n] = swap(Gram[m], Gram[n])
                Gram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])
                c = Gram[n_active, n_active]
                L[n_active, :n_active] = Gram[n_active, :n_active]
                        if n_active:
                linalg.solve_triangular(L[:n_active, :n_active],
                                        L[n_active, :n_active],
                                        trans=0, lower=1,
                                        overwrite_b=True,
                                        **solve_triangular_args)
            v = np.dot(L[n_active, :n_active], L[n_active, :n_active])
            diag = max(np.sqrt(np.abs(c - v)), eps)
            L[n_active, n_active] = diag
            if diag < 1e-7:
                                                
                                                                                                warnings.warn('Regressors in active set degenerate. '
                              'Dropping a regressor, after %i iterations, '
                              'i.e. alpha=%.3e, '
                              'with an active set of %i regressors, and '
                              'the smallest cholesky pivot element being %.3e'
                              % (n_iter, alpha, n_active, diag),
                              ConvergenceWarning)
                                Cov = Cov_not_shortened
                Cov[0] = 0
                Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])
                continue
            active.append(indices[n_active])
            n_active += 1
            if verbose > 1:
                print("%s\t\t%s\t\t%s\t\t%s\t\t%s" % (n_iter, active[-1], '',
                                                      n_active, C))
        if method == 'lasso' and n_iter > 0 and prev_alpha[0] < alpha[0]:
                                                            warnings.warn('Early stopping the lars path, as the residues '
                          'are small and the current value of alpha is no '
                          'longer well controlled. %i iterations, alpha=%.3e, '
                          'previous alpha=%.3e, with an active set of %i '
                          'regressors.'
                          % (n_iter, alpha, prev_alpha, n_active),
                          ConvergenceWarning)
            break
                least_squares, info = solve_cholesky(L[:n_active, :n_active],
                                             sign_active[:n_active],
                                             lower=True)
        if least_squares.size == 1 and least_squares == 0:
                        least_squares[...] = 1
            AA = 1.
        else:
                        AA = 1. / np.sqrt(np.sum(least_squares * sign_active[:n_active]))
            if not np.isfinite(AA):
                                i = 0
                L_ = L[:n_active, :n_active].copy()
                while not np.isfinite(AA):
                    L_.flat[::n_active + 1] += (2 ** i) * eps
                    least_squares, info = solve_cholesky(
                        L_, sign_active[:n_active], lower=True)
                    tmp = max(np.sum(least_squares * sign_active[:n_active]),
                              eps)
                    AA = 1. / np.sqrt(tmp)
                    i += 1
            least_squares *= AA
        if Gram is None:
                        eq_dir = np.dot(X.T[:n_active].T, least_squares)
                                    corr_eq_dir = np.dot(X.T[n_active:], eq_dir)
        else:
                                                corr_eq_dir = np.dot(Gram[:n_active, n_active:].T,
                                 least_squares)
        g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))
        if positive:
            gamma_ = min(g1, C / AA)
        else:
            g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))
            gamma_ = min(g1, g2, C / AA)
                drop = False
        z = -coef[active] / (least_squares + tiny32)
        z_pos = arrayfuncs.min_pos(z)
        if z_pos < gamma_:
                        idx = np.where(z == z_pos)[0][::-1]
                        sign_active[idx] = -sign_active[idx]
            if method == 'lasso':
                gamma_ = z_pos
            drop = True
        n_iter += 1
        if return_path:
            if n_iter >= coefs.shape[0]:
                del coef, alpha, prev_alpha, prev_coef
                                add_features = 2 * max(1, (max_features - n_active))
                coefs = np.resize(coefs, (n_iter + add_features, n_features))
                coefs[-add_features:] = 0
                alphas = np.resize(alphas, n_iter + add_features)
                alphas[-add_features:] = 0
            coef = coefs[n_iter]
            prev_coef = coefs[n_iter - 1]
            alpha = alphas[n_iter, np.newaxis]
            prev_alpha = alphas[n_iter - 1, np.newaxis]
        else:
                        prev_coef = coef
            prev_alpha[0] = alpha[0]
            coef = np.zeros_like(coef)
        coef[active] = prev_coef[active] + gamma_ * least_squares
                Cov -= gamma_ * corr_eq_dir
                if drop and method == 'lasso':
                        [arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii) for ii in
                idx]
            n_active -= 1
            m, n = idx, n_active
                        drop_idx = [active.pop(ii) for ii in idx]
            if Gram is None:
                                for ii in idx:
                    for i in range(ii, n_active):
                        X.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])
                                                indices[i], indices[i + 1] = indices[i + 1], indices[i]
                                residual = y - np.dot(X[:, :n_active], coef[active])
                temp = np.dot(X.T[n_active], residual)
                Cov = np.r_[temp, Cov]
            else:
                for ii in idx:
                    for i in range(ii, n_active):
                        indices[i], indices[i + 1] = indices[i + 1], indices[i]
                        Gram[i], Gram[i + 1] = swap(Gram[i], Gram[i + 1])
                        Gram[:, i], Gram[:, i + 1] = swap(Gram[:, i],
                                                          Gram[:, i + 1])
                                
                                
                                residual = y - np.dot(X, coef)
                temp = np.dot(X.T[drop_idx], residual)
                Cov = np.r_[temp, Cov]
            sign_active = np.delete(sign_active, idx)
            sign_active = np.append(sign_active, 0.)              if verbose > 1:
                print("%s\t\t%s\t\t%s\t\t%s\t\t%s" % (n_iter, '', drop_idx,
                                                      n_active, abs(temp)))
    if return_path:
                alphas = alphas[:n_iter + 1]
        coefs = coefs[:n_iter + 1]
        if return_n_iter:
            return alphas, active, coefs.T, n_iter
        else:
            return alphas, active, coefs.T
    else:
        if return_n_iter:
            return alpha, active, coef, n_iter
        else:
            return alpha, active, coef

class Lars(LinearModel, RegressorMixin):
        method = 'lar'
    def __init__(self, fit_intercept=True, verbose=False, normalize=True,
                 precompute='auto', n_nonzero_coefs=500,
                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
                 positive=False):
        self.fit_intercept = fit_intercept
        self.verbose = verbose
        self.normalize = normalize
        self.precompute = precompute
        self.n_nonzero_coefs = n_nonzero_coefs
        self.positive = positive
        self.eps = eps
        self.copy_X = copy_X
        self.fit_path = fit_path
    def _get_gram(self):
                precompute = self.precompute
        if hasattr(precompute, '__array__'):
            Gram = precompute
        elif precompute == 'auto':
            Gram = 'auto'
        else:
            Gram = None
        return Gram
    def fit(self, X, y, Xy=None):
                X, y = check_X_y(X, y, y_numeric=True, multi_output=True)
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y,
                                                        self.fit_intercept,
                                                        self.normalize,
                                                        self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        alpha = getattr(self, 'alpha', 0.)
        if hasattr(self, 'n_nonzero_coefs'):
            alpha = 0.              max_iter = self.n_nonzero_coefs
        else:
            max_iter = self.max_iter
        precompute = self.precompute
        if not hasattr(precompute, '__array__') and (
                precompute is True or
                (precompute == 'auto' and X.shape[0] > X.shape[1]) or
                (precompute == 'auto' and y.shape[1] > 1)):
            Gram = np.dot(X.T, X)
        else:
            Gram = self._get_gram()
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features))
        if self.fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in xrange(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
                alphas, active, coef_path, n_iter_ = lars_path(
                    X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X,
                    copy_Gram=True, alpha_min=alpha, method=self.method,
                    verbose=max(0, self.verbose - 1), max_iter=max_iter,
                    eps=self.eps, return_path=True,
                    return_n_iter=True, positive=self.positive)
                self.alphas_.append(alphas)
                self.active_.append(active)
                self.n_iter_.append(n_iter_)
                self.coef_path_.append(coef_path)
                self.coef_[k] = coef_path[:, -1]
            if n_targets == 1:
                self.alphas_, self.active_, self.coef_path_, self.coef_ = [
                    a[0] for a in (self.alphas_, self.active_, self.coef_path_,
                                   self.coef_)]
                self.n_iter_ = self.n_iter_[0]
        else:
            for k in xrange(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
                alphas, _, self.coef_[k], n_iter_ = lars_path(
                    X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X,
                    copy_Gram=True, alpha_min=alpha, method=self.method,
                    verbose=max(0, self.verbose - 1), max_iter=max_iter,
                    eps=self.eps, return_path=False, return_n_iter=True,
                    positive=self.positive)
                self.alphas_.append(alphas)
                self.n_iter_.append(n_iter_)
            if n_targets == 1:
                self.alphas_ = self.alphas_[0]
                self.n_iter_ = self.n_iter_[0]
        self._set_intercept(X_offset, y_offset, X_scale)
        return self

class LassoLars(Lars):
        method = 'lasso'
    def __init__(self, alpha=1.0, fit_intercept=True, verbose=False,
                 normalize=True, precompute='auto', max_iter=500,
                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
                 positive=False):
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.max_iter = max_iter
        self.verbose = verbose
        self.normalize = normalize
        self.positive = positive
        self.precompute = precompute
        self.copy_X = copy_X
        self.eps = eps
        self.fit_path = fit_path

def _check_copy_and_writeable(array, copy=False):
    if copy or not array.flags.writeable:
        return array.copy()
    return array

def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None,
                        copy=True, method='lars', verbose=False,
                        fit_intercept=True, normalize=True, max_iter=500,
                        eps=np.finfo(np.float).eps, positive=False):
        X_train = _check_copy_and_writeable(X_train, copy)
    y_train = _check_copy_and_writeable(y_train, copy)
    X_test = _check_copy_and_writeable(X_test, copy)
    y_test = _check_copy_and_writeable(y_test, copy)
    if fit_intercept:
        X_mean = X_train.mean(axis=0)
        X_train -= X_mean
        X_test -= X_mean
        y_mean = y_train.mean(axis=0)
        y_train = as_float_array(y_train, copy=False)
        y_train -= y_mean
        y_test = as_float_array(y_test, copy=False)
        y_test -= y_mean
    if normalize:
        norms = np.sqrt(np.sum(X_train ** 2, axis=0))
        nonzeros = np.flatnonzero(norms)
        X_train[:, nonzeros] /= norms[nonzeros]
    alphas, active, coefs = lars_path(
        X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False,
        method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps,
        positive=positive)
    if normalize:
        coefs[nonzeros] /= norms[nonzeros][:, np.newaxis]
    residues = np.dot(X_test, coefs) - y_test[:, np.newaxis]
    return alphas, active, coefs, residues.T

class LarsCV(Lars):
    
    method = 'lar'
    def __init__(self, fit_intercept=True, verbose=False, max_iter=500,
                 normalize=True, precompute='auto', cv=None,
                 max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
                 copy_X=True, positive=False):
        self.max_iter = max_iter
        self.cv = cv
        self.max_n_alphas = max_n_alphas
        self.n_jobs = n_jobs
        super(LarsCV, self).__init__(fit_intercept=fit_intercept,
                                     verbose=verbose, normalize=normalize,
                                     precompute=precompute,
                                     n_nonzero_coefs=500,
                                     eps=eps, copy_X=copy_X, fit_path=True,
                                     positive=positive)
    def fit(self, X, y):
                X, y = check_X_y(X, y, y_numeric=True)
        X = as_float_array(X, copy=self.copy_X)
        y = as_float_array(y, copy=self.copy_X)
                cv = check_cv(self.cv, classifier=False)
        Gram = 'auto' if self.precompute else None
        cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            delayed(_lars_path_residues)(
                X[train], y[train], X[test], y[test], Gram=Gram, copy=False,
                method=self.method, verbose=max(0, self.verbose - 1),
                normalize=self.normalize, fit_intercept=self.fit_intercept,
                max_iter=self.max_iter, eps=self.eps, positive=self.positive)
            for train, test in cv.split(X, y))
        all_alphas = np.concatenate(list(zip(*cv_paths))[0])
                all_alphas = np.unique(all_alphas)
                stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))
        all_alphas = all_alphas[::stride]
        mse_path = np.empty((len(all_alphas), len(cv_paths)))
        for index, (alphas, active, coefs, residues) in enumerate(cv_paths):
            alphas = alphas[::-1]
            residues = residues[::-1]
            if alphas[0] != 0:
                alphas = np.r_[0, alphas]
                residues = np.r_[residues[0, np.newaxis], residues]
            if alphas[-1] != all_alphas[-1]:
                alphas = np.r_[alphas, all_alphas[-1]]
                residues = np.r_[residues, residues[-1, np.newaxis]]
            this_residues = interpolate.interp1d(alphas,
                                                 residues,
                                                 axis=0)(all_alphas)
            this_residues **= 2
            mse_path[:, index] = np.mean(this_residues, axis=-1)
        mask = np.all(np.isfinite(mse_path), axis=-1)
        all_alphas = all_alphas[mask]
        mse_path = mse_path[mask]
                i_best_alpha = np.argmin(mse_path.mean(axis=-1))
        best_alpha = all_alphas[i_best_alpha]
                self.alpha_ = best_alpha
        self.cv_alphas_ = all_alphas
        self.mse_path_ = mse_path
                                Lars.fit(self, X, y)
        return self
    @property
    def alpha(self):
                return self.alpha_
    @property
    @deprecated("Attribute cv_mse_path_ is deprecated in 0.18 and "
                "will be removed in 0.20. Use 'mse_path_' instead")
    def cv_mse_path_(self):
        return self.mse_path_

class LassoLarsCV(LarsCV):
    
    method = 'lasso'

class LassoLarsIC(LassoLars):
        def __init__(self, criterion='aic', fit_intercept=True, verbose=False,
                 normalize=True, precompute='auto', max_iter=500,
                 eps=np.finfo(np.float).eps, copy_X=True, positive=False):
        self.criterion = criterion
        self.fit_intercept = fit_intercept
        self.positive = positive
        self.max_iter = max_iter
        self.verbose = verbose
        self.normalize = normalize
        self.copy_X = copy_X
        self.precompute = precompute
        self.eps = eps
        self.fit_path = True
    def fit(self, X, y, copy_X=True):
                X, y = check_X_y(X, y, y_numeric=True)
        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(
            X, y, self.fit_intercept, self.normalize, self.copy_X)
        max_iter = self.max_iter
        Gram = self._get_gram()
        alphas_, active_, coef_path_, self.n_iter_ = lars_path(
            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,
            method='lasso', verbose=self.verbose, max_iter=max_iter,
            eps=self.eps, return_n_iter=True, positive=self.positive)
        n_samples = X.shape[0]
        if self.criterion == 'aic':
            K = 2          elif self.criterion == 'bic':
            K = log(n_samples)          else:
            raise ValueError('criterion should be either bic or aic')
        R = y[:, np.newaxis] - np.dot(X, coef_path_)          mean_squared_error = np.mean(R ** 2, axis=0)
        df = np.zeros(coef_path_.shape[1], dtype=np.int)          for k, coef in enumerate(coef_path_.T):
            mask = np.abs(coef) > np.finfo(coef.dtype).eps
            if not np.any(mask):
                continue
                                                df[k] = np.sum(mask)
        self.alphas_ = alphas_
        with np.errstate(divide='ignore'):
            self.criterion_ = n_samples * np.log(mean_squared_error) + K * df
        n_best = np.argmin(self.criterion_)
        self.alpha_ = alphas_[n_best]
        self.coef_ = coef_path_[:, n_best]
        self._set_intercept(Xmean, ymean, Xstd)
        return self

import numbers
import warnings
import numpy as np
from scipy import optimize, sparse
from .base import LinearClassifierMixin, SparseCoefMixin, BaseEstimator
from .sag import sag_solver
from ..preprocessing import LabelEncoder, LabelBinarizer
from ..svm.base import _fit_liblinear
from ..utils import check_array, check_consistent_length, compute_class_weight
from ..utils import check_random_state
from ..utils.extmath import (logsumexp, log_logistic, safe_sparse_dot,
                             softmax, squared_norm)
from ..utils.extmath import row_norms
from ..utils.optimize import newton_cg
from ..utils.validation import check_X_y
from ..exceptions import NotFittedError
from ..utils.fixes import expit
from ..utils.multiclass import check_classification_targets
from ..externals.joblib import Parallel, delayed
from ..model_selection import check_cv
from ..externals import six
from ..metrics import SCORERS

def _intercept_dot(w, X, y):
        c = 0.
    if w.size == X.shape[1] + 1:
        c = w[-1]
        w = w[:-1]
    z = safe_sparse_dot(X, w) + c
    yz = y * z
    return w, c, yz

def _logistic_loss_and_grad(w, X, y, alpha, sample_weight=None):
        n_samples, n_features = X.shape
    grad = np.empty_like(w)
    w, c, yz = _intercept_dot(w, X, y)
    if sample_weight is None:
        sample_weight = np.ones(n_samples)
        out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)
    z = expit(yz)
    z0 = sample_weight * (z - 1) * y
    grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w
        if grad.shape[0] > n_features:
        grad[-1] = z0.sum()
    return out, grad

def _logistic_loss(w, X, y, alpha, sample_weight=None):
        w, c, yz = _intercept_dot(w, X, y)
    if sample_weight is None:
        sample_weight = np.ones(y.shape[0])
        out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)
    return out

def _logistic_grad_hess(w, X, y, alpha, sample_weight=None):
        n_samples, n_features = X.shape
    grad = np.empty_like(w)
    fit_intercept = grad.shape[0] > n_features
    w, c, yz = _intercept_dot(w, X, y)
    if sample_weight is None:
        sample_weight = np.ones(y.shape[0])
    z = expit(yz)
    z0 = sample_weight * (z - 1) * y
    grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w
        if fit_intercept:
        grad[-1] = z0.sum()
        d = sample_weight * z * (1 - z)
    if sparse.issparse(X):
        dX = safe_sparse_dot(sparse.dia_matrix((d, 0),
                             shape=(n_samples, n_samples)), X)
    else:
                dX = d[:, np.newaxis] * X
    if fit_intercept:
                        dd_intercept = np.squeeze(np.array(dX.sum(axis=0)))
    def Hs(s):
        ret = np.empty_like(s)
        ret[:n_features] = X.T.dot(dX.dot(s[:n_features]))
        ret[:n_features] += alpha * s[:n_features]
                if fit_intercept:
            ret[:n_features] += s[-1] * dd_intercept
            ret[-1] = dd_intercept.dot(s[:n_features])
            ret[-1] += d.sum() * s[-1]
        return ret
    return grad, Hs

def _multinomial_loss(w, X, Y, alpha, sample_weight):
        n_classes = Y.shape[1]
    n_features = X.shape[1]
    fit_intercept = w.size == (n_classes * (n_features + 1))
    w = w.reshape(n_classes, -1)
    sample_weight = sample_weight[:, np.newaxis]
    if fit_intercept:
        intercept = w[:, -1]
        w = w[:, :-1]
    else:
        intercept = 0
    p = safe_sparse_dot(X, w.T)
    p += intercept
    p -= logsumexp(p, axis=1)[:, np.newaxis]
    loss = -(sample_weight * Y * p).sum()
    loss += 0.5 * alpha * squared_norm(w)
    p = np.exp(p, p)
    return loss, p, w

def _multinomial_loss_grad(w, X, Y, alpha, sample_weight):
        n_classes = Y.shape[1]
    n_features = X.shape[1]
    fit_intercept = (w.size == n_classes * (n_features + 1))
    grad = np.zeros((n_classes, n_features + bool(fit_intercept)))
    loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
    sample_weight = sample_weight[:, np.newaxis]
    diff = sample_weight * (p - Y)
    grad[:, :n_features] = safe_sparse_dot(diff.T, X)
    grad[:, :n_features] += alpha * w
    if fit_intercept:
        grad[:, -1] = diff.sum(axis=0)
    return loss, grad.ravel(), p

def _multinomial_grad_hess(w, X, Y, alpha, sample_weight):
        n_features = X.shape[1]
    n_classes = Y.shape[1]
    fit_intercept = w.size == (n_classes * (n_features + 1))
            loss, grad, p = _multinomial_loss_grad(w, X, Y, alpha, sample_weight)
    sample_weight = sample_weight[:, np.newaxis]
            def hessp(v):
        v = v.reshape(n_classes, -1)
        if fit_intercept:
            inter_terms = v[:, -1]
            v = v[:, :-1]
        else:
            inter_terms = 0
                        r_yhat = safe_sparse_dot(X, v.T)
        r_yhat += inter_terms
        r_yhat += (-p * r_yhat).sum(axis=1)[:, np.newaxis]
        r_yhat *= p
        r_yhat *= sample_weight
        hessProd = np.zeros((n_classes, n_features + bool(fit_intercept)))
        hessProd[:, :n_features] = safe_sparse_dot(r_yhat.T, X)
        hessProd[:, :n_features] += v * alpha
        if fit_intercept:
            hessProd[:, -1] = r_yhat.sum(axis=0)
        return hessProd.ravel()
    return grad, hessp

def _check_solver_option(solver, multi_class, penalty, dual):
    if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag']:
        raise ValueError("Logistic Regression supports only liblinear,"
                         " newton-cg, lbfgs and sag solvers, got %s" % solver)
    if multi_class not in ['multinomial', 'ovr']:
        raise ValueError("multi_class should be either multinomial or "
                         "ovr, got %s" % multi_class)
    if multi_class == 'multinomial' and solver == 'liblinear':
        raise ValueError("Solver %s does not support "
                         "a multinomial backend." % solver)
    if solver != 'liblinear':
        if penalty != 'l2':
            raise ValueError("Solver %s supports only l2 penalties, "
                             "got %s penalty." % (solver, penalty))
        if dual:
            raise ValueError("Solver %s supports only "
                             "dual=False, got dual=%s" % (solver, dual))

def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                             max_iter=100, tol=1e-4, verbose=0,
                             solver='lbfgs', coef=None,
                             class_weight=None, dual=False, penalty='l2',
                             intercept_scaling=1., multi_class='ovr',
                             random_state=None, check_input=True,
                             max_squared_sum=None, sample_weight=None):
        if isinstance(Cs, numbers.Integral):
        Cs = np.logspace(-4, 4, Cs)
    _check_solver_option(solver, multi_class, penalty, dual)
        if check_input:
        X = check_array(X, accept_sparse='csr', dtype=np.float64)
        y = check_array(y, ensure_2d=False, dtype=None)
        check_consistent_length(X, y)
    _, n_features = X.shape
    classes = np.unique(y)
    random_state = check_random_state(random_state)
    if pos_class is None and multi_class != 'multinomial':
        if (classes.size > 2):
            raise ValueError('To fit OvR, use the pos_class argument')
                pos_class = classes[1]
                if sample_weight is not None:
        sample_weight = np.array(sample_weight, dtype=np.float64, order='C')
        check_consistent_length(y, sample_weight)
    else:
        sample_weight = np.ones(X.shape[0])
                le = LabelEncoder()
    if isinstance(class_weight, dict) or multi_class == 'multinomial':
        class_weight_ = compute_class_weight(class_weight, classes, y)
        sample_weight *= class_weight_[le.fit_transform(y)]
            if multi_class == 'ovr':
        w0 = np.zeros(n_features + int(fit_intercept))
        mask_classes = np.array([-1, 1])
        mask = (y == pos_class)
        y_bin = np.ones(y.shape, dtype=np.float64)
        y_bin[~mask] = -1.
        
        if class_weight == "balanced":
            class_weight_ = compute_class_weight(class_weight, mask_classes,
                                                 y_bin)
            sample_weight *= class_weight_[le.fit_transform(y_bin)]
    else:
        if solver != 'sag':
            lbin = LabelBinarizer()
            Y_multi = lbin.fit_transform(y)
            if Y_multi.shape[1] == 1:
                Y_multi = np.hstack([1 - Y_multi, Y_multi])
        else:
                        le = LabelEncoder()
            Y_multi = le.fit_transform(y)
        w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
                      order='F')
    if coef is not None:
                if multi_class == 'ovr':
            if coef.size not in (n_features, w0.size):
                raise ValueError(
                    'Initialization coef is of shape %d, expected shape '
                    '%d or %d' % (coef.size, n_features, w0.size))
            w0[:coef.size] = coef
        else:
                                    n_classes = classes.size
            if n_classes == 2:
                n_classes = 1
            if (coef.shape[0] != n_classes or
                    coef.shape[1] not in (n_features, n_features + 1)):
                raise ValueError(
                    'Initialization coef is of shape (%d, %d), expected '
                    'shape (%d, %d) or (%d, %d)' % (
                        coef.shape[0], coef.shape[1], classes.size,
                        n_features, classes.size, n_features + 1))
            w0[:, :coef.shape[1]] = coef
    if multi_class == 'multinomial':
                if solver in ['lbfgs', 'newton-cg']:
            w0 = w0.ravel()
        target = Y_multi
        if solver == 'lbfgs':
            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
        elif solver == 'newton-cg':
            func = lambda x, *args: _multinomial_loss(x, *args)[0]
            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]
            hess = _multinomial_grad_hess
        warm_start_sag = {'coef': w0.T}
    else:
        target = y_bin
        if solver == 'lbfgs':
            func = _logistic_loss_and_grad
        elif solver == 'newton-cg':
            func = _logistic_loss
            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]
            hess = _logistic_grad_hess
        warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
    coefs = list()
    n_iter = np.zeros(len(Cs), dtype=np.int32)
    for i, C in enumerate(Cs):
        if solver == 'lbfgs':
            try:
                w0, loss, info = optimize.fmin_l_bfgs_b(
                    func, w0, fprime=None,
                    args=(X, target, 1. / C, sample_weight),
                    iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
            except TypeError:
                                w0, loss, info = optimize.fmin_l_bfgs_b(
                    func, w0, fprime=None,
                    args=(X, target, 1. / C, sample_weight),
                    iprint=(verbose > 0) - 1, pgtol=tol)
            if info["warnflag"] == 1 and verbose > 0:
                warnings.warn("lbfgs failed to converge. Increase the number "
                              "of iterations.")
            try:
                n_iter_i = info['nit'] - 1
            except:
                n_iter_i = info['funcalls'] - 1
        elif solver == 'newton-cg':
            args = (X, target, 1. / C, sample_weight)
            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,
                                     maxiter=max_iter, tol=tol)
        elif solver == 'liblinear':
            coef_, intercept_, n_iter_i, = _fit_liblinear(
                X, target, C, fit_intercept, intercept_scaling, None,
                penalty, dual, verbose, max_iter, tol, random_state,
                sample_weight=sample_weight)
            if fit_intercept:
                w0 = np.concatenate([coef_.ravel(), intercept_])
            else:
                w0 = coef_.ravel()
        elif solver == 'sag':
            if multi_class == 'multinomial':
                target = target.astype(np.float64)
                loss = 'multinomial'
            else:
                loss = 'log'
            w0, n_iter_i, warm_start_sag = sag_solver(
                X, target, sample_weight, loss, 1. / C, max_iter, tol,
                verbose, random_state, False, max_squared_sum, warm_start_sag)
        else:
            raise ValueError("solver must be one of {'liblinear', 'lbfgs', "
                             "'newton-cg', 'sag'}, got '%s' instead" % solver)
        if multi_class == 'multinomial':
            multi_w0 = np.reshape(w0, (classes.size, -1))
            if classes.size == 2:
                multi_w0 = multi_w0[1][np.newaxis, :]
            coefs.append(multi_w0)
        else:
            coefs.append(w0.copy())
        n_iter[i] = n_iter_i
    return coefs, np.array(Cs), n_iter

def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
                          scoring=None, fit_intercept=False,
                          max_iter=100, tol=1e-4, class_weight=None,
                          verbose=0, solver='lbfgs', penalty='l2',
                          dual=False, intercept_scaling=1.,
                          multi_class='ovr', random_state=None,
                          max_squared_sum=None, sample_weight=None):
        _check_solver_option(solver, multi_class, penalty, dual)
    X_train = X[train]
    X_test = X[test]
    y_train = y[train]
    y_test = y[test]
    if sample_weight is not None:
        sample_weight = check_array(sample_weight, ensure_2d=False)
        check_consistent_length(y, sample_weight)
        sample_weight = sample_weight[train]
    coefs, Cs, n_iter = logistic_regression_path(
        X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
        solver=solver, max_iter=max_iter, class_weight=class_weight,
        pos_class=pos_class, multi_class=multi_class,
        tol=tol, verbose=verbose, dual=dual, penalty=penalty,
        intercept_scaling=intercept_scaling, random_state=random_state,
        check_input=False, max_squared_sum=max_squared_sum,
        sample_weight=sample_weight)
    log_reg = LogisticRegression(fit_intercept=fit_intercept)
        if multi_class == 'ovr':
        log_reg.classes_ = np.array([-1, 1])
    elif multi_class == 'multinomial':
        log_reg.classes_ = np.unique(y_train)
    else:
        raise ValueError("multi_class should be either multinomial or ovr, "
                         "got %d" % multi_class)
    if pos_class is not None:
        mask = (y_test == pos_class)
        y_test = np.ones(y_test.shape, dtype=np.float64)
        y_test[~mask] = -1.
    scores = list()
    if isinstance(scoring, six.string_types):
        scoring = SCORERS[scoring]
    for w in coefs:
        if multi_class == 'ovr':
            w = w[np.newaxis, :]
        if fit_intercept:
            log_reg.coef_ = w[:, :-1]
            log_reg.intercept_ = w[:, -1]
        else:
            log_reg.coef_ = w
            log_reg.intercept_ = 0.
        if scoring is None:
            scores.append(log_reg.score(X_test, y_test))
        else:
            scores.append(scoring(log_reg, X_test, y_test))
    return coefs, Cs, np.array(scores), n_iter

class LogisticRegression(BaseEstimator, LinearClassifierMixin,
                         SparseCoefMixin):
    
    def __init__(self, penalty='l2', dual=False, tol=1e-4, C=1.0,
                 fit_intercept=True, intercept_scaling=1, class_weight=None,
                 random_state=None, solver='liblinear', max_iter=100,
                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):
        self.penalty = penalty
        self.dual = dual
        self.tol = tol
        self.C = C
        self.fit_intercept = fit_intercept
        self.intercept_scaling = intercept_scaling
        self.class_weight = class_weight
        self.random_state = random_state
        self.solver = solver
        self.max_iter = max_iter
        self.multi_class = multi_class
        self.verbose = verbose
        self.warm_start = warm_start
        self.n_jobs = n_jobs
    def fit(self, X, y, sample_weight=None):
                if not isinstance(self.C, numbers.Number) or self.C < 0:
            raise ValueError("Penalty term must be positive; got (C=%r)"
                             % self.C)
        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
            raise ValueError("Maximum number of iteration must be positive;"
                             " got (max_iter=%r)" % self.max_iter)
        if not isinstance(self.tol, numbers.Number) or self.tol < 0:
            raise ValueError("Tolerance for stopping criteria must be "
                             "positive; got (tol=%r)" % self.tol)
        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
                         order="C")
        check_classification_targets(y)
        self.classes_ = np.unique(y)
        n_samples, n_features = X.shape
        _check_solver_option(self.solver, self.multi_class, self.penalty,
                             self.dual)
        if self.solver == 'liblinear':
            self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
                X, y, self.C, self.fit_intercept, self.intercept_scaling,
                self.class_weight, self.penalty, self.dual, self.verbose,
                self.max_iter, self.tol, self.random_state,
                sample_weight=sample_weight)
            self.n_iter_ = np.array([n_iter_])
            return self
        if self.solver == 'sag':
            max_squared_sum = row_norms(X, squared=True).max()
        else:
            max_squared_sum = None
        n_classes = len(self.classes_)
        classes_ = self.classes_
        if n_classes < 2:
            raise ValueError("This solver needs samples of at least 2 classes"
                             " in the data, but the data contains only one"
                             " class: %r" % classes_[0])
        if len(self.classes_) == 2:
            n_classes = 1
            classes_ = classes_[1:]
        if self.warm_start:
            warm_start_coef = getattr(self, 'coef_', None)
        else:
            warm_start_coef = None
        if warm_start_coef is not None and self.fit_intercept:
            warm_start_coef = np.append(warm_start_coef,
                                        self.intercept_[:, np.newaxis],
                                        axis=1)
        self.coef_ = list()
        self.intercept_ = np.zeros(n_classes)
                if self.multi_class == 'multinomial':
            classes_ = [None]
            warm_start_coef = [warm_start_coef]
        if warm_start_coef is None:
            warm_start_coef = [None] * n_classes
        path_func = delayed(logistic_regression_path)
                        backend = 'threading' if self.solver == 'sag' else 'multiprocessing'
        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                               backend=backend)(
            path_func(X, y, pos_class=class_, Cs=[self.C],
                      fit_intercept=self.fit_intercept, tol=self.tol,
                      verbose=self.verbose, solver=self.solver,
                      multi_class=self.multi_class, max_iter=self.max_iter,
                      class_weight=self.class_weight, check_input=False,
                      random_state=self.random_state, coef=warm_start_coef_,
                      max_squared_sum=max_squared_sum,
                      sample_weight=sample_weight)
            for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))
        fold_coefs_, _, n_iter_ = zip(*fold_coefs_)
        self.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, 0]
        if self.multi_class == 'multinomial':
            self.coef_ = fold_coefs_[0][0]
        else:
            self.coef_ = np.asarray(fold_coefs_)
            self.coef_ = self.coef_.reshape(n_classes, n_features +
                                            int(self.fit_intercept))
        if self.fit_intercept:
            self.intercept_ = self.coef_[:, -1]
            self.coef_ = self.coef_[:, :-1]
        return self
    def predict_proba(self, X):
                if not hasattr(self, "coef_"):
            raise NotFittedError("Call fit before prediction")
        calculate_ovr = self.coef_.shape[0] == 1 or self.multi_class == "ovr"
        if calculate_ovr:
            return super(LogisticRegression, self)._predict_proba_lr(X)
        else:
            return softmax(self.decision_function(X), copy=False)
    def predict_log_proba(self, X):
                return np.log(self.predict_proba(X))

class LogisticRegressionCV(LogisticRegression, BaseEstimator,
                           LinearClassifierMixin):
    
    def __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False,
                 penalty='l2', scoring=None, solver='lbfgs', tol=1e-4,
                 max_iter=100, class_weight=None, n_jobs=1, verbose=0,
                 refit=True, intercept_scaling=1., multi_class='ovr',
                 random_state=None):
        self.Cs = Cs
        self.fit_intercept = fit_intercept
        self.cv = cv
        self.dual = dual
        self.penalty = penalty
        self.scoring = scoring
        self.tol = tol
        self.max_iter = max_iter
        self.class_weight = class_weight
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.solver = solver
        self.refit = refit
        self.intercept_scaling = intercept_scaling
        self.multi_class = multi_class
        self.random_state = random_state
    def fit(self, X, y, sample_weight=None):
                _check_solver_option(self.solver, self.multi_class, self.penalty,
                             self.dual)
        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
            raise ValueError("Maximum number of iteration must be positive;"
                             " got (max_iter=%r)" % self.max_iter)
        if not isinstance(self.tol, numbers.Number) or self.tol < 0:
            raise ValueError("Tolerance for stopping criteria must be "
                             "positive; got (tol=%r)" % self.tol)
        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
                         order="C")
        check_classification_targets(y)
        class_weight = self.class_weight
                label_encoder = LabelEncoder().fit(y)
        y = label_encoder.transform(y)
        if isinstance(class_weight, dict):
            class_weight = dict((label_encoder.transform([cls])[0], v)
                                for cls, v in class_weight.items())
                classes = self.classes_ = label_encoder.classes_
        encoded_labels = label_encoder.transform(label_encoder.classes_)
        if self.solver == 'sag':
            max_squared_sum = row_norms(X, squared=True).max()
        else:
            max_squared_sum = None
                cv = check_cv(self.cv, y, classifier=True)
        folds = list(cv.split(X, y))
                n_classes = len(encoded_labels)
        if n_classes < 2:
            raise ValueError("This solver needs samples of at least 2 classes"
                             " in the data, but the data contains only one"
                             " class: %r" % classes[0])
        if n_classes == 2:
                                    n_classes = 1
            encoded_labels = encoded_labels[1:]
            classes = classes[1:]
                        if self.multi_class == 'multinomial':
            iter_encoded_labels = iter_classes = [None]
        else:
            iter_encoded_labels = encoded_labels
            iter_classes = classes
                if class_weight == "balanced":
            class_weight = compute_class_weight(class_weight,
                                                np.arange(len(self.classes_)),
                                                y)
            class_weight = dict(enumerate(class_weight))
        path_func = delayed(_log_reg_scoring_path)
                        backend = 'threading' if self.solver == 'sag' else 'multiprocessing'
        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                               backend=backend)(
            path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
                      fit_intercept=self.fit_intercept, penalty=self.penalty,
                      dual=self.dual, solver=self.solver, tol=self.tol,
                      max_iter=self.max_iter, verbose=self.verbose,
                      class_weight=class_weight, scoring=self.scoring,
                      multi_class=self.multi_class,
                      intercept_scaling=self.intercept_scaling,
                      random_state=self.random_state,
                      max_squared_sum=max_squared_sum,
                      sample_weight=sample_weight
                      )
            for label in iter_encoded_labels
            for train, test in folds)
        if self.multi_class == 'multinomial':
            multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)
            multi_coefs_paths = np.asarray(multi_coefs_paths)
            multi_scores = np.asarray(multi_scores)
                                                                        coefs_paths = np.rollaxis(multi_coefs_paths, 2, 0)
                                                scores = np.tile(multi_scores, (n_classes, 1, 1))
            self.Cs_ = Cs[0]
            self.n_iter_ = np.reshape(n_iter_, (1, len(folds),
                                                len(self.Cs_)))
        else:
            coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)
            self.Cs_ = Cs[0]
            coefs_paths = np.reshape(coefs_paths, (n_classes, len(folds),
                                                   len(self.Cs_), -1))
            self.n_iter_ = np.reshape(n_iter_, (n_classes, len(folds),
                                                len(self.Cs_)))
        self.coefs_paths_ = dict(zip(classes, coefs_paths))
        scores = np.reshape(scores, (n_classes, len(folds), -1))
        self.scores_ = dict(zip(classes, scores))
        self.C_ = list()
        self.coef_ = np.empty((n_classes, X.shape[1]))
        self.intercept_ = np.zeros(n_classes)
                if self.multi_class == 'multinomial':
            scores = multi_scores
            coefs_paths = multi_coefs_paths
        for index, (cls, encoded_label) in enumerate(
                zip(iter_classes, iter_encoded_labels)):
            if self.multi_class == 'ovr':
                                                scores = self.scores_[cls]
                coefs_paths = self.coefs_paths_[cls]
            if self.refit:
                best_index = scores.sum(axis=0).argmax()
                C_ = self.Cs_[best_index]
                self.C_.append(C_)
                if self.multi_class == 'multinomial':
                    coef_init = np.mean(coefs_paths[:, best_index, :, :],
                                        axis=0)
                else:
                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)
                                                w, _, _ = logistic_regression_path(
                    X, y, pos_class=encoded_label, Cs=[C_], solver=self.solver,
                    fit_intercept=self.fit_intercept, coef=coef_init,
                    max_iter=self.max_iter, tol=self.tol,
                    penalty=self.penalty,
                    class_weight=class_weight,
                    multi_class=self.multi_class,
                    verbose=max(0, self.verbose - 1),
                    random_state=self.random_state,
                    check_input=False, max_squared_sum=max_squared_sum,
                    sample_weight=sample_weight)
                w = w[0]
            else:
                                                best_indices = np.argmax(scores, axis=1)
                w = np.mean([coefs_paths[i][best_indices[i]]
                             for i in range(len(folds))], axis=0)
                self.C_.append(np.mean(self.Cs_[best_indices]))
            if self.multi_class == 'multinomial':
                self.C_ = np.tile(self.C_, n_classes)
                self.coef_ = w[:, :X.shape[1]]
                if self.fit_intercept:
                    self.intercept_ = w[:, -1]
            else:
                self.coef_[index] = w[: X.shape[1]]
                if self.fit_intercept:
                    self.intercept_[index] = w[-1]
        self.C_ = np.asarray(self.C_)
        return self

import warnings
from distutils.version import LooseVersion
import numpy as np
from scipy import linalg
from scipy.linalg.lapack import get_lapack_funcs
from .base import LinearModel, _pre_fit
from ..base import RegressorMixin
from ..utils import as_float_array, check_array, check_X_y
from ..model_selection import check_cv
from ..externals.joblib import Parallel, delayed
import scipy
solve_triangular_args = {}
if LooseVersion(scipy.__version__) >= LooseVersion('0.12'):
        solve_triangular_args = {'check_finite': False}

premature = 
def _cholesky_omp(X, y, n_nonzero_coefs, tol=None, copy_X=True,
                  return_path=False):
        if copy_X:
        X = X.copy('F')
    else:          X = np.asfortranarray(X)
    min_float = np.finfo(X.dtype).eps
    nrm2, swap = linalg.get_blas_funcs(('nrm2', 'swap'), (X,))
    potrs, = get_lapack_funcs(('potrs',), (X,))
    alpha = np.dot(X.T, y)
    residual = y
    gamma = np.empty(0)
    n_active = 0
    indices = np.arange(X.shape[1])  
    max_features = X.shape[1] if tol is not None else n_nonzero_coefs
    if solve_triangular_args:
                L = np.empty((max_features, max_features), dtype=X.dtype)
    else:
                L = np.zeros((max_features, max_features), dtype=X.dtype)
    L[0, 0] = 1.
    if return_path:
        coefs = np.empty_like(L)
    while True:
        lam = np.argmax(np.abs(np.dot(X.T, residual)))
        if lam < n_active or alpha[lam] ** 2 < min_float:
                        warnings.warn(premature, RuntimeWarning, stacklevel=2)
            break
        if n_active > 0:
                        L[n_active, :n_active] = np.dot(X[:, :n_active].T, X[:, lam])
            linalg.solve_triangular(L[:n_active, :n_active],
                                    L[n_active, :n_active],
                                    trans=0, lower=1,
                                    overwrite_b=True,
                                    **solve_triangular_args)
            v = nrm2(L[n_active, :n_active]) ** 2
            if 1 - v <= min_float:                  warnings.warn(premature, RuntimeWarning, stacklevel=2)
                break
            L[n_active, n_active] = np.sqrt(1 - v)
        X.T[n_active], X.T[lam] = swap(X.T[n_active], X.T[lam])
        alpha[n_active], alpha[lam] = alpha[lam], alpha[n_active]
        indices[n_active], indices[lam] = indices[lam], indices[n_active]
        n_active += 1
                gamma, _ = potrs(L[:n_active, :n_active], alpha[:n_active], lower=True,
                         overwrite_b=False)
        if return_path:
            coefs[:n_active, n_active - 1] = gamma
        residual = y - np.dot(X[:, :n_active], gamma)
        if tol is not None and nrm2(residual) ** 2 <= tol:
            break
        elif n_active == max_features:
            break
    if return_path:
        return gamma, indices[:n_active], coefs[:, :n_active], n_active
    else:
        return gamma, indices[:n_active], n_active

def _gram_omp(Gram, Xy, n_nonzero_coefs, tol_0=None, tol=None,
              copy_Gram=True, copy_Xy=True, return_path=False):
        Gram = Gram.copy('F') if copy_Gram else np.asfortranarray(Gram)
    if copy_Xy:
        Xy = Xy.copy()
    min_float = np.finfo(Gram.dtype).eps
    nrm2, swap = linalg.get_blas_funcs(('nrm2', 'swap'), (Gram,))
    potrs, = get_lapack_funcs(('potrs',), (Gram,))
    indices = np.arange(len(Gram))      alpha = Xy
    tol_curr = tol_0
    delta = 0
    gamma = np.empty(0)
    n_active = 0
    max_features = len(Gram) if tol is not None else n_nonzero_coefs
    if solve_triangular_args:
                L = np.empty((max_features, max_features), dtype=Gram.dtype)
    else:
                L = np.zeros((max_features, max_features), dtype=Gram.dtype)
    L[0, 0] = 1.
    if return_path:
        coefs = np.empty_like(L)
    while True:
        lam = np.argmax(np.abs(alpha))
        if lam < n_active or alpha[lam] ** 2 < min_float:
                        warnings.warn(premature, RuntimeWarning, stacklevel=3)
            break
        if n_active > 0:
            L[n_active, :n_active] = Gram[lam, :n_active]
            linalg.solve_triangular(L[:n_active, :n_active],
                                    L[n_active, :n_active],
                                    trans=0, lower=1,
                                    overwrite_b=True,
                                    **solve_triangular_args)
            v = nrm2(L[n_active, :n_active]) ** 2
            if 1 - v <= min_float:                  warnings.warn(premature, RuntimeWarning, stacklevel=3)
                break
            L[n_active, n_active] = np.sqrt(1 - v)
        Gram[n_active], Gram[lam] = swap(Gram[n_active], Gram[lam])
        Gram.T[n_active], Gram.T[lam] = swap(Gram.T[n_active], Gram.T[lam])
        indices[n_active], indices[lam] = indices[lam], indices[n_active]
        Xy[n_active], Xy[lam] = Xy[lam], Xy[n_active]
        n_active += 1
                gamma, _ = potrs(L[:n_active, :n_active], Xy[:n_active], lower=True,
                         overwrite_b=False)
        if return_path:
            coefs[:n_active, n_active - 1] = gamma
        beta = np.dot(Gram[:, :n_active], gamma)
        alpha = Xy - beta
        if tol is not None:
            tol_curr += delta
            delta = np.inner(gamma, beta[:n_active])
            tol_curr -= delta
            if abs(tol_curr) <= tol:
                break
        elif n_active == max_features:
            break
    if return_path:
        return gamma, indices[:n_active], coefs[:, :n_active], n_active
    else:
        return gamma, indices[:n_active], n_active

def orthogonal_mp(X, y, n_nonzero_coefs=None, tol=None, precompute=False,
                  copy_X=True, return_path=False,
                  return_n_iter=False):
        X = check_array(X, order='F', copy=copy_X)
    copy_X = False
    if y.ndim == 1:
        y = y.reshape(-1, 1)
    y = check_array(y)
    if y.shape[1] > 1:          copy_X = True
    if n_nonzero_coefs is None and tol is None:
                        n_nonzero_coefs = max(int(0.1 * X.shape[1]), 1)
    if tol is not None and tol < 0:
        raise ValueError("Epsilon cannot be negative")
    if tol is None and n_nonzero_coefs <= 0:
        raise ValueError("The number of atoms must be positive")
    if tol is None and n_nonzero_coefs > X.shape[1]:
        raise ValueError("The number of atoms cannot be more than the number "
                         "of features")
    if precompute == 'auto':
        precompute = X.shape[0] > X.shape[1]
    if precompute:
        G = np.dot(X.T, X)
        G = np.asfortranarray(G)
        Xy = np.dot(X.T, y)
        if tol is not None:
            norms_squared = np.sum((y ** 2), axis=0)
        else:
            norms_squared = None
        return orthogonal_mp_gram(G, Xy, n_nonzero_coefs, tol, norms_squared,
                                  copy_Gram=copy_X, copy_Xy=False,
                                  return_path=return_path)
    if return_path:
        coef = np.zeros((X.shape[1], y.shape[1], X.shape[1]))
    else:
        coef = np.zeros((X.shape[1], y.shape[1]))
    n_iters = []
    for k in range(y.shape[1]):
        out = _cholesky_omp(
            X, y[:, k], n_nonzero_coefs, tol,
            copy_X=copy_X, return_path=return_path)
        if return_path:
            _, idx, coefs, n_iter = out
            coef = coef[:, :, :len(idx)]
            for n_active, x in enumerate(coefs.T):
                coef[idx[:n_active + 1], k, n_active] = x[:n_active + 1]
        else:
            x, idx, n_iter = out
            coef[idx, k] = x
        n_iters.append(n_iter)
    if y.shape[1] == 1:
        n_iters = n_iters[0]
    if return_n_iter:
        return np.squeeze(coef), n_iters
    else:
        return np.squeeze(coef)

def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None,
                       norms_squared=None, copy_Gram=True,
                       copy_Xy=True, return_path=False,
                       return_n_iter=False):
        Gram = check_array(Gram, order='F', copy=copy_Gram)
    Xy = np.asarray(Xy)
    if Xy.ndim > 1 and Xy.shape[1] > 1:
                copy_Gram = True
    if Xy.ndim == 1:
        Xy = Xy[:, np.newaxis]
        if tol is not None:
            norms_squared = [norms_squared]
    if n_nonzero_coefs is None and tol is None:
        n_nonzero_coefs = int(0.1 * len(Gram))
    if tol is not None and norms_squared is None:
        raise ValueError('Gram OMP needs the precomputed norms in order '
                         'to evaluate the error sum of squares.')
    if tol is not None and tol < 0:
        raise ValueError("Epsilon cannot be negative")
    if tol is None and n_nonzero_coefs <= 0:
        raise ValueError("The number of atoms must be positive")
    if tol is None and n_nonzero_coefs > len(Gram):
        raise ValueError("The number of atoms cannot be more than the number "
                         "of features")
    if return_path:
        coef = np.zeros((len(Gram), Xy.shape[1], len(Gram)))
    else:
        coef = np.zeros((len(Gram), Xy.shape[1]))
    n_iters = []
    for k in range(Xy.shape[1]):
        out = _gram_omp(
            Gram, Xy[:, k], n_nonzero_coefs,
            norms_squared[k] if tol is not None else None, tol,
            copy_Gram=copy_Gram, copy_Xy=copy_Xy,
            return_path=return_path)
        if return_path:
            _, idx, coefs, n_iter = out
            coef = coef[:, :, :len(idx)]
            for n_active, x in enumerate(coefs.T):
                coef[idx[:n_active + 1], k, n_active] = x[:n_active + 1]
        else:
            x, idx, n_iter = out
            coef[idx, k] = x
        n_iters.append(n_iter)
    if Xy.shape[1] == 1:
        n_iters = n_iters[0]
    if return_n_iter:
        return np.squeeze(coef), n_iters
    else:
        return np.squeeze(coef)

class OrthogonalMatchingPursuit(LinearModel, RegressorMixin):
        def __init__(self, n_nonzero_coefs=None, tol=None, fit_intercept=True,
                 normalize=True, precompute='auto'):
        self.n_nonzero_coefs = n_nonzero_coefs
        self.tol = tol
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.precompute = precompute
    def fit(self, X, y):
                X, y = check_X_y(X, y, multi_output=True, y_numeric=True)
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale, Gram, Xy = \
            _pre_fit(X, y, None, self.precompute, self.normalize,
                     self.fit_intercept, copy=True)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        if self.n_nonzero_coefs is None and self.tol is None:
                                    self.n_nonzero_coefs_ = max(int(0.1 * n_features), 1)
        else:
            self.n_nonzero_coefs_ = self.n_nonzero_coefs
        if Gram is False:
            coef_, self.n_iter_ = orthogonal_mp(
                X, y, self.n_nonzero_coefs_, self.tol,
                precompute=False, copy_X=True,
                return_n_iter=True)
        else:
            norms_sq = np.sum(y ** 2, axis=0) if self.tol is not None else None
            coef_, self.n_iter_ = orthogonal_mp_gram(
                Gram, Xy=Xy, n_nonzero_coefs=self.n_nonzero_coefs_,
                tol=self.tol, norms_squared=norms_sq,
                copy_Gram=True, copy_Xy=True,
                return_n_iter=True)
        self.coef_ = coef_.T
        self._set_intercept(X_offset, y_offset, X_scale)
        return self

def _omp_path_residues(X_train, y_train, X_test, y_test, copy=True,
                       fit_intercept=True, normalize=True, max_iter=100):
    
    if copy:
        X_train = X_train.copy()
        y_train = y_train.copy()
        X_test = X_test.copy()
        y_test = y_test.copy()
    if fit_intercept:
        X_mean = X_train.mean(axis=0)
        X_train -= X_mean
        X_test -= X_mean
        y_mean = y_train.mean(axis=0)
        y_train = as_float_array(y_train, copy=False)
        y_train -= y_mean
        y_test = as_float_array(y_test, copy=False)
        y_test -= y_mean
    if normalize:
        norms = np.sqrt(np.sum(X_train ** 2, axis=0))
        nonzeros = np.flatnonzero(norms)
        X_train[:, nonzeros] /= norms[nonzeros]
    coefs = orthogonal_mp(X_train, y_train, n_nonzero_coefs=max_iter, tol=None,
                          precompute=False, copy_X=False,
                          return_path=True)
    if coefs.ndim == 1:
        coefs = coefs[:, np.newaxis]
    if normalize:
        coefs[nonzeros] /= norms[nonzeros][:, np.newaxis]
    return np.dot(coefs.T, X_test.T) - y_test

class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):
        def __init__(self, copy=True, fit_intercept=True, normalize=True,
                 max_iter=None, cv=None, n_jobs=1, verbose=False):
        self.copy = copy
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.max_iter = max_iter
        self.cv = cv
        self.n_jobs = n_jobs
        self.verbose = verbose
    def fit(self, X, y):
                X, y = check_X_y(X, y, y_numeric=True, ensure_min_features=2,
                         estimator=self)
        X = as_float_array(X, copy=False, force_all_finite=False)
        cv = check_cv(self.cv, classifier=False)
        max_iter = (min(max(int(0.1 * X.shape[1]), 5), X.shape[1])
                    if not self.max_iter
                    else self.max_iter)
        cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            delayed(_omp_path_residues)(
                X[train], y[train], X[test], y[test], self.copy,
                self.fit_intercept, self.normalize, max_iter)
            for train, test in cv.split(X))
        min_early_stop = min(fold.shape[0] for fold in cv_paths)
        mse_folds = np.array([(fold[:min_early_stop] ** 2).mean(axis=1)
                              for fold in cv_paths])
        best_n_nonzero_coefs = np.argmin(mse_folds.mean(axis=0)) + 1
        self.n_nonzero_coefs_ = best_n_nonzero_coefs
        omp = OrthogonalMatchingPursuit(n_nonzero_coefs=best_n_nonzero_coefs,
                                        fit_intercept=self.fit_intercept,
                                        normalize=self.normalize)
        omp.fit(X, y)
        self.coef_ = omp.coef_
        self.intercept_ = omp.intercept_
        self.n_iter_ = omp.n_iter_
        return self
from .stochastic_gradient import BaseSGDClassifier
from .stochastic_gradient import BaseSGDRegressor
from .stochastic_gradient import DEFAULT_EPSILON

class PassiveAggressiveClassifier(BaseSGDClassifier):
    
    def __init__(self, C=1.0, fit_intercept=True, n_iter=5, shuffle=True,
                 verbose=0, loss="hinge", n_jobs=1, random_state=None,
                 warm_start=False, class_weight=None, average=False):
        super(PassiveAggressiveClassifier, self).__init__(
            penalty=None,
            fit_intercept=fit_intercept,
            n_iter=n_iter,
            shuffle=shuffle,
            verbose=verbose,
            random_state=random_state,
            eta0=1.0,
            warm_start=warm_start,
            class_weight=class_weight,
            average=average,
            n_jobs=n_jobs)
        self.C = C
        self.loss = loss
    def partial_fit(self, X, y, classes=None):
                if self.class_weight == 'balanced':
            raise ValueError("class_weight 'balanced' is not supported for "
                             "partial_fit. For 'balanced' weights, use "
                             "`sklearn.utils.compute_class_weight` with "
                             "`class_weight='balanced'`. In place of y you "
                             "can use a large enough subset of the full "
                             "training set target to properly estimate the "
                             "class frequency distributions. Pass the "
                             "resulting weights as the class_weight "
                             "parameter.")
        lr = "pa1" if self.loss == "hinge" else "pa2"
        return self._partial_fit(X, y, alpha=1.0, C=self.C,
                                 loss="hinge", learning_rate=lr, n_iter=1,
                                 classes=classes, sample_weight=None,
                                 coef_init=None, intercept_init=None)
    def fit(self, X, y, coef_init=None, intercept_init=None):
                lr = "pa1" if self.loss == "hinge" else "pa2"
        return self._fit(X, y, alpha=1.0, C=self.C,
                         loss="hinge", learning_rate=lr,
                         coef_init=coef_init, intercept_init=intercept_init)

class PassiveAggressiveRegressor(BaseSGDRegressor):
        def __init__(self, C=1.0, fit_intercept=True, n_iter=5, shuffle=True,
                 verbose=0, loss="epsilon_insensitive",
                 epsilon=DEFAULT_EPSILON, random_state=None, warm_start=False,
                 average=False):
        super(PassiveAggressiveRegressor, self).__init__(
            penalty=None,
            l1_ratio=0,
            epsilon=epsilon,
            eta0=1.0,
            fit_intercept=fit_intercept,
            n_iter=n_iter,
            shuffle=shuffle,
            verbose=verbose,
            random_state=random_state,
            warm_start=warm_start,
            average=average)
        self.C = C
        self.loss = loss
    def partial_fit(self, X, y):
                lr = "pa1" if self.loss == "epsilon_insensitive" else "pa2"
        return self._partial_fit(X, y, alpha=1.0, C=self.C,
                                 loss="epsilon_insensitive",
                                 learning_rate=lr, n_iter=1,
                                 sample_weight=None,
                                 coef_init=None, intercept_init=None)
    def fit(self, X, y, coef_init=None, intercept_init=None):
                lr = "pa1" if self.loss == "epsilon_insensitive" else "pa2"
        return self._fit(X, y, alpha=1.0, C=self.C,
                         loss="epsilon_insensitive",
                         learning_rate=lr,
                         coef_init=coef_init,
                         intercept_init=intercept_init)
from .stochastic_gradient import BaseSGDClassifier

class Perceptron(BaseSGDClassifier):
        def __init__(self, penalty=None, alpha=0.0001, fit_intercept=True,
                 n_iter=5, shuffle=True, verbose=0, eta0=1.0, n_jobs=1,
                 random_state=0, class_weight=None, warm_start=False):
        super(Perceptron, self).__init__(loss="perceptron",
                                         penalty=penalty,
                                         alpha=alpha, l1_ratio=0,
                                         fit_intercept=fit_intercept,
                                         n_iter=n_iter,
                                         shuffle=shuffle,
                                         verbose=verbose,
                                         random_state=random_state,
                                         learning_rate="constant",
                                         eta0=eta0,
                                         power_t=0.5,
                                         warm_start=warm_start,
                                         class_weight=class_weight,
                                         n_jobs=n_jobs)
import itertools
from abc import ABCMeta, abstractmethod
import warnings
import numpy as np
from scipy.sparse import issparse
from scipy import sparse
from scipy.interpolate import interp1d
from .base import _preprocess_data
from ..base import BaseEstimator
from ..externals import six
from ..externals.joblib import Memory, Parallel, delayed
from ..feature_selection.base import SelectorMixin
from ..utils import (as_float_array, check_random_state, check_X_y, safe_mask)
from ..utils.validation import check_is_fitted
from .least_angle import lars_path, LassoLarsIC
from .logistic import LogisticRegression
from ..exceptions import ConvergenceWarning

def _resample_model(estimator_func, X, y, scaling=.5, n_resampling=200,
                    n_jobs=1, verbose=False, pre_dispatch='3*n_jobs',
                    random_state=None, sample_fraction=.75, **params):
    random_state = check_random_state(random_state)
        n_samples, n_features = X.shape
    if not (0 < scaling < 1):
        raise ValueError(
            "'scaling' should be between 0 and 1. Got %r instead." % scaling)
    scaling = 1. - scaling
    scores_ = 0.0
    for active_set in Parallel(n_jobs=n_jobs, verbose=verbose,
                               pre_dispatch=pre_dispatch)(
            delayed(estimator_func)(
                X, y, weights=scaling * random_state.randint(
                    0, 2, size=(n_features,)),
                mask=(random_state.rand(n_samples) < sample_fraction),
                verbose=max(0, verbose - 1),
                **params)
            for _ in range(n_resampling)):
        scores_ += active_set
    scores_ /= n_resampling
    return scores_

class BaseRandomizedLinearModel(six.with_metaclass(ABCMeta, BaseEstimator,
                                                   SelectorMixin)):
    
    @abstractmethod
    def __init__(self):
        pass
    _preprocess_data = staticmethod(_preprocess_data)
    def fit(self, X, y):
                X, y = check_X_y(X, y, ['csr', 'csc'], y_numeric=True,
                         ensure_min_samples=2, estimator=self)
        X = as_float_array(X, copy=False)
        n_samples, n_features = X.shape
        X, y, X_offset, y_offset, X_scale = \
            self._preprocess_data(X, y, self.fit_intercept, self.normalize)
        estimator_func, params = self._make_estimator_and_params(X, y)
        memory = self.memory
        if isinstance(memory, six.string_types):
            memory = Memory(cachedir=memory)
        scores_ = memory.cache(
            _resample_model, ignore=['verbose', 'n_jobs', 'pre_dispatch']
        )(
            estimator_func, X, y,
            scaling=self.scaling, n_resampling=self.n_resampling,
            n_jobs=self.n_jobs, verbose=self.verbose,
            pre_dispatch=self.pre_dispatch, random_state=self.random_state,
            sample_fraction=self.sample_fraction, **params)
        if scores_.ndim == 1:
            scores_ = scores_[:, np.newaxis]
        self.all_scores_ = scores_
        self.scores_ = np.max(self.all_scores_, axis=1)
        return self
    def _make_estimator_and_params(self, X, y):
            def __init__(self, alpha='aic', scaling=.5, sample_fraction=.75,
                 n_resampling=200, selection_threshold=.25,
                 fit_intercept=True, verbose=False,
                 normalize=True, precompute='auto',
                 max_iter=500,
                 eps=np.finfo(np.float).eps, random_state=None,
                 n_jobs=1, pre_dispatch='3*n_jobs',
                 memory=Memory(cachedir=None, verbose=0)):
        self.alpha = alpha
        self.scaling = scaling
        self.sample_fraction = sample_fraction
        self.n_resampling = n_resampling
        self.fit_intercept = fit_intercept
        self.max_iter = max_iter
        self.verbose = verbose
        self.normalize = normalize
        self.precompute = precompute
        self.eps = eps
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.selection_threshold = selection_threshold
        self.pre_dispatch = pre_dispatch
        self.memory = memory
    def _make_estimator_and_params(self, X, y):
        assert self.precompute in (True, False, None, 'auto')
        alpha = self.alpha
        if isinstance(alpha, six.string_types) and alpha in ('aic', 'bic'):
            model = LassoLarsIC(precompute=self.precompute,
                                criterion=self.alpha,
                                max_iter=self.max_iter,
                                eps=self.eps)
            model.fit(X, y)
            self.alpha_ = alpha = model.alpha_
        return _randomized_lasso, dict(alpha=alpha, max_iter=self.max_iter,
                                       eps=self.eps,
                                       precompute=self.precompute)

def _randomized_logistic(X, y, weights, mask, C=1., verbose=False,
                         fit_intercept=True, tol=1e-3):
    X = X[safe_mask(X, mask)]
    y = y[mask]
    if issparse(X):
        size = len(weights)
        weight_dia = sparse.dia_matrix((1 - weights, 0), (size, size))
        X = X * weight_dia
    else:
        X *= (1 - weights)
    C = np.atleast_1d(np.asarray(C, dtype=np.float64))
    if C.ndim > 1:
        raise ValueError("C should be 1-dimensional array-like, "
                         "but got a {}-dimensional array-like instead: {}."
                         .format(C.ndim, C))
    scores = np.zeros((X.shape[1], len(C)), dtype=np.bool)
    for this_C, this_scores in zip(C, scores.T):
                clf = LogisticRegression(C=this_C, tol=tol, penalty='l1', dual=False,
                                 fit_intercept=fit_intercept)
        clf.fit(X, y)
        this_scores[:] = np.any(
            np.abs(clf.coef_) > 10 * np.finfo(np.float).eps, axis=0)
    return scores

class RandomizedLogisticRegression(BaseRandomizedLinearModel):
        def __init__(self, C=1, scaling=.5, sample_fraction=.75,
                 n_resampling=200,
                 selection_threshold=.25, tol=1e-3,
                 fit_intercept=True, verbose=False,
                 normalize=True,
                 random_state=None,
                 n_jobs=1, pre_dispatch='3*n_jobs',
                 memory=Memory(cachedir=None, verbose=0)):
        self.C = C
        self.scaling = scaling
        self.sample_fraction = sample_fraction
        self.n_resampling = n_resampling
        self.fit_intercept = fit_intercept
        self.verbose = verbose
        self.normalize = normalize
        self.tol = tol
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.selection_threshold = selection_threshold
        self.pre_dispatch = pre_dispatch
        self.memory = memory
    def _make_estimator_and_params(self, X, y):
        params = dict(C=self.C, tol=self.tol,
                      fit_intercept=self.fit_intercept)
        return _randomized_logistic, params
    def _preprocess_data(self, X, y, fit_intercept, normalize=False):
                X, _, X_offset, _, X_scale = _preprocess_data(X, y, fit_intercept,
                                                      normalize=normalize)
        return X, y, X_offset, y, X_scale

def _lasso_stability_path(X, y, mask, weights, eps):
    "Inner loop of lasso_stability_path"
    X = X * weights[np.newaxis, :]
    X = X[safe_mask(X, mask), :]
    y = y[mask]
    alpha_max = np.max(np.abs(np.dot(X.T, y))) / X.shape[0]
    alpha_min = eps * alpha_max      with warnings.catch_warnings():
        warnings.simplefilter('ignore', ConvergenceWarning)
        alphas, _, coefs = lars_path(X, y, method='lasso', verbose=False,
                                     alpha_min=alpha_min)
        alphas /= alphas[0]
        alphas = alphas[::-1]
    coefs = coefs[:, ::-1]
        mask = alphas >= eps
            mask[0] = True
    alphas = alphas[mask]
    coefs = coefs[:, mask]
    return alphas, coefs

def lasso_stability_path(X, y, scaling=0.5, random_state=None,
                         n_resampling=200, n_grid=100,
                         sample_fraction=0.75,
                         eps=4 * np.finfo(np.float).eps, n_jobs=1,
                         verbose=False):
        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'])
    rng = check_random_state(random_state)
    if not (0 < scaling < 1):
        raise ValueError("Parameter 'scaling' should be between 0 and 1."
                         " Got %r instead." % scaling)
    n_samples, n_features = X.shape
    paths = Parallel(n_jobs=n_jobs, verbose=verbose)(
        delayed(_lasso_stability_path)(
            X, y, mask=rng.rand(n_samples) < sample_fraction,
            weights=1. - scaling * rng.randint(0, 2, size=(n_features,)),
            eps=eps)
        for k in range(n_resampling))
    all_alphas = sorted(list(set(itertools.chain(*[p[0] for p in paths]))))
        stride = int(max(1, int(len(all_alphas) / float(n_grid))))
    all_alphas = all_alphas[::stride]
    if not all_alphas[-1] == 1:
        all_alphas.append(1.)
    all_alphas = np.array(all_alphas)
    scores_path = np.zeros((n_features, len(all_alphas)))
    for alphas, coefs in paths:
        if alphas[0] != 0:
            alphas = np.r_[0, alphas]
            coefs = np.c_[np.ones((n_features, 1)), coefs]
        if alphas[-1] != all_alphas[-1]:
            alphas = np.r_[alphas, all_alphas[-1]]
            coefs = np.c_[coefs, np.zeros((n_features, 1))]
        scores_path += (interp1d(alphas, coefs,
                        kind='nearest', bounds_error=False,
                        fill_value=0, axis=-1)(all_alphas) != 0)
    scores_path /= n_resampling
    return all_alphas, scores_path

import numpy as np
import warnings
from ..base import BaseEstimator, MetaEstimatorMixin, RegressorMixin, clone
from ..utils import check_random_state, check_array, check_consistent_length
from ..utils.random import sample_without_replacement
from ..utils.validation import check_is_fitted
from .base import LinearRegression
from ..utils.validation import has_fit_parameter
_EPSILON = np.spacing(1)

def _dynamic_max_trials(n_inliers, n_samples, min_samples, probability):
        inlier_ratio = n_inliers / float(n_samples)
    nom = max(_EPSILON, 1 - probability)
    denom = max(_EPSILON, 1 - inlier_ratio ** min_samples)
    if nom == 1:
        return 0
    if denom == 1:
        return float('inf')
    return abs(float(np.ceil(np.log(nom) / np.log(denom))))

class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin):
    
    def __init__(self, base_estimator=None, min_samples=None,
                 residual_threshold=None, is_data_valid=None,
                 is_model_valid=None, max_trials=100, max_skips=np.inf,
                 stop_n_inliers=np.inf, stop_score=np.inf,
                 stop_probability=0.99, residual_metric=None,
                 loss='absolute_loss', random_state=None):
        self.base_estimator = base_estimator
        self.min_samples = min_samples
        self.residual_threshold = residual_threshold
        self.is_data_valid = is_data_valid
        self.is_model_valid = is_model_valid
        self.max_trials = max_trials
        self.max_skips = max_skips
        self.stop_n_inliers = stop_n_inliers
        self.stop_score = stop_score
        self.stop_probability = stop_probability
        self.residual_metric = residual_metric
        self.random_state = random_state
        self.loss = loss
    def fit(self, X, y, sample_weight=None):
                X = check_array(X, accept_sparse='csr')
        y = check_array(y, ensure_2d=False)
        check_consistent_length(X, y)
        if self.base_estimator is not None:
            base_estimator = clone(self.base_estimator)
        else:
            base_estimator = LinearRegression()
        if self.min_samples is None:
                        min_samples = X.shape[1] + 1
        elif 0 < self.min_samples < 1:
            min_samples = np.ceil(self.min_samples * X.shape[0])
        elif self.min_samples >= 1:
            if self.min_samples % 1 != 0:
                raise ValueError("Absolute number of samples must be an "
                                 "integer value.")
            min_samples = self.min_samples
        else:
            raise ValueError("Value for `min_samples` must be scalar and "
                             "positive.")
        if min_samples > X.shape[0]:
            raise ValueError("`min_samples` may not be larger than number "
                             "of samples ``X.shape[0]``.")
        if self.stop_probability < 0 or self.stop_probability > 1:
            raise ValueError("`stop_probability` must be in range [0, 1].")
        if self.residual_threshold is None:
                        residual_threshold = np.median(np.abs(y - np.median(y)))
        else:
            residual_threshold = self.residual_threshold
        if self.residual_metric is not None:
            warnings.warn(
                "'residual_metric' was deprecated in version 0.18 and "
                "will be removed in version 0.20. Use 'loss' instead.",
                DeprecationWarning)
        if self.loss == "absolute_loss":
            if y.ndim == 1:
                loss_function = lambda y_true, y_pred: np.abs(y_true - y_pred)
            else:
                loss_function = lambda \
                    y_true, y_pred: np.sum(np.abs(y_true - y_pred), axis=1)
        elif self.loss == "squared_loss":
            if y.ndim == 1:
                loss_function = lambda y_true, y_pred: (y_true - y_pred) ** 2
            else:
                loss_function = lambda \
                    y_true, y_pred: np.sum((y_true - y_pred) ** 2, axis=1)
        elif callable(self.loss):
            loss_function = self.loss
        else:
            raise ValueError(
                "loss should be 'absolute_loss', 'squared_loss' or a callable."
                "Got %s. " % self.loss)

        random_state = check_random_state(self.random_state)
        try:              base_estimator.set_params(random_state=random_state)
        except ValueError:
            pass
        estimator_fit_has_sample_weight = has_fit_parameter(base_estimator,
                                                            "sample_weight")
        estimator_name = type(base_estimator).__name__
        if (sample_weight is not None and not
                estimator_fit_has_sample_weight):
            raise ValueError("%s does not support sample_weight. Samples"
                             " weights are only used for the calibration"
                             " itself." % estimator_name)
        if sample_weight is not None:
            sample_weight = np.asarray(sample_weight)
        n_inliers_best = 1
        score_best = -np.inf
        inlier_mask_best = None
        X_inlier_best = None
        y_inlier_best = None
        self.n_skips_no_inliers_ = 0
        self.n_skips_invalid_data_ = 0
        self.n_skips_invalid_model_ = 0
                n_samples = X.shape[0]
        sample_idxs = np.arange(n_samples)
        n_samples, _ = X.shape
        for self.n_trials_ in range(1, self.max_trials + 1):
            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +
                    self.n_skips_invalid_model_) > self.max_skips:
                break
                        subset_idxs = sample_without_replacement(n_samples, min_samples,
                                                     random_state=random_state)
            X_subset = X[subset_idxs]
            y_subset = y[subset_idxs]
                        if (self.is_data_valid is not None
                    and not self.is_data_valid(X_subset, y_subset)):
                self.n_skips_invalid_data_ += 1
                continue
                        if sample_weight is None:
                base_estimator.fit(X_subset, y_subset)
            else:
                base_estimator.fit(X_subset, y_subset,
                                   sample_weight=sample_weight[subset_idxs])
                        if (self.is_model_valid is not None and not
                    self.is_model_valid(base_estimator, X_subset, y_subset)):
                self.n_skips_invalid_model_ += 1
                continue
                        y_pred = base_estimator.predict(X)
                        if self.residual_metric is not None:
                diff = y_pred - y
                if diff.ndim == 1:
                    diff = diff.reshape(-1, 1)
                residuals_subset = self.residual_metric(diff)
            else:
                residuals_subset = loss_function(y, y_pred)
                        inlier_mask_subset = residuals_subset < residual_threshold
            n_inliers_subset = np.sum(inlier_mask_subset)
                        if n_inliers_subset < n_inliers_best:
                self.n_skips_no_inliers_ += 1
                continue
                        inlier_idxs_subset = sample_idxs[inlier_mask_subset]
            X_inlier_subset = X[inlier_idxs_subset]
            y_inlier_subset = y[inlier_idxs_subset]
                        score_subset = base_estimator.score(X_inlier_subset,
                                                y_inlier_subset)
                                    if (n_inliers_subset == n_inliers_best
                    and score_subset < score_best):
                continue
                        n_inliers_best = n_inliers_subset
            score_best = score_subset
            inlier_mask_best = inlier_mask_subset
            X_inlier_best = X_inlier_subset
            y_inlier_best = y_inlier_subset
                        if (n_inliers_best >= self.stop_n_inliers
                    or score_best >= self.stop_score
                    or self.n_trials_
                       >= _dynamic_max_trials(n_inliers_best, n_samples,
                                              min_samples,
                                              self.stop_probability)):
                break
                if inlier_mask_best is None:
            if ((self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +
                    self.n_skips_invalid_model_) > self.max_skips):
                raise ValueError(
                    "RANSAC skipped more iterations than `max_skips` without"
                    " finding a valid consensus set. Iterations were skipped"
                    " because each randomly chosen sub-sample failed the"
                    " passing criteria. See estimator attributes for"
                    " diagnostics (n_skips*).")
            else:
                raise ValueError(
                    "RANSAC could not find a valid consensus set. All"
                    " `max_trials` iterations were skipped because each"
                    " randomly chosen sub-sample failed the passing criteria."
                    " See estimator attributes for diagnostics (n_skips*).")
        else:
            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +
                    self.n_skips_invalid_model_) > self.max_skips:
                warnings.warn("RANSAC found a valid consensus set but exited"
                              " early due to skipping more iterations than"
                              " `max_skips`. See estimator attributes for"
                              " diagnostics (n_skips*).",
                              UserWarning)
                base_estimator.fit(X_inlier_best, y_inlier_best)
        self.estimator_ = base_estimator
        self.inlier_mask_ = inlier_mask_best
        return self
    def predict(self, X):
                check_is_fitted(self, 'estimator_')
        return self.estimator_.predict(X)
    def score(self, X, y):
                check_is_fitted(self, 'estimator_')
        return self.estimator_.score(X, y)

from abc import ABCMeta, abstractmethod
import warnings
import numpy as np
from scipy import linalg
from scipy import sparse
from scipy.sparse import linalg as sp_linalg
from .base import LinearClassifierMixin, LinearModel, _rescale_data
from .sag import sag_solver
from ..base import RegressorMixin
from ..utils.extmath import safe_sparse_dot
from ..utils.extmath import row_norms
from ..utils import check_X_y
from ..utils import check_array
from ..utils import check_consistent_length
from ..utils import compute_sample_weight
from ..utils import column_or_1d
from ..preprocessing import LabelBinarizer
from ..model_selection import GridSearchCV
from ..externals import six
from ..metrics.scorer import check_scoring

def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0):
    n_samples, n_features = X.shape
    X1 = sp_linalg.aslinearoperator(X)
    coefs = np.empty((y.shape[1], n_features))
    if n_features > n_samples:
        def create_mv(curr_alpha):
            def _mv(x):
                return X1.matvec(X1.rmatvec(x)) + curr_alpha * x
            return _mv
    else:
        def create_mv(curr_alpha):
            def _mv(x):
                return X1.rmatvec(X1.matvec(x)) + curr_alpha * x
            return _mv
    for i in range(y.shape[1]):
        y_column = y[:, i]
        mv = create_mv(alpha[i])
        if n_features > n_samples:
                                    C = sp_linalg.LinearOperator(
                (n_samples, n_samples), matvec=mv, dtype=X.dtype)
            coef, info = sp_linalg.cg(C, y_column, tol=tol)
            coefs[i] = X1.rmatvec(coef)
        else:
                                    y_column = X1.rmatvec(y_column)
            C = sp_linalg.LinearOperator(
                (n_features, n_features), matvec=mv, dtype=X.dtype)
            coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,
                                          tol=tol)
        if info < 0:
            raise ValueError("Failed with error code %d" % info)
        if max_iter is None and info > 0 and verbose:
            warnings.warn("sparse_cg did not converge after %d iterations." %
                          info)
    return coefs

def _solve_lsqr(X, y, alpha, max_iter=None, tol=1e-3):
    n_samples, n_features = X.shape
    coefs = np.empty((y.shape[1], n_features))
    n_iter = np.empty(y.shape[1], dtype=np.int32)
        sqrt_alpha = np.sqrt(alpha)
    for i in range(y.shape[1]):
        y_column = y[:, i]
        info = sp_linalg.lsqr(X, y_column, damp=sqrt_alpha[i],
                              atol=tol, btol=tol, iter_lim=max_iter)
        coefs[i] = info[0]
        n_iter[i] = info[2]
    return coefs, n_iter

def _solve_cholesky(X, y, alpha):
        n_samples, n_features = X.shape
    n_targets = y.shape[1]
    A = safe_sparse_dot(X.T, X, dense_output=True)
    Xy = safe_sparse_dot(X.T, y, dense_output=True)
    one_alpha = np.array_equal(alpha, len(alpha) * [alpha[0]])
    if one_alpha:
        A.flat[::n_features + 1] += alpha[0]
        return linalg.solve(A, Xy, sym_pos=True,
                            overwrite_a=True).T
    else:
        coefs = np.empty([n_targets, n_features])
        for coef, target, current_alpha in zip(coefs, Xy.T, alpha):
            A.flat[::n_features + 1] += current_alpha
            coef[:] = linalg.solve(A, target, sym_pos=True,
                                   overwrite_a=False).ravel()
            A.flat[::n_features + 1] -= current_alpha
        return coefs

def _solve_cholesky_kernel(K, y, alpha, sample_weight=None, copy=False):
        n_samples = K.shape[0]
    n_targets = y.shape[1]
    if copy:
        K = K.copy()
    alpha = np.atleast_1d(alpha)
    one_alpha = (alpha == alpha[0]).all()
    has_sw = isinstance(sample_weight, np.ndarray) \
        or sample_weight not in [1.0, None]
    if has_sw:
                        sw = np.sqrt(np.atleast_1d(sample_weight))
        y = y * sw[:, np.newaxis]
        K *= np.outer(sw, sw)
    if one_alpha:
                K.flat[::n_samples + 1] += alpha[0]
        try:
                                                dual_coef = linalg.solve(K, y, sym_pos=True,
                                     overwrite_a=False)
        except np.linalg.LinAlgError:
            warnings.warn("Singular matrix in solving dual problem. Using "
                          "least-squares solution instead.")
            dual_coef = linalg.lstsq(K, y)[0]
                        K.flat[::n_samples + 1] -= alpha[0]
        if has_sw:
            dual_coef *= sw[:, np.newaxis]
        return dual_coef
    else:
                dual_coefs = np.empty([n_targets, n_samples])
        for dual_coef, target, current_alpha in zip(dual_coefs, y.T, alpha):
            K.flat[::n_samples + 1] += current_alpha
            dual_coef[:] = linalg.solve(K, target, sym_pos=True,
                                        overwrite_a=False).ravel()
            K.flat[::n_samples + 1] -= current_alpha
        if has_sw:
            dual_coefs *= sw[np.newaxis, :]
        return dual_coefs.T

def _solve_svd(X, y, alpha):
    U, s, Vt = linalg.svd(X, full_matrices=False)
    idx = s > 1e-15      s_nnz = s[idx][:, np.newaxis]
    UTy = np.dot(U.T, y)
    d = np.zeros((s.size, alpha.size))
    d[idx] = s_nnz / (s_nnz ** 2 + alpha)
    d_UT_y = d * UTy
    return np.dot(Vt.T, d_UT_y).T

def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',
                     max_iter=None, tol=1e-3, verbose=0, random_state=None,
                     return_n_iter=False, return_intercept=False):
        if return_intercept and sparse.issparse(X) and solver != 'sag':
        if solver != 'auto':
            warnings.warn("In Ridge, only 'sag' solver can currently fit the "
                          "intercept when X is sparse. Solver has been "
                          "automatically changed into 'sag'.")
        solver = 'sag'
        if solver == 'sag':
        X = check_array(X, accept_sparse=['csr'],
                        dtype=np.float64, order='C')
        y = check_array(y, dtype=np.float64, ensure_2d=False, order='F')
    else:
        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'],
                        dtype=np.float64)
        y = check_array(y, dtype='numeric', ensure_2d=False)
    check_consistent_length(X, y)
    n_samples, n_features = X.shape
    if y.ndim > 2:
        raise ValueError("Target y has the wrong shape %s" % str(y.shape))
    ravel = False
    if y.ndim == 1:
        y = y.reshape(-1, 1)
        ravel = True
    n_samples_, n_targets = y.shape
    if n_samples != n_samples_:
        raise ValueError("Number of samples in X and y does not correspond:"
                         " %d != %d" % (n_samples, n_samples_))
    has_sw = sample_weight is not None
    if solver == 'auto':
                if not sparse.issparse(X) or has_sw:
            solver = 'cholesky'
        else:
            solver = 'sparse_cg'
    elif solver == 'lsqr' and not hasattr(sp_linalg, 'lsqr'):
        warnings.warn(    def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,
                 copy_X=True, max_iter=None, tol=1e-3, solver="auto",
                 random_state=None):
        super(Ridge, self).__init__(alpha=alpha, fit_intercept=fit_intercept,
                                    normalize=normalize, copy_X=copy_X,
                                    max_iter=max_iter, tol=tol, solver=solver,
                                    random_state=random_state)
    def fit(self, X, y, sample_weight=None):
                return super(Ridge, self).fit(X, y, sample_weight=sample_weight)

class RidgeClassifier(LinearClassifierMixin, _BaseRidge):
        def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,
                 copy_X=True, max_iter=None, tol=1e-3, class_weight=None,
                 solver="auto", random_state=None):
        super(RidgeClassifier, self).__init__(
            alpha=alpha, fit_intercept=fit_intercept, normalize=normalize,
            copy_X=copy_X, max_iter=max_iter, tol=tol, solver=solver,
            random_state=random_state)
        self.class_weight = class_weight
    def fit(self, X, y, sample_weight=None):
                self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)
        Y = self._label_binarizer.fit_transform(y)
        if not self._label_binarizer.y_type_.startswith('multilabel'):
            y = column_or_1d(y, warn=True)
        else:
                        raise ValueError(
                "%s doesn't support multi-label classification" % (
                    self.__class__.__name__))
        if self.class_weight:
            if sample_weight is None:
                sample_weight = 1.
                        sample_weight = (sample_weight *
                             compute_sample_weight(self.class_weight, y))
        super(RidgeClassifier, self).fit(X, Y, sample_weight=sample_weight)
        return self
    @property
    def classes_(self):
        return self._label_binarizer.classes_

class _RidgeGCV(LinearModel):
    
    def __init__(self, alphas=(0.1, 1.0, 10.0),
                 fit_intercept=True, normalize=False,
                 scoring=None, copy_X=True,
                 gcv_mode=None, store_cv_values=False):
        self.alphas = np.asarray(alphas)
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.scoring = scoring
        self.copy_X = copy_X
        self.gcv_mode = gcv_mode
        self.store_cv_values = store_cv_values
    def _pre_compute(self, X, y, centered_kernel=True):
                K = safe_sparse_dot(X, X.T, dense_output=True)
                                if centered_kernel:
            K += np.ones_like(K)
        v, Q = linalg.eigh(K)
        QT_y = np.dot(Q.T, y)
        return v, Q, QT_y
    def _decomp_diag(self, v_prime, Q):
                return (v_prime * Q ** 2).sum(axis=-1)
    def _diag_dot(self, D, B):
                if len(B.shape) > 1:
                        D = D[(slice(None), ) + (np.newaxis, ) * (len(B.shape) - 1)]
        return D * B
    def _errors_and_values_helper(self, alpha, y, v, Q, QT_y):
                w = 1. / (v + alpha)
        constant_column = np.var(Q, 0) < 1.e-12
                w[constant_column] = 0  
        c = np.dot(Q, self._diag_dot(w, QT_y))
        G_diag = self._decomp_diag(w, Q)
                if len(y.shape) != 1:
            G_diag = G_diag[:, np.newaxis]
        return G_diag, c
    def _errors(self, alpha, y, v, Q, QT_y):
        G_diag, c = self._errors_and_values_helper(alpha, y, v, Q, QT_y)
        return (c / G_diag) ** 2, c
    def _values(self, alpha, y, v, Q, QT_y):
        G_diag, c = self._errors_and_values_helper(alpha, y, v, Q, QT_y)
        return y - (c / G_diag), c
    def _pre_compute_svd(self, X, y, centered_kernel=True):
        if sparse.issparse(X):
            raise TypeError("SVD not supported for sparse matrices")
        if centered_kernel:
            X = np.hstack((X, np.ones((X.shape[0], 1))))
                        U, s, _ = linalg.svd(X, full_matrices=0)
        v = s ** 2
        UT_y = np.dot(U.T, y)
        return v, U, UT_y
    def _errors_and_values_svd_helper(self, alpha, y, v, U, UT_y):
                constant_column = np.var(U, 0) < 1.e-12
                w = ((v + alpha) ** -1) - (alpha ** -1)
        w[constant_column] = - (alpha ** -1)
                c = np.dot(U, self._diag_dot(w, UT_y)) + (alpha ** -1) * y
        G_diag = self._decomp_diag(w, U) + (alpha ** -1)
        if len(y.shape) != 1:
                        G_diag = G_diag[:, np.newaxis]
        return G_diag, c
    def _errors_svd(self, alpha, y, v, U, UT_y):
        G_diag, c = self._errors_and_values_svd_helper(alpha, y, v, U, UT_y)
        return (c / G_diag) ** 2, c
    def _values_svd(self, alpha, y, v, U, UT_y):
        G_diag, c = self._errors_and_values_svd_helper(alpha, y, v, U, UT_y)
        return y - (c / G_diag), c
    def fit(self, X, y, sample_weight=None):
                X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64,
                         multi_output=True, y_numeric=True)
        if sample_weight is not None and not isinstance(sample_weight, float):
            sample_weight = check_array(sample_weight, ensure_2d=False)
        n_samples, n_features = X.shape
        X, y, X_offset, y_offset, X_scale = LinearModel._preprocess_data(
            X, y, self.fit_intercept, self.normalize, self.copy_X,
            sample_weight=sample_weight)
        gcv_mode = self.gcv_mode
        with_sw = len(np.shape(sample_weight))
        if gcv_mode is None or gcv_mode == 'auto':
            if sparse.issparse(X) or n_features > n_samples or with_sw:
                gcv_mode = 'eigen'
            else:
                gcv_mode = 'svd'
        elif gcv_mode == "svd" and with_sw:
                        warnings.warn("non-uniform sample weights unsupported for svd, "
                          "forcing usage of eigen")
            gcv_mode = 'eigen'
        if gcv_mode == 'eigen':
            _pre_compute = self._pre_compute
            _errors = self._errors
            _values = self._values
        elif gcv_mode == 'svd':
                        _pre_compute = self._pre_compute_svd
            _errors = self._errors_svd
            _values = self._values_svd
        else:
            raise ValueError('bad gcv_mode "%s"' % gcv_mode)
        if sample_weight is not None:
            X, y = _rescale_data(X, y, sample_weight)
        centered_kernel = not sparse.issparse(X) and self.fit_intercept
        v, Q, QT_y = _pre_compute(X, y, centered_kernel)
        n_y = 1 if len(y.shape) == 1 else y.shape[1]
        cv_values = np.zeros((n_samples * n_y, len(self.alphas)))
        C = []
        scorer = check_scoring(self, scoring=self.scoring, allow_none=True)
        error = scorer is None
        for i, alpha in enumerate(self.alphas):
            if error:
                out, c = _errors(alpha, y, v, Q, QT_y)
            else:
                out, c = _values(alpha, y, v, Q, QT_y)
            cv_values[:, i] = out.ravel()
            C.append(c)
        if error:
            best = cv_values.mean(axis=0).argmin()
        else:
                                                def identity_estimator():
                pass
            identity_estimator.decision_function = lambda y_predict: y_predict
            identity_estimator.predict = lambda y_predict: y_predict
            out = [scorer(identity_estimator, y.ravel(), cv_values[:, i])
                   for i in range(len(self.alphas))]
            best = np.argmax(out)
        self.alpha_ = self.alphas[best]
        self.dual_coef_ = C[best]
        self.coef_ = safe_sparse_dot(self.dual_coef_.T, X)
        self._set_intercept(X_offset, y_offset, X_scale)
        if self.store_cv_values:
            if len(y.shape) == 1:
                cv_values_shape = n_samples, len(self.alphas)
            else:
                cv_values_shape = n_samples, n_y, len(self.alphas)
            self.cv_values_ = cv_values.reshape(cv_values_shape)
        return self

class _BaseRidgeCV(LinearModel):
    def __init__(self, alphas=(0.1, 1.0, 10.0),
                 fit_intercept=True, normalize=False, scoring=None,
                 cv=None, gcv_mode=None,
                 store_cv_values=False):
        self.alphas = alphas
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.scoring = scoring
        self.cv = cv
        self.gcv_mode = gcv_mode
        self.store_cv_values = store_cv_values
    def fit(self, X, y, sample_weight=None):
                if self.cv is None:
            estimator = _RidgeGCV(self.alphas,
                                  fit_intercept=self.fit_intercept,
                                  normalize=self.normalize,
                                  scoring=self.scoring,
                                  gcv_mode=self.gcv_mode,
                                  store_cv_values=self.store_cv_values)
            estimator.fit(X, y, sample_weight=sample_weight)
            self.alpha_ = estimator.alpha_
            if self.store_cv_values:
                self.cv_values_ = estimator.cv_values_
        else:
            if self.store_cv_values:
                raise ValueError("cv!=None and store_cv_values=True "
                                 " are incompatible")
            parameters = {'alpha': self.alphas}
            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept),
                              parameters, cv=self.cv, scoring=self.scoring)
            gs.fit(X, y, sample_weight=sample_weight)
            estimator = gs.best_estimator_
            self.alpha_ = gs.best_estimator_.alpha
        self.coef_ = estimator.coef_
        self.intercept_ = estimator.intercept_
        return self

class RidgeCV(_BaseRidgeCV, RegressorMixin):
        pass

class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
        def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,
                 normalize=False, scoring=None, cv=None, class_weight=None):
        super(RidgeClassifierCV, self).__init__(
            alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,
            scoring=scoring, cv=cv)
        self.class_weight = class_weight
    def fit(self, X, y, sample_weight=None):
                self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)
        Y = self._label_binarizer.fit_transform(y)
        if not self._label_binarizer.y_type_.startswith('multilabel'):
            y = column_or_1d(y, warn=True)
        if self.class_weight:
            if sample_weight is None:
                sample_weight = 1.
                        sample_weight = (sample_weight *
                             compute_sample_weight(self.class_weight, y))
        _BaseRidgeCV.fit(self, X, Y, sample_weight=sample_weight)
        return self
    @property
    def classes_(self):
        return self._label_binarizer.classes_

import numpy as np
import warnings
from ..exceptions import ConvergenceWarning
from ..utils import check_array
from ..utils.extmath import row_norms
from .base import make_dataset
from .sag_fast import sag

def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept):
        if loss in ('log', 'multinomial'):
                return 4.0 / (max_squared_sum + int(fit_intercept)
                      + 4.0 * alpha_scaled)
    elif loss == 'squared':
                return 1.0 / (max_squared_sum + int(fit_intercept) + alpha_scaled)
    else:
        raise ValueError("Unknown loss function for SAG solver, got %s "
                         "instead of 'log' or 'squared'" % loss)

def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
               max_iter=1000, tol=0.001, verbose=0, random_state=None,
               check_input=True, max_squared_sum=None,
               warm_start_mem=None):
        if warm_start_mem is None:
        warm_start_mem = {}
        if max_iter is None:
        max_iter = 1000
    if check_input:
        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
    n_samples, n_features = X.shape[0], X.shape[1]
        alpha_scaled = float(alpha) / n_samples
        n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
        if sample_weight is None:
        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
    if 'coef' in warm_start_mem.keys():
        coef_init = warm_start_mem['coef']
    else:
                coef_init = np.zeros((n_features, n_classes), dtype=np.float64,
                             order='C')
            fit_intercept = coef_init.shape[0] == (n_features + 1)
    if fit_intercept:
        intercept_init = coef_init[-1, :]
        coef_init = coef_init[:-1, :]
    else:
        intercept_init = np.zeros(n_classes, dtype=np.float64)
    if 'intercept_sum_gradient' in warm_start_mem.keys():
        intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']
    else:
        intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)
    if 'gradient_memory' in warm_start_mem.keys():
        gradient_memory_init = warm_start_mem['gradient_memory']
    else:
        gradient_memory_init = np.zeros((n_samples, n_classes),
                                        dtype=np.float64, order='C')
    if 'sum_gradient' in warm_start_mem.keys():
        sum_gradient_init = warm_start_mem['sum_gradient']
    else:
        sum_gradient_init = np.zeros((n_features, n_classes),
                                     dtype=np.float64, order='C')
    if 'seen' in warm_start_mem.keys():
        seen_init = warm_start_mem['seen']
    else:
        seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
    if 'num_seen' in warm_start_mem.keys():
        num_seen_init = warm_start_mem['num_seen']
    else:
        num_seen_init = 0
    dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
    if max_squared_sum is None:
        max_squared_sum = row_norms(X, squared=True).max()
    step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
                                   fit_intercept)
    if step_size * alpha_scaled == 1:
        raise ZeroDivisionError("Current sag implementation does not handle "
                                "the case step_size * alpha_scaled == 1")
    num_seen, n_iter_ = sag(dataset, coef_init,
                            intercept_init, n_samples,
                            n_features, n_classes, tol,
                            max_iter,
                            loss,
                            step_size, alpha_scaled,
                            sum_gradient_init,
                            gradient_memory_init,
                            seen_init,
                            num_seen_init,
                            fit_intercept,
                            intercept_sum_gradient,
                            intercept_decay,
                            verbose)
    if n_iter_ == max_iter:
        warnings.warn("The max_iter was reached which means "
                      "the coef_ did not converge", ConvergenceWarning)
    if fit_intercept:
        coef_init = np.vstack((coef_init, intercept_init))
    warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init,
                      'intercept_sum_gradient': intercept_sum_gradient,
                      'gradient_memory': gradient_memory_init,
                      'seen': seen_init, 'num_seen': num_seen}
    if loss == 'multinomial':
        coef_ = coef_init.T
    else:
        coef_ = coef_init[:, 0]
    return coef_, n_iter_, warm_start_mem
import numpy as np
cimport numpy as np
import scipy.sparse as sp
from libc.math cimport fabs, exp, log
from libc.time cimport time, time_t
from ..utils.seq_dataset cimport SequentialDataset
from .sgd_fast cimport LossFunction
from .sgd_fast cimport Log, SquaredLoss

cdef extern from "sgd_fast_helpers.h":
    bint skl_isfinite(double) nogil

cdef inline double fmax(double x, double y) nogil:
    if x > y:
        return x
    return y

cdef double _logsumexp(double* arr, int n_classes) nogil:
                cdef double vmax = arr[0]
    cdef double out = 0.0
    cdef int i
    for i in range(1, n_classes):
        if vmax < arr[i]:
            vmax = arr[i]
    for i in range(n_classes):
        out += exp(arr[i] - vmax)
    return log(out) + vmax

cdef class MultinomialLogLoss:
    cdef double _loss(self, double* prediction, double y, int n_classes,
                      double sample_weight) nogil:
                cdef double logsumexp_prediction = _logsumexp(prediction, n_classes)
        cdef double loss
                loss = (logsumexp_prediction - prediction[int(y)]) * sample_weight
        return loss
    cdef void _dloss(self, double* prediction, double y, int n_classes,
                     double sample_weight, double* gradient_ptr) nogil:
                cdef double logsumexp_prediction = _logsumexp(prediction, n_classes)
        cdef int class_ind
        for class_ind in range(n_classes):
            gradient_ptr[class_ind] = exp(prediction[class_ind] -
                                          logsumexp_prediction)
                        if class_ind == y:
                gradient_ptr[class_ind] -= 1.0
            gradient_ptr[class_ind] *= sample_weight
    def __reduce__(self):
        return MultinomialLogLoss, ()

def _multinomial_grad_loss_all_samples(
        SequentialDataset dataset,
        np.ndarray[double, ndim=2, mode='c'] weights_array,
        np.ndarray[double, ndim=1, mode='c'] intercept_array,
        int n_samples, int n_features, int n_classes):
        cdef double* weights = <double * >weights_array.data
    cdef double* intercept = <double * >intercept_array.data
    cdef double *x_data_ptr = NULL
    cdef int *x_ind_ptr = NULL
    cdef int xnnz = -1
    cdef double y
    cdef double sample_weight
    cdef double wscale = 1.0
    cdef int i, j, class_ind, feature_ind
    cdef double val
    cdef double sum_loss = 0.0
    cdef MultinomialLogLoss multiloss = MultinomialLogLoss()
    cdef np.ndarray[double, ndim=2] sum_gradient_array = \
        np.zeros((n_features, n_classes), dtype=np.double, order="c")
    cdef double* sum_gradient = <double*> sum_gradient_array.data
    cdef np.ndarray[double, ndim=1] prediction_array = \
        np.zeros(n_classes, dtype=np.double, order="c")
    cdef double* prediction = <double*> prediction_array.data
    cdef np.ndarray[double, ndim=1] gradient_array = \
        np.zeros(n_classes, dtype=np.double, order="c")
    cdef double* gradient = <double*> gradient_array.data
    with nogil:
        for i in range(n_samples):
                        dataset.next(&x_data_ptr, &x_ind_ptr, &xnnz,
                         &y, &sample_weight)
                        predict_sample(x_data_ptr, x_ind_ptr, xnnz, weights, wscale,
                           intercept, prediction, n_classes)
                        multiloss._dloss(prediction, y, n_classes, sample_weight, gradient)
                        sum_loss += multiloss._loss(prediction, y, n_classes, sample_weight)
                        for j in range(xnnz):
                feature_ind = x_ind_ptr[j]
                val = x_data_ptr[j]
                for class_ind in range(n_classes):
                    sum_gradient[feature_ind * n_classes + class_ind] += \
                        gradient[class_ind] * val
    return sum_loss, sum_gradient_array

def sag(SequentialDataset dataset,
        np.ndarray[double, ndim=2, mode='c'] weights_array,
        np.ndarray[double, ndim=1, mode='c'] intercept_array,
        int n_samples,
        int n_features,
        int n_classes,
        double tol,
        int max_iter,
        str loss_function,
        double step_size,
        double alpha,
        np.ndarray[double, ndim=2, mode='c'] sum_gradient_init,
        np.ndarray[double, ndim=2, mode='c'] gradient_memory_init,
        np.ndarray[bint, ndim=1, mode='c'] seen_init,
        int num_seen,
        bint fit_intercept,
        np.ndarray[double, ndim=1, mode='c'] intercept_sum_gradient_init,
        double intercept_decay,
        bint verbose):
            cdef double *x_data_ptr = NULL
        cdef int *x_ind_ptr = NULL
        cdef int xnnz = -1
        cdef double y
        cdef double sample_weight
        cdef int f_idx, s_idx, feature_ind, class_ind, j
        cdef int n_iter = 0
        cdef int sample_itr
        cdef int sample_ind
        cdef double max_change
        cdef double max_weight
        cdef time_t start_time
        cdef time_t end_time
        cdef double wscale_update = 1.0 - step_size * alpha
        cdef bint* seen = <bint*> seen_init.data
        cdef double cum_sum
        cdef double* weights = <double * >weights_array.data
        cdef double* intercept = <double * >intercept_array.data
        cdef double* intercept_sum_gradient = \
        <double * >intercept_sum_gradient_init.data
        cdef double* sum_gradient = <double*> sum_gradient_init.data
        cdef double* gradient_memory = <double*> gradient_memory_init.data
        cdef np.ndarray[double, ndim=1] cumulative_sums_array = \
        np.empty(n_samples, dtype=np.double, order="c")
    cdef double* cumulative_sums = <double*> cumulative_sums_array.data
        cdef np.ndarray[int, ndim=1] feature_hist_array = \
        np.zeros(n_features, dtype=np.int32, order="c")
    cdef int* feature_hist = <int*> feature_hist_array.data
        cdef np.ndarray[double, ndim=2] previous_weights_array = \
        np.zeros((n_features, n_classes), dtype=np.double, order="c")
    cdef double* previous_weights = <double*> previous_weights_array.data
    cdef np.ndarray[double, ndim=1] prediction_array = \
        np.zeros(n_classes, dtype=np.double, order="c")
    cdef double* prediction = <double*> prediction_array.data
    cdef np.ndarray[double, ndim=1] gradient_array = \
        np.zeros(n_classes, dtype=np.double, order="c")
    cdef double* gradient = <double*> gradient_array.data
        cdef double wscale = 1.0
        cumulative_sums[0] = 0.0
        cdef LossFunction loss
        cdef bint multinomial = False
        cdef MultinomialLogLoss multiloss
    if loss_function == "multinomial":
        multinomial = True
        multiloss = MultinomialLogLoss()
    elif loss_function == "log":
        loss = Log()
    elif loss_function == "squared":
        loss = SquaredLoss()
    else:
        raise ValueError("Invalid loss parameter: got %s instead of "
                         "one of ('log', 'squared', 'multinomial')"
                         % loss_function)
    with nogil:
        start_time = time(NULL)
        for n_iter in range(max_iter):
            for sample_itr in range(n_samples):
                                sample_ind = dataset.random(&x_data_ptr, &x_ind_ptr, &xnnz,
                                              &y, &sample_weight)
                                s_idx = sample_ind * n_classes
                                if seen[sample_ind] == 0:
                    num_seen += 1
                    seen[sample_ind] = 1
                                if sample_itr > 0:
                    for j in range(xnnz):
                        feature_ind = x_ind_ptr[j]
                                                f_idx = feature_ind * n_classes
                        cum_sum = cumulative_sums[sample_itr - 1]
                        if feature_hist[feature_ind] != 0:
                            cum_sum -= \
                                cumulative_sums[feature_hist[feature_ind] - 1]
                        for class_ind in range(n_classes):
                            weights[f_idx + class_ind] -= \
                                cum_sum * sum_gradient[f_idx + class_ind]
                                                        if not skl_isfinite(weights[f_idx + class_ind]):
                                with gil:
                                    raise_infinite_error(n_iter)
                        feature_hist[feature_ind] = sample_itr
                                predict_sample(x_data_ptr, x_ind_ptr, xnnz, weights, wscale,
                               intercept, prediction, n_classes)
                                if multinomial:
                    multiloss._dloss(prediction, y, n_classes, sample_weight,
                                     gradient)
                else:
                    gradient[0] = loss._dloss(prediction[0], y) * sample_weight
                                for j in range(xnnz):
                    feature_ind = x_ind_ptr[j]
                    val = x_data_ptr[j]
                    f_idx = feature_ind * n_classes
                    for class_ind in range(n_classes):
                        sum_gradient[f_idx + class_ind] += \
                            val * (gradient[class_ind] -
                                   gradient_memory[s_idx + class_ind])
                                if fit_intercept:
                    for class_ind in range(n_classes):
                        intercept_sum_gradient[class_ind] += \
                            (gradient[class_ind] -
                             gradient_memory[s_idx + class_ind])
                        intercept[class_ind] -= \
                            (step_size * intercept_sum_gradient[class_ind] /
                             num_seen * intercept_decay)
                                                if not skl_isfinite(intercept[class_ind]):
                            with gil:
                                raise_infinite_error(n_iter)
                                for class_ind in range(n_classes):
                    gradient_memory[s_idx + class_ind] = gradient[class_ind]
                                wscale *= wscale_update
                if sample_itr == 0:
                    cumulative_sums[0] = step_size / (wscale * num_seen)
                else:
                    cumulative_sums[sample_itr] = \
                        (cumulative_sums[sample_itr - 1] +
                         step_size / (wscale * num_seen))
                                if wscale < 1e-9:
                    if verbose:
                        with gil:
                            print("rescaling...")
                    wscale = scale_weights(
                        weights, wscale, n_features, n_samples, n_classes,
                        sample_itr, cumulative_sums, feature_hist, sum_gradient)
                                    wscale = scale_weights(weights, wscale, n_features, n_samples,
                                   n_classes, n_samples - 1, cumulative_sums,
                                   feature_hist, sum_gradient)
                        max_change = 0.0
            max_weight = 0.0
            for feature_ind in range(n_features):
                max_weight = fmax(max_weight, fabs(weights[feature_ind]))
                max_change = fmax(max_change,
                                  fabs(weights[feature_ind] -
                                       previous_weights[feature_ind]))
                previous_weights[feature_ind] = weights[feature_ind]
            if max_change / max_weight <= tol:
                if verbose:
                    end_time = time(NULL)
                    with gil:
                        print("convergence after %d epochs took %d seconds" %
                              (n_iter + 1, end_time - start_time))
                break
    n_iter += 1
    if verbose and n_iter >= max_iter:
        end_time = time(NULL)
        print(("max_iter reached after %d seconds") %
              (end_time - start_time))
    return num_seen, n_iter

cdef void raise_infinite_error(int n_iter):
    raise ValueError("Floating-point under-/overflow occurred at "
                     "epoch                      "scaling the input data with StandardScaler "
                     "or MinMaxScaler might help." % (n_iter + 1))

cdef double scale_weights(double* weights, double wscale, int n_features,
                          int n_samples, int n_classes, int sample_itr,
                          double* cumulative_sums, int* feature_hist,
                          double* sum_gradient) nogil:
        cdef int feature_ind, class_ind, idx
    cdef double cum_sum
    idx = -1
    for feature_ind in range(n_features):
        cum_sum = cumulative_sums[sample_itr]
        if feature_hist[feature_ind] != 0:
            cum_sum -= cumulative_sums[feature_hist[feature_ind] - 1]
        for class_ind in range(n_classes):
            idx += 1              weights[idx] -= cum_sum * sum_gradient[idx]
            weights[idx] *= wscale
        feature_hist[feature_ind] = (sample_itr + 1) % n_samples
    cumulative_sums[sample_itr % n_samples] = 0.0
        return 1.0

cdef void predict_sample(double* x_data_ptr, int* x_ind_ptr, int xnnz,
                         double* w_data_ptr, double wscale, double* intercept,
                         double* prediction, int n_classes) nogil:
        cdef int feature_ind, class_ind, j
    cdef double innerprod
    for class_ind in range(n_classes):
        innerprod = 0.0
                for j in range(xnnz):
            feature_ind = x_ind_ptr[j]
            innerprod += (w_data_ptr[feature_ind * n_classes + class_ind] *
                          x_data_ptr[j])
        prediction[class_ind] = wscale * innerprod + intercept[class_ind]
import os
from os.path import join
import numpy
from sklearn._build_utils import get_blas_info

def configuration(parent_package='', top_path=None):
    from numpy.distutils.misc_util import Configuration
    config = Configuration('linear_model', parent_package, top_path)
    cblas_libs, blas_info = get_blas_info()
    if os.name == 'posix':
        cblas_libs.append('m')
    config.add_extension('cd_fast', sources=['cd_fast.pyx'],
                         libraries=cblas_libs,
                         include_dirs=[join('..', 'src', 'cblas'),
                                       numpy.get_include(),
                                       blas_info.pop('include_dirs', [])],
                         extra_compile_args=blas_info.pop('extra_compile_args',
                                                          []), **blas_info)
    config.add_extension('sgd_fast',
                         sources=['sgd_fast.pyx'],
                         include_dirs=[join('..', 'src', 'cblas'),
                                       numpy.get_include(),
                                       blas_info.pop('include_dirs', [])],
                         libraries=cblas_libs,
                         extra_compile_args=blas_info.pop('extra_compile_args',
                                                          []),
                         **blas_info)
    config.add_extension('sag_fast',
                         sources=['sag_fast.pyx'],
                         include_dirs=numpy.get_include())
        config.add_subpackage('tests')
    return config
if __name__ == '__main__':
    from numpy.distutils.core import setup
    setup(**configuration(top_path='').todict())

import numpy as np
import sys
from time import time
cimport cython
from libc.math cimport exp, log, sqrt, pow, fabs
cimport numpy as np
cdef extern from "sgd_fast_helpers.h":
    bint skl_isfinite(double) nogil
from sklearn.utils.weight_vector cimport WeightVector
from sklearn.utils.seq_dataset cimport SequentialDataset
np.import_array()
DEF NO_PENALTY = 0
DEF L1 = 1
DEF L2 = 2
DEF ELASTICNET = 3
DEF CONSTANT = 1
DEF OPTIMAL = 2
DEF INVSCALING = 3
DEF PA1 = 4
DEF PA2 = 5

cdef class LossFunction:
    
    cdef double loss(self, double p, double y) nogil:
                return 0.
    def dloss(self, double p, double y):
                return self._dloss(p, y)
    cdef double _dloss(self, double p, double y) nogil:
                        return 0.

cdef class Regression(LossFunction):
    
    cdef double loss(self, double p, double y) nogil:
        return 0.
    cdef double _dloss(self, double p, double y) nogil:
        return 0.

cdef class Classification(LossFunction):
    
    cdef double loss(self, double p, double y) nogil:
        return 0.
    cdef double _dloss(self, double p, double y) nogil:
        return 0.

cdef class ModifiedHuber(Classification):
        cdef double loss(self, double p, double y) nogil:
        cdef double z = p * y
        if z >= 1.0:
            return 0.0
        elif z >= -1.0:
            return (1.0 - z) * (1.0 - z)
        else:
            return -4.0 * z
    cdef double _dloss(self, double p, double y) nogil:
        cdef double z = p * y
        if z >= 1.0:
            return 0.0
        elif z >= -1.0:
            return 2.0 * (1.0 - z) * -y
        else:
            return -4.0 * y
    def __reduce__(self):
        return ModifiedHuber, ()

cdef class Hinge(Classification):
    
    cdef double threshold
    def __init__(self, double threshold=1.0):
        self.threshold = threshold
    cdef double loss(self, double p, double y) nogil:
        cdef double z = p * y
        if z <= self.threshold:
            return (self.threshold - z)
        return 0.0
    cdef double _dloss(self, double p, double y) nogil:
        cdef double z = p * y
        if z <= self.threshold:
            return -y
        return 0.0
    def __reduce__(self):
        return Hinge, (self.threshold,)

cdef class SquaredHinge(Classification):
    
    cdef double threshold
    def __init__(self, double threshold=1.0):
        self.threshold = threshold
    cdef double loss(self, double p, double y) nogil:
        cdef double z = self.threshold - p * y
        if z > 0:
            return z * z
        return 0.0
    cdef double _dloss(self, double p, double y) nogil:
        cdef double z = self.threshold - p * y
        if z > 0:
            return -2 * y * z
        return 0.0
    def __reduce__(self):
        return SquaredHinge, (self.threshold,)

cdef class Log(Classification):
    
    cdef double loss(self, double p, double y) nogil:
        cdef double z = p * y
                if z > 18:
            return exp(-z)
        if z < -18:
            return -z
        return log(1.0 + exp(-z))
    cdef double _dloss(self, double p, double y) nogil:
        cdef double z = p * y
                if z > 18.0:
            return exp(-z) * -y
        if z < -18.0:
            return -y
        return -y / (exp(z) + 1.0)
    def __reduce__(self):
        return Log, ()

cdef class SquaredLoss(Regression):
        cdef double loss(self, double p, double y) nogil:
        return 0.5 * (p - y) * (p - y)
    cdef double _dloss(self, double p, double y) nogil:
        return p - y
    def __reduce__(self):
        return SquaredLoss, ()

cdef class Huber(Regression):
    
    cdef double c
    def __init__(self, double c):
        self.c = c
    cdef double loss(self, double p, double y) nogil:
        cdef double r = p - y
        cdef double abs_r = fabs(r)
        if abs_r <= self.c:
            return 0.5 * r * r
        else:
            return self.c * abs_r - (0.5 * self.c * self.c)
    cdef double _dloss(self, double p, double y) nogil:
        cdef double r = p - y
        cdef double abs_r = fabs(r)
        if abs_r <= self.c:
            return r
        elif r > 0.0:
            return self.c
        else:
            return -self.c
    def __reduce__(self):
        return Huber, (self.c,)

cdef class EpsilonInsensitive(Regression):
    
    cdef double epsilon
    def __init__(self, double epsilon):
        self.epsilon = epsilon
    cdef double loss(self, double p, double y) nogil:
        cdef double ret = fabs(y - p) - self.epsilon
        return ret if ret > 0 else 0
    cdef double _dloss(self, double p, double y) nogil:
        if y - p > self.epsilon:
            return -1
        elif p - y > self.epsilon:
            return 1
        else:
            return 0
    def __reduce__(self):
        return EpsilonInsensitive, (self.epsilon,)

cdef class SquaredEpsilonInsensitive(Regression):
    
    cdef double epsilon
    def __init__(self, double epsilon):
        self.epsilon = epsilon
    cdef double loss(self, double p, double y) nogil:
        cdef double ret = fabs(y - p) - self.epsilon
        return ret * ret if ret > 0 else 0
    cdef double _dloss(self, double p, double y) nogil:
        cdef double z
        z = y - p
        if z > self.epsilon:
            return -2 * (z - self.epsilon)
        elif z < -self.epsilon:
            return 2 * (-z - self.epsilon)
        else:
            return 0
    def __reduce__(self):
        return SquaredEpsilonInsensitive, (self.epsilon,)

def plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
              double intercept,
              LossFunction loss,
              int penalty_type,
              double alpha, double C,
              double l1_ratio,
              SequentialDataset dataset,
              int n_iter, int fit_intercept,
              int verbose, bint shuffle, np.uint32_t seed,
              double weight_pos, double weight_neg,
              int learning_rate, double eta0,
              double power_t,
              double t=1.0,
              double intercept_decay=1.0):
        standard_weights, standard_intercept,\
        _, _ = _plain_sgd(weights,
                          intercept,
                          None,
                          0,
                          loss,
                          penalty_type,
                          alpha, C,
                          l1_ratio,
                          dataset,
                          n_iter, fit_intercept,
                          verbose, shuffle, seed,
                          weight_pos, weight_neg,
                          learning_rate, eta0,
                          power_t,
                          t,
                          intercept_decay,
                          0)
    return standard_weights, standard_intercept

def average_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
                double intercept,
                np.ndarray[double, ndim=1, mode='c'] average_weights,
                double average_intercept,
                LossFunction loss,
                int penalty_type,
                double alpha, double C,
                double l1_ratio,
                SequentialDataset dataset,
                int n_iter, int fit_intercept,
                int verbose, bint shuffle, np.uint32_t seed,
                double weight_pos, double weight_neg,
                int learning_rate, double eta0,
                double power_t,
                double t=1.0,
                double intercept_decay=1.0,
                int average=1):
        return _plain_sgd(weights,
                      intercept,
                      average_weights,
                      average_intercept,
                      loss,
                      penalty_type,
                      alpha, C,
                      l1_ratio,
                      dataset,
                      n_iter, fit_intercept,
                      verbose, shuffle, seed,
                      weight_pos, weight_neg,
                      learning_rate, eta0,
                      power_t,
                      t,
                      intercept_decay,
                      average)

def _plain_sgd(np.ndarray[double, ndim=1, mode='c'] weights,
               double intercept,
               np.ndarray[double, ndim=1, mode='c'] average_weights,
               double average_intercept,
               LossFunction loss,
               int penalty_type,
               double alpha, double C,
               double l1_ratio,
               SequentialDataset dataset,
               int n_iter, int fit_intercept,
               int verbose, bint shuffle, np.uint32_t seed,
               double weight_pos, double weight_neg,
               int learning_rate, double eta0,
               double power_t,
               double t=1.0,
               double intercept_decay=1.0,
               int average=0):
        cdef Py_ssize_t n_samples = dataset.n_samples
    cdef Py_ssize_t n_features = weights.shape[0]
    cdef WeightVector w = WeightVector(weights, average_weights)
    cdef double* w_ptr = &weights[0]
    cdef double *x_data_ptr = NULL
    cdef int *x_ind_ptr = NULL
    cdef double* ps_ptr = NULL
        cdef bint infinity = False
    cdef int xnnz
    cdef double eta = 0.0
    cdef double p = 0.0
    cdef double update = 0.0
    cdef double sumloss = 0.0
    cdef double y = 0.0
    cdef double sample_weight
    cdef double class_weight = 1.0
    cdef unsigned int count = 0
    cdef unsigned int epoch = 0
    cdef unsigned int i = 0
    cdef int is_hinge = isinstance(loss, Hinge)
    cdef double optimal_init = 0.0
    cdef double dloss = 0.0
    cdef double MAX_DLOSS = 1e12
        cdef np.ndarray[double, ndim = 1, mode = "c"] q = None
    cdef double * q_data_ptr = NULL
    if penalty_type == L1 or penalty_type == ELASTICNET:
        q = np.zeros((n_features,), dtype=np.float64, order="c")
        q_data_ptr = <double * > q.data
    cdef double u = 0.0
    if penalty_type == L2:
        l1_ratio = 0.0
    elif penalty_type == L1:
        l1_ratio = 1.0
    eta = eta0
    if learning_rate == OPTIMAL:
        typw = np.sqrt(1.0 / np.sqrt(alpha))
                initial_eta0 = typw / max(1.0, loss.dloss(-typw, 1.0))
                optimal_init = 1.0 / (initial_eta0 * alpha)
    t_start = time()
    with nogil:
        for epoch in range(n_iter):
            if verbose > 0:
                with gil:
                    print("-- Epoch %d" % (epoch + 1))
            if shuffle:
                dataset.shuffle(seed)
            for i in range(n_samples):
                dataset.next(&x_data_ptr, &x_ind_ptr, &xnnz,
                             &y, &sample_weight)
                p = w.dot(x_data_ptr, x_ind_ptr, xnnz) + intercept
                if learning_rate == OPTIMAL:
                    eta = 1.0 / (alpha * (optimal_init + t - 1))
                elif learning_rate == INVSCALING:
                    eta = eta0 / pow(t, power_t)
                if verbose > 0:
                    sumloss += loss.loss(p, y)
                if y > 0.0:
                    class_weight = weight_pos
                else:
                    class_weight = weight_neg
                if learning_rate == PA1:
                    update = sqnorm(x_data_ptr, x_ind_ptr, xnnz)
                    if update == 0:
                        continue
                    update = min(C, loss.loss(p, y) / update)
                elif learning_rate == PA2:
                    update = sqnorm(x_data_ptr, x_ind_ptr, xnnz)
                    update = loss.loss(p, y) / (update + 0.5 / C)
                else:
                    dloss = loss._dloss(p, y)
                                                            if dloss < -MAX_DLOSS:
                        dloss = -MAX_DLOSS
                    elif dloss > MAX_DLOSS:
                        dloss = MAX_DLOSS
                    update = -eta * dloss
                if learning_rate >= PA1:
                    if is_hinge:
                                                update *= y
                    elif y - p < 0:
                                                update *= -1
                update *= class_weight * sample_weight
                if penalty_type >= L2:
                                                            w.scale(max(0, 1.0 - ((1.0 - l1_ratio) * eta * alpha)))
                if update != 0.0:
                    w.add(x_data_ptr, x_ind_ptr, xnnz, update)
                    if fit_intercept == 1:
                        intercept += update * intercept_decay
                if 0 < average <= t:
                                                            
                    w.add_average(x_data_ptr, x_ind_ptr, xnnz,
                                  update, (t - average + 1))
                    average_intercept += ((intercept - average_intercept) /
                                          (t - average + 1))
                if penalty_type == L1 or penalty_type == ELASTICNET:
                    u += (l1_ratio * eta * alpha)
                    l1penalty(w, q_data_ptr, x_ind_ptr, xnnz, u)
                t += 1
                count += 1
                        if verbose > 0:
                with gil:
                    print("Norm: %.2f, NNZs: %d, "
                          "Bias: %.6f, T: %d, Avg. loss: %.6f"
                          % (w.norm(), weights.nonzero()[0].shape[0],
                             intercept, count, sumloss / count))
                    print("Total training time: %.2f seconds."
                          % (time() - t_start))
                        if (not skl_isfinite(intercept)
                or any_nonfinite(<double *>weights.data, n_features)):
                infinity = True
                break
    if infinity:
        raise ValueError(("Floating-point under-/overflow occurred at epoch"
                          "                           " MinMaxScaler might help.") % (epoch + 1))
    w.reset_wscale()
    return weights, intercept, average_weights, average_intercept

cdef bint any_nonfinite(double *w, int n) nogil:
    for i in range(n):
        if not skl_isfinite(w[i]):
            return True
    return 0

cdef double sqnorm(double * x_data_ptr, int * x_ind_ptr, int xnnz) nogil:
    cdef double x_norm = 0.0
    cdef int j
    cdef double z
    for j in range(xnnz):
        z = x_data_ptr[j]
        x_norm += z * z
    return x_norm

cdef void l1penalty(WeightVector w, double * q_data_ptr,
                    int *x_ind_ptr, int xnnz, double u) nogil:
        cdef double z = 0.0
    cdef int j = 0
    cdef int idx = 0
    cdef double wscale = w.wscale
    cdef double *w_data_ptr = w.w_data_ptr
    for j in range(xnnz):
        idx = x_ind_ptr[j]
        z = w_data_ptr[idx]
        if wscale * w_data_ptr[idx] > 0.0:
            w_data_ptr[idx] = max(
                0.0, w_data_ptr[idx] - ((u + q_data_ptr[idx]) / wscale))
        elif wscale * w_data_ptr[idx] < 0.0:
            w_data_ptr[idx] = min(
                0.0, w_data_ptr[idx] + ((u - q_data_ptr[idx]) / wscale))
        q_data_ptr[idx] += wscale * (w_data_ptr[idx] - z)
import numpy as np
from abc import ABCMeta, abstractmethod
from ..externals.joblib import Parallel, delayed
from .base import LinearClassifierMixin, SparseCoefMixin
from .base import make_dataset
from ..base import BaseEstimator, RegressorMixin
from ..utils import check_array, check_random_state, check_X_y
from ..utils.extmath import safe_sparse_dot
from ..utils.multiclass import _check_partial_fit_first_call
from ..utils.validation import check_is_fitted
from ..externals import six
from .sgd_fast import plain_sgd, average_sgd
from ..utils.fixes import astype
from ..utils import compute_class_weight
from ..utils import deprecated
from .sgd_fast import Hinge
from .sgd_fast import SquaredHinge
from .sgd_fast import Log
from .sgd_fast import ModifiedHuber
from .sgd_fast import SquaredLoss
from .sgd_fast import Huber
from .sgd_fast import EpsilonInsensitive
from .sgd_fast import SquaredEpsilonInsensitive

LEARNING_RATE_TYPES = {"constant": 1, "optimal": 2, "invscaling": 3,
                       "pa1": 4, "pa2": 5}
PENALTY_TYPES = {"none": 0, "l2": 2, "l1": 1, "elasticnet": 3}
DEFAULT_EPSILON = 0.1

class BaseSGD(six.with_metaclass(ABCMeta, BaseEstimator, SparseCoefMixin)):
    
    def __init__(self, loss, penalty='l2', alpha=0.0001, C=1.0,
                 l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True,
                 verbose=0, epsilon=0.1, random_state=None,
                 learning_rate="optimal", eta0=0.0, power_t=0.5,
                 warm_start=False, average=False):
        self.loss = loss
        self.penalty = penalty
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.alpha = alpha
        self.C = C
        self.l1_ratio = l1_ratio
        self.fit_intercept = fit_intercept
        self.n_iter = n_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.verbose = verbose
        self.eta0 = eta0
        self.power_t = power_t
        self.warm_start = warm_start
        self.average = average
        self._validate_params()
    def set_params(self, *args, **kwargs):
        super(BaseSGD, self).set_params(*args, **kwargs)
        self._validate_params()
        return self
    @abstractmethod
    def fit(self, X, y):
        
    def _validate_params(self):
                if not isinstance(self.shuffle, bool):
            raise ValueError("shuffle must be either True or False")
        if self.n_iter <= 0:
            raise ValueError("n_iter must be > zero")
        if not (0.0 <= self.l1_ratio <= 1.0):
            raise ValueError("l1_ratio must be in [0, 1]")
        if self.alpha < 0.0:
            raise ValueError("alpha must be >= 0")
        if self.learning_rate in ("constant", "invscaling"):
            if self.eta0 <= 0.0:
                raise ValueError("eta0 must be > 0")
        if self.learning_rate == "optimal" and self.alpha == 0:
            raise ValueError("alpha must be > 0 since "
                             "learning_rate is 'optimal'. alpha is used "
                             "to compute the optimal learning rate.")
                self._get_penalty_type(self.penalty)
        self._get_learning_rate_type(self.learning_rate)
        if self.loss not in self.loss_functions:
            raise ValueError("The loss %s is not supported. " % self.loss)
    def _get_loss_function(self, loss):
                try:
            loss_ = self.loss_functions[loss]
            loss_class, args = loss_[0], loss_[1:]
            if loss in ('huber', 'epsilon_insensitive',
                        'squared_epsilon_insensitive'):
                args = (self.epsilon, )
            return loss_class(*args)
        except KeyError:
            raise ValueError("The loss %s is not supported. " % loss)
    def _get_learning_rate_type(self, learning_rate):
        try:
            return LEARNING_RATE_TYPES[learning_rate]
        except KeyError:
            raise ValueError("learning rate %s "
                             "is not supported. " % learning_rate)
    def _get_penalty_type(self, penalty):
        penalty = str(penalty).lower()
        try:
            return PENALTY_TYPES[penalty]
        except KeyError:
            raise ValueError("Penalty %s is not supported. " % penalty)
    def _validate_sample_weight(self, sample_weight, n_samples):
                if sample_weight is None:
                        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
        else:
                        sample_weight = np.asarray(sample_weight, dtype=np.float64,
                                       order="C")
        if sample_weight.shape[0] != n_samples:
            raise ValueError("Shapes of X and sample_weight do not match.")
        return sample_weight
    def _allocate_parameter_mem(self, n_classes, n_features, coef_init=None,
                                intercept_init=None):
                if n_classes > 2:
                        if coef_init is not None:
                coef_init = np.asarray(coef_init, order="C")
                if coef_init.shape != (n_classes, n_features):
                    raise ValueError("Provided ``coef_`` does not match "
                                     "dataset. ")
                self.coef_ = coef_init
            else:
                self.coef_ = np.zeros((n_classes, n_features),
                                      dtype=np.float64, order="C")
                        if intercept_init is not None:
                intercept_init = np.asarray(intercept_init, order="C")
                if intercept_init.shape != (n_classes, ):
                    raise ValueError("Provided intercept_init "
                                     "does not match dataset.")
                self.intercept_ = intercept_init
            else:
                self.intercept_ = np.zeros(n_classes, dtype=np.float64,
                                           order="C")
        else:
                        if coef_init is not None:
                coef_init = np.asarray(coef_init, dtype=np.float64,
                                       order="C")
                coef_init = coef_init.ravel()
                if coef_init.shape != (n_features,):
                    raise ValueError("Provided coef_init does not "
                                     "match dataset.")
                self.coef_ = coef_init
            else:
                self.coef_ = np.zeros(n_features,
                                      dtype=np.float64,
                                      order="C")
                        if intercept_init is not None:
                intercept_init = np.asarray(intercept_init, dtype=np.float64)
                if intercept_init.shape != (1,) and intercept_init.shape != ():
                    raise ValueError("Provided intercept_init "
                                     "does not match dataset.")
                self.intercept_ = intercept_init.reshape(1,)
            else:
                self.intercept_ = np.zeros(1, dtype=np.float64, order="C")
                if self.average > 0:
            self.standard_coef_ = self.coef_
            self.standard_intercept_ = self.intercept_
            self.average_coef_ = np.zeros(self.coef_.shape,
                                          dtype=np.float64,
                                          order="C")
            self.average_intercept_ = np.zeros(self.standard_intercept_.shape,
                                               dtype=np.float64,
                                               order="C")

def _prepare_fit_binary(est, y, i):
        y_i = np.ones(y.shape, dtype=np.float64, order="C")
    y_i[y != est.classes_[i]] = -1.0
    average_intercept = 0
    average_coef = None
    if len(est.classes_) == 2:
        if not est.average:
            coef = est.coef_.ravel()
            intercept = est.intercept_[0]
        else:
            coef = est.standard_coef_.ravel()
            intercept = est.standard_intercept_[0]
            average_coef = est.average_coef_.ravel()
            average_intercept = est.average_intercept_[0]
    else:
        if not est.average:
            coef = est.coef_[i]
            intercept = est.intercept_[i]
        else:
            coef = est.standard_coef_[i]
            intercept = est.standard_intercept_[i]
            average_coef = est.average_coef_[i]
            average_intercept = est.average_intercept_[i]
    return y_i, coef, intercept, average_coef, average_intercept

def fit_binary(est, i, X, y, alpha, C, learning_rate, n_iter,
               pos_weight, neg_weight, sample_weight):
                y_i, coef, intercept, average_coef, average_intercept = \
        _prepare_fit_binary(est, y, i)
    assert y_i.shape[0] == y.shape[0] == sample_weight.shape[0]
    dataset, intercept_decay = make_dataset(X, y_i, sample_weight)
    penalty_type = est._get_penalty_type(est.penalty)
    learning_rate_type = est._get_learning_rate_type(learning_rate)
        random_state = check_random_state(est.random_state)
            seed = random_state.randint(0, np.iinfo(np.int32).max)
    if not est.average:
        return plain_sgd(coef, intercept, est.loss_function_,
                         penalty_type, alpha, C, est.l1_ratio,
                         dataset, n_iter, int(est.fit_intercept),
                         int(est.verbose), int(est.shuffle), seed,
                         pos_weight, neg_weight,
                         learning_rate_type, est.eta0,
                         est.power_t, est.t_, intercept_decay)
    else:
        standard_coef, standard_intercept, average_coef, \
            average_intercept = average_sgd(coef, intercept, average_coef,
                                            average_intercept,
                                            est.loss_function_, penalty_type,
                                            alpha, C, est.l1_ratio, dataset,
                                            n_iter, int(est.fit_intercept),
                                            int(est.verbose), int(est.shuffle),
                                            seed, pos_weight, neg_weight,
                                            learning_rate_type, est.eta0,
                                            est.power_t, est.t_,
                                            intercept_decay,
                                            est.average)
        if len(est.classes_) == 2:
            est.average_intercept_[0] = average_intercept
        else:
            est.average_intercept_[i] = average_intercept
        return standard_coef, standard_intercept

class BaseSGDClassifier(six.with_metaclass(ABCMeta, BaseSGD,
                                           LinearClassifierMixin)):
    loss_functions = {
        "hinge": (Hinge, 1.0),
        "squared_hinge": (SquaredHinge, 1.0),
        "perceptron": (Hinge, 0.0),
        "log": (Log, ),
        "modified_huber": (ModifiedHuber, ),
        "squared_loss": (SquaredLoss, ),
        "huber": (Huber, DEFAULT_EPSILON),
        "epsilon_insensitive": (EpsilonInsensitive, DEFAULT_EPSILON),
        "squared_epsilon_insensitive": (SquaredEpsilonInsensitive,
                                        DEFAULT_EPSILON),
    }
    @abstractmethod
    def __init__(self, loss="hinge", penalty='l2', alpha=0.0001, l1_ratio=0.15,
                 fit_intercept=True, n_iter=5, shuffle=True, verbose=0,
                 epsilon=DEFAULT_EPSILON, n_jobs=1, random_state=None,
                 learning_rate="optimal", eta0=0.0, power_t=0.5,
                 class_weight=None, warm_start=False, average=False):
        super(BaseSGDClassifier, self).__init__(loss=loss, penalty=penalty,
                                                alpha=alpha, l1_ratio=l1_ratio,
                                                fit_intercept=fit_intercept,
                                                n_iter=n_iter, shuffle=shuffle,
                                                verbose=verbose,
                                                epsilon=epsilon,
                                                random_state=random_state,
                                                learning_rate=learning_rate,
                                                eta0=eta0, power_t=power_t,
                                                warm_start=warm_start,
                                                average=average)
        self.class_weight = class_weight
        self.n_jobs = int(n_jobs)
    @property
    @deprecated("Attribute loss_function was deprecated in version 0.19 and "
                "will be removed in 0.21. Use 'loss_function_' instead")
    def loss_function(self):
        return self.loss_function_
    def _partial_fit(self, X, y, alpha, C,
                     loss, learning_rate, n_iter,
                     classes, sample_weight,
                     coef_init, intercept_init):
        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order="C")
        n_samples, n_features = X.shape
        self._validate_params()
        _check_partial_fit_first_call(self, classes)
        n_classes = self.classes_.shape[0]
                self._expanded_class_weight = compute_class_weight(self.class_weight,
                                                           self.classes_, y)
        sample_weight = self._validate_sample_weight(sample_weight, n_samples)
        if getattr(self, "coef_", None) is None or coef_init is not None:
            self._allocate_parameter_mem(n_classes, n_features,
                                         coef_init, intercept_init)
        elif n_features != self.coef_.shape[-1]:
            raise ValueError("Number of features %d does not match previous "
                             "data %d." % (n_features, self.coef_.shape[-1]))
        self.loss_function_ = self._get_loss_function(loss)
        if not hasattr(self, "t_"):
            self.t_ = 1.0
                if n_classes > 2:
            self._fit_multiclass(X, y, alpha=alpha, C=C,
                                 learning_rate=learning_rate,
                                 sample_weight=sample_weight, n_iter=n_iter)
        elif n_classes == 2:
            self._fit_binary(X, y, alpha=alpha, C=C,
                             learning_rate=learning_rate,
                             sample_weight=sample_weight, n_iter=n_iter)
        else:
            raise ValueError("The number of class labels must be "
                             "greater than one.")
        return self
    def _fit(self, X, y, alpha, C, loss, learning_rate, coef_init=None,
             intercept_init=None, sample_weight=None):
        if hasattr(self, "classes_"):
            self.classes_ = None
        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order="C")
        n_samples, n_features = X.shape
                        classes = np.unique(y)
        if self.warm_start and hasattr(self, "coef_"):
            if coef_init is None:
                coef_init = self.coef_
            if intercept_init is None:
                intercept_init = self.intercept_
        else:
            self.coef_ = None
            self.intercept_ = None
        if self.average > 0:
            self.standard_coef_ = self.coef_
            self.standard_intercept_ = self.intercept_
            self.average_coef_ = None
            self.average_intercept_ = None
                self.t_ = 1.0
        self._partial_fit(X, y, alpha, C, loss, learning_rate, self.n_iter,
                          classes, sample_weight, coef_init, intercept_init)
        return self
    def _fit_binary(self, X, y, alpha, C, sample_weight,
                    learning_rate, n_iter):
                coef, intercept = fit_binary(self, 1, X, y, alpha, C,
                                     learning_rate, n_iter,
                                     self._expanded_class_weight[1],
                                     self._expanded_class_weight[0],
                                     sample_weight)
        self.t_ += n_iter * X.shape[0]
                if self.average > 0:
            if self.average <= self.t_ - 1:
                self.coef_ = self.average_coef_.reshape(1, -1)
                self.intercept_ = self.average_intercept_
            else:
                self.coef_ = self.standard_coef_.reshape(1, -1)
                self.standard_intercept_ = np.atleast_1d(intercept)
                self.intercept_ = self.standard_intercept_
        else:
            self.coef_ = coef.reshape(1, -1)
                        self.intercept_ = np.atleast_1d(intercept)
    def _fit_multiclass(self, X, y, alpha, C, learning_rate,
                        sample_weight, n_iter):
                        result = Parallel(n_jobs=self.n_jobs, backend="threading",
                          verbose=self.verbose)(
            delayed(fit_binary)(self, i, X, y, alpha, C, learning_rate,
                                n_iter, self._expanded_class_weight[i], 1.,
                                sample_weight)
            for i in range(len(self.classes_)))
        for i, (_, intercept) in enumerate(result):
            self.intercept_[i] = intercept
        self.t_ += n_iter * X.shape[0]
        if self.average > 0:
            if self.average <= self.t_ - 1.0:
                self.coef_ = self.average_coef_
                self.intercept_ = self.average_intercept_
            else:
                self.coef_ = self.standard_coef_
                self.standard_intercept_ = np.atleast_1d(self.intercept_)
                self.intercept_ = self.standard_intercept_
    def partial_fit(self, X, y, classes=None, sample_weight=None):
                if self.class_weight in ['balanced']:
            raise ValueError("class_weight '{0}' is not supported for "
                             "partial_fit. In order to use 'balanced' weights,"
                             " use compute_class_weight('{0}', classes, y). "
                             "In place of y you can us a large enough sample "
                             "of the full training set target to properly "
                             "estimate the class frequency distributions. "
                             "Pass the resulting weights as the class_weight "
                             "parameter.".format(self.class_weight))
        return self._partial_fit(X, y, alpha=self.alpha, C=1.0, loss=self.loss,
                                 learning_rate=self.learning_rate, n_iter=1,
                                 classes=classes, sample_weight=sample_weight,
                                 coef_init=None, intercept_init=None)
    def fit(self, X, y, coef_init=None, intercept_init=None,
            sample_weight=None):
                return self._fit(X, y, alpha=self.alpha, C=1.0,
                         loss=self.loss, learning_rate=self.learning_rate,
                         coef_init=coef_init, intercept_init=intercept_init,
                         sample_weight=sample_weight)

class SGDClassifier(BaseSGDClassifier):
    
    def __init__(self, loss="hinge", penalty='l2', alpha=0.0001, l1_ratio=0.15,
                 fit_intercept=True, n_iter=5, shuffle=True, verbose=0,
                 epsilon=DEFAULT_EPSILON, n_jobs=1, random_state=None,
                 learning_rate="optimal", eta0=0.0, power_t=0.5,
                 class_weight=None, warm_start=False, average=False):
        super(SGDClassifier, self).__init__(
            loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio,
            fit_intercept=fit_intercept, n_iter=n_iter, shuffle=shuffle,
            verbose=verbose, epsilon=epsilon, n_jobs=n_jobs,
            random_state=random_state, learning_rate=learning_rate, eta0=eta0,
            power_t=power_t, class_weight=class_weight, warm_start=warm_start,
            average=average)
    def _check_proba(self):
        check_is_fitted(self, "t_")
        if self.loss not in ("log", "modified_huber"):
            raise AttributeError("probability estimates are not available for"
                                 " loss=%r" % self.loss)
    @property
    def predict_proba(self):
                self._check_proba()
        return self._predict_proba
    def _predict_proba(self, X):
        if self.loss == "log":
            return self._predict_proba_lr(X)
        elif self.loss == "modified_huber":
            binary = (len(self.classes_) == 2)
            scores = self.decision_function(X)
            if binary:
                prob2 = np.ones((scores.shape[0], 2))
                prob = prob2[:, 1]
            else:
                prob = scores
            np.clip(scores, -1, 1, prob)
            prob += 1.
            prob /= 2.
            if binary:
                prob2[:, 0] -= prob
                prob = prob2
            else:
                                                                prob_sum = prob.sum(axis=1)
                all_zero = (prob_sum == 0)
                if np.any(all_zero):
                    prob[all_zero, :] = 1
                    prob_sum[all_zero] = len(self.classes_)
                                prob /= prob_sum.reshape((prob.shape[0], -1))
            return prob
        else:
            raise NotImplementedError("predict_(log_)proba only supported when"
                                      " loss='log' or loss='modified_huber' "
                                      "(%r given)" % self.loss)
    @property
    def predict_log_proba(self):
                self._check_proba()
        return self._predict_log_proba
    def _predict_log_proba(self, X):
        return np.log(self.predict_proba(X))

class BaseSGDRegressor(BaseSGD, RegressorMixin):
    loss_functions = {
        "squared_loss": (SquaredLoss, ),
        "huber": (Huber, DEFAULT_EPSILON),
        "epsilon_insensitive": (EpsilonInsensitive, DEFAULT_EPSILON),
        "squared_epsilon_insensitive": (SquaredEpsilonInsensitive,
                                        DEFAULT_EPSILON),
    }
    @abstractmethod
    def __init__(self, loss="squared_loss", penalty="l2", alpha=0.0001,
                 l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True,
                 verbose=0, epsilon=DEFAULT_EPSILON, random_state=None,
                 learning_rate="invscaling", eta0=0.01, power_t=0.25,
                 warm_start=False, average=False):
        super(BaseSGDRegressor, self).__init__(loss=loss, penalty=penalty,
                                               alpha=alpha, l1_ratio=l1_ratio,
                                               fit_intercept=fit_intercept,
                                               n_iter=n_iter, shuffle=shuffle,
                                               verbose=verbose,
                                               epsilon=epsilon,
                                               random_state=random_state,
                                               learning_rate=learning_rate,
                                               eta0=eta0, power_t=power_t,
                                               warm_start=warm_start,
                                               average=average)
    def _partial_fit(self, X, y, alpha, C, loss, learning_rate,
                     n_iter, sample_weight,
                     coef_init, intercept_init):
        X, y = check_X_y(X, y, "csr", copy=False, order='C', dtype=np.float64)
        y = astype(y, np.float64, copy=False)
        n_samples, n_features = X.shape
        self._validate_params()
                sample_weight = self._validate_sample_weight(sample_weight, n_samples)
        if getattr(self, "coef_", None) is None:
            self._allocate_parameter_mem(1, n_features,
                                         coef_init, intercept_init)
        elif n_features != self.coef_.shape[-1]:
            raise ValueError("Number of features %d does not match previous "
                             "data %d." % (n_features, self.coef_.shape[-1]))
        if self.average > 0 and getattr(self, "average_coef_", None) is None:
            self.average_coef_ = np.zeros(n_features,
                                          dtype=np.float64,
                                          order="C")
            self.average_intercept_ = np.zeros(1,
                                               dtype=np.float64,
                                               order="C")
        self._fit_regressor(X, y, alpha, C, loss, learning_rate,
                            sample_weight, n_iter)
        return self
    def partial_fit(self, X, y, sample_weight=None):
                return self._partial_fit(X, y, self.alpha, C=1.0,
                                 loss=self.loss,
                                 learning_rate=self.learning_rate, n_iter=1,
                                 sample_weight=sample_weight,
                                 coef_init=None, intercept_init=None)
    def _fit(self, X, y, alpha, C, loss, learning_rate, coef_init=None,
             intercept_init=None, sample_weight=None):
        if self.warm_start and getattr(self, "coef_", None) is not None:
            if coef_init is None:
                coef_init = self.coef_
            if intercept_init is None:
                intercept_init = self.intercept_
        else:
            self.coef_ = None
            self.intercept_ = None
        if self.average > 0:
            self.standard_intercept_ = self.intercept_
            self.standard_coef_ = self.coef_
            self.average_coef_ = None
            self.average_intercept_ = None
                self.t_ = 1.0
        return self._partial_fit(X, y, alpha, C, loss, learning_rate,
                                 self.n_iter, sample_weight,
                                 coef_init, intercept_init)
    def fit(self, X, y, coef_init=None, intercept_init=None,
            sample_weight=None):
                return self._fit(X, y, alpha=self.alpha, C=1.0,
                         loss=self.loss, learning_rate=self.learning_rate,
                         coef_init=coef_init,
                         intercept_init=intercept_init,
                         sample_weight=sample_weight)
    def _decision_function(self, X):
                check_is_fitted(self, ["t_", "coef_", "intercept_"], all_or_any=all)
        X = check_array(X, accept_sparse='csr')
        scores = safe_sparse_dot(X, self.coef_.T,
                                 dense_output=True) + self.intercept_
        return scores.ravel()
    def predict(self, X):
                return self._decision_function(X)
    def _fit_regressor(self, X, y, alpha, C, loss, learning_rate,
                       sample_weight, n_iter):
        dataset, intercept_decay = make_dataset(X, y, sample_weight)
        loss_function = self._get_loss_function(loss)
        penalty_type = self._get_penalty_type(self.penalty)
        learning_rate_type = self._get_learning_rate_type(learning_rate)
        if not hasattr(self, "t_"):
            self.t_ = 1.0
        random_state = check_random_state(self.random_state)
                        seed = random_state.randint(0, np.iinfo(np.int32).max)
        if self.average > 0:
            self.standard_coef_, self.standard_intercept_, \
                self.average_coef_, self.average_intercept_ =\
                average_sgd(self.standard_coef_,
                            self.standard_intercept_[0],
                            self.average_coef_,
                            self.average_intercept_[0],
                            loss_function,
                            penalty_type,
                            alpha, C,
                            self.l1_ratio,
                            dataset,
                            n_iter,
                            int(self.fit_intercept),
                            int(self.verbose),
                            int(self.shuffle),
                            seed,
                            1.0, 1.0,
                            learning_rate_type,
                            self.eta0, self.power_t, self.t_,
                            intercept_decay, self.average)
            self.average_intercept_ = np.atleast_1d(self.average_intercept_)
            self.standard_intercept_ = np.atleast_1d(self.standard_intercept_)
            self.t_ += n_iter * X.shape[0]
            if self.average <= self.t_ - 1.0:
                self.coef_ = self.average_coef_
                self.intercept_ = self.average_intercept_
            else:
                self.coef_ = self.standard_coef_
                self.intercept_ = self.standard_intercept_
        else:
            self.coef_, self.intercept_ = \
                plain_sgd(self.coef_,
                          self.intercept_[0],
                          loss_function,
                          penalty_type,
                          alpha, C,
                          self.l1_ratio,
                          dataset,
                          n_iter,
                          int(self.fit_intercept),
                          int(self.verbose),
                          int(self.shuffle),
                          seed,
                          1.0, 1.0,
                          learning_rate_type,
                          self.eta0, self.power_t, self.t_,
                          intercept_decay)
            self.t_ += n_iter * X.shape[0]
            self.intercept_ = np.atleast_1d(self.intercept_)

class SGDRegressor(BaseSGDRegressor):
        def __init__(self, loss="squared_loss", penalty="l2", alpha=0.0001,
                 l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True,
                 verbose=0, epsilon=DEFAULT_EPSILON, random_state=None,
                 learning_rate="invscaling", eta0=0.01, power_t=0.25,
                 warm_start=False, average=False):
        super(SGDRegressor, self).__init__(loss=loss, penalty=penalty,
                                           alpha=alpha, l1_ratio=l1_ratio,
                                           fit_intercept=fit_intercept,
                                           n_iter=n_iter, shuffle=shuffle,
                                           verbose=verbose,
                                           epsilon=epsilon,
                                           random_state=random_state,
                                           learning_rate=learning_rate,
                                           eta0=eta0, power_t=power_t,
                                           warm_start=warm_start,
                                           average=average)

from __future__ import division, print_function, absolute_import
import warnings
from itertools import combinations
import numpy as np
from scipy import linalg
from scipy.special import binom
from scipy.linalg.lapack import get_lapack_funcs
from .base import LinearModel
from ..base import RegressorMixin
from ..utils import check_random_state
from ..utils import check_X_y, _get_n_jobs
from ..utils.random import choice
from ..externals.joblib import Parallel, delayed
from ..externals.six.moves import xrange as range
from ..exceptions import ConvergenceWarning
_EPSILON = np.finfo(np.double).eps

def _modified_weiszfeld_step(X, x_old):
        diff = X - x_old
    diff_norm = np.sqrt(np.sum(diff ** 2, axis=1))
    mask = diff_norm >= _EPSILON
        is_x_old_in_X = int(mask.sum() < X.shape[0])
    diff = diff[mask]
    diff_norm = diff_norm[mask][:, np.newaxis]
    quotient_norm = linalg.norm(np.sum(diff / diff_norm, axis=0))
    if quotient_norm > _EPSILON:          new_direction = (np.sum(X[mask, :] / diff_norm, axis=0)
                         / np.sum(1 / diff_norm, axis=0))
    else:
        new_direction = 1.
        quotient_norm = 1.
    return (max(0., 1. - is_x_old_in_X / quotient_norm) * new_direction
            + min(1., is_x_old_in_X / quotient_norm) * x_old)

def _spatial_median(X, max_iter=300, tol=1.e-3):
        if X.shape[1] == 1:
        return 1, np.median(X.ravel())
    tol **= 2      spatial_median_old = np.mean(X, axis=0)
    for n_iter in range(max_iter):
        spatial_median = _modified_weiszfeld_step(X, spatial_median_old)
        if np.sum((spatial_median_old - spatial_median) ** 2) < tol:
            break
        else:
            spatial_median_old = spatial_median
    else:
        warnings.warn("Maximum number of iterations {max_iter} reached in "
                      "spatial median for TheilSen regressor."
                      "".format(max_iter=max_iter), ConvergenceWarning)
    return n_iter, spatial_median

def _breakdown_point(n_samples, n_subsamples):
        return 1 - (0.5 ** (1 / n_subsamples) * (n_samples - n_subsamples + 1) +
                n_subsamples - 1) / n_samples

def _lstsq(X, y, indices, fit_intercept):
        fit_intercept = int(fit_intercept)
    n_features = X.shape[1] + fit_intercept
    n_subsamples = indices.shape[1]
    weights = np.empty((indices.shape[0], n_features))
    X_subpopulation = np.ones((n_subsamples, n_features))
        y_subpopulation = np.zeros((max(n_subsamples, n_features)))
    lstsq, = get_lapack_funcs(('gelss',), (X_subpopulation, y_subpopulation))
    for index, subset in enumerate(indices):
        X_subpopulation[:, fit_intercept:] = X[subset, :]
        y_subpopulation[:n_subsamples] = y[subset]
        weights[index] = lstsq(X_subpopulation,
                               y_subpopulation)[1][:n_features]
    return weights

class TheilSenRegressor(LinearModel, RegressorMixin):
    
    def __init__(self, fit_intercept=True, copy_X=True,
                 max_subpopulation=1e4, n_subsamples=None, max_iter=300,
                 tol=1.e-3, random_state=None, n_jobs=1, verbose=False):
        self.fit_intercept = fit_intercept
        self.copy_X = copy_X
        self.max_subpopulation = int(max_subpopulation)
        self.n_subsamples = n_subsamples
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.verbose = verbose
    def _check_subparams(self, n_samples, n_features):
        n_subsamples = self.n_subsamples
        if self.fit_intercept:
            n_dim = n_features + 1
        else:
            n_dim = n_features
        if n_subsamples is not None:
            if n_subsamples > n_samples:
                raise ValueError("Invalid parameter since n_subsamples > "
                                 "n_samples ({0} > {1}).".format(n_subsamples,
                                                                 n_samples))
            if n_samples >= n_features:
                if n_dim > n_subsamples:
                    plus_1 = "+1" if self.fit_intercept else ""
                    raise ValueError("Invalid parameter since n_features{0} "
                                     "> n_subsamples ({1} > {2})."
                                     "".format(plus_1, n_dim, n_samples))
            else:                  if n_subsamples != n_samples:
                    raise ValueError("Invalid parameter since n_subsamples != "
                                     "n_samples ({0} != {1}) while n_samples "
                                     "< n_features.".format(n_subsamples,
                                                            n_samples))
        else:
            n_subsamples = min(n_dim, n_samples)
        if self.max_subpopulation <= 0:
            raise ValueError("Subpopulation must be strictly positive "
                             "({0} <= 0).".format(self.max_subpopulation))
        all_combinations = max(1, np.rint(binom(n_samples, n_subsamples)))
        n_subpopulation = int(min(self.max_subpopulation, all_combinations))
        return n_subsamples, n_subpopulation
    def fit(self, X, y):
                random_state = check_random_state(self.random_state)
        X, y = check_X_y(X, y, y_numeric=True)
        n_samples, n_features = X.shape
        n_subsamples, self.n_subpopulation_ = self._check_subparams(n_samples,
                                                                    n_features)
        self.breakdown_ = _breakdown_point(n_samples, n_subsamples)
        if self.verbose:
            print("Breakdown point: {0}".format(self.breakdown_))
            print("Number of samples: {0}".format(n_samples))
            tol_outliers = int(self.breakdown_ * n_samples)
            print("Tolerable outliers: {0}".format(tol_outliers))
            print("Number of subpopulations: {0}".format(
                self.n_subpopulation_))
                if np.rint(binom(n_samples, n_subsamples)) <= self.max_subpopulation:
            indices = list(combinations(range(n_samples), n_subsamples))
        else:
            indices = [choice(n_samples,
                              size=n_subsamples,
                              replace=False,
                              random_state=random_state)
                       for _ in range(self.n_subpopulation_)]
        n_jobs = _get_n_jobs(self.n_jobs)
        index_list = np.array_split(indices, n_jobs)
        weights = Parallel(n_jobs=n_jobs,
                           verbose=self.verbose)(
            delayed(_lstsq)(X, y, index_list[job], self.fit_intercept)
            for job in range(n_jobs))
        weights = np.vstack(weights)
        self.n_iter_, coefs = _spatial_median(weights,
                                              max_iter=self.max_iter,
                                              tol=self.tol)
        if self.fit_intercept:
            self.intercept_ = coefs[0]
            self.coef_ = coefs[1:]
        else:
            self.intercept_ = 0.
            self.coef_ = coefs
        return self

from .base import LinearRegression
from .bayes import BayesianRidge, ARDRegression
from .least_angle import (Lars, LassoLars, lars_path, LarsCV, LassoLarsCV,
                          LassoLarsIC)
from .coordinate_descent import (Lasso, ElasticNet, LassoCV, ElasticNetCV,
                                 lasso_path, enet_path, MultiTaskLasso,
                                 MultiTaskElasticNet, MultiTaskElasticNetCV,
                                 MultiTaskLassoCV)
from .huber import HuberRegressor
from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber
from .stochastic_gradient import SGDClassifier, SGDRegressor
from .ridge import (Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV,
                    ridge_regression)
from .logistic import (LogisticRegression, LogisticRegressionCV,
                       logistic_regression_path)
from .omp import (orthogonal_mp, orthogonal_mp_gram, OrthogonalMatchingPursuit,
                  OrthogonalMatchingPursuitCV)
from .passive_aggressive import PassiveAggressiveClassifier
from .passive_aggressive import PassiveAggressiveRegressor
from .perceptron import Perceptron
from .randomized_l1 import (RandomizedLasso, RandomizedLogisticRegression,
                            lasso_stability_path)
from .ransac import RANSACRegressor
from .theil_sen import TheilSenRegressor
__all__ = ['ARDRegression',
           'BayesianRidge',
           'ElasticNet',
           'ElasticNetCV',
           'Hinge',
           'HuberRegressor',
           'Lars',
           'LarsCV',
           'Lasso',
           'LassoCV',
           'LassoLars',
           'LassoLarsCV',
           'LassoLarsIC',
           'LinearRegression',
           'Log',
           'LogisticRegression',
           'LogisticRegressionCV',
           'ModifiedHuber',
           'MultiTaskElasticNet',
           'MultiTaskElasticNetCV',
           'MultiTaskLasso',
           'MultiTaskLassoCV',
           'OrthogonalMatchingPursuit',
           'OrthogonalMatchingPursuitCV',
           'PassiveAggressiveClassifier',
           'PassiveAggressiveRegressor',
           'Perceptron',
           'RandomizedLasso',
           'RandomizedLogisticRegression',
           'Ridge',
           'RidgeCV',
           'RidgeClassifier',
           'RidgeClassifierCV',
           'SGDClassifier',
           'SGDRegressor',
           'SquaredLoss',
           'TheilSenRegressor',
           'enet_path',
           'lars_path',
           'lasso_path',
           'lasso_stability_path',
           'logistic_regression_path',
           'orthogonal_mp',
           'orthogonal_mp_gram',
           'ridge_regression',
           'RANSACRegressor']

import numpy as np
from ..base import BaseEstimator, TransformerMixin
from ..neighbors import NearestNeighbors, kneighbors_graph
from ..utils import check_array
from ..utils.graph import graph_shortest_path
from ..decomposition import KernelPCA
from ..preprocessing import KernelCenterer

class Isomap(BaseEstimator, TransformerMixin):
    
    def __init__(self, n_neighbors=5, n_components=2, eigen_solver='auto',
                 tol=0, max_iter=None, path_method='auto',
                 neighbors_algorithm='auto', n_jobs=1):
        self.n_neighbors = n_neighbors
        self.n_components = n_components
        self.eigen_solver = eigen_solver
        self.tol = tol
        self.max_iter = max_iter
        self.path_method = path_method
        self.neighbors_algorithm = neighbors_algorithm
        self.n_jobs = n_jobs
    def _fit_transform(self, X):
        X = check_array(X)
        self.nbrs_ = NearestNeighbors(n_neighbors=self.n_neighbors,
                                      algorithm=self.neighbors_algorithm,
                                      n_jobs=self.n_jobs)
        self.nbrs_.fit(X)
        self.training_data_ = self.nbrs_._fit_X
        self.kernel_pca_ = KernelPCA(n_components=self.n_components,
                                     kernel="precomputed",
                                     eigen_solver=self.eigen_solver,
                                     tol=self.tol, max_iter=self.max_iter,
                                     n_jobs=self.n_jobs)
        kng = kneighbors_graph(self.nbrs_, self.n_neighbors,
                               mode='distance', n_jobs=self.n_jobs)
        self.dist_matrix_ = graph_shortest_path(kng,
                                                method=self.path_method,
                                                directed=False)
        G = self.dist_matrix_ ** 2
        G *= -0.5
        self.embedding_ = self.kernel_pca_.fit_transform(G)
    def reconstruction_error(self):
                G = -0.5 * self.dist_matrix_ ** 2
        G_center = KernelCenterer().fit_transform(G)
        evals = self.kernel_pca_.lambdas_
        return np.sqrt(np.sum(G_center ** 2) - np.sum(evals ** 2)) / G.shape[0]
    def fit(self, X, y=None):
                self._fit_transform(X)
        return self
    def fit_transform(self, X, y=None):
                self._fit_transform(X)
        return self.embedding_
    def transform(self, X):
                X = check_array(X)
        distances, indices = self.nbrs_.kneighbors(X, return_distance=True)
                                        G_X = np.zeros((X.shape[0], self.training_data_.shape[0]))
        for i in range(X.shape[0]):
            G_X[i] = np.min(self.dist_matrix_[indices[i]] +
                            distances[i][:, None], 0)
        G_X **= 2
        G_X *= -0.5
        return self.kernel_pca_.transform(G_X)

import numpy as np
from scipy.linalg import eigh, svd, qr, solve
from scipy.sparse import eye, csr_matrix
from ..base import BaseEstimator, TransformerMixin
from ..utils import check_random_state, check_array
from ..utils.arpack import eigsh
from ..utils.extmath import stable_cumsum
from ..utils.validation import check_is_fitted
from ..utils.validation import FLOAT_DTYPES
from ..neighbors import NearestNeighbors

def barycenter_weights(X, Z, reg=1e-3):
        X = check_array(X, dtype=FLOAT_DTYPES)
    Z = check_array(Z, dtype=FLOAT_DTYPES, allow_nd=True)
    n_samples, n_neighbors = X.shape[0], Z.shape[1]
    B = np.empty((n_samples, n_neighbors), dtype=X.dtype)
    v = np.ones(n_neighbors, dtype=X.dtype)
            for i, A in enumerate(Z.transpose(0, 2, 1)):
        C = A.T - X[i]          G = np.dot(C, C.T)
        trace = np.trace(G)
        if trace > 0:
            R = reg * trace
        else:
            R = reg
        G.flat[::Z.shape[1] + 1] += R
        w = solve(G, v, sym_pos=True)
        B[i, :] = w / np.sum(w)
    return B

def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=1):
        knn = NearestNeighbors(n_neighbors + 1, n_jobs=n_jobs).fit(X)
    X = knn._fit_X
    n_samples = X.shape[0]
    ind = knn.kneighbors(X, return_distance=False)[:, 1:]
    data = barycenter_weights(X, X[ind], reg=reg)
    indptr = np.arange(0, n_samples * n_neighbors + 1, n_neighbors)
    return csr_matrix((data.ravel(), ind.ravel(), indptr),
                      shape=(n_samples, n_samples))

def null_space(M, k, k_skip=1, eigen_solver='arpack', tol=1E-6, max_iter=100,
               random_state=None):
        if eigen_solver == 'auto':
        if M.shape[0] > 200 and k + k_skip < 10:
            eigen_solver = 'arpack'
        else:
            eigen_solver = 'dense'
    if eigen_solver == 'arpack':
        random_state = check_random_state(random_state)
                v0 = random_state.uniform(-1, 1, M.shape[0])
        try:
            eigen_values, eigen_vectors = eigsh(M, k + k_skip, sigma=0.0,
                                                tol=tol, maxiter=max_iter,
                                                v0=v0)
        except RuntimeError as msg:
            raise ValueError("Error in determining null-space with ARPACK. "
                             "Error message: '%s'. "
                             "Note that method='arpack' can fail when the "
                             "weight matrix is singular or otherwise "
                             "ill-behaved.  method='dense' is recommended. "
                             "See online documentation for more information."
                             % msg)
        return eigen_vectors[:, k_skip:], np.sum(eigen_values[k_skip:])
    elif eigen_solver == 'dense':
        if hasattr(M, 'toarray'):
            M = M.toarray()
        eigen_values, eigen_vectors = eigh(
            M, eigvals=(k_skip, k + k_skip - 1), overwrite_a=True)
        index = np.argsort(np.abs(eigen_values))
        return eigen_vectors[:, index], np.sum(eigen_values)
    else:
        raise ValueError("Unrecognized eigen_solver '%s'" % eigen_solver)

def locally_linear_embedding(
        X, n_neighbors, n_components, reg=1e-3, eigen_solver='auto', tol=1e-6,
        max_iter=100, method='standard', hessian_tol=1E-4, modified_tol=1E-12,
        random_state=None, n_jobs=1):
        if eigen_solver not in ('auto', 'arpack', 'dense'):
        raise ValueError("unrecognized eigen_solver '%s'" % eigen_solver)
    if method not in ('standard', 'hessian', 'modified', 'ltsa'):
        raise ValueError("unrecognized method '%s'" % method)
    nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs)
    nbrs.fit(X)
    X = nbrs._fit_X
    N, d_in = X.shape
    if n_components > d_in:
        raise ValueError("output dimension must be less than or equal "
                         "to input dimension")
    if n_neighbors >= N:
        raise ValueError("n_neighbors must be less than number of points")
    if n_neighbors <= 0:
        raise ValueError("n_neighbors must be positive")
    M_sparse = (eigen_solver != 'dense')
    if method == 'standard':
        W = barycenter_kneighbors_graph(
            nbrs, n_neighbors=n_neighbors, reg=reg, n_jobs=n_jobs)
                        if M_sparse:
            M = eye(*W.shape, format=W.format) - W
            M = (M.T * M).tocsr()
        else:
            M = (W.T * W - W.T - W).toarray()
            M.flat[::M.shape[0] + 1] += 1  
    elif method == 'hessian':
        dp = n_components * (n_components + 1) // 2
        if n_neighbors <= n_components + dp:
            raise ValueError("for method='hessian', n_neighbors must be "
                             "greater than "
                             "[n_components * (n_components + 3) / 2]")
        neighbors = nbrs.kneighbors(X, n_neighbors=n_neighbors + 1,
                                    return_distance=False)
        neighbors = neighbors[:, 1:]
        Yi = np.empty((n_neighbors, 1 + n_components + dp), dtype=np.float64)
        Yi[:, 0] = 1
        M = np.zeros((N, N), dtype=np.float64)
        use_svd = (n_neighbors > d_in)
        for i in range(N):
            Gi = X[neighbors[i]]
            Gi -= Gi.mean(0)
                        if use_svd:
                U = svd(Gi, full_matrices=0)[0]
            else:
                Ci = np.dot(Gi, Gi.T)
                U = eigh(Ci)[1][:, ::-1]
            Yi[:, 1:1 + n_components] = U[:, :n_components]
            j = 1 + n_components
            for k in range(n_components):
                Yi[:, j:j + n_components - k] = (U[:, k:k + 1] *
                                                 U[:, k:n_components])
                j += n_components - k
            Q, R = qr(Yi)
            w = Q[:, n_components + 1:]
            S = w.sum(0)
            S[np.where(abs(S) < hessian_tol)] = 1
            w /= S
            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])
            M[nbrs_x, nbrs_y] += np.dot(w, w.T)
        if M_sparse:
            M = csr_matrix(M)
    elif method == 'modified':
        if n_neighbors < n_components:
            raise ValueError("modified LLE requires "
                             "n_neighbors >= n_components")
        neighbors = nbrs.kneighbors(X, n_neighbors=n_neighbors + 1,
                                    return_distance=False)
        neighbors = neighbors[:, 1:]
                                V = np.zeros((N, n_neighbors, n_neighbors))
        nev = min(d_in, n_neighbors)
        evals = np.zeros([N, nev])
                use_svd = (n_neighbors > d_in)
        if use_svd:
            for i in range(N):
                X_nbrs = X[neighbors[i]] - X[i]
                V[i], evals[i], _ = svd(X_nbrs,
                                        full_matrices=True)
            evals **= 2
        else:
            for i in range(N):
                X_nbrs = X[neighbors[i]] - X[i]
                C_nbrs = np.dot(X_nbrs, X_nbrs.T)
                evi, vi = eigh(C_nbrs)
                evals[i] = evi[::-1]
                V[i] = vi[:, ::-1]
                                reg = 1E-3 * evals.sum(1)
        tmp = np.dot(V.transpose(0, 2, 1), np.ones(n_neighbors))
        tmp[:, :nev] /= evals + reg[:, None]
        tmp[:, nev:] /= reg[:, None]
        w_reg = np.zeros((N, n_neighbors))
        for i in range(N):
            w_reg[i] = np.dot(V[i], tmp[i])
        w_reg /= w_reg.sum(1)[:, None]
                        rho = evals[:, n_components:].sum(1) / evals[:, :n_components].sum(1)
        eta = np.median(rho)
                                s_range = np.zeros(N, dtype=int)
        evals_cumsum = stable_cumsum(evals, 1)
        eta_range = evals_cumsum[:, -1:] / evals_cumsum[:, :-1] - 1
        for i in range(N):
            s_range[i] = np.searchsorted(eta_range[i, ::-1], eta)
        s_range += n_neighbors - nev  
                        M = np.zeros((N, N), dtype=np.float64)
        for i in range(N):
            s_i = s_range[i]
                        Vi = V[i, :, n_neighbors - s_i:]
            alpha_i = np.linalg.norm(Vi.sum(0)) / np.sqrt(s_i)
                                                h = alpha_i * np.ones(s_i) - np.dot(Vi.T, np.ones(n_neighbors))
            norm_h = np.linalg.norm(h)
            if norm_h < modified_tol:
                h *= 0
            else:
                h /= norm_h
                                                                        Wi = (Vi - 2 * np.outer(np.dot(Vi, h), h) +
                  (1 - alpha_i) * w_reg[i, :, None])
                                                                                    nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])
            M[nbrs_x, nbrs_y] += np.dot(Wi, Wi.T)
            Wi_sum1 = Wi.sum(1)
            M[i, neighbors[i]] -= Wi_sum1
            M[neighbors[i], i] -= Wi_sum1
            M[i, i] += s_i
        if M_sparse:
            M = csr_matrix(M)
    elif method == 'ltsa':
        neighbors = nbrs.kneighbors(X, n_neighbors=n_neighbors + 1,
                                    return_distance=False)
        neighbors = neighbors[:, 1:]
        M = np.zeros((N, N))
        use_svd = (n_neighbors > d_in)
        for i in range(N):
            Xi = X[neighbors[i]]
            Xi -= Xi.mean(0)
                        if use_svd:
                v = svd(Xi, full_matrices=True)[0]
            else:
                Ci = np.dot(Xi, Xi.T)
                v = eigh(Ci)[1][:, ::-1]
            Gi = np.zeros((n_neighbors, n_components + 1))
            Gi[:, 1:] = v[:, :n_components]
            Gi[:, 0] = 1. / np.sqrt(n_neighbors)
            GiGiT = np.dot(Gi, Gi.T)
            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])
            M[nbrs_x, nbrs_y] -= GiGiT
            M[neighbors[i], neighbors[i]] += 1
    return null_space(M, n_components, k_skip=1, eigen_solver=eigen_solver,
                      tol=tol, max_iter=max_iter, random_state=random_state)

class LocallyLinearEmbedding(BaseEstimator, TransformerMixin):
    
    def __init__(self, n_neighbors=5, n_components=2, reg=1E-3,
                 eigen_solver='auto', tol=1E-6, max_iter=100,
                 method='standard', hessian_tol=1E-4, modified_tol=1E-12,
                 neighbors_algorithm='auto', random_state=None, n_jobs=1):
        self.n_neighbors = n_neighbors
        self.n_components = n_components
        self.reg = reg
        self.eigen_solver = eigen_solver
        self.tol = tol
        self.max_iter = max_iter
        self.method = method
        self.hessian_tol = hessian_tol
        self.modified_tol = modified_tol
        self.random_state = random_state
        self.neighbors_algorithm = neighbors_algorithm
        self.n_jobs = n_jobs
    def _fit_transform(self, X):
        self.nbrs_ = NearestNeighbors(self.n_neighbors,
                                      algorithm=self.neighbors_algorithm,
                                      n_jobs=self.n_jobs)
        random_state = check_random_state(self.random_state)
        X = check_array(X, dtype=float)
        self.nbrs_.fit(X)
        self.embedding_, self.reconstruction_error_ = \
            locally_linear_embedding(
                self.nbrs_, self.n_neighbors, self.n_components,
                eigen_solver=self.eigen_solver, tol=self.tol,
                max_iter=self.max_iter, method=self.method,
                hessian_tol=self.hessian_tol, modified_tol=self.modified_tol,
                random_state=random_state, reg=self.reg, n_jobs=self.n_jobs)
    def fit(self, X, y=None):
                self._fit_transform(X)
        return self
    def fit_transform(self, X, y=None):
                self._fit_transform(X)
        return self.embedding_
    def transform(self, X):
                check_is_fitted(self, "nbrs_")
        X = check_array(X)
        ind = self.nbrs_.kneighbors(X, n_neighbors=self.n_neighbors,
                                    return_distance=False)
        weights = barycenter_weights(X, self.nbrs_._fit_X[ind],
                                     reg=self.reg)
        X_new = np.empty((X.shape[0], self.n_components))
        for i in range(X.shape[0]):
            X_new[i] = np.dot(self.embedding_[ind[i]].T, weights[i])
        return X_new

import numpy as np
import warnings
from ..base import BaseEstimator
from ..metrics import euclidean_distances
from ..utils import check_random_state, check_array, check_symmetric
from ..externals.joblib import Parallel
from ..externals.joblib import delayed
from ..isotonic import IsotonicRegression

def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
                   max_iter=300, verbose=0, eps=1e-3, random_state=None):
        dissimilarities = check_symmetric(dissimilarities, raise_exception=True)
    n_samples = dissimilarities.shape[0]
    random_state = check_random_state(random_state)
    sim_flat = ((1 - np.tri(n_samples)) * dissimilarities).ravel()
    sim_flat_w = sim_flat[sim_flat != 0]
    if init is None:
                X = random_state.rand(n_samples * n_components)
        X = X.reshape((n_samples, n_components))
    else:
                n_components = init.shape[1]
        if n_samples != init.shape[0]:
            raise ValueError("init matrix should be of shape (%d, %d)" %
                             (n_samples, n_components))
        X = init
    old_stress = None
    ir = IsotonicRegression()
    for it in range(max_iter):
                dis = euclidean_distances(X)
        if metric:
            disparities = dissimilarities
        else:
            dis_flat = dis.ravel()
                        dis_flat_w = dis_flat[sim_flat != 0]
                        disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)
            disparities = dis_flat.copy()
            disparities[sim_flat != 0] = disparities_flat
            disparities = disparities.reshape((n_samples, n_samples))
            disparities *= np.sqrt((n_samples * (n_samples - 1) / 2) /
                                   (disparities ** 2).sum())
                stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2
                dis[dis == 0] = 1e-5
        ratio = disparities / dis
        B = - ratio
        B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)
        X = 1. / n_samples * np.dot(B, X)
        dis = np.sqrt((X ** 2).sum(axis=1)).sum()
        if verbose >= 2:
            print('it: %d, stress %s' % (it, stress))
        if old_stress is not None:
            if(old_stress - stress / dis) < eps:
                if verbose:
                    print('breaking at iteration %d with stress %s' % (it,
                                                                       stress))
                break
        old_stress = stress / dis
    return X, stress, it + 1

def smacof(dissimilarities, metric=True, n_components=2, init=None, n_init=8,
           n_jobs=1, max_iter=300, verbose=0, eps=1e-3, random_state=None,
           return_n_iter=False):
    
    dissimilarities = check_array(dissimilarities)
    random_state = check_random_state(random_state)
    if hasattr(init, '__array__'):
        init = np.asarray(init).copy()
        if not n_init == 1:
            warnings.warn(
                'Explicit initial positions passed: '
                'performing only one init of the MDS instead of %d'
                % n_init)
            n_init = 1
    best_pos, best_stress = None, None
    if n_jobs == 1:
        for it in range(n_init):
            pos, stress, n_iter_ = _smacof_single(
                dissimilarities, metric=metric,
                n_components=n_components, init=init,
                max_iter=max_iter, verbose=verbose,
                eps=eps, random_state=random_state)
            if best_stress is None or stress < best_stress:
                best_stress = stress
                best_pos = pos.copy()
                best_iter = n_iter_
    else:
        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
        results = Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))(
            delayed(_smacof_single)(
                dissimilarities, metric=metric, n_components=n_components,
                init=init, max_iter=max_iter, verbose=verbose, eps=eps,
                random_state=seed)
            for seed in seeds)
        positions, stress, n_iters = zip(*results)
        best = np.argmin(stress)
        best_stress = stress[best]
        best_pos = positions[best]
        best_iter = n_iters[best]
    if return_n_iter:
        return best_pos, best_stress, best_iter
    else:
        return best_pos, best_stress

class MDS(BaseEstimator):
        def __init__(self, n_components=2, metric=True, n_init=4,
                 max_iter=300, verbose=0, eps=1e-3, n_jobs=1,
                 random_state=None, dissimilarity="euclidean"):
        self.n_components = n_components
        self.dissimilarity = dissimilarity
        self.metric = metric
        self.n_init = n_init
        self.max_iter = max_iter
        self.eps = eps
        self.verbose = verbose
        self.n_jobs = n_jobs
        self.random_state = random_state
    @property
    def _pairwise(self):
        return self.kernel == "precomputed"
    def fit(self, X, y=None, init=None):
                self.fit_transform(X, init=init)
        return self
    def fit_transform(self, X, y=None, init=None):
                X = check_array(X)
        if X.shape[0] == X.shape[1] and self.dissimilarity != "precomputed":
            warnings.warn("The MDS API has changed. ``fit`` now constructs an"
                          " dissimilarity matrix from data. To use a custom "
                          "dissimilarity matrix, set "
                          "``dissimilarity='precomputed'``.")
        if self.dissimilarity == "precomputed":
            self.dissimilarity_matrix_ = X
        elif self.dissimilarity == "euclidean":
            self.dissimilarity_matrix_ = euclidean_distances(X)
        else:
            raise ValueError("Proximity must be 'precomputed' or 'euclidean'."
                             " Got %s instead" % str(self.dissimilarity))
        self.embedding_, self.stress_, self.n_iter_ = smacof(
            self.dissimilarity_matrix_, metric=self.metric,
            n_components=self.n_components, init=init, n_init=self.n_init,
            n_jobs=self.n_jobs, max_iter=self.max_iter, verbose=self.verbose,
            eps=self.eps, random_state=self.random_state,
            return_n_iter=True)
        return self.embedding_
import os
from os.path import join
import numpy
from numpy.distutils.misc_util import Configuration
from sklearn._build_utils import get_blas_info

def configuration(parent_package="", top_path=None):
    config = Configuration("manifold", parent_package, top_path)
    libraries = []
    if os.name == 'posix':
        libraries.append('m')
    config.add_extension("_utils",
                         sources=["_utils.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries,
                         extra_compile_args=["-O3"])
    cblas_libs, blas_info = get_blas_info()
    eca = blas_info.pop('extra_compile_args', [])
    eca.append("-O4")
    config.add_extension("_barnes_hut_tsne",
                         libraries=cblas_libs,
                         sources=["_barnes_hut_tsne.pyx"],
                         include_dirs=[join('..', 'src', 'cblas'),
                                       numpy.get_include(),
                                       blas_info.pop('include_dirs', [])],
                         extra_compile_args=eca, **blas_info)
    config.add_subpackage('tests')
    return config
if __name__ == "__main__":
    from numpy.distutils.core import setup
    setup(**configuration().todict())

import warnings
import numpy as np
from scipy import sparse
from scipy.linalg import eigh
from scipy.sparse.linalg import lobpcg
from ..base import BaseEstimator
from ..externals import six
from ..utils import check_random_state, check_array, check_symmetric
from ..utils.extmath import _deterministic_vector_sign_flip
from ..utils.graph import graph_laplacian
from ..utils.sparsetools import connected_components
from ..utils.arpack import eigsh
from ..metrics.pairwise import rbf_kernel
from ..neighbors import kneighbors_graph

def _graph_connected_component(graph, node_id):
        n_node = graph.shape[0]
    if sparse.issparse(graph):
                graph = graph.tocsr()
    connected_nodes = np.zeros(n_node, dtype=np.bool)
    nodes_to_explore = np.zeros(n_node, dtype=np.bool)
    nodes_to_explore[node_id] = True
    for _ in range(n_node):
        last_num_component = connected_nodes.sum()
        np.logical_or(connected_nodes, nodes_to_explore, out=connected_nodes)
        if last_num_component >= connected_nodes.sum():
            break
        indices = np.where(nodes_to_explore)[0]
        nodes_to_explore.fill(False)
        for i in indices:
            if sparse.issparse(graph):
                neighbors = graph[i].toarray().ravel()
            else:
                neighbors = graph[i]
            np.logical_or(nodes_to_explore, neighbors, out=nodes_to_explore)
    return connected_nodes

def _graph_is_connected(graph):
        if sparse.isspmatrix(graph):
                n_connected_components, _ = connected_components(graph)
        return n_connected_components == 1
    else:
                return _graph_connected_component(graph, 0).sum() == graph.shape[0]

def _set_diag(laplacian, value, norm_laplacian):
        n_nodes = laplacian.shape[0]
        if not sparse.isspmatrix(laplacian):
        if norm_laplacian:
            laplacian.flat[::n_nodes + 1] = value
    else:
        laplacian = laplacian.tocoo()
        if norm_laplacian:
            diag_idx = (laplacian.row == laplacian.col)
            laplacian.data[diag_idx] = value
                                n_diags = np.unique(laplacian.row - laplacian.col).size
        if n_diags <= 7:
                        laplacian = laplacian.todia()
        else:
                                    laplacian = laplacian.tocsr()
    return laplacian

def spectral_embedding(adjacency, n_components=8, eigen_solver=None,
                       random_state=None, eigen_tol=0.0,
                       norm_laplacian=True, drop_first=True):
        adjacency = check_symmetric(adjacency)
    try:
        from pyamg import smoothed_aggregation_solver
    except ImportError:
        if eigen_solver == "amg":
            raise ValueError("The eigen_solver was set to 'amg', but pyamg is "
                             "not available.")
    if eigen_solver is None:
        eigen_solver = 'arpack'
    elif eigen_solver not in ('arpack', 'lobpcg', 'amg'):
        raise ValueError("Unknown value for eigen_solver: '%s'."
                         "Should be 'amg', 'arpack', or 'lobpcg'"
                         % eigen_solver)
    random_state = check_random_state(random_state)
    n_nodes = adjacency.shape[0]
        if drop_first:
        n_components = n_components + 1
    if not _graph_is_connected(adjacency):
        warnings.warn("Graph is not fully connected, spectral embedding"
                      " may not work as expected.")
    laplacian, dd = graph_laplacian(adjacency,
                                    normed=norm_laplacian, return_diag=True)
    if (eigen_solver == 'arpack' or eigen_solver != 'lobpcg' and
       (not sparse.isspmatrix(laplacian) or n_nodes < 5 * n_components)):
                                                        laplacian = _set_diag(laplacian, 1, norm_laplacian)
                                                                                                                        try:
                                    laplacian *= -1
            v0 = random_state.uniform(-1, 1, laplacian.shape[0])
            lambdas, diffusion_map = eigsh(laplacian, k=n_components,
                                           sigma=1.0, which='LM',
                                           tol=eigen_tol, v0=v0)
            embedding = diffusion_map.T[n_components::-1] * dd
        except RuntimeError:
                                    eigen_solver = "lobpcg"
                        laplacian *= -1
    if eigen_solver == 'amg':
                        if not sparse.issparse(laplacian):
            warnings.warn("AMG works better for sparse matrices")
                laplacian = check_array(laplacian, dtype=np.float64,
                                accept_sparse=True)
        laplacian = _set_diag(laplacian, 1, norm_laplacian)
        ml = smoothed_aggregation_solver(check_array(laplacian, 'csr'))
        M = ml.aspreconditioner()
        X = random_state.rand(laplacian.shape[0], n_components + 1)
        X[:, 0] = dd.ravel()
        lambdas, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,
                                        largest=False)
        embedding = diffusion_map.T * dd
        if embedding.shape[0] == 1:
            raise ValueError
    elif eigen_solver == "lobpcg":
                laplacian = check_array(laplacian, dtype=np.float64,
                                accept_sparse=True)
        if n_nodes < 5 * n_components + 1:
                                                if sparse.isspmatrix(laplacian):
                laplacian = laplacian.toarray()
            lambdas, diffusion_map = eigh(laplacian)
            embedding = diffusion_map.T[:n_components] * dd
        else:
            laplacian = _set_diag(laplacian, 1, norm_laplacian)
                                    X = random_state.rand(laplacian.shape[0], n_components + 1)
            X[:, 0] = dd.ravel()
            lambdas, diffusion_map = lobpcg(laplacian, X, tol=1e-15,
                                            largest=False, maxiter=2000)
            embedding = diffusion_map.T[:n_components] * dd
            if embedding.shape[0] == 1:
                raise ValueError
    embedding = _deterministic_vector_sign_flip(embedding)
    if drop_first:
        return embedding[1:n_components].T
    else:
        return embedding[:n_components].T

class SpectralEmbedding(BaseEstimator):
    
    def __init__(self, n_components=2, affinity="nearest_neighbors",
                 gamma=None, random_state=None, eigen_solver=None,
                 n_neighbors=None, n_jobs=1):
        self.n_components = n_components
        self.affinity = affinity
        self.gamma = gamma
        self.random_state = random_state
        self.eigen_solver = eigen_solver
        self.n_neighbors = n_neighbors
        self.n_jobs = n_jobs
    @property
    def _pairwise(self):
        return self.affinity == "precomputed"
    def _get_affinity_matrix(self, X, Y=None):
                if self.affinity == 'precomputed':
            self.affinity_matrix_ = X
            return self.affinity_matrix_
        if self.affinity == 'nearest_neighbors':
            if sparse.issparse(X):
                warnings.warn("Nearest neighbors affinity currently does "
                              "not support sparse input, falling back to "
                              "rbf affinity")
                self.affinity = "rbf"
            else:
                self.n_neighbors_ = (self.n_neighbors
                                     if self.n_neighbors is not None
                                     else max(int(X.shape[0] / 10), 1))
                self.affinity_matrix_ = kneighbors_graph(X, self.n_neighbors_,
                                                         include_self=True,
                                                         n_jobs=self.n_jobs)
                                self.affinity_matrix_ = 0.5 * (self.affinity_matrix_ +
                                               self.affinity_matrix_.T)
                return self.affinity_matrix_
        if self.affinity == 'rbf':
            self.gamma_ = (self.gamma
                           if self.gamma is not None else 1.0 / X.shape[1])
            self.affinity_matrix_ = rbf_kernel(X, gamma=self.gamma_)
            return self.affinity_matrix_
        self.affinity_matrix_ = self.affinity(X)
        return self.affinity_matrix_
    def fit(self, X, y=None):
        
        X = check_array(X, ensure_min_samples=2, estimator=self)
        random_state = check_random_state(self.random_state)
        if isinstance(self.affinity, six.string_types):
            if self.affinity not in set(("nearest_neighbors", "rbf",
                                         "precomputed")):
                raise ValueError(("%s is not a valid affinity. Expected "
                                  "'precomputed', 'rbf', 'nearest_neighbors' "
                                  "or a callable.") % self.affinity)
        elif not callable(self.affinity):
            raise ValueError(("'affinity' is expected to be an affinity "
                              "name or a callable. Got: %s") % self.affinity)
        affinity_matrix = self._get_affinity_matrix(X)
        self.embedding_ = spectral_embedding(affinity_matrix,
                                             n_components=self.n_components,
                                             eigen_solver=self.eigen_solver,
                                             random_state=random_state)
        return self
    def fit_transform(self, X, y=None):
                self.fit(X)
        return self.embedding_

import numpy as np
from scipy import linalg
import scipy.sparse as sp
from scipy.spatial.distance import pdist
from scipy.spatial.distance import squareform
from ..neighbors import BallTree
from ..base import BaseEstimator
from ..utils import check_array
from ..utils import check_random_state
from ..utils.extmath import _ravel
from ..decomposition import PCA
from ..metrics.pairwise import pairwise_distances
from . import _utils
from . import _barnes_hut_tsne
from ..utils.fixes import astype
from ..externals.six import string_types
from ..utils import deprecated

MACHINE_EPSILON = np.finfo(np.double).eps

def _joint_probabilities(distances, desired_perplexity, verbose):
                distances = astype(distances, np.float32, copy=False)
    conditional_P = _utils._binary_search_perplexity(
        distances, None, desired_perplexity, verbose)
    P = conditional_P + conditional_P.T
    sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)
    P = np.maximum(squareform(P) / sum_P, MACHINE_EPSILON)
    return P

def _joint_probabilities_nn(distances, neighbors, desired_perplexity, verbose):
                distances = astype(distances, np.float32, copy=False)
    neighbors = astype(neighbors, np.int64, copy=False)
    conditional_P = _utils._binary_search_perplexity(
        distances, neighbors, desired_perplexity, verbose)
    m = "All probabilities should be finite"
    assert np.all(np.isfinite(conditional_P)), m
    P = conditional_P + conditional_P.T
    sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)
    P = np.maximum(squareform(P) / sum_P, MACHINE_EPSILON)
    assert np.all(np.abs(P) <= 1.0)
    return P

def _kl_divergence(params, P, degrees_of_freedom, n_samples, n_components,
                   skip_num_points=0):
        X_embedded = params.reshape(n_samples, n_components)
        n = pdist(X_embedded, "sqeuclidean")
    n += 1.
    n /= degrees_of_freedom
    n **= (degrees_of_freedom + 1.0) / -2.0
    Q = np.maximum(n / (2.0 * np.sum(n)), MACHINE_EPSILON)
        
        kl_divergence = 2.0 * np.dot(P, np.log(P / Q))
        grad = np.ndarray((n_samples, n_components))
    PQd = squareform((P - Q) * n)
    for i in range(skip_num_points, n_samples):
        np.dot(_ravel(PQd[i]), X_embedded[i] - X_embedded, out=grad[i])
    grad = grad.ravel()
    c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom
    grad *= c
    return kl_divergence, grad

def _kl_divergence_error(params, P, neighbors, degrees_of_freedom, n_samples,
                         n_components):
        X_embedded = params.reshape(n_samples, n_components)
        n = pdist(X_embedded, "sqeuclidean")
    n += 1.
    n /= degrees_of_freedom
    n **= (degrees_of_freedom + 1.0) / -2.0
    Q = np.maximum(n / (2.0 * np.sum(n)), MACHINE_EPSILON)
        
        if len(P.shape) == 2:
        P = squareform(P)
    kl_divergence = 2.0 * np.dot(P, np.log(P / Q))
    return kl_divergence

def _kl_divergence_bh(params, P, neighbors, degrees_of_freedom, n_samples,
                      n_components, angle=0.5, skip_num_points=0,
                      verbose=False):
        params = astype(params, np.float32, copy=False)
    X_embedded = params.reshape(n_samples, n_components)
    neighbors = astype(neighbors, np.int64, copy=False)
    if len(P.shape) == 1:
        sP = squareform(P).astype(np.float32)
    else:
        sP = P.astype(np.float32)
    grad = np.zeros(X_embedded.shape, dtype=np.float32)
    error = _barnes_hut_tsne.gradient(sP, X_embedded, neighbors,
                                      grad, angle, n_components, verbose,
                                      dof=degrees_of_freedom)
    c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom
    grad = grad.ravel()
    grad *= c
    return error, grad

def _gradient_descent(objective, p0, it, n_iter, objective_error=None,
                      n_iter_check=1, n_iter_without_progress=50,
                      momentum=0.5, learning_rate=1000.0, min_gain=0.01,
                      min_grad_norm=1e-7, min_error_diff=1e-7, verbose=0,
                      args=None, kwargs=None):
        if args is None:
        args = []
    if kwargs is None:
        kwargs = {}
    p = p0.copy().ravel()
    update = np.zeros_like(p)
    gains = np.ones_like(p)
    error = np.finfo(np.float).max
    best_error = np.finfo(np.float).max
    best_iter = 0
    for i in range(it, n_iter):
        new_error, grad = objective(p, *args, **kwargs)
        grad_norm = linalg.norm(grad)
        inc = update * grad >= 0.0
        dec = np.invert(inc)
        gains[inc] += 0.05
        gains[dec] *= 0.95
        np.clip(gains, min_gain, np.inf)
        grad *= gains
        update = momentum * update - learning_rate * grad
        p += update
        if (i + 1) % n_iter_check == 0:
            if new_error is None:
                new_error = objective_error(p, *args)
            error_diff = np.abs(new_error - error)
            error = new_error
            if verbose >= 2:
                m = "[t-SNE] Iteration %d: error = %.7f, gradient norm = %.7f"
                print(m % (i + 1, error, grad_norm))
            if error < best_error:
                best_error = error
                best_iter = i
            elif i - best_iter > n_iter_without_progress:
                if verbose >= 2:
                    print("[t-SNE] Iteration %d: did not make any progress "
                          "during the last %d episodes. Finished."
                          % (i + 1, n_iter_without_progress))
                break
            if grad_norm <= min_grad_norm:
                if verbose >= 2:
                    print("[t-SNE] Iteration %d: gradient norm %f. Finished."
                          % (i + 1, grad_norm))
                break
            if error_diff <= min_error_diff:
                if verbose >= 2:
                    m = "[t-SNE] Iteration %d: error difference %f. Finished."
                    print(m % (i + 1, error_diff))
                break
        if new_error is not None:
            error = new_error
    return p, error, i

def trustworthiness(X, X_embedded, n_neighbors=5, precomputed=False):
        if precomputed:
        dist_X = X
    else:
        dist_X = pairwise_distances(X, squared=True)
    dist_X_embedded = pairwise_distances(X_embedded, squared=True)
    ind_X = np.argsort(dist_X, axis=1)
    ind_X_embedded = np.argsort(dist_X_embedded, axis=1)[:, 1:n_neighbors + 1]
    n_samples = X.shape[0]
    t = 0.0
    ranks = np.zeros(n_neighbors)
    for i in range(n_samples):
        for j in range(n_neighbors):
            ranks[j] = np.where(ind_X[i] == ind_X_embedded[i, j])[0][0]
        ranks -= n_neighbors
        t += np.sum(ranks[ranks > 0])
    t = 1.0 - t * (2.0 / (n_samples * n_neighbors *
                          (2.0 * n_samples - 3.0 * n_neighbors - 1.0)))
    return t

class TSNE(BaseEstimator):
    
    def __init__(self, n_components=2, perplexity=30.0,
                 early_exaggeration=4.0, learning_rate=1000.0, n_iter=1000,
                 n_iter_without_progress=30, min_grad_norm=1e-7,
                 metric="euclidean", init="random", verbose=0,
                 random_state=None, method='barnes_hut', angle=0.5):
        if not ((isinstance(init, string_types) and
                init in ["pca", "random"]) or
                isinstance(init, np.ndarray)):
            msg = "'init' must be 'pca', 'random', or a numpy array"
            raise ValueError(msg)
        self.n_components = n_components
        self.perplexity = perplexity
        self.early_exaggeration = early_exaggeration
        self.learning_rate = learning_rate
        self.n_iter = n_iter
        self.n_iter_without_progress = n_iter_without_progress
        self.min_grad_norm = min_grad_norm
        self.metric = metric
        self.init = init
        self.verbose = verbose
        self.random_state = random_state
        self.method = method
        self.angle = angle
    def _fit(self, X, skip_num_points=0):
                if self.method not in ['barnes_hut', 'exact']:
            raise ValueError("'method' must be 'barnes_hut' or 'exact'")
        if self.angle < 0.0 or self.angle > 1.0:
            raise ValueError("'angle' must be between 0.0 - 1.0")
        if self.method == 'barnes_hut' and sp.issparse(X):
            raise TypeError('A sparse matrix was passed, but dense '
                            'data is required for method="barnes_hut". Use '
                            'X.toarray() to convert to a dense numpy array if '
                            'the array is small enough for it to fit in '
                            'memory. Otherwise consider dimensionality '
                            'reduction techniques (e.g. TruncatedSVD)')
        else:
            X = check_array(X, accept_sparse=['csr', 'csc', 'coo'],
                            dtype=np.float64)
        random_state = check_random_state(self.random_state)
        if self.early_exaggeration < 1.0:
            raise ValueError("early_exaggeration must be at least 1, but is "
                             "%f" % self.early_exaggeration)
        if self.n_iter < 200:
            raise ValueError("n_iter should be at least 200")
        if self.metric == "precomputed":
            if isinstance(self.init, string_types) and self.init == 'pca':
                raise ValueError("The parameter init=\"pca\" cannot be used "
                                 "with metric=\"precomputed\".")
            if X.shape[0] != X.shape[1]:
                raise ValueError("X should be a square distance matrix")
            distances = X
        else:
            if self.verbose:
                print("[t-SNE] Computing pairwise distances...")
            if self.metric == "euclidean":
                distances = pairwise_distances(X, metric=self.metric,
                                               squared=True)
            else:
                distances = pairwise_distances(X, metric=self.metric)
        if not np.all(distances >= 0):
            raise ValueError("All distances should be positive, either "
                             "the metric or precomputed distances given "
                             "as X are not correct")
                                        degrees_of_freedom = max(self.n_components - 1.0, 1)
        n_samples = X.shape[0]
                k = min(n_samples - 1, int(3. * self.perplexity + 1))
        neighbors_nn = None
        if self.method == 'barnes_hut':
            if self.verbose:
                print("[t-SNE] Computing %i nearest neighbors..." % k)
            if self.metric == 'precomputed':
                                                neighbors_nn = np.argsort(distances, axis=1)[:, :k]
            else:
                                bt = BallTree(X)
                                                                                distances_nn, neighbors_nn = bt.query(X, k=k + 1)
                neighbors_nn = neighbors_nn[:, 1:]
            P = _joint_probabilities_nn(distances, neighbors_nn,
                                        self.perplexity, self.verbose)
        else:
            P = _joint_probabilities(distances, self.perplexity, self.verbose)
        assert np.all(np.isfinite(P)), "All probabilities should be finite"
        assert np.all(P >= 0), "All probabilities should be zero or positive"
        assert np.all(P <= 1), ("All probabilities should be less "
                                "or then equal to one")
        if isinstance(self.init, np.ndarray):
            X_embedded = self.init
        elif self.init == 'pca':
            pca = PCA(n_components=self.n_components, svd_solver='randomized',
                      random_state=random_state)
            X_embedded = pca.fit_transform(X)
        elif self.init == 'random':
            X_embedded = None
        else:
            raise ValueError("Unsupported initialization scheme: %s"
                             % self.init)
        return self._tsne(P, degrees_of_freedom, n_samples, random_state,
                          X_embedded=X_embedded,
                          neighbors=neighbors_nn,
                          skip_num_points=skip_num_points)
    @property
    @deprecated("Attribute n_iter_final was deprecated in version 0.19 and "
                "will be removed in 0.21. Use 'n_iter_' instead")
    def n_iter_final(self):
        return self.n_iter_
    def _tsne(self, P, degrees_of_freedom, n_samples, random_state,
              X_embedded=None, neighbors=None, skip_num_points=0):
                                                                        
        if X_embedded is None:
                        X_embedded = 1e-4 * random_state.randn(n_samples,
                                                   self.n_components)
        params = X_embedded.ravel()
        opt_args = {"n_iter": 50, "momentum": 0.5, "it": 0,
                    "learning_rate": self.learning_rate,
                    "n_iter_without_progress": self.n_iter_without_progress,
                    "verbose": self.verbose, "n_iter_check": 25,
                    "kwargs": dict(skip_num_points=skip_num_points)}
        if self.method == 'barnes_hut':
            m = "Must provide an array of neighbors to use Barnes-Hut"
            assert neighbors is not None, m
            obj_func = _kl_divergence_bh
            objective_error = _kl_divergence_error
            sP = squareform(P).astype(np.float32)
            neighbors = neighbors.astype(np.int64)
            args = [sP, neighbors, degrees_of_freedom, n_samples,
                    self.n_components]
            opt_args['args'] = args
            opt_args['min_grad_norm'] = 1e-3
            opt_args['n_iter_without_progress'] = 30
                                    opt_args['objective_error'] = objective_error
            opt_args['kwargs']['angle'] = self.angle
            opt_args['kwargs']['verbose'] = self.verbose
        else:
            obj_func = _kl_divergence
            opt_args['args'] = [P, degrees_of_freedom, n_samples,
                                self.n_components]
            opt_args['min_error_diff'] = 0.0
            opt_args['min_grad_norm'] = self.min_grad_norm
                P *= self.early_exaggeration
        params, kl_divergence, it = _gradient_descent(obj_func, params,
                                                      **opt_args)
        opt_args['n_iter'] = 100
        opt_args['momentum'] = 0.8
        opt_args['it'] = it + 1
        params, kl_divergence, it = _gradient_descent(obj_func, params,
                                                      **opt_args)
        if self.verbose:
            print("[t-SNE] KL divergence after %d iterations with early "
                  "exaggeration: %f" % (it + 1, kl_divergence))
                self.n_iter_ = it
                P /= self.early_exaggeration
        opt_args['n_iter'] = self.n_iter
        opt_args['it'] = it + 1
        params, error, it = _gradient_descent(obj_func, params, **opt_args)
        if self.verbose:
            print("[t-SNE] Error after %d iterations: %f"
                  % (it + 1, kl_divergence))
        X_embedded = params.reshape(n_samples, self.n_components)
        self.kl_divergence_ = kl_divergence
        return X_embedded
    def fit_transform(self, X, y=None):
                embedding = self._fit(X)
        self.embedding_ = embedding
        return self.embedding_
    def fit(self, X, y=None):
                self.fit_transform(X)
        return self

from libc.stdlib cimport malloc, free
from libc.stdio cimport printf
from libc.math cimport sqrt, log
cimport numpy as np
import numpy as np
cdef char* EMPTY_STRING = ""
cdef extern from "math.h":
    float fabsf(float x) nogil
cdef float EPSILON = 1e-6
cdef enum:
    DEBUGFLAG = 0
cdef extern from "time.h":
        ctypedef long clock_t
    clock_t clock() nogil
    double CLOCKS_PER_SEC

cdef extern from "cblas.h":
    float snrm2 "cblas_snrm2"(int N, float *X, int incX) nogil

cdef struct Node:
        float* barycenter
        float* leaf_point_position
            long cumulative_size
        long size
            long point_index
            long level
        float* left_edge
        float* center
            float* width
        float max_width
            int is_leaf
        Node **children
        Node *parent
        Tree* tree
cdef struct Tree:
        Node* root_node 
        int n_dimensions
        long n_cells
        long n_points
        int verbose
        int n_cell_per_node
cdef Tree* init_tree(float[:] left_edge, float[:] width, int n_dimensions, 
                     int verbose) nogil:
        cdef Tree* tree = <Tree*> malloc(sizeof(Tree))
    tree.n_dimensions = n_dimensions
    tree.n_cells = 0
    tree.n_points = 0
    tree.verbose = verbose
    tree.root_node = create_root(left_edge, width, n_dimensions)
    tree.root_node.tree = tree
    tree.n_cells += 1
    tree.n_cell_per_node = 2 ** n_dimensions
    if DEBUGFLAG:
        printf("[t-SNE] Tree initialised. Left_edge = (%1.9e, %1.9e, %1.9e)\n",
               left_edge[0], left_edge[1], left_edge[2])
        printf("[t-SNE] Tree initialised. Width = (%1.9e, %1.9e, %1.9e)\n",
                width[0], width[1], width[2])
    return tree
cdef Node* create_root(float[:] left_edge, float[:] width, int n_dimensions) nogil:
        cdef int ax
    cdef int n_cell_per_node = 2 ** n_dimensions
        root = <Node*> malloc(sizeof(Node))
    root.is_leaf = 1
    root.parent = NULL
    root.level = 0
    root.cumulative_size = 0
    root.size = 0
    root.point_index = -1
    root.max_width = 0.0
    root.width = <float*> malloc(sizeof(float) * n_dimensions)
    root.left_edge = <float*> malloc(sizeof(float) * n_dimensions)
    root.center = <float*> malloc(sizeof(float) * n_dimensions)
    root.barycenter = <float*> malloc(sizeof(float) * n_dimensions)
    root.leaf_point_position= <float*> malloc(sizeof(float) * n_dimensions)
    root.children = NULL
    for ax in range(n_dimensions):
        root.width[ax] = width[ax]
        root.left_edge[ax] = left_edge[ax]
        root.center[ax] = 0.0
        root.barycenter[ax] = 0.
        root.leaf_point_position[ax] = -1
    for ax in range(n_dimensions):
        root.max_width = max(root.max_width, root.width[ax])
    if DEBUGFLAG:
        printf("[t-SNE] Created root node %p\n", root)
    return root
cdef Node* create_child(Node *parent, int[3] offset) nogil:
        cdef int ax
        child = <Node *> malloc(sizeof(Node))
    child.is_leaf = 1
    child.parent = parent
    child.level = parent.level + 1
    child.size = 0
    child.cumulative_size = 0
    child.point_index = -1
    child.tree = parent.tree
    child.max_width = 0.0
    child.width = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)
    child.left_edge = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)
    child.center = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)
    child.barycenter = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)
    child.leaf_point_position = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)
    child.children = NULL
    for ax in range(parent.tree.n_dimensions):
        child.width[ax] = parent.width[ax] / 2.0
        child.left_edge[ax] = parent.left_edge[ax] + offset[ax] * parent.width[ax] / 2.0
        child.center[ax] = child.left_edge[ax] + child.width[ax] / 2.0
        child.barycenter[ax] = 0.
        child.leaf_point_position[ax] = -1.
    for ax in range(parent.tree.n_dimensions):
        child.max_width = max(child.max_width, child.width[ax])
    child.tree.n_cells += 1
    return child
cdef Node* select_child(Node *node, float[3] pos, long index) nogil:
            cdef int* offset = <int*> malloc(sizeof(int) * node.tree.n_dimensions)
    cdef int ax, idx
    cdef Node* child
    cdef int error
    for ax in range(node.tree.n_dimensions):
        offset[ax] = (pos[ax] - (node.left_edge[ax] + node.width[ax] / 2.0)) > 0.
    idx = offset2index(offset, node.tree.n_dimensions)
    child = node.children[idx]
    if DEBUGFLAG:
        printf("[t-SNE] Offset [%i, %i] with LE [%f, %f]\n",
               offset[0], offset[1], child.left_edge[0], child.left_edge[1])
    free(offset)
    return child

cdef inline void index2offset(int* offset, int index, int n_dimensions) nogil:
                                cdef int rem, k, shift
    for k in range(n_dimensions):
        shift = n_dimensions -k -1
        rem = ((index >> shift) << shift)
        offset[k] = rem > 0
        if DEBUGFLAG:
            printf("i2o index %i k %i rem %i offset", index, k, rem)
            for j in range(n_dimensions):
                printf(" %i", offset[j])
            printf(" n_dimensions %i\n", n_dimensions)
        index -= rem

cdef inline int offset2index(int* offset, int n_dimensions) nogil:
                cdef int dim
    cdef int index = 0
    for dim in range(n_dimensions):
        index += (2 ** dim) * offset[n_dimensions - dim - 1]
        if DEBUGFLAG:
            printf("o2i index %i dim %i            offset", index, dim)
            for j in range(n_dimensions):
                printf(" %i", offset[j])
            printf(" n_dimensions %i\n", n_dimensions)
    return index

cdef void subdivide(Node* node) nogil:
        cdef int idx = 0
    cdef int* offset = <int*> malloc(sizeof(int) * node.tree.n_dimensions)
    node.is_leaf = False
    node.children = <Node**> malloc(sizeof(Node*) * node.tree.n_cell_per_node)
    for idx in range(node.tree.n_cell_per_node):
        index2offset(offset, idx, node.tree.n_dimensions)
        node.children[idx] = create_child(node, offset)
    free(offset)

cdef int insert(Node *root, float pos[3], long point_index, long depth, long
        duplicate_count) nogil:
                    cdef Node *child
    cdef long i
    cdef int ax
    cdef int not_identical = 1
    cdef int n_dimensions = root.tree.n_dimensions
    if DEBUGFLAG:
        printf("[t-SNE] [d=%i] Inserting pos %i [%f, %f] duplicate_count=%i "
                "into child %p\n", depth, point_index, pos[0], pos[1],
                duplicate_count, root)    
            root.cumulative_size += duplicate_count
            cdef double frac_seen = <double>(root.cumulative_size - 1) / (<double>
            root.cumulative_size)
    cdef double frac_new  = 1.0 / <double> root.cumulative_size
        if duplicate_count < 1:
        return -1
        for ax in range(n_dimensions):
        root.barycenter[ax] *= frac_seen
        if (pos[ax] > (root.left_edge[ax] + root.width[ax] + EPSILON)):
            printf("[t-SNE] Error: point (%1.9e) is above right edge of node "
                    "(%1.9e)\n", pos[ax], root.left_edge[ax] + root.width[ax])
            return -1
        if (pos[ax] < root.left_edge[ax] - EPSILON):
            printf("[t-SNE] Error: point (%1.9e) is below left edge of node "
                   "(%1.9e)\n", pos[ax], root.left_edge[ax])
            return -1
    for ax in range(n_dimensions):
        root.barycenter[ax] += pos[ax] * frac_new
                        if (root.size == 0) & root.is_leaf:
                if DEBUGFLAG:
            printf("[t-SNE] [d=%i] Inserting [%f, %f] into blank cell\n", depth,
                   pos[0], pos[1])
        for ax in range(n_dimensions):
            root.leaf_point_position[ax] = pos[ax]
        root.point_index = point_index
        root.size = duplicate_count
        return 0
    else:
                if DEBUGFLAG:
            printf("[t-SNE] [d=%i] Node %p is occupied or is a leaf.\n", depth,
                    root)
            printf("[t-SNE] [d=%i] Node %p leaf = %i. Size %i\n", depth, root,
                    root.is_leaf, root.size)
        if root.is_leaf & (root.size > 0):
                        for ax in range(n_dimensions):
                not_identical &= (fabsf(pos[ax] - root.leaf_point_position[ax]) < EPSILON)
                not_identical &= (root.point_index != point_index)
            if not_identical == 1:
                root.size += duplicate_count
                if DEBUGFLAG:
                    printf("[t-SNE] Warning: [d=%i] Detected identical "
                            "points. Returning. Leaf now has size %i\n",
                            depth, root.size)
                return 0
                        if root.is_leaf:
            if DEBUGFLAG:
                printf("[t-SNE] [d=%i] Subdividing this leaf node %p\n", depth,
                        root)
            subdivide(root)
                                if root.size > 0:
            child = select_child(root, root.leaf_point_position, root.point_index)
            if DEBUGFLAG:
                printf("[t-SNE] [d=%i] Relocating old point to node %p\n",
                        depth, child)
            insert(child, root.leaf_point_position, root.point_index, depth + 1, root.size)
                if DEBUGFLAG:
            printf("[t-SNE] [d=%i] Selecting node for new point\n", depth)
        child = select_child(root, pos, point_index)
        if root.size > 0:
                        for ax in range(n_dimensions):
                root.leaf_point_position[ax] = -1            
            root.size = 0
            root.point_index = -1            
        return insert(child, pos, point_index, depth + 1, 1)
cdef int insert_many(Tree* tree, float[:,:] pos_array) nogil:
        cdef long nrows = pos_array.shape[0]
    cdef long i
    cdef int ax
    cdef float row[3]
    cdef long err = 0
    for i in range(nrows):
        for ax in range(tree.n_dimensions):
            row[ax] = pos_array[i, ax]
        if DEBUGFLAG:
            printf("[t-SNE] inserting point %i: [%f, %f]\n", i, row[0], row[1])
        err = insert(tree.root_node, row, i, 0, 1)
        if err != 0:
            printf("[t-SNE] ERROR\n%s", EMPTY_STRING)
            return err
        tree.n_points += 1
    return err
cdef int free_tree(Tree* tree) nogil:
    cdef int check
    cdef long* cnt = <long*> malloc(sizeof(long) * 3)
    for i in range(3):
        cnt[i] = 0
    free_recursive(tree, tree.root_node, cnt)
    check = cnt[0] == tree.n_cells
    check &= cnt[2] == tree.n_points
    free(tree)
    free(cnt)
    return check
cdef void free_post_children(Node *node) nogil:
    free(node.width)
    free(node.left_edge)
    free(node.center)
    free(node.barycenter)
    free(node.leaf_point_position)
    free(node)
cdef void free_recursive(Tree* tree, Node *root, long* counts) nogil:
                cdef int idx
    cdef Node* child
    if not root.is_leaf:
        for idx in range(tree.n_cell_per_node):
            child = root.children[idx]
            free_recursive(tree, child, counts)
            counts[0] += 1
            if child.is_leaf:
                counts[1] += 1
                if child.size > 0:
                    counts[2] +=1
            else:
                free(child.children)
            free_post_children(child)
    if root == tree.root_node:
        if not root.is_leaf:
            free(root.children)
        free_post_children(root)
cdef long count_points(Node* root, long count) nogil:
            if DEBUGFLAG:
        printf("[t-SNE] Counting nodes at root node %p\n", root)
    cdef Node* child
    cdef int idx
    if root.is_leaf:
        count += root.size
        if DEBUGFLAG : 
            printf("[t-SNE] %p is a leaf node, no children\n", root)
            printf("[t-SNE] %i points in node %p\n", count, root)
        return count
        for idx in range(root.tree.n_cell_per_node):
        child = root.children[idx]
        if DEBUGFLAG:
            printf("[t-SNE] Counting points for child %p\n", child)
        if child.is_leaf and child.size > 0:
            if DEBUGFLAG:
                printf("[t-SNE] Child has size %d\n", child.size)
            count += child.size
        elif not child.is_leaf:
            if DEBUGFLAG:
                printf("[t-SNE] Child is not a leaf. Descending\n%s", EMPTY_STRING)
            count = count_points(child, count)
                                    if DEBUGFLAG:
        printf("[t-SNE] %i points in this node\n", count)
    return count

cdef float compute_gradient(float[:,:] val_P,
                            float[:,:] pos_reference,
                            np.int64_t[:,:] neighbors,
                            float[:,:] tot_force,
                            Node* root_node,
                            float theta,
                            float dof,
                            long start,
                            long stop) nogil:
            cdef long i, coord
    cdef int ax
    cdef long n = pos_reference.shape[0]
    cdef int n_dimensions = root_node.tree.n_dimensions
    if root_node.tree.verbose > 11:
        printf("[t-SNE] Allocating %i elements in force arrays\n",
                n * n_dimensions * 2)
    cdef float* sum_Q = <float*> malloc(sizeof(float))
    cdef float* neg_f = <float*> malloc(sizeof(float) * n * n_dimensions)
    cdef float* neg_f_fast = <float*> malloc(sizeof(float) * n * n_dimensions)
    cdef float* pos_f = <float*> malloc(sizeof(float) * n * n_dimensions)
    cdef clock_t t1, t2
    cdef float sQ, error
    sum_Q[0] = 0.0
    t1 = clock()
    compute_gradient_negative(val_P, pos_reference, neg_f, root_node, sum_Q,
                              dof, theta, start, stop)
    t2 = clock()
    if root_node.tree.verbose > 15:
        printf("[t-SNE] Computing negative gradient: %e ticks\n", ((float) (t2 - t1)))
    sQ = sum_Q[0]
    t1 = clock()
    error = compute_gradient_positive(val_P, pos_reference, neighbors, pos_f,
                              n_dimensions, dof, sQ, start, root_node.tree.verbose)
    t2 = clock()
    if root_node.tree.verbose > 15:
        printf("[t-SNE] Computing positive gradient: %e ticks\n", ((float) (t2 - t1)))
    for i in range(start, n):
        for ax in range(n_dimensions):
            coord = i * n_dimensions + ax
            tot_force[i, ax] = pos_f[coord] - (neg_f[coord] / sum_Q[0])
    free(sum_Q)
    free(neg_f)
    free(neg_f_fast)
    free(pos_f)
    return sQ

cdef float compute_gradient_positive(float[:,:] val_P,
                                     float[:,:] pos_reference,
                                     np.int64_t[:,:] neighbors,
                                     float* pos_f,
                                     int n_dimensions,
                                     float dof,
                                     float sum_Q,
                                     np.int64_t start,
                                     int verbose) nogil:
                        cdef:
        int ax
        long i, j, k
        long K = neighbors.shape[1]
        long n = val_P.shape[0]
        float[3] buff
        float D, Q, pij
        float C = 0.0
        float exponent = (dof + 1.0) / -2.0
    cdef clock_t t1, t2
    t1 = clock()
    for i in range(start, n):
        for ax in range(n_dimensions):
            pos_f[i * n_dimensions + ax] = 0.0
        for k in range(K):
            j = neighbors[i, k]
                                    D = 0.0
            Q = 0.0
            pij = val_P[i, j]
            for ax in range(n_dimensions):
                buff[ax] = pos_reference[i, ax] - pos_reference[j, ax]
                D += buff[ax] ** 2.0  
            Q = (((1.0 + D) / dof) ** exponent)
            D = pij * Q
            Q /= sum_Q
            C += pij * log((pij + EPSILON) / (Q + EPSILON))
            for ax in range(n_dimensions):
                pos_f[i * n_dimensions + ax] += D * buff[ax]
    t2 = clock()
    dt = ((float) (t2 - t1))
    if verbose > 10:
        printf("[t-SNE] Computed error=%1.4f in %1.1e ticks\n", C, dt)
    return C

cdef void compute_gradient_negative(float[:,:] val_P, 
                                    float[:,:] pos_reference,
                                    float* neg_f,
                                    Node *root_node,
                                    float* sum_Q,
                                    float dof,
                                    float theta, 
                                    long start, 
                                    long stop) nogil:
    if stop == -1:
        stop = pos_reference.shape[0] 
    cdef:
        int ax
        long i, j
        long n = stop - start
        float* force
        float* iQ 
        float* pos
        float* dist2s
        long* sizes
        float* deltas
        long* l
        int n_dimensions = root_node.tree.n_dimensions
        float qijZ, mult
        long idx, 
        long dta = 0
        long dtb = 0
        clock_t t1, t2, t3
        float* neg_force
    iQ = <float*> malloc(sizeof(float))
    force = <float*> malloc(sizeof(float) * n_dimensions)
    pos = <float*> malloc(sizeof(float) * n_dimensions)
    dist2s = <float*> malloc(sizeof(float) * n)
    sizes = <long*> malloc(sizeof(long) * n)
    deltas = <float*> malloc(sizeof(float) * n * n_dimensions)
    l = <long*> malloc(sizeof(long))
    neg_force= <float*> malloc(sizeof(float) * n_dimensions)
    for i in range(start, stop):
                for ax in range(n_dimensions):
            force[ax] = 0.0
            neg_force[ax] = 0.0
            pos[ax] = pos_reference[i, ax]
        iQ[0] = 0.0
        l[0] = 0
                        t1 = clock()
        compute_non_edge_forces(root_node, theta, i, pos, force, dist2s,
                                     sizes, deltas, l)
        t2 = clock()
                                        exponent = (dof + 1.0) / -2.0
        for j in range(l[0]):
            qijZ = ((1.0 + dist2s[j]) / dof) ** exponent
            sum_Q[0] += sizes[j] * qijZ
            mult = sizes[j] * qijZ * qijZ
            for ax in range(n_dimensions):
                idx = j * n_dimensions + ax
                neg_force[ax] += mult * deltas[idx]
        t3 = clock()
        for ax in range(n_dimensions):
            neg_f[i * n_dimensions + ax] = neg_force[ax]
        dta += t2 - t1
        dtb += t3 - t2
    if root_node.tree.verbose > 20:
        printf("[t-SNE] Tree: %i clock ticks | ", dta)
        printf("Force computation: %i clock ticks\n", dtb)
    free(iQ)
    free(force)
    free(pos)
    free(dist2s)
    free(sizes)
    free(deltas)
    free(l)
    free(neg_force)

cdef void compute_non_edge_forces(Node* node, 
                                  float theta,
                                  long point_index,
                                  float* pos,
                                  float* force,
                                  float* dist2s,
                                  long* sizes,
                                  float* deltas,
                                  long* l) nogil:
        cdef:
        Node* child
        int i, j
        int n_dimensions = node.tree.n_dimensions
        long idx, idx1
        float dist_check
    
                if node.cumulative_size > 0 and not (node.is_leaf and (node.point_index ==
        point_index)):
                                idx1 = l[0] * n_dimensions
        deltas[idx1] = pos[0] - node.barycenter[0]
        idx = idx1
        for i in range(1, n_dimensions):
            idx += 1
            deltas[idx] = pos[i] - node.barycenter[i] 
                dist2s[l[0]] = snrm2(n_dimensions, &deltas[idx1], 1)
                                                if node.is_leaf or ((node.max_width / dist2s[l[0]]) < theta):
                                    sizes[l[0]] = node.cumulative_size
            dist2s[l[0]] = dist2s[l[0]] * dist2s[l[0]]
            l[0] += 1
        else:
                        for idx in range(node.tree.n_cell_per_node):
                child = node.children[idx]
                if child.cumulative_size == 0: 
                    continue
                compute_non_edge_forces(child, theta,
                        point_index, pos, force, dist2s, sizes, deltas,
                        l)

cdef float compute_error(float[:, :] val_P,
                        float[:, :] pos_reference,
                        np.int64_t[:,:] neighbors,
                        float sum_Q,
                        int n_dimensions,
                        int verbose) nogil:
    cdef int i, j, ax
    cdef int I = neighbors.shape[0]
    cdef int K = neighbors.shape[1]
    cdef float pij, Q
    cdef float C = 0.0
    cdef clock_t t1, t2
    cdef float dt, delta
    t1 = clock()
    for i in range(I):
        for k in range(K):
            j = neighbors[i, k]
            pij = val_P[i, j]
            Q = 0.0
            for ax in range(n_dimensions):
                delta = (pos_reference[i, ax] - pos_reference[j, ax])
                Q += delta * delta
            Q = (1.0 / (sum_Q + Q * sum_Q))
            C += pij * log((pij + EPSILON) / (Q + EPSILON))
    t2 = clock()
    dt = ((float) (t2 - t1))
    if verbose > 10:
        printf("[t-SNE] Computed error=%1.4f in %1.1e ticks\n", C, dt)
    return C

def calculate_edge(pos_output):
            left_edge = np.min(pos_output, axis=0)
    right_edge = np.max(pos_output, axis=0) 
    center = (right_edge + left_edge) * 0.5
    width = np.maximum(np.subtract(right_edge, left_edge), EPSILON)
        width = width.astype(np.float32) * 1.001
    left_edge = center - width / 2.0
    right_edge = center + width / 2.0
    return left_edge, right_edge, width
def gradient(float[:,:] pij_input, 
             float[:,:] pos_output, 
             np.int64_t[:,:] neighbors, 
             float[:,:] forces, 
             float theta,
             int n_dimensions,
             int verbose,
             float dof = 1.0,
             long skip_num_points=0):
                cdef float C
    n = pos_output.shape[0]
    left_edge, right_edge, width = calculate_edge(pos_output)
    assert width.itemsize == 4
    assert pij_input.itemsize == 4
    assert pos_output.itemsize == 4
    assert forces.itemsize == 4
    m = "Number of neighbors must be <     assert n - 1 >= neighbors.shape[1], m
    m = "neighbors array and pos_output shapes are incompatible"
    assert n == neighbors.shape[0], m
    m = "Forces array and pos_output shapes are incompatible"
    assert n == forces.shape[0], m
    m = "Pij and pos_output shapes are incompatible"
    assert n == pij_input.shape[0], m
    m = "Pij and pos_output shapes are incompatible"
    assert n == pij_input.shape[1], m
    if verbose > 10:
        printf("[t-SNE] Initializing tree of n_dimensions %i\n", n_dimensions)
    cdef Tree* qt = init_tree(left_edge, width, n_dimensions, verbose)
    if verbose > 10:
        printf("[t-SNE] Inserting %i points\n", pos_output.shape[0])
    err = insert_many(qt, pos_output)
    assert err == 0, "[t-SNE] Insertion failed"
    if verbose > 10:
                                printf("[t-SNE] Computing gradient\n%s", EMPTY_STRING)
    sum_Q = compute_gradient(pij_input, pos_output, neighbors, forces,
                             qt.root_node, theta, dof, skip_num_points, -1)
    C = compute_error(pij_input, pos_output, neighbors, sum_Q, n_dimensions,
                      verbose)
    if verbose > 10:
                                printf("[t-SNE] Checking tree consistency\n%s", EMPTY_STRING)
    cdef long count = count_points(qt.root_node, 0)
    m = ("Tree consistency failed: unexpected number of points=%i "
         "at root node=%i" % (count, qt.root_node.cumulative_size))
    assert count == qt.root_node.cumulative_size, m 
    m = "Tree consistency failed: unexpected number of points on the tree"
    assert count == qt.n_points, m
    free_tree(qt)
    return C

def check_quadtree(X, np.int64_t[:] counts):
        
    X = X.astype(np.float32)
    left_edge, right_edge, width = calculate_edge(X)
        qt = init_tree(left_edge, width, 2, 2)
        insert_many(qt, X)
    cdef long count = count_points(qt.root_node, 0)
    counts[0] = count
    counts[1] = qt.root_node.cumulative_size
    counts[2] = qt.n_points
    free_tree(qt)
    return counts

cdef int helper_test_index2offset(int* check, int index, int n_dimensions):
    cdef int* offset = <int*> malloc(sizeof(int) * n_dimensions)
    cdef int error_check = 1
    for i in range(n_dimensions):
        offset[i] = 0
    index2offset(offset, index, n_dimensions)
    for i in range(n_dimensions):
        error_check &= offset[i] == check[i]
    free(offset)
    return error_check

def test_index2offset():
    ret = 1
    ret &= helper_test_index2offset([1, 0, 1], 5, 3) == 1
    ret &= helper_test_index2offset([0, 0, 0], 0, 3) == 1
    ret &= helper_test_index2offset([0, 0, 1], 1, 3) == 1
    ret &= helper_test_index2offset([0, 1, 0], 2, 3) == 1
    ret &= helper_test_index2offset([0, 1, 1], 3, 3) == 1
    ret &= helper_test_index2offset([1, 0, 0], 4, 3) == 1
    return ret

def test_index_offset():
    cdef int n_dimensions, idx, tidx, k
    cdef int error_check = 1
    cdef int* offset 
    for n_dimensions in range(2, 10):
        offset = <int*> malloc(sizeof(int) * n_dimensions)
        for k in range(n_dimensions):
            offset[k] = 0
        for idx in range(2 ** n_dimensions):
            index2offset(offset, idx, n_dimensions)
            tidx = offset2index(offset, n_dimensions)
            error_check &= tidx == idx
            assert error_check == 1
        free(offset)
    return error_check
from libc cimport math
cimport cython
import numpy as np
cimport numpy as np
from libc.stdio cimport printf
cdef extern from "numpy/npy_math.h":
    float NPY_INFINITY

cdef float EPSILON_DBL = 1e-8
cdef float PERPLEXITY_TOLERANCE = 1e-5
@cython.boundscheck(False)
cpdef np.ndarray[np.float32_t, ndim=2] _binary_search_perplexity(
        np.ndarray[np.float32_t, ndim=2] affinities, 
        np.ndarray[np.int64_t, ndim=2] neighbors, 
        float desired_perplexity,
        int verbose):
            cdef long n_steps = 100
    cdef long n_samples = affinities.shape[0]
            cdef np.ndarray[np.float64_t, ndim=2] P = np.zeros((n_samples, n_samples),
                                                       dtype=np.float64)
        cdef float beta
    cdef float beta_min
    cdef float beta_max
    cdef float beta_sum = 0.0
        cdef float desired_entropy = math.log(desired_perplexity)
    cdef float entropy_diff
    cdef float entropy
    cdef float sum_Pi
    cdef float sum_disti_Pi
    cdef long i, j, k, l = 0
    cdef long K = n_samples
    cdef int using_neighbors = neighbors is not None
    if using_neighbors:
        K = neighbors.shape[1]
    for i in range(n_samples):
        beta_min = -NPY_INFINITY
        beta_max = NPY_INFINITY
        beta = 1.0
                for l in range(n_steps):
                                                if using_neighbors:
                for k in range(K):
                    j = neighbors[i, k]
                    P[i, j] = math.exp(-affinities[i, j] * beta)
            else:
                for j in range(K):
                    P[i, j] = math.exp(-affinities[i, j] * beta)
            P[i, i] = 0.0
            sum_Pi = 0.0
            if using_neighbors:
                for k in range(K):
                    j = neighbors[i, k]
                    sum_Pi += P[i, j]
            else:
                for j in range(K):
                    sum_Pi += P[i, j]
            if sum_Pi == 0.0:
                sum_Pi = EPSILON_DBL
            sum_disti_Pi = 0.0
            if using_neighbors:
                for k in range(K):
                    j = neighbors[i, k]
                    P[i, j] /= sum_Pi
                    sum_disti_Pi += affinities[i, j] * P[i, j]
            else:
                for j in range(K):
                    P[i, j] /= sum_Pi
                    sum_disti_Pi += affinities[i, j] * P[i, j]
            entropy = math.log(sum_Pi) + beta * sum_disti_Pi
            entropy_diff = entropy - desired_entropy
            if math.fabs(entropy_diff) <= PERPLEXITY_TOLERANCE:
                break
            if entropy_diff > 0.0:
                beta_min = beta
                if beta_max == NPY_INFINITY:
                    beta *= 2.0
                else:
                    beta = (beta + beta_max) / 2.0
            else:
                beta_max = beta
                if beta_min == -NPY_INFINITY:
                    beta /= 2.0
                else:
                    beta = (beta + beta_min) / 2.0
        beta_sum += beta
        if verbose and ((i + 1) % 1000 == 0 or i + 1 == n_samples):
            print("[t-SNE] Computed conditional probabilities for sample "
                  "%d / %d" % (i + 1, n_samples))
    if verbose:
        print("[t-SNE] Mean sigma: %f"
              % np.mean(math.sqrt(n_samples / beta_sum)))
    return P
from .locally_linear import locally_linear_embedding, LocallyLinearEmbedding
from .isomap import Isomap
from .mds import MDS, smacof
from .spectral_embedding_ import SpectralEmbedding, spectral_embedding
from .t_sne import TSNE
__all__ = ['locally_linear_embedding', 'LocallyLinearEmbedding', 'Isomap',
           'MDS', 'smacof', 'SpectralEmbedding', 'spectral_embedding', "TSNE"]
from __future__ import division
import numpy as np
from ..utils import check_array, check_consistent_length
from ..utils.multiclass import type_of_target

def _average_binary_score(binary_metric, y_true, y_score, average,
                          sample_weight=None):
        average_options = (None, 'micro', 'macro', 'weighted', 'samples')
    if average not in average_options:
        raise ValueError('average has to be one of {0}'
                         ''.format(average_options))
    y_type = type_of_target(y_true)
    if y_type not in ("binary", "multilabel-indicator"):
        raise ValueError("{0} format is not supported".format(y_type))
    if y_type == "binary":
        return binary_metric(y_true, y_score, sample_weight=sample_weight)
    check_consistent_length(y_true, y_score, sample_weight)
    y_true = check_array(y_true)
    y_score = check_array(y_score)
    not_average_axis = 1
    score_weight = sample_weight
    average_weight = None
    if average == "micro":
        if score_weight is not None:
            score_weight = np.repeat(score_weight, y_true.shape[1])
        y_true = y_true.ravel()
        y_score = y_score.ravel()
    elif average == 'weighted':
        if score_weight is not None:
            average_weight = np.sum(np.multiply(
                y_true, np.reshape(score_weight, (-1, 1))), axis=0)
        else:
            average_weight = np.sum(y_true, axis=0)
        if average_weight.sum() == 0:
            return 0
    elif average == 'samples':
                average_weight = score_weight
        score_weight = None
        not_average_axis = 0
    if y_true.ndim == 1:
        y_true = y_true.reshape((-1, 1))
    if y_score.ndim == 1:
        y_score = y_score.reshape((-1, 1))
    n_classes = y_score.shape[not_average_axis]
    score = np.zeros((n_classes,))
    for c in range(n_classes):
        y_true_c = y_true.take([c], axis=not_average_axis).ravel()
        y_score_c = y_score.take([c], axis=not_average_axis).ravel()
        score[c] = binary_metric(y_true_c, y_score_c,
                                 sample_weight=score_weight)
        if average is not None:
        return np.average(score, weights=average_weight)
    else:
        return score

from __future__ import division
import warnings
import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse import csr_matrix
from ..preprocessing import LabelBinarizer, label_binarize
from ..preprocessing import LabelEncoder
from ..utils import assert_all_finite
from ..utils import check_array
from ..utils import check_consistent_length
from ..utils import column_or_1d
from ..utils.multiclass import unique_labels
from ..utils.multiclass import type_of_target
from ..utils.validation import _num_samples
from ..utils.sparsefuncs import count_nonzero
from ..utils.fixes import bincount
from ..exceptions import UndefinedMetricWarning

def _check_targets(y_true, y_pred):
        check_consistent_length(y_true, y_pred)
    type_true = type_of_target(y_true)
    type_pred = type_of_target(y_pred)
    y_type = set([type_true, type_pred])
    if y_type == set(["binary", "multiclass"]):
        y_type = set(["multiclass"])
    if len(y_type) > 1:
        raise ValueError("Can't handle mix of {0} and {1}"
                         "".format(type_true, type_pred))
        y_type = y_type.pop()
        if (y_type not in ["binary", "multiclass", "multilabel-indicator"]):
        raise ValueError("{0} is not supported".format(y_type))
    if y_type in ["binary", "multiclass"]:
        y_true = column_or_1d(y_true)
        y_pred = column_or_1d(y_pred)
    if y_type.startswith('multilabel'):
        y_true = csr_matrix(y_true)
        y_pred = csr_matrix(y_pred)
        y_type = 'multilabel-indicator'
    return y_type, y_true, y_pred

def _weighted_sum(sample_score, sample_weight, normalize=False):
    if normalize:
        return np.average(sample_score, weights=sample_weight)
    elif sample_weight is not None:
        return np.dot(sample_score, sample_weight)
    else:
        return sample_score.sum()

def accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):
    
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if y_type.startswith('multilabel'):
        differing_labels = count_nonzero(y_true - y_pred, axis=1)
        score = differing_labels == 0
    else:
        score = y_true == y_pred
    return _weighted_sum(score, sample_weight, normalize)

def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if y_type not in ("binary", "multiclass"):
        raise ValueError("%s is not supported" % y_type)
    if labels is None:
        labels = unique_labels(y_true, y_pred)
    else:
        labels = np.asarray(labels)
        if np.all([l not in y_true for l in labels]):
            raise ValueError("At least one label specified must be in y_true")
    if sample_weight is None:
        sample_weight = np.ones(y_true.shape[0], dtype=np.int)
    else:
        sample_weight = np.asarray(sample_weight)
    check_consistent_length(sample_weight, y_true, y_pred)
    n_labels = labels.size
    label_to_ind = dict((y, x) for x, y in enumerate(labels))
        y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
        ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
        sample_weight = sample_weight[ind]
    CM = coo_matrix((sample_weight, (y_true, y_pred)),
                    shape=(n_labels, n_labels)
                    ).toarray()
    return CM

def cohen_kappa_score(y1, y2, labels=None, weights=None, sample_weight=None):
        confusion = confusion_matrix(y1, y2, labels=labels,
                                 sample_weight=sample_weight)
    n_classes = confusion.shape[0]
    sum0 = np.sum(confusion, axis=0)
    sum1 = np.sum(confusion, axis=1)
    expected = np.outer(sum0, sum1) / np.sum(sum0)
    if weights is None:
        w_mat = np.ones([n_classes, n_classes], dtype=np.int)
        w_mat.flat[:: n_classes + 1] = 0
    elif weights == "linear" or weights == "quadratic":
        w_mat = np.zeros([n_classes, n_classes], dtype=np.int)
        w_mat += np.arange(n_classes)
        if weights == "linear":
            w_mat = np.abs(w_mat - w_mat.T)
        else:
            w_mat = (w_mat - w_mat.T) ** 2
    else:
        raise ValueError("Unknown kappa weighting type.")
    k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)
    return 1 - k

def jaccard_similarity_score(y_true, y_pred, normalize=True,
                             sample_weight=None):
    
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if y_type.startswith('multilabel'):
        with np.errstate(divide='ignore', invalid='ignore'):
                        pred_or_true = count_nonzero(y_true + y_pred, axis=1)
            pred_and_true = count_nonzero(y_true.multiply(y_pred), axis=1)
            score = pred_and_true / pred_or_true
            score[pred_or_true == 0.0] = 1.0
    else:
        score = y_true == y_pred
    return _weighted_sum(score, sample_weight, normalize)

def matthews_corrcoef(y_true, y_pred, sample_weight=None):
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if y_type != "binary":
        raise ValueError("%s is not supported" % y_type)
    lb = LabelEncoder()
    lb.fit(np.hstack([y_true, y_pred]))
    y_true = lb.transform(y_true)
    y_pred = lb.transform(y_pred)
    mean_yt = np.average(y_true, weights=sample_weight)
    mean_yp = np.average(y_pred, weights=sample_weight)
    y_true_u_cent = y_true - mean_yt
    y_pred_u_cent = y_pred - mean_yp
    cov_ytyp = np.average(y_true_u_cent * y_pred_u_cent, weights=sample_weight)
    var_yt = np.average(y_true_u_cent ** 2, weights=sample_weight)
    var_yp = np.average(y_pred_u_cent ** 2, weights=sample_weight)
    mcc = cov_ytyp / np.sqrt(var_yt * var_yp)
    if np.isnan(mcc):
        return 0.
    else:
        return mcc

def zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None):
        score = accuracy_score(y_true, y_pred,
                           normalize=normalize,
                           sample_weight=sample_weight)
    if normalize:
        return 1 - score
    else:
        if sample_weight is not None:
            n_samples = np.sum(sample_weight)
        else:
            n_samples = _num_samples(y_true)
        return n_samples - score

def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
             sample_weight=None):
        return fbeta_score(y_true, y_pred, 1, labels=labels,
                       pos_label=pos_label, average=average,
                       sample_weight=sample_weight)

def fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,
                average='binary', sample_weight=None):
        _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,
                                                 beta=beta,
                                                 labels=labels,
                                                 pos_label=pos_label,
                                                 average=average,
                                                 warn_for=('f-score',),
                                                 sample_weight=sample_weight)
    return f

def _prf_divide(numerator, denominator, metric, modifier, average, warn_for):
        result = numerator / denominator
    mask = denominator == 0.0
    if not np.any(mask):
        return result
        result[mask] = 0.0
                axis0 = 'sample'
    axis1 = 'label'
    if average == 'samples':
        axis0, axis1 = axis1, axis0
    if metric in warn_for and 'f-score' in warn_for:
        msg_start = '{0} and F-score are'.format(metric.title())
    elif metric in warn_for:
        msg_start = '{0} is'.format(metric.title())
    elif 'f-score' in warn_for:
        msg_start = 'F-score is'
    else:
        return result
    msg = ('{0} ill-defined and being set to 0.0 {{0}} '
           'no {1} {2}s.'.format(msg_start, modifier, axis0))
    if len(mask) == 1:
        msg = msg.format('due to')
    else:
        msg = msg.format('in {0}s with'.format(axis1))
    warnings.warn(msg, UndefinedMetricWarning, stacklevel=2)
    return result

def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
                                    pos_label=1, average=None,
                                    warn_for=('precision', 'recall',
                                              'f-score'),
                                    sample_weight=None):
        average_options = (None, 'micro', 'macro', 'weighted', 'samples')
    if average not in average_options and average != 'binary':
        raise ValueError('average has to be one of ' +
                         str(average_options))
    if beta <= 0:
        raise ValueError("beta should be >0 in the F-beta score")
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    present_labels = unique_labels(y_true, y_pred)
    if average == 'binary':
        if y_type == 'binary':
            if pos_label not in present_labels:
                if len(present_labels) < 2:
                                        return (0., 0., 0., 0)
                else:
                    raise ValueError("pos_label=%r is not a valid label: %r" %
                                     (pos_label, present_labels))
            labels = [pos_label]
        else:
            raise ValueError("Target is %s but average='binary'. Please "
                             "choose another average setting." % y_type)
    elif pos_label not in (None, 1):
        warnings.warn("Note that pos_label (set to %r) is ignored when "
                      "average != 'binary' (got %r). You may use "
                      "labels=[pos_label] to specify a single positive class."
                      % (pos_label, average), UserWarning)
    if labels is None:
        labels = present_labels
        n_labels = None
    else:
        n_labels = len(labels)
        labels = np.hstack([labels, np.setdiff1d(present_labels, labels,
                                                 assume_unique=True)])
    
    if y_type.startswith('multilabel'):
        sum_axis = 1 if average == 'samples' else 0
                        if not np.all(labels == present_labels):
            if np.max(labels) > np.max(present_labels):
                raise ValueError('All labels must be in [0, n labels). '
                                 'Got %d > %d' %
                                 (np.max(labels), np.max(present_labels)))
            if np.min(labels) < 0:
                raise ValueError('All labels must be in [0, n labels). '
                                 'Got %d < 0' % np.min(labels))
            y_true = y_true[:, labels[:n_labels]]
            y_pred = y_pred[:, labels[:n_labels]]
                true_and_pred = y_true.multiply(y_pred)
        tp_sum = count_nonzero(true_and_pred, axis=sum_axis,
                               sample_weight=sample_weight)
        pred_sum = count_nonzero(y_pred, axis=sum_axis,
                                 sample_weight=sample_weight)
        true_sum = count_nonzero(y_true, axis=sum_axis,
                                 sample_weight=sample_weight)
    elif average == 'samples':
        raise ValueError("Sample-based precision, recall, fscore is "
                         "not meaningful outside multilabel "
                         "classification. See the accuracy_score instead.")
    else:
        le = LabelEncoder()
        le.fit(labels)
        y_true = le.transform(y_true)
        y_pred = le.transform(y_pred)
        sorted_labels = le.classes_
                tp = y_true == y_pred
        tp_bins = y_true[tp]
        if sample_weight is not None:
            tp_bins_weights = np.asarray(sample_weight)[tp]
        else:
            tp_bins_weights = None
        if len(tp_bins):
            tp_sum = bincount(tp_bins, weights=tp_bins_weights,
                              minlength=len(labels))
        else:
                        true_sum = pred_sum = tp_sum = np.zeros(len(labels))
        if len(y_pred):
            pred_sum = bincount(y_pred, weights=sample_weight,
                                minlength=len(labels))
        if len(y_true):
            true_sum = bincount(y_true, weights=sample_weight,
                                minlength=len(labels))
                indices = np.searchsorted(sorted_labels, labels[:n_labels])
        tp_sum = tp_sum[indices]
        true_sum = true_sum[indices]
        pred_sum = pred_sum[indices]
    if average == 'micro':
        tp_sum = np.array([tp_sum.sum()])
        pred_sum = np.array([pred_sum.sum()])
        true_sum = np.array([true_sum.sum()])
    
    beta2 = beta ** 2
    with np.errstate(divide='ignore', invalid='ignore'):
        
                        precision = _prf_divide(tp_sum, pred_sum,
                                'precision', 'predicted', average, warn_for)
        recall = _prf_divide(tp_sum, true_sum,
                             'recall', 'true', average, warn_for)
                        f_score = ((1 + beta2) * precision * recall /
                   (beta2 * precision + recall))
        f_score[tp_sum == 0] = 0.0
    
    if average == 'weighted':
        weights = true_sum
        if weights.sum() == 0:
            return 0, 0, 0, None
    elif average == 'samples':
        weights = sample_weight
    else:
        weights = None
    if average is not None:
        assert average != 'binary' or len(precision) == 1
        precision = np.average(precision, weights=weights)
        recall = np.average(recall, weights=weights)
        f_score = np.average(f_score, weights=weights)
        true_sum = None  
    return precision, recall, f_score, true_sum

def precision_score(y_true, y_pred, labels=None, pos_label=1,
                    average='binary', sample_weight=None):
        p, _, _, _ = precision_recall_fscore_support(y_true, y_pred,
                                                 labels=labels,
                                                 pos_label=pos_label,
                                                 average=average,
                                                 warn_for=('precision',),
                                                 sample_weight=sample_weight)
    return p

def recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
                 sample_weight=None):
        _, r, _, _ = precision_recall_fscore_support(y_true, y_pred,
                                                 labels=labels,
                                                 pos_label=pos_label,
                                                 average=average,
                                                 warn_for=('recall',),
                                                 sample_weight=sample_weight)
    return r

def classification_report(y_true, y_pred, labels=None, target_names=None,
                          sample_weight=None, digits=2):
    
    if labels is None:
        labels = unique_labels(y_true, y_pred)
    else:
        labels = np.asarray(labels)
    if target_names is not None and len(labels) != len(target_names):
        warnings.warn(
            "labels size, {0}, does not match size of target_names, {1}"
            .format(len(labels), len(target_names))
        )
    last_line_heading = 'avg / total'
    if target_names is None:
        target_names = [u'%s' % l for l in labels]
    name_width = max(len(cn) for cn in target_names)
    width = max(name_width, len(last_line_heading), digits)
    headers = ["precision", "recall", "f1-score", "support"]
    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)
    report = head_fmt.format(u'', *headers, width=width)
    report += u'\n\n'
    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred,
                                                  labels=labels,
                                                  average=None,
                                                  sample_weight=sample_weight)
    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\n'
    rows = zip(target_names, p, r, f1, s)
    for row in rows:
        report += row_fmt.format(*row, width=width, digits=digits)
    report += u'\n'
        report += row_fmt.format(last_line_heading,
                             np.average(p, weights=s),
                             np.average(r, weights=s),
                             np.average(f1, weights=s),
                             np.sum(s),
                             width=width, digits=digits)
    return report

def hamming_loss(y_true, y_pred, labels=None, sample_weight=None,
                 classes=None):
        if classes is not None:
        warnings.warn("'classes' was renamed to 'labels' in version 0.18 and "
                      "will be removed in 0.20.", DeprecationWarning)
        labels = classes
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if labels is None:
        labels = unique_labels(y_true, y_pred)
    else:
        labels = np.asarray(labels)
    if sample_weight is None:
        weight_average = 1.
    else:
        weight_average = np.mean(sample_weight)
    if y_type.startswith('multilabel'):
        n_differences = count_nonzero(y_true - y_pred,
                                      sample_weight=sample_weight)
        return (n_differences /
                (y_true.shape[0] * len(labels) * weight_average))
    elif y_type in ["binary", "multiclass"]:
        return _weighted_sum(y_true != y_pred, sample_weight, normalize=True)
    else:
        raise ValueError("{0} is not supported".format(y_type))

def log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None,
             labels=None):
        y_pred = check_array(y_pred, ensure_2d=False)
    check_consistent_length(y_pred, y_true)
    lb = LabelBinarizer()
    if labels is not None:
        lb.fit(labels)
    else:
        lb.fit(y_true)
    if len(lb.classes_) == 1:
        if labels is None:
            raise ValueError('y_true contains only one label ({0}). Please '
                             'provide the true labels explicitly through the '
                             'labels argument.'.format(lb.classes_[0]))
        else:
            raise ValueError('The labels array needs to contain at least two '
                             'labels for log_loss, '
                             'got {0}.'.format(lb.classes_))
    transformed_labels = lb.transform(y_true)
    if transformed_labels.shape[1] == 1:
        transformed_labels = np.append(1 - transformed_labels,
                                       transformed_labels, axis=1)
        y_pred = np.clip(y_pred, eps, 1 - eps)
            if y_pred.ndim == 1:
        y_pred = y_pred[:, np.newaxis]
    if y_pred.shape[1] == 1:
        y_pred = np.append(1 - y_pred, y_pred, axis=1)
        transformed_labels = check_array(transformed_labels)
    if len(lb.classes_) != y_pred.shape[1]:
        if labels is None:
            raise ValueError("y_true and y_pred contain different number of "
                             "classes {0}, {1}. Please provide the true "
                             "labels explicitly through the labels argument. "
                             "Classes found in "
                             "y_true: {2}".format(transformed_labels.shape[1],
                                                  y_pred.shape[1],
                                                  lb.classes_))
        else:
            raise ValueError('The number of classes in labels is different '
                             'from that in y_pred. Classes found in '
                             'labels: {0}'.format(lb.classes_))
        y_pred /= y_pred.sum(axis=1)[:, np.newaxis]
    loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)
    return _weighted_sum(loss, sample_weight, normalize)

def hinge_loss(y_true, pred_decision, labels=None, sample_weight=None):
        check_consistent_length(y_true, pred_decision, sample_weight)
    pred_decision = check_array(pred_decision, ensure_2d=False)
    y_true = column_or_1d(y_true)
    y_true_unique = np.unique(y_true)
    if y_true_unique.size > 2:
        if (labels is None and pred_decision.ndim > 1 and
                (np.size(y_true_unique) != pred_decision.shape[1])):
            raise ValueError("Please include all labels in y_true "
                             "or pass labels as third argument")
        if labels is None:
            labels = y_true_unique
        le = LabelEncoder()
        le.fit(labels)
        y_true = le.transform(y_true)
        mask = np.ones_like(pred_decision, dtype=bool)
        mask[np.arange(y_true.shape[0]), y_true] = False
        margin = pred_decision[~mask]
        margin -= np.max(pred_decision[mask].reshape(y_true.shape[0], -1),
                         axis=1)
    else:
                                pred_decision = column_or_1d(pred_decision)
        pred_decision = np.ravel(pred_decision)
        lbin = LabelBinarizer(neg_label=-1)
        y_true = lbin.fit_transform(y_true)[:, 0]
        try:
            margin = y_true * pred_decision
        except TypeError:
            raise TypeError("pred_decision should be an array of floats.")
    losses = 1 - margin
        losses[losses <= 0] = 0
    return np.average(losses, weights=sample_weight)

def _check_binary_probabilistic_predictions(y_true, y_prob):
        check_consistent_length(y_true, y_prob)
    labels = np.unique(y_true)
    if len(labels) > 2:
        raise ValueError("Only binary classification is supported. "
                         "Provided labels %s." % labels)
    if y_prob.max() > 1:
        raise ValueError("y_prob contains values greater than 1.")
    if y_prob.min() < 0:
        raise ValueError("y_prob contains values less than 0.")
    return label_binarize(y_true, labels)[:, 0]

def brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):
        y_true = column_or_1d(y_true)
    y_prob = column_or_1d(y_prob)
    assert_all_finite(y_true)
    assert_all_finite(y_prob)
    if pos_label is None:
        pos_label = y_true.max()
    y_true = np.array(y_true == pos_label, int)
    y_true = _check_binary_probabilistic_predictions(y_true, y_prob)
    return np.average((y_true - y_prob) ** 2, weights=sample_weight)

import itertools
from functools import partial
import numpy as np
from scipy.spatial import distance
from scipy.sparse import csr_matrix
from scipy.sparse import issparse
from ..utils import check_array
from ..utils import gen_even_slices
from ..utils import gen_batches
from ..utils.extmath import row_norms, safe_sparse_dot
from ..preprocessing import normalize
from ..externals.joblib import Parallel
from ..externals.joblib import delayed
from ..externals.joblib import cpu_count
from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan

def _return_float_dtype(X, Y):
        if not issparse(X) and not isinstance(X, np.ndarray):
        X = np.asarray(X)
    if Y is None:
        Y_dtype = X.dtype
    elif not issparse(Y) and not isinstance(Y, np.ndarray):
        Y = np.asarray(Y)
        Y_dtype = Y.dtype
    else:
        Y_dtype = Y.dtype
    if X.dtype == Y_dtype == np.float32:
        dtype = np.float32
    else:
        dtype = np.float
    return X, Y, dtype

def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):
        X, Y, dtype_float = _return_float_dtype(X, Y)
    warn_on_dtype = dtype is not None
    estimator = 'check_pairwise_arrays'
    if dtype is None:
        dtype = dtype_float
    if Y is X or Y is None:
        X = Y = check_array(X, accept_sparse='csr', dtype=dtype,
                            warn_on_dtype=warn_on_dtype, estimator=estimator)
    else:
        X = check_array(X, accept_sparse='csr', dtype=dtype,
                        warn_on_dtype=warn_on_dtype, estimator=estimator)
        Y = check_array(Y, accept_sparse='csr', dtype=dtype,
                        warn_on_dtype=warn_on_dtype, estimator=estimator)
    if precomputed:
        if X.shape[1] != Y.shape[0]:
            raise ValueError("Precomputed metric requires shape "
                             "(n_queries, n_indexed). Got (%d, %d) "
                             "for %d indexed." %
                             (X.shape[0], X.shape[1], Y.shape[0]))
    elif X.shape[1] != Y.shape[1]:
        raise ValueError("Incompatible dimension for X and Y matrices: "
                         "X.shape[1] == %d while Y.shape[1] == %d" % (
                             X.shape[1], Y.shape[1]))
    return X, Y

def check_paired_arrays(X, Y):
        X, Y = check_pairwise_arrays(X, Y)
    if X.shape != Y.shape:
        raise ValueError("X and Y should be of same shape. They were "
                         "respectively %r and %r long." % (X.shape, Y.shape))
    return X, Y

def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,
                        X_norm_squared=None):
        X, Y = check_pairwise_arrays(X, Y)
    if X_norm_squared is not None:
        XX = check_array(X_norm_squared)
        if XX.shape == (1, X.shape[0]):
            XX = XX.T
        elif XX.shape != (X.shape[0], 1):
            raise ValueError(
                "Incompatible dimensions for X and X_norm_squared")
    else:
        XX = row_norms(X, squared=True)[:, np.newaxis]
    if X is Y:          YY = XX.T
    elif Y_norm_squared is not None:
        YY = np.atleast_2d(Y_norm_squared)
        if YY.shape != (1, Y.shape[0]):
            raise ValueError(
                "Incompatible dimensions for Y and Y_norm_squared")
    else:
        YY = row_norms(Y, squared=True)[np.newaxis, :]
    distances = safe_sparse_dot(X, Y.T, dense_output=True)
    distances *= -2
    distances += XX
    distances += YY
    np.maximum(distances, 0, out=distances)
    if X is Y:
                        distances.flat[::distances.shape[0] + 1] = 0.0
    return distances if squared else np.sqrt(distances, out=distances)

def pairwise_distances_argmin_min(X, Y, axis=1, metric="euclidean",
                                  batch_size=500, metric_kwargs=None):
        dist_func = None
    if metric in PAIRWISE_DISTANCE_FUNCTIONS:
        dist_func = PAIRWISE_DISTANCE_FUNCTIONS[metric]
    elif not callable(metric) and not isinstance(metric, str):
        raise ValueError("'metric' must be a string or a callable")
    X, Y = check_pairwise_arrays(X, Y)
    if metric_kwargs is None:
        metric_kwargs = {}
    if axis == 0:
        X, Y = Y, X
        indices = np.empty(X.shape[0], dtype=np.intp)
    values = np.empty(X.shape[0])
    values.fill(np.infty)
    for chunk_x in gen_batches(X.shape[0], batch_size):
        X_chunk = X[chunk_x, :]
        for chunk_y in gen_batches(Y.shape[0], batch_size):
            Y_chunk = Y[chunk_y, :]
            if dist_func is not None:
                if metric == 'euclidean':                      d_chunk = safe_sparse_dot(X_chunk, Y_chunk.T,
                                              dense_output=True)
                    d_chunk *= -2
                    d_chunk += row_norms(X_chunk, squared=True)[:, np.newaxis]
                    d_chunk += row_norms(Y_chunk, squared=True)[np.newaxis, :]
                    np.maximum(d_chunk, 0, d_chunk)
                else:
                    d_chunk = dist_func(X_chunk, Y_chunk, **metric_kwargs)
            else:
                d_chunk = pairwise_distances(X_chunk, Y_chunk,
                                             metric=metric, **metric_kwargs)
                        min_indices = d_chunk.argmin(axis=1)
            min_values = d_chunk[np.arange(chunk_x.stop - chunk_x.start),
                                 min_indices]
            flags = values[chunk_x] > min_values
            indices[chunk_x][flags] = min_indices[flags] + chunk_y.start
            values[chunk_x][flags] = min_values[flags]
    if metric == "euclidean" and not metric_kwargs.get("squared", False):
        np.sqrt(values, values)
    return indices, values

def pairwise_distances_argmin(X, Y, axis=1, metric="euclidean",
                              batch_size=500, metric_kwargs=None):
        if metric_kwargs is None:
        metric_kwargs = {}
    return pairwise_distances_argmin_min(X, Y, axis, metric, batch_size,
                                         metric_kwargs)[0]

def manhattan_distances(X, Y=None, sum_over_features=True,
                        size_threshold=5e8):
        X, Y = check_pairwise_arrays(X, Y)
    if issparse(X) or issparse(Y):
        if not sum_over_features:
            raise TypeError("sum_over_features=%r not supported"
                            " for sparse matrices" % sum_over_features)
        X = csr_matrix(X, copy=False)
        Y = csr_matrix(Y, copy=False)
        D = np.zeros((X.shape[0], Y.shape[0]))
        _sparse_manhattan(X.data, X.indices, X.indptr,
                          Y.data, Y.indices, Y.indptr,
                          X.shape[1], D)
        return D
    if sum_over_features:
        return distance.cdist(X, Y, 'cityblock')
    D = X[:, np.newaxis, :] - Y[np.newaxis, :, :]
    D = np.abs(D, D)
    return D.reshape((-1, X.shape[1]))

def cosine_distances(X, Y=None):
            S = cosine_similarity(X, Y)
    S *= -1
    S += 1
    np.clip(S, 0, 2, out=S)
    if X is Y or Y is None:
                        S[np.diag_indices_from(S)] = 0.0
    return S

def paired_euclidean_distances(X, Y):
        X, Y = check_paired_arrays(X, Y)
    return row_norms(X - Y)

def paired_manhattan_distances(X, Y):
        X, Y = check_paired_arrays(X, Y)
    diff = X - Y
    if issparse(diff):
        diff.data = np.abs(diff.data)
        return np.squeeze(np.array(diff.sum(axis=1)))
    else:
        return np.abs(diff).sum(axis=-1)

def paired_cosine_distances(X, Y):
        X, Y = check_paired_arrays(X, Y)
    return .5 * row_norms(normalize(X) - normalize(Y), squared=True)

PAIRED_DISTANCES = {
    'cosine': paired_cosine_distances,
    'euclidean': paired_euclidean_distances,
    'l2': paired_euclidean_distances,
    'l1': paired_manhattan_distances,
    'manhattan': paired_manhattan_distances,
    'cityblock': paired_manhattan_distances}

def paired_distances(X, Y, metric="euclidean", **kwds):
    
    if metric in PAIRED_DISTANCES:
        func = PAIRED_DISTANCES[metric]
        return func(X, Y)
    elif callable(metric):
                X, Y = check_paired_arrays(X, Y)
        distances = np.zeros(len(X))
        for i in range(len(X)):
            distances[i] = metric(X[i], Y[i])
        return distances
    else:
        raise ValueError('Unknown distance %s' % metric)

def linear_kernel(X, Y=None):
        X, Y = check_pairwise_arrays(X, Y)
    return safe_sparse_dot(X, Y.T, dense_output=True)

def polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1):
        X, Y = check_pairwise_arrays(X, Y)
    if gamma is None:
        gamma = 1.0 / X.shape[1]
    K = safe_sparse_dot(X, Y.T, dense_output=True)
    K *= gamma
    K += coef0
    K **= degree
    return K

def sigmoid_kernel(X, Y=None, gamma=None, coef0=1):
        X, Y = check_pairwise_arrays(X, Y)
    if gamma is None:
        gamma = 1.0 / X.shape[1]
    K = safe_sparse_dot(X, Y.T, dense_output=True)
    K *= gamma
    K += coef0
    np.tanh(K, K)       return K

def rbf_kernel(X, Y=None, gamma=None):
        X, Y = check_pairwise_arrays(X, Y)
    if gamma is None:
        gamma = 1.0 / X.shape[1]
    K = euclidean_distances(X, Y, squared=True)
    K *= -gamma
    np.exp(K, K)        return K

def laplacian_kernel(X, Y=None, gamma=None):
        X, Y = check_pairwise_arrays(X, Y)
    if gamma is None:
        gamma = 1.0 / X.shape[1]
    K = -gamma * manhattan_distances(X, Y)
    np.exp(K, K)        return K

def cosine_similarity(X, Y=None, dense_output=True):
        
    X, Y = check_pairwise_arrays(X, Y)
    X_normalized = normalize(X, copy=True)
    if X is Y:
        Y_normalized = X_normalized
    else:
        Y_normalized = normalize(Y, copy=True)
    K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)
    return K

def additive_chi2_kernel(X, Y=None):
        if issparse(X) or issparse(Y):
        raise ValueError("additive_chi2 does not support sparse matrices.")
    X, Y = check_pairwise_arrays(X, Y)
    if (X < 0).any():
        raise ValueError("X contains negative values.")
    if Y is not X and (Y < 0).any():
        raise ValueError("Y contains negative values.")
    result = np.zeros((X.shape[0], Y.shape[0]), dtype=X.dtype)
    _chi2_kernel_fast(X, Y, result)
    return result

def chi2_kernel(X, Y=None, gamma=1.):
        K = additive_chi2_kernel(X, Y)
    K *= gamma
    return np.exp(K, K)

PAIRWISE_DISTANCE_FUNCTIONS = {
            'cityblock': manhattan_distances,
    'cosine': cosine_distances,
    'euclidean': euclidean_distances,
    'l2': euclidean_distances,
    'l1': manhattan_distances,
    'manhattan': manhattan_distances,
    'precomputed': None,  }

def distance_metrics():
        return PAIRWISE_DISTANCE_FUNCTIONS

def _parallel_pairwise(X, Y, func, n_jobs, **kwds):
        if n_jobs < 0:
        n_jobs = max(cpu_count() + 1 + n_jobs, 1)
    if Y is None:
        Y = X
    if n_jobs == 1:
                return func(X, Y, **kwds)
        fd = delayed(func)
    ret = Parallel(n_jobs=n_jobs, verbose=0)(
        fd(X, Y[s], **kwds)
        for s in gen_even_slices(Y.shape[0], n_jobs))
    return np.hstack(ret)

def _pairwise_callable(X, Y, metric, **kwds):
        X, Y = check_pairwise_arrays(X, Y)
    if X is Y:
                out = np.zeros((X.shape[0], Y.shape[0]), dtype='float')
        iterator = itertools.combinations(range(X.shape[0]), 2)
        for i, j in iterator:
            out[i, j] = metric(X[i], Y[j], **kwds)
                        out = out + out.T
                        for i in range(X.shape[0]):
            x = X[i]
            out[i, i] = metric(x, x, **kwds)
    else:
                out = np.empty((X.shape[0], Y.shape[0]), dtype='float')
        iterator = itertools.product(range(X.shape[0]), range(Y.shape[0]))
        for i, j in iterator:
            out[i, j] = metric(X[i], Y[j], **kwds)
    return out

_VALID_METRICS = ['euclidean', 'l2', 'l1', 'manhattan', 'cityblock',
                  'braycurtis', 'canberra', 'chebyshev', 'correlation',
                  'cosine', 'dice', 'hamming', 'jaccard', 'kulsinski',
                  'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
                  'russellrao', 'seuclidean', 'sokalmichener',
                  'sokalsneath', 'sqeuclidean', 'yule', "wminkowski"]

def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=1, **kwds):
        if (metric not in _VALID_METRICS and
            not callable(metric) and metric != "precomputed"):
        raise ValueError("Unknown metric %s. "
                         "Valid metrics are %s, or 'precomputed', or a "
                         "callable" % (metric, _VALID_METRICS))
    if metric == "precomputed":
        X, _ = check_pairwise_arrays(X, Y, precomputed=True)
        return X
    elif metric in PAIRWISE_DISTANCE_FUNCTIONS:
        func = PAIRWISE_DISTANCE_FUNCTIONS[metric]
    elif callable(metric):
        func = partial(_pairwise_callable, metric=metric, **kwds)
    else:
        if issparse(X) or issparse(Y):
            raise TypeError("scipy distance metrics do not"
                            " support sparse matrices.")
        dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else None
        X, Y = check_pairwise_arrays(X, Y, dtype=dtype)
        if n_jobs == 1 and X is Y:
            return distance.squareform(distance.pdist(X, metric=metric,
                                                      **kwds))
        func = partial(distance.cdist, metric=metric, **kwds)
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)

PAIRWISE_BOOLEAN_FUNCTIONS = [
    'dice',
    'jaccard',
    'kulsinski',
    'matching',
    'rogerstanimoto',
    'russellrao',
    'sokalmichener',
    'sokalsneath',
    'yule',
]

PAIRWISE_KERNEL_FUNCTIONS = {
            'additive_chi2': additive_chi2_kernel,
    'chi2': chi2_kernel,
    'linear': linear_kernel,
    'polynomial': polynomial_kernel,
    'poly': polynomial_kernel,
    'rbf': rbf_kernel,
    'laplacian': laplacian_kernel,
    'sigmoid': sigmoid_kernel,
    'cosine': cosine_similarity, }

def kernel_metrics():
        return PAIRWISE_KERNEL_FUNCTIONS

KERNEL_PARAMS = {
    "additive_chi2": (),
    "chi2": (),
    "cosine": (),
    "exp_chi2": frozenset(["gamma"]),
    "linear": (),
    "poly": frozenset(["gamma", "degree", "coef0"]),
    "polynomial": frozenset(["gamma", "degree", "coef0"]),
    "rbf": frozenset(["gamma"]),
    "laplacian": frozenset(["gamma"]),
    "sigmoid": frozenset(["gamma", "coef0"]),
}

def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
                     n_jobs=1, **kwds):
            from ..gaussian_process.kernels import Kernel as GPKernel
    if metric == "precomputed":
        X, _ = check_pairwise_arrays(X, Y, precomputed=True)
        return X
    elif isinstance(metric, GPKernel):
        func = metric.__call__
    elif metric in PAIRWISE_KERNEL_FUNCTIONS:
        if filter_params:
            kwds = dict((k, kwds[k]) for k in kwds
                        if k in KERNEL_PARAMS[metric])
        func = PAIRWISE_KERNEL_FUNCTIONS[metric]
    elif callable(metric):
        func = partial(_pairwise_callable, metric=metric, **kwds)
    else:
        raise ValueError("Unknown kernel %r" % metric)
    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)

from libc.string cimport memset
import numpy as np
cimport numpy as np
cdef extern from "cblas.h":
    double cblas_dasum(int, const double *, int) nogil
ctypedef float [:, :] float_array_2d_t
ctypedef double [:, :] double_array_2d_t
cdef fused floating1d:
    float[::1]
    double[::1]
cdef fused floating_array_2d_t:
    float_array_2d_t
    double_array_2d_t

np.import_array()

def _chi2_kernel_fast(floating_array_2d_t X,
                      floating_array_2d_t Y,
                      floating_array_2d_t result):
    cdef np.npy_intp i, j, k
    cdef np.npy_intp n_samples_X = X.shape[0]
    cdef np.npy_intp n_samples_Y = Y.shape[0]
    cdef np.npy_intp n_features = X.shape[1]
    cdef double res, nom, denom
    with nogil:
        for i in range(n_samples_X):
            for j in range(n_samples_Y):
                res = 0
                for k in range(n_features):
                    denom = (X[i, k] - Y[j, k])
                    nom = (X[i, k] + Y[j, k])
                    if nom != 0:
                        res  += denom * denom / nom
                result[i, j] = -res

def _sparse_manhattan(floating1d X_data, int[:] X_indices, int[:] X_indptr,
                      floating1d Y_data, int[:] Y_indices, int[:] Y_indptr,
                      np.npy_intp n_features, double[:, ::1] D):
        cdef double[::1] row = np.empty(n_features)
    cdef np.npy_intp ix, iy, j
    with nogil:
        for ix in range(D.shape[0]):
            for iy in range(D.shape[1]):
                                                memset(&row[0], 0, n_features * sizeof(double))
                for j in range(X_indptr[ix], X_indptr[ix + 1]):
                    row[X_indices[j]] = X_data[j]
                for j in range(Y_indptr[iy], Y_indptr[iy + 1]):
                    row[Y_indices[j]] -= Y_data[j]
                D[ix, iy] = cblas_dasum(n_features, &row[0], 1)

from __future__ import division
import warnings
import numpy as np
from scipy.sparse import csr_matrix
from ..utils import assert_all_finite
from ..utils import check_consistent_length
from ..utils import column_or_1d, check_array
from ..utils.multiclass import type_of_target
from ..utils.extmath import stable_cumsum
from ..utils.fixes import bincount
from ..utils.fixes import array_equal
from ..utils.stats import rankdata
from ..utils.sparsefuncs import count_nonzero
from ..exceptions import UndefinedMetricWarning
from .base import _average_binary_score

def auc(x, y, reorder=False):
        check_consistent_length(x, y)
    x = column_or_1d(x)
    y = column_or_1d(y)
    if x.shape[0] < 2:
        raise ValueError('At least 2 points are needed to compute'
                         ' area under curve, but x.shape = %s' % x.shape)
    direction = 1
    if reorder:
                        order = np.lexsort((y, x))
        x, y = x[order], y[order]
    else:
        dx = np.diff(x)
        if np.any(dx < 0):
            if np.all(dx <= 0):
                direction = -1
            else:
                raise ValueError("Reordering is not turned on, and "
                                 "the x array is not increasing: %s" % x)
    area = direction * np.trapz(y, x)
    if isinstance(area, np.memmap):
                                area = area.dtype.type(area)
    return area

def average_precision_score(y_true, y_score, average="macro",
                            sample_weight=None):
        def _binary_average_precision(y_true, y_score, sample_weight=None):
        precision, recall, thresholds = precision_recall_curve(
            y_true, y_score, sample_weight=sample_weight)
        return auc(recall, precision)
    return _average_binary_score(_binary_average_precision, y_true, y_score,
                                 average, sample_weight=sample_weight)

def roc_auc_score(y_true, y_score, average="macro", sample_weight=None):
        def _binary_roc_auc_score(y_true, y_score, sample_weight=None):
        if len(np.unique(y_true)) != 2:
            raise ValueError("Only one class present in y_true. ROC AUC score "
                             "is not defined in that case.")
        fpr, tpr, tresholds = roc_curve(y_true, y_score,
                                        sample_weight=sample_weight)
        return auc(fpr, tpr, reorder=True)
    return _average_binary_score(
        _binary_roc_auc_score, y_true, y_score, average,
        sample_weight=sample_weight)

def _binary_clf_curve(y_true, y_score, pos_label=None, sample_weight=None):
        check_consistent_length(y_true, y_score)
    y_true = column_or_1d(y_true)
    y_score = column_or_1d(y_score)
    assert_all_finite(y_true)
    assert_all_finite(y_score)
    if sample_weight is not None:
        sample_weight = column_or_1d(sample_weight)
        classes = np.unique(y_true)
    if (pos_label is None and
        not (array_equal(classes, [0, 1]) or
             array_equal(classes, [-1, 1]) or
             array_equal(classes, [0]) or
             array_equal(classes, [-1]) or
             array_equal(classes, [1]))):
        raise ValueError("Data is not binary and pos_label is not specified")
    elif pos_label is None:
        pos_label = 1.
        y_true = (y_true == pos_label)
        desc_score_indices = np.argsort(y_score, kind="mergesort")[::-1]
    y_score = y_score[desc_score_indices]
    y_true = y_true[desc_score_indices]
    if sample_weight is not None:
        weight = sample_weight[desc_score_indices]
    else:
        weight = 1.
                distinct_value_indices = np.where(np.diff(y_score))[0]
    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]
        tps = stable_cumsum(y_true * weight)[threshold_idxs]
    if sample_weight is not None:
        fps = stable_cumsum(weight)[threshold_idxs] - tps
    else:
        fps = 1 + threshold_idxs - tps
    return fps, tps, y_score[threshold_idxs]

def precision_recall_curve(y_true, probas_pred, pos_label=None,
                           sample_weight=None):
        fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,
                                             pos_label=pos_label,
                                             sample_weight=sample_weight)
    precision = tps / (tps + fps)
    recall = tps / tps[-1]
            last_ind = tps.searchsorted(tps[-1])
    sl = slice(last_ind, None, -1)
    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]

def roc_curve(y_true, y_score, pos_label=None, sample_weight=None,
              drop_intermediate=True):
        fps, tps, thresholds = _binary_clf_curve(
        y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
                                        if drop_intermediate and len(fps) > 2:
        optimal_idxs = np.where(np.r_[True,
                                      np.logical_or(np.diff(fps, 2),
                                                    np.diff(tps, 2)),
                                      True])[0]
        fps = fps[optimal_idxs]
        tps = tps[optimal_idxs]
        thresholds = thresholds[optimal_idxs]
    if tps.size == 0 or fps[0] != 0:
                tps = np.r_[0, tps]
        fps = np.r_[0, fps]
        thresholds = np.r_[thresholds[0] + 1, thresholds]
    if fps[-1] <= 0:
        warnings.warn("No negative samples in y_true, "
                      "false positive value should be meaningless",
                      UndefinedMetricWarning)
        fpr = np.repeat(np.nan, fps.shape)
    else:
        fpr = fps / fps[-1]
    if tps[-1] <= 0:
        warnings.warn("No positive samples in y_true, "
                      "true positive value should be meaningless",
                      UndefinedMetricWarning)
        tpr = np.repeat(np.nan, tps.shape)
    else:
        tpr = tps / tps[-1]
    return fpr, tpr, thresholds

def label_ranking_average_precision_score(y_true, y_score):
        check_consistent_length(y_true, y_score)
    y_true = check_array(y_true, ensure_2d=False)
    y_score = check_array(y_score, ensure_2d=False)
    if y_true.shape != y_score.shape:
        raise ValueError("y_true and y_score have different shape")
        y_type = type_of_target(y_true)
    if (y_type != "multilabel-indicator" and
            not (y_type == "binary" and y_true.ndim == 2)):
        raise ValueError("{0} format is not supported".format(y_type))
    y_true = csr_matrix(y_true)
    y_score = -y_score
    n_samples, n_labels = y_true.shape
    out = 0.
    for i, (start, stop) in enumerate(zip(y_true.indptr, y_true.indptr[1:])):
        relevant = y_true.indices[start:stop]
        if (relevant.size == 0 or relevant.size == n_labels):
                                    out += 1.
            continue
        scores_i = y_score[i]
        rank = rankdata(scores_i, 'max')[relevant]
        L = rankdata(scores_i[relevant], 'max')
        out += (L / rank).mean()
    return out / n_samples

def coverage_error(y_true, y_score, sample_weight=None):
        y_true = check_array(y_true, ensure_2d=False)
    y_score = check_array(y_score, ensure_2d=False)
    check_consistent_length(y_true, y_score, sample_weight)
    y_type = type_of_target(y_true)
    if y_type != "multilabel-indicator":
        raise ValueError("{0} format is not supported".format(y_type))
    if y_true.shape != y_score.shape:
        raise ValueError("y_true and y_score have different shape")
    y_score_mask = np.ma.masked_array(y_score, mask=np.logical_not(y_true))
    y_min_relevant = y_score_mask.min(axis=1).reshape((-1, 1))
    coverage = (y_score >= y_min_relevant).sum(axis=1)
    coverage = coverage.filled(0)
    return np.average(coverage, weights=sample_weight)

def label_ranking_loss(y_true, y_score, sample_weight=None):
        y_true = check_array(y_true, ensure_2d=False, accept_sparse='csr')
    y_score = check_array(y_score, ensure_2d=False)
    check_consistent_length(y_true, y_score, sample_weight)
    y_type = type_of_target(y_true)
    if y_type not in ("multilabel-indicator",):
        raise ValueError("{0} format is not supported".format(y_type))
    if y_true.shape != y_score.shape:
        raise ValueError("y_true and y_score have different shape")
    n_samples, n_labels = y_true.shape
    y_true = csr_matrix(y_true)
    loss = np.zeros(n_samples)
    for i, (start, stop) in enumerate(zip(y_true.indptr, y_true.indptr[1:])):
                unique_scores, unique_inverse = np.unique(y_score[i],
                                                  return_inverse=True)
        true_at_reversed_rank = bincount(
            unique_inverse[y_true.indices[start:stop]],
            minlength=len(unique_scores))
        all_at_reversed_rank = bincount(unique_inverse,
                                        minlength=len(unique_scores))
        false_at_reversed_rank = all_at_reversed_rank - true_at_reversed_rank
                                        loss[i] = np.dot(true_at_reversed_rank.cumsum(),
                         false_at_reversed_rank)
    n_positives = count_nonzero(y_true, axis=1)
    with np.errstate(divide="ignore", invalid="ignore"):
        loss /= ((n_labels - n_positives) * n_positives)
            loss[np.logical_or(n_positives == 0, n_positives == n_labels)] = 0.
    return np.average(loss, weights=sample_weight)

from __future__ import division
import numpy as np
from ..utils.validation import check_array, check_consistent_length
from ..utils.validation import column_or_1d
from ..externals.six import string_types

__ALL__ = [
    "mean_absolute_error",
    "mean_squared_error",
    "mean_squared_log_error",
    "median_absolute_error",
    "r2_score",
    "explained_variance_score"
]

def _check_reg_targets(y_true, y_pred, multioutput):
        check_consistent_length(y_true, y_pred)
    y_true = check_array(y_true, ensure_2d=False)
    y_pred = check_array(y_pred, ensure_2d=False)
    if y_true.ndim == 1:
        y_true = y_true.reshape((-1, 1))
    if y_pred.ndim == 1:
        y_pred = y_pred.reshape((-1, 1))
    if y_true.shape[1] != y_pred.shape[1]:
        raise ValueError("y_true and y_pred have different number of output "
                         "({0}!={1})".format(y_true.shape[1], y_pred.shape[1]))
    n_outputs = y_true.shape[1]
    allowed_multioutput_str = ('raw_values', 'uniform_average',
                               'variance_weighted')
    if isinstance(multioutput, string_types):
        if multioutput not in allowed_multioutput_str:
            raise ValueError("Allowed 'multioutput' string values are {}. "
                             "You provided multioutput={!r}".format(
                                 allowed_multioutput_str,
                                 multioutput))
    elif multioutput is not None:
        multioutput = check_array(multioutput, ensure_2d=False)
        if n_outputs == 1:
            raise ValueError("Custom weights are useful only in "
                             "multi-output cases.")
        elif n_outputs != len(multioutput):
            raise ValueError(("There must be equally many custom weights "
                              "(%d) as outputs (%d).") %
                             (len(multioutput), n_outputs))
    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'
    return y_type, y_true, y_pred, multioutput

def mean_absolute_error(y_true, y_pred,
                        sample_weight=None,
                        multioutput='uniform_average'):
        y_type, y_true, y_pred, multioutput = _check_reg_targets(
        y_true, y_pred, multioutput)
    output_errors = np.average(np.abs(y_pred - y_true),
                               weights=sample_weight, axis=0)
    if isinstance(multioutput, string_types):
        if multioutput == 'raw_values':
            return output_errors
        elif multioutput == 'uniform_average':
                        multioutput = None
    return np.average(output_errors, weights=multioutput)

def mean_squared_error(y_true, y_pred,
                       sample_weight=None,
                       multioutput='uniform_average'):
        y_type, y_true, y_pred, multioutput = _check_reg_targets(
        y_true, y_pred, multioutput)
    output_errors = np.average((y_true - y_pred) ** 2, axis=0,
                               weights=sample_weight)
    if isinstance(multioutput, string_types):
        if multioutput == 'raw_values':
            return output_errors
        elif multioutput == 'uniform_average':
                        multioutput = None
    return np.average(output_errors, weights=multioutput)

def mean_squared_log_error(y_true, y_pred,
                           sample_weight=None,
                           multioutput='uniform_average'):
        y_type, y_true, y_pred, multioutput = _check_reg_targets(
        y_true, y_pred, multioutput)
    if not (y_true >= 0).all() and not (y_pred >= 0).all():
        raise ValueError("Mean Squared Logarithmic Error cannot be used when "
                         "targets contain negative values.")
    return mean_squared_error(np.log(y_true + 1), np.log(y_pred + 1),
                              sample_weight, multioutput)

def median_absolute_error(y_true, y_pred):
        y_type, y_true, y_pred, _ = _check_reg_targets(y_true, y_pred,
                                                   'uniform_average')
    if y_type == 'continuous-multioutput':
        raise ValueError("Multioutput not supported in median_absolute_error")
    return np.median(np.abs(y_pred - y_true))

def explained_variance_score(y_true, y_pred,
                             sample_weight=None,
                             multioutput='uniform_average'):
        y_type, y_true, y_pred, multioutput = _check_reg_targets(
        y_true, y_pred, multioutput)
    y_diff_avg = np.average(y_true - y_pred, weights=sample_weight, axis=0)
    numerator = np.average((y_true - y_pred - y_diff_avg) ** 2,
                           weights=sample_weight, axis=0)
    y_true_avg = np.average(y_true, weights=sample_weight, axis=0)
    denominator = np.average((y_true - y_true_avg) ** 2,
                             weights=sample_weight, axis=0)
    nonzero_numerator = numerator != 0
    nonzero_denominator = denominator != 0
    valid_score = nonzero_numerator & nonzero_denominator
    output_scores = np.ones(y_true.shape[1])
    output_scores[valid_score] = 1 - (numerator[valid_score] /
                                      denominator[valid_score])
    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.
    if isinstance(multioutput, string_types):
        if multioutput == 'raw_values':
                        return output_scores
        elif multioutput == 'uniform_average':
                        avg_weights = None
        elif multioutput == 'variance_weighted':
            avg_weights = denominator
    else:
        avg_weights = multioutput
    return np.average(output_scores, weights=avg_weights)

def r2_score(y_true, y_pred, sample_weight=None,
             multioutput="uniform_average"):
        y_type, y_true, y_pred, multioutput = _check_reg_targets(
        y_true, y_pred, multioutput)
    if sample_weight is not None:
        sample_weight = column_or_1d(sample_weight)
        weight = sample_weight[:, np.newaxis]
    else:
        weight = 1.
    numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,
                                                      dtype=np.float64)
    denominator = (weight * (y_true - np.average(
        y_true, axis=0, weights=sample_weight)) ** 2).sum(axis=0,
                                                          dtype=np.float64)
    nonzero_denominator = denominator != 0
    nonzero_numerator = numerator != 0
    valid_score = nonzero_denominator & nonzero_numerator
    output_scores = np.ones([y_true.shape[1]])
    output_scores[valid_score] = 1 - (numerator[valid_score] /
                                      denominator[valid_score])
            output_scores[nonzero_numerator & ~nonzero_denominator] = 0.
    if isinstance(multioutput, string_types):
        if multioutput == 'raw_values':
                        return output_scores
        elif multioutput == 'uniform_average':
                        avg_weights = None
        elif multioutput == 'variance_weighted':
            avg_weights = denominator
                        if not np.any(nonzero_denominator):
                if not np.any(nonzero_numerator):
                    return 1.0
                else:
                    return 0.0
    else:
        avg_weights = multioutput
    return np.average(output_scores, weights=avg_weights)

from abc import ABCMeta, abstractmethod
import warnings
import numpy as np
from . import (r2_score, median_absolute_error, mean_absolute_error,
               mean_squared_error, mean_squared_log_error, accuracy_score,
               f1_score, roc_auc_score, average_precision_score,
               precision_score, recall_score, log_loss)
from .cluster import adjusted_rand_score
from .cluster import homogeneity_score
from .cluster import completeness_score
from .cluster import v_measure_score
from .cluster import mutual_info_score
from .cluster import adjusted_mutual_info_score
from .cluster import normalized_mutual_info_score
from .cluster import fowlkes_mallows_score
from ..utils.multiclass import type_of_target
from ..externals import six
from ..base import is_regressor

class _BaseScorer(six.with_metaclass(ABCMeta, object)):
    def __init__(self, score_func, sign, kwargs):
        self._kwargs = kwargs
        self._score_func = score_func
        self._sign = sign
                        self._deprecation_msg = None
    @abstractmethod
    def __call__(self, estimator, X, y, sample_weight=None):
        if self._deprecation_msg is not None:
            warnings.warn(self._deprecation_msg,
                          category=DeprecationWarning,
                          stacklevel=2)
    def __repr__(self):
        kwargs_string = "".join([", %s=%s" % (str(k), str(v))
                                 for k, v in self._kwargs.items()])
        return ("make_scorer(%s%s%s%s)"
                % (self._score_func.__name__,
                   "" if self._sign > 0 else ", greater_is_better=False",
                   self._factory_args(), kwargs_string))
    def _factory_args(self):
                return ""

class _PredictScorer(_BaseScorer):
    def __call__(self, estimator, X, y_true, sample_weight=None):
                super(_PredictScorer, self).__call__(estimator, X, y_true,
                                             sample_weight=sample_weight)
        y_pred = estimator.predict(X)
        if sample_weight is not None:
            return self._sign * self._score_func(y_true, y_pred,
                                                 sample_weight=sample_weight,
                                                 **self._kwargs)
        else:
            return self._sign * self._score_func(y_true, y_pred,
                                                 **self._kwargs)

class _ProbaScorer(_BaseScorer):
    def __call__(self, clf, X, y, sample_weight=None):
                super(_ProbaScorer, self).__call__(clf, X, y,
                                           sample_weight=sample_weight)
        y_pred = clf.predict_proba(X)
        if sample_weight is not None:
            return self._sign * self._score_func(y, y_pred,
                                                 sample_weight=sample_weight,
                                                 **self._kwargs)
        else:
            return self._sign * self._score_func(y, y_pred, **self._kwargs)
    def _factory_args(self):
        return ", needs_proba=True"

class _ThresholdScorer(_BaseScorer):
    def __call__(self, clf, X, y, sample_weight=None):
                super(_ThresholdScorer, self).__call__(clf, X, y,
                                               sample_weight=sample_weight)
        y_type = type_of_target(y)
        if y_type not in ("binary", "multilabel-indicator"):
            raise ValueError("{0} format is not supported".format(y_type))
        if is_regressor(clf):
            y_pred = clf.predict(X)
        else:
            try:
                y_pred = clf.decision_function(X)
                                if isinstance(y_pred, list):
                    y_pred = np.vstack(p for p in y_pred).T
            except (NotImplementedError, AttributeError):
                y_pred = clf.predict_proba(X)
                if y_type == "binary":
                    y_pred = y_pred[:, 1]
                elif isinstance(y_pred, list):
                    y_pred = np.vstack([p[:, -1] for p in y_pred]).T
        if sample_weight is not None:
            return self._sign * self._score_func(y, y_pred,
                                                 sample_weight=sample_weight,
                                                 **self._kwargs)
        else:
            return self._sign * self._score_func(y, y_pred, **self._kwargs)
    def _factory_args(self):
        return ", needs_threshold=True"

def get_scorer(scoring):
    if isinstance(scoring, six.string_types):
        try:
            scorer = SCORERS[scoring]
        except KeyError:
            scorers = [scorer for scorer in SCORERS
                       if SCORERS[scorer]._deprecation_msg is None]
            raise ValueError('%r is not a valid scoring value. '
                             'Valid options are %s'
                             % (scoring, sorted(scorers)))
    else:
        scorer = scoring
    return scorer

def _passthrough_scorer(estimator, *args, **kwargs):
        return estimator.score(*args, **kwargs)

def check_scoring(estimator, scoring=None, allow_none=False):
        has_scoring = scoring is not None
    if not hasattr(estimator, 'fit'):
        raise TypeError("estimator should be an estimator implementing "
                        "'fit' method, %r was passed" % estimator)
    if isinstance(scoring, six.string_types):
        return get_scorer(scoring)
    elif has_scoring:
                module = getattr(scoring, '__module__', None)
        if hasattr(module, 'startswith') and \
           module.startswith('sklearn.metrics.') and \
           not module.startswith('sklearn.metrics.scorer') and \
           not module.startswith('sklearn.metrics.tests.'):
            raise ValueError('scoring value %r looks like it is a metric '
                             'function rather than a scorer. A scorer should '
                             'require an estimator as its first parameter. '
                             'Please use `make_scorer` to convert a metric '
                             'to a scorer.' % scoring)
        return get_scorer(scoring)
    elif hasattr(estimator, 'score'):
        return _passthrough_scorer
    elif allow_none:
        return None
    else:
        raise TypeError(
            "If no scoring is specified, the estimator passed should "
            "have a 'score' method. The estimator %r does not." % estimator)

def make_scorer(score_func, greater_is_better=True, needs_proba=False,
                needs_threshold=False, **kwargs):
        sign = 1 if greater_is_better else -1
    if needs_proba and needs_threshold:
        raise ValueError("Set either needs_proba or needs_threshold to True,"
                         " but not both.")
    if needs_proba:
        cls = _ProbaScorer
    elif needs_threshold:
        cls = _ThresholdScorer
    else:
        cls = _PredictScorer
    return cls(score_func, sign, kwargs)

r2_scorer = make_scorer(r2_score)
neg_mean_squared_error_scorer = make_scorer(mean_squared_error,
                                            greater_is_better=False)
deprecation_msg = ('Scoring method mean_squared_error was renamed to '
                   'neg_mean_squared_error in version 0.18 and will '
                   'be removed in 0.20.')
mean_squared_error_scorer = make_scorer(mean_squared_error,
                                        greater_is_better=False)
mean_squared_error_scorer._deprecation_msg = deprecation_msg
neg_mean_squared_log_error_scorer = make_scorer(mean_squared_log_error,
                                                greater_is_better=False)
neg_mean_absolute_error_scorer = make_scorer(mean_absolute_error,
                                             greater_is_better=False)
deprecation_msg = ('Scoring method mean_absolute_error was renamed to '
                   'neg_mean_absolute_error in version 0.18 and will '
                   'be removed in 0.20.')
mean_absolute_error_scorer = make_scorer(mean_absolute_error,
                                         greater_is_better=False)
mean_absolute_error_scorer._deprecation_msg = deprecation_msg
neg_median_absolute_error_scorer = make_scorer(median_absolute_error,
                                               greater_is_better=False)
deprecation_msg = ('Scoring method median_absolute_error was renamed to '
                   'neg_median_absolute_error in version 0.18 and will '
                   'be removed in 0.20.')
median_absolute_error_scorer = make_scorer(median_absolute_error,
                                           greater_is_better=False)
median_absolute_error_scorer._deprecation_msg = deprecation_msg

accuracy_scorer = make_scorer(accuracy_score)
f1_scorer = make_scorer(f1_score)
roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True,
                             needs_threshold=True)
average_precision_scorer = make_scorer(average_precision_score,
                                       needs_threshold=True)
precision_scorer = make_scorer(precision_score)
recall_scorer = make_scorer(recall_score)
neg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False,
                                  needs_proba=True)
deprecation_msg = ('Scoring method log_loss was renamed to '
                   'neg_log_loss in version 0.18 and will be removed in 0.20.')
log_loss_scorer = make_scorer(log_loss, greater_is_better=False,
                              needs_proba=True)
log_loss_scorer._deprecation_msg = deprecation_msg

adjusted_rand_scorer = make_scorer(adjusted_rand_score)
homogeneity_scorer = make_scorer(homogeneity_score)
completeness_scorer = make_scorer(completeness_score)
v_measure_scorer = make_scorer(v_measure_score)
mutual_info_scorer = make_scorer(mutual_info_score)
adjusted_mutual_info_scorer = make_scorer(adjusted_mutual_info_score)
normalized_mutual_info_scorer = make_scorer(normalized_mutual_info_score)
fowlkes_mallows_scorer = make_scorer(fowlkes_mallows_score)

SCORERS = dict(r2=r2_scorer,
               neg_median_absolute_error=neg_median_absolute_error_scorer,
               neg_mean_absolute_error=neg_mean_absolute_error_scorer,
               neg_mean_squared_error=neg_mean_squared_error_scorer,
               neg_mean_squared_log_error=neg_mean_squared_log_error_scorer,
               median_absolute_error=median_absolute_error_scorer,
               mean_absolute_error=mean_absolute_error_scorer,
               mean_squared_error=mean_squared_error_scorer,
               accuracy=accuracy_scorer, roc_auc=roc_auc_scorer,
               average_precision=average_precision_scorer,
               log_loss=log_loss_scorer,
               neg_log_loss=neg_log_loss_scorer,
                              adjusted_rand_score=adjusted_rand_scorer,
               homogeneity_score=homogeneity_scorer,
               completeness_score=completeness_scorer,
               v_measure_score=v_measure_scorer,
               mutual_info_score=mutual_info_scorer,
               adjusted_mutual_info_score=adjusted_mutual_info_scorer,
               normalized_mutual_info_score=normalized_mutual_info_scorer,
               fowlkes_mallows_score=fowlkes_mallows_scorer)

for name, metric in [('precision', precision_score),
                     ('recall', recall_score), ('f1', f1_score)]:
    SCORERS[name] = make_scorer(metric)
    for average in ['macro', 'micro', 'samples', 'weighted']:
        qualified_name = '{0}_{1}'.format(name, average)
        SCORERS[qualified_name] = make_scorer(metric, pos_label=None,
                                              average=average)
import os
import os.path
import numpy
from numpy.distutils.misc_util import Configuration
from sklearn._build_utils import get_blas_info

def configuration(parent_package="", top_path=None):
    config = Configuration("metrics", parent_package, top_path)
    cblas_libs, blas_info = get_blas_info()
    if os.name == 'posix':
        cblas_libs.append('m')
    config.add_extension("pairwise_fast",
                         sources=["pairwise_fast.pyx"],
                         include_dirs=[os.path.join('..', 'src', 'cblas'),
                                       numpy.get_include(),
                                       blas_info.pop('include_dirs', [])],
                         libraries=cblas_libs,
                         extra_compile_args=blas_info.pop('extra_compile_args',
                                                          []),
                         **blas_info)
    config.add_subpackage('tests')
    return config
if __name__ == "__main__":
    from numpy.distutils.core import setup
    setup(**configuration().todict())

from .ranking import auc
from .ranking import average_precision_score
from .ranking import coverage_error
from .ranking import label_ranking_average_precision_score
from .ranking import label_ranking_loss
from .ranking import precision_recall_curve
from .ranking import roc_auc_score
from .ranking import roc_curve
from .classification import accuracy_score
from .classification import classification_report
from .classification import cohen_kappa_score
from .classification import confusion_matrix
from .classification import f1_score
from .classification import fbeta_score
from .classification import hamming_loss
from .classification import hinge_loss
from .classification import jaccard_similarity_score
from .classification import log_loss
from .classification import matthews_corrcoef
from .classification import precision_recall_fscore_support
from .classification import precision_score
from .classification import recall_score
from .classification import zero_one_loss
from .classification import brier_score_loss
from . import cluster
from .cluster import adjusted_mutual_info_score
from .cluster import adjusted_rand_score
from .cluster import completeness_score
from .cluster import consensus_score
from .cluster import homogeneity_completeness_v_measure
from .cluster import homogeneity_score
from .cluster import mutual_info_score
from .cluster import normalized_mutual_info_score
from .cluster import fowlkes_mallows_score
from .cluster import silhouette_samples
from .cluster import silhouette_score
from .cluster import calinski_harabaz_score
from .cluster import v_measure_score
from .pairwise import euclidean_distances
from .pairwise import pairwise_distances
from .pairwise import pairwise_distances_argmin
from .pairwise import pairwise_distances_argmin_min
from .pairwise import pairwise_kernels
from .regression import explained_variance_score
from .regression import mean_absolute_error
from .regression import mean_squared_error
from .regression import mean_squared_log_error
from .regression import median_absolute_error
from .regression import r2_score
from .scorer import make_scorer
from .scorer import SCORERS
from .scorer import get_scorer
__all__ = [
    'accuracy_score',
    'adjusted_mutual_info_score',
    'adjusted_rand_score',
    'auc',
    'average_precision_score',
    'classification_report',
    'cluster',
    'completeness_score',
    'confusion_matrix',
    'consensus_score',
    'coverage_error',
    'euclidean_distances',
    'explained_variance_score',
    'f1_score',
    'fbeta_score',
    'get_scorer',
    'hamming_loss',
    'hinge_loss',
    'homogeneity_completeness_v_measure',
    'homogeneity_score',
    'jaccard_similarity_score',
    'label_ranking_average_precision_score',
    'label_ranking_loss',
    'log_loss',
    'make_scorer',
    'matthews_corrcoef',
    'mean_absolute_error',
    'mean_squared_error',
    'mean_squared_log_error',
    'median_absolute_error',
    'mutual_info_score',
    'normalized_mutual_info_score',
    'pairwise_distances',
    'pairwise_distances_argmin',
    'pairwise_distances_argmin_min',
    'pairwise_distances_argmin_min',
    'pairwise_kernels',
    'precision_recall_curve',
    'precision_recall_fscore_support',
    'precision_score',
    'r2_score',
    'recall_score',
    'roc_auc_score',
    'roc_curve',
    'SCORERS',
    'silhouette_samples',
    'silhouette_score',
    'v_measure_score',
    'zero_one_loss',
    'brier_score_loss',
]
from __future__ import division
import numpy as np
from sklearn.utils.linear_assignment_ import linear_assignment
from sklearn.utils.validation import check_consistent_length, check_array
__all__ = ["consensus_score"]

def _check_rows_and_columns(a, b):
        check_consistent_length(*a)
    check_consistent_length(*b)
    checks = lambda x: check_array(x, ensure_2d=False)
    a_rows, a_cols = map(checks, a)
    b_rows, b_cols = map(checks, b)
    return a_rows, a_cols, b_rows, b_cols

def _jaccard(a_rows, a_cols, b_rows, b_cols):
        intersection = ((a_rows * b_rows).sum() *
                    (a_cols * b_cols).sum())
    a_size = a_rows.sum() * a_cols.sum()
    b_size = b_rows.sum() * b_cols.sum()
    return intersection / (a_size + b_size - intersection)

def _pairwise_similarity(a, b, similarity):
        a_rows, a_cols, b_rows, b_cols = _check_rows_and_columns(a, b)
    n_a = a_rows.shape[0]
    n_b = b_rows.shape[0]
    result = np.array(list(list(similarity(a_rows[i], a_cols[i],
                                           b_rows[j], b_cols[j])
                                for j in range(n_b))
                           for i in range(n_a)))
    return result

def consensus_score(a, b, similarity="jaccard"):
        if similarity == "jaccard":
        similarity = _jaccard
    matrix = _pairwise_similarity(a, b, similarity)
    indices = linear_assignment(1. - matrix)
    n_a = len(a[0])
    n_b = len(b[0])
    return matrix[indices[:, 0], indices[:, 1]].sum() / max(n_a, n_b)
from libc.math cimport exp
from scipy.special import gammaln
import numpy as np
cimport numpy as np
cimport cython
from sklearn.utils.lgamma cimport lgamma
np.import_array()
ctypedef np.float64_t DOUBLE
@cython.boundscheck(False)
@cython.wraparound(False)
def expected_mutual_information(contingency, int n_samples):
        cdef int R, C
    cdef DOUBLE N, gln_N, emi, term2, term3, gln
    cdef np.ndarray[DOUBLE] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij
    cdef np.ndarray[DOUBLE] nijs, term1
    cdef np.ndarray[DOUBLE, ndim=2] log_ab_outer
    cdef np.ndarray[np.int32_t] a, b
        R, C = contingency.shape
    N = <DOUBLE>n_samples
    a = np.ravel(contingency.sum(axis=1).astype(np.int32))
    b = np.ravel(contingency.sum(axis=0).astype(np.int32))
                nijs = np.arange(0, max(np.max(a), np.max(b)) + 1, dtype='float')
    nijs[0] = 1          term1 = nijs / N
            log_ab_outer = np.log(a)[:, np.newaxis] + np.log(b)
        log_Nnij = np.log(N * nijs)
            gln_a = gammaln(a + 1)
    gln_b = gammaln(b + 1)
    gln_Na = gammaln(N - a + 1)
    gln_Nb = gammaln(N - b + 1)
    gln_N = gammaln(N + 1)
    gln_nij = gammaln(nijs + 1)
        start = np.array([[v - N + w for w in b] for v in a], dtype='int')
    start = np.maximum(start, 1)
    end = np.minimum(np.resize(a, (C, R)).T, np.resize(b, (R, C))) + 1
        emi = 0
    cdef Py_ssize_t i, j, nij
    for i in range(R):
        for j in range(C):
            for nij in range(start[i,j], end[i,j]):
                term2 = log_Nnij[nij] - log_ab_outer[i,j]
                                gln = (gln_a[i] + gln_b[j] + gln_Na[i] + gln_Nb[j]
                     - gln_N - gln_nij[nij] - lgamma(a[i] - nij + 1)
                     - lgamma(b[j] - nij + 1)
                     - lgamma(N - a[i] - b[j] + nij + 1))
                term3 = exp(gln)
                emi += (term1[nij] * term2 * term3)
    return emi
import os
import numpy
from numpy.distutils.misc_util import Configuration

def configuration(parent_package="", top_path=None):
    config = Configuration("metrics/cluster", parent_package, top_path)
    libraries = []
    if os.name == 'posix':
        libraries.append('m')
    config.add_extension("expected_mutual_info_fast",
                         sources=["expected_mutual_info_fast.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_subpackage("tests")
    return config
if __name__ == "__main__":
    from numpy.distutils.core import setup
    setup(**configuration().todict())

from __future__ import division
from math import log
import numpy as np
from scipy.misc import comb
from scipy import sparse as sp
from .expected_mutual_info_fast import expected_mutual_information
from ...utils.fixes import bincount
from ...utils.validation import check_array

def comb2(n):
            return comb(n, 2, exact=1)

def check_clusterings(labels_true, labels_pred):
        labels_true = np.asarray(labels_true)
    labels_pred = np.asarray(labels_pred)
        if labels_true.ndim != 1:
        raise ValueError(
            "labels_true must be 1D: shape is %r" % (labels_true.shape,))
    if labels_pred.ndim != 1:
        raise ValueError(
            "labels_pred must be 1D: shape is %r" % (labels_pred.shape,))
    if labels_true.shape != labels_pred.shape:
        raise ValueError(
            "labels_true and labels_pred must have same size, got %d and %d"
            % (labels_true.shape[0], labels_pred.shape[0]))
    return labels_true, labels_pred

def contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):
    
    if eps is not None and sparse:
        raise ValueError("Cannot set 'eps' when sparse=True")
    classes, class_idx = np.unique(labels_true, return_inverse=True)
    clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)
    n_classes = classes.shape[0]
    n_clusters = clusters.shape[0]
                contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),
                                 (class_idx, cluster_idx)),
                                shape=(n_classes, n_clusters),
                                dtype=np.int)
    if sparse:
        contingency = contingency.tocsr()
        contingency.sum_duplicates()
    else:
        contingency = contingency.toarray()
        if eps is not None:
                        contingency = contingency + eps
    return contingency

def adjusted_rand_score(labels_true, labels_pred):
        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
    n_samples = labels_true.shape[0]
    n_classes = np.unique(labels_true).shape[0]
    n_clusters = np.unique(labels_pred).shape[0]
                if (n_classes == n_clusters == 1 or
            n_classes == n_clusters == 0 or
            n_classes == n_clusters == n_samples):
        return 1.0
        contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
    sum_comb_c = sum(comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1)))
    sum_comb_k = sum(comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0)))
    sum_comb = sum(comb2(n_ij) for n_ij in contingency.data)
    prod_comb = (sum_comb_c * sum_comb_k) / comb(n_samples, 2)
    mean_comb = (sum_comb_k + sum_comb_c) / 2.
    return (sum_comb - prod_comb) / (mean_comb - prod_comb)

def homogeneity_completeness_v_measure(labels_true, labels_pred):
        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
    if len(labels_true) == 0:
        return 1.0, 1.0, 1.0
    entropy_C = entropy(labels_true)
    entropy_K = entropy(labels_pred)
    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
    MI = mutual_info_score(None, None, contingency=contingency)
    homogeneity = MI / (entropy_C) if entropy_C else 1.0
    completeness = MI / (entropy_K) if entropy_K else 1.0
    if homogeneity + completeness == 0.0:
        v_measure_score = 0.0
    else:
        v_measure_score = (2.0 * homogeneity * completeness /
                           (homogeneity + completeness))
    return homogeneity, completeness, v_measure_score

def homogeneity_score(labels_true, labels_pred):
        return homogeneity_completeness_v_measure(labels_true, labels_pred)[0]

def completeness_score(labels_true, labels_pred):
        return homogeneity_completeness_v_measure(labels_true, labels_pred)[1]

def v_measure_score(labels_true, labels_pred):
        return homogeneity_completeness_v_measure(labels_true, labels_pred)[2]

def mutual_info_score(labels_true, labels_pred, contingency=None):
        if contingency is None:
        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
        contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
    else:
        contingency = check_array(contingency,
                                  accept_sparse=['csr', 'csc', 'coo'],
                                  dtype=[int, np.int32, np.int64])
    if isinstance(contingency, np.ndarray):
                nzx, nzy = np.nonzero(contingency)
        nz_val = contingency[nzx, nzy]
    elif sp.issparse(contingency):
                nzx, nzy, nz_val = sp.find(contingency)
    else:
        raise ValueError("Unsupported type for 'contingency': %s" %
                         type(contingency))
    contingency_sum = contingency.sum()
    pi = np.ravel(contingency.sum(axis=1))
    pj = np.ravel(contingency.sum(axis=0))
    log_contingency_nm = np.log(nz_val)
    contingency_nm = nz_val / contingency_sum
        outer = pi.take(nzx) * pj.take(nzy)
    log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())
    mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +
          contingency_nm * log_outer)
    return mi.sum()

def adjusted_mutual_info_score(labels_true, labels_pred):
        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
    n_samples = labels_true.shape[0]
    classes = np.unique(labels_true)
    clusters = np.unique(labels_pred)
            if (classes.shape[0] == clusters.shape[0] == 1 or
            classes.shape[0] == clusters.shape[0] == 0):
        return 1.0
    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
    contingency = contingency.astype(np.float64)
        mi = mutual_info_score(labels_true, labels_pred,
                           contingency=contingency)
        emi = expected_mutual_information(contingency, n_samples)
        h_true, h_pred = entropy(labels_true), entropy(labels_pred)
    ami = (mi - emi) / (max(h_true, h_pred) - emi)
    return ami

def normalized_mutual_info_score(labels_true, labels_pred):
        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
    classes = np.unique(labels_true)
    clusters = np.unique(labels_pred)
            if (classes.shape[0] == clusters.shape[0] == 1 or
            classes.shape[0] == clusters.shape[0] == 0):
        return 1.0
    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
    contingency = contingency.astype(np.float64)
        mi = mutual_info_score(labels_true, labels_pred,
                           contingency=contingency)
            h_true, h_pred = entropy(labels_true), entropy(labels_pred)
    nmi = mi / max(np.sqrt(h_true * h_pred), 1e-10)
    return nmi

def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):
        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
    n_samples, = labels_true.shape
    c = contingency_matrix(labels_true, labels_pred, sparse=True)
    tk = np.dot(c.data, c.data) - n_samples
    pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
    qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
    return tk / np.sqrt(pk * qk) if tk != 0. else 0.

def entropy(labels):
        if len(labels) == 0:
        return 1.0
    label_idx = np.unique(labels, return_inverse=True)[1]
    pi = bincount(label_idx).astype(np.float64)
    pi = pi[pi > 0]
    pi_sum = np.sum(pi)
            return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))

import numpy as np
from ...utils import check_random_state
from ...utils import check_X_y
from ...utils.fixes import bincount
from ..pairwise import pairwise_distances
from ...preprocessing import LabelEncoder

def check_number_of_labels(n_labels, n_samples):
    if not 1 < n_labels < n_samples:
        raise ValueError("Number of labels is %d. Valid values are 2 "
                         "to n_samples - 1 (inclusive)" % n_labels)

def silhouette_score(X, labels, metric='euclidean', sample_size=None,
                     random_state=None, **kwds):
        if sample_size is not None:
        X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])
        random_state = check_random_state(random_state)
        indices = random_state.permutation(X.shape[0])[:sample_size]
        if metric == "precomputed":
            X, labels = X[indices].T[indices].T, labels[indices]
        else:
            X, labels = X[indices], labels[indices]
    return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))

def silhouette_samples(X, labels, metric='euclidean', **kwds):
        X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])
    le = LabelEncoder()
    labels = le.fit_transform(labels)
    check_number_of_labels(len(le.classes_), X.shape[0])
    distances = pairwise_distances(X, metric=metric, **kwds)
    unique_labels = le.classes_
    n_samples_per_label = bincount(labels, minlength=len(unique_labels))
            intra_clust_dists = np.zeros(distances.shape[0], dtype=distances.dtype)
            inter_clust_dists = np.inf + intra_clust_dists
    for curr_label in range(len(unique_labels)):
                        mask = labels == curr_label
        current_distances = distances[mask]
                n_samples_curr_lab = n_samples_per_label[curr_label] - 1
        if n_samples_curr_lab != 0:
            intra_clust_dists[mask] = np.sum(
                current_distances[:, mask], axis=1) / n_samples_curr_lab
                        for other_label in range(len(unique_labels)):
            if other_label != curr_label:
                other_mask = labels == other_label
                other_distances = np.mean(
                    current_distances[:, other_mask], axis=1)
                inter_clust_dists[mask] = np.minimum(
                    inter_clust_dists[mask], other_distances)
    sil_samples = inter_clust_dists - intra_clust_dists
    sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)
        sil_samples[n_samples_per_label.take(labels) == 1] = 0
    return sil_samples

def calinski_harabaz_score(X, labels):
        X, labels = check_X_y(X, labels)
    le = LabelEncoder()
    labels = le.fit_transform(labels)
    n_samples, _ = X.shape
    n_labels = len(le.classes_)
    check_number_of_labels(n_labels, n_samples)
    extra_disp, intra_disp = 0., 0.
    mean = np.mean(X, axis=0)
    for k in range(n_labels):
        cluster_k = X[labels == k]
        mean_k = np.mean(cluster_k, axis=0)
        extra_disp += len(cluster_k) * np.sum((mean_k - mean) ** 2)
        intra_disp += np.sum((cluster_k - mean_k) ** 2)
    return (1. if intra_disp == 0. else
            extra_disp * (n_samples - n_labels) /
            (intra_disp * (n_labels - 1.)))
from .supervised import adjusted_mutual_info_score
from .supervised import normalized_mutual_info_score
from .supervised import adjusted_rand_score
from .supervised import completeness_score
from .supervised import contingency_matrix
from .supervised import expected_mutual_information
from .supervised import homogeneity_completeness_v_measure
from .supervised import homogeneity_score
from .supervised import mutual_info_score
from .supervised import v_measure_score
from .supervised import fowlkes_mallows_score
from .supervised import entropy
from .unsupervised import silhouette_samples
from .unsupervised import silhouette_score
from .unsupervised import calinski_harabaz_score
from .bicluster import consensus_score
__all__ = ["adjusted_mutual_info_score", "normalized_mutual_info_score",
           "adjusted_rand_score", "completeness_score", "contingency_matrix",
           "expected_mutual_information", "homogeneity_completeness_v_measure",
           "homogeneity_score", "mutual_info_score", "v_measure_score",
           "fowlkes_mallows_score", "entropy", "silhouette_samples",
           "silhouette_score", "calinski_harabaz_score", "consensus_score"]

from __future__ import print_function
import warnings
from abc import ABCMeta, abstractmethod
from time import time
import numpy as np
from .. import cluster
from ..base import BaseEstimator
from ..base import DensityMixin
from ..externals import six
from ..exceptions import ConvergenceWarning
from ..utils import check_array, check_random_state
from ..utils.extmath import logsumexp

def _check_shape(param, param_shape, name):
        param = np.array(param)
    if param.shape != param_shape:
        raise ValueError("The parameter '%s' should have the shape of %s, "
                         "but got %s" % (name, param_shape, param.shape))

def _check_X(X, n_components=None, n_features=None):
        X = check_array(X, dtype=[np.float64, np.float32])
    if n_components is not None and X.shape[0] < n_components:
        raise ValueError('Expected n_samples >= n_components '
                         'but got n_components = %d, n_samples = %d'
                         % (n_components, X.shape[0]))
    if n_features is not None and X.shape[1] != n_features:
        raise ValueError("Expected the input data X have %d features, "
                         "but got %d features"
                         % (n_features, X.shape[1]))
    return X

class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):
    
    def __init__(self, n_components, tol, reg_covar,
                 max_iter, n_init, init_params, random_state, warm_start,
                 verbose, verbose_interval):
        self.n_components = n_components
        self.tol = tol
        self.reg_covar = reg_covar
        self.max_iter = max_iter
        self.n_init = n_init
        self.init_params = init_params
        self.random_state = random_state
        self.warm_start = warm_start
        self.verbose = verbose
        self.verbose_interval = verbose_interval
    def _check_initial_parameters(self, X):
                if self.n_components < 1:
            raise ValueError("Invalid value for 'n_components': %d "
                             "Estimation requires at least one component"
                             % self.n_components)
        if self.tol < 0.:
            raise ValueError("Invalid value for 'tol': %.5f "
                             "Tolerance used by the EM must be non-negative"
                             % self.tol)
        if self.n_init < 1:
            raise ValueError("Invalid value for 'n_init': %d "
                             "Estimation requires at least one run"
                             % self.n_init)
        if self.max_iter < 1:
            raise ValueError("Invalid value for 'max_iter': %d "
                             "Estimation requires at least one iteration"
                             % self.max_iter)
        if self.reg_covar < 0.:
            raise ValueError("Invalid value for 'reg_covar': %.5f "
                             "regularization on covariance must be "
                             "non-negative"
                             % self.reg_covar)
                self._check_parameters(X)
    @abstractmethod
    def _check_parameters(self, X):
                pass
    def _initialize_parameters(self, X, random_state):
                n_samples, _ = X.shape
        if self.init_params == 'kmeans':
            resp = np.zeros((n_samples, self.n_components))
            label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
                                   random_state=random_state).fit(X).labels_
            resp[np.arange(n_samples), label] = 1
        elif self.init_params == 'random':
            resp = random_state.rand(n_samples, self.n_components)
            resp /= resp.sum(axis=1)[:, np.newaxis]
        else:
            raise ValueError("Unimplemented initialization method '%s'"
                             % self.init_params)
        self._initialize(X, resp)
    @abstractmethod
    def _initialize(self, X, resp):
                pass
    def fit(self, X, y=None):
                X = _check_X(X, self.n_components)
        self._check_initial_parameters(X)
                do_init = not(self.warm_start and hasattr(self, 'converged_'))
        n_init = self.n_init if do_init else 1
        max_lower_bound = -np.infty
        self.converged_ = False
        random_state = check_random_state(self.random_state)
        n_samples, _ = X.shape
        for init in range(n_init):
            self._print_verbose_msg_init_beg(init)
            if do_init:
                self._initialize_parameters(X, random_state)
                self.lower_bound_ = -np.infty
            for n_iter in range(self.max_iter):
                prev_lower_bound = self.lower_bound_
                log_prob_norm, log_resp = self._e_step(X)
                self._m_step(X, log_resp)
                self.lower_bound_ = self._compute_lower_bound(
                    log_resp, log_prob_norm)
                change = self.lower_bound_ - prev_lower_bound
                self._print_verbose_msg_iter_end(n_iter, change)
                if abs(change) < self.tol:
                    self.converged_ = True
                    break
            self._print_verbose_msg_init_end(self.lower_bound_)
            if self.lower_bound_ > max_lower_bound:
                max_lower_bound = self.lower_bound_
                best_params = self._get_parameters()
                best_n_iter = n_iter
        if not self.converged_:
            warnings.warn('Initialization %d did not converged. '
                          'Try different init parameters, '
                          'or increase max_iter, tol '
                          'or check for degenerate data.'
                          % (init + 1), ConvergenceWarning)
        self._set_parameters(best_params)
        self.n_iter_ = best_n_iter
        return self
    def _e_step(self, X):
                log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
        return np.mean(log_prob_norm), log_resp
    @abstractmethod
    def _m_step(self, X, log_resp):
                pass
    @abstractmethod
    def _check_is_fitted(self):
        pass
    @abstractmethod
    def _get_parameters(self):
        pass
    @abstractmethod
    def _set_parameters(self, params):
        pass
    def score_samples(self, X):
                self._check_is_fitted()
        X = _check_X(X, None, self.means_.shape[1])
        return logsumexp(self._estimate_weighted_log_prob(X), axis=1)
    def score(self, X, y=None):
                return self.score_samples(X).mean()
    def predict(self, X, y=None):
                self._check_is_fitted()
        X = _check_X(X, None, self.means_.shape[1])
        return self._estimate_weighted_log_prob(X).argmax(axis=1)
    def predict_proba(self, X):
                self._check_is_fitted()
        X = _check_X(X, None, self.means_.shape[1])
        _, log_resp = self._estimate_log_prob_resp(X)
        return np.exp(log_resp)
    def sample(self, n_samples=1):
                self._check_is_fitted()
        if n_samples < 1:
            raise ValueError(
                "Invalid value for 'n_samples': %d . The sampling requires at "
                "least one sample." % (self.n_components))
        _, n_features = self.means_.shape
        rng = check_random_state(self.random_state)
        n_samples_comp = rng.multinomial(n_samples, self.weights_)
        if self.covariance_type == 'full':
            X = np.vstack([
                rng.multivariate_normal(mean, covariance, int(sample))
                for (mean, covariance, sample) in zip(
                    self.means_, self.covariances_, n_samples_comp)])
        elif self.covariance_type == "tied":
            X = np.vstack([
                rng.multivariate_normal(mean, self.covariances_, int(sample))
                for (mean, sample) in zip(
                    self.means_, n_samples_comp)])
        else:
            X = np.vstack([
                mean + rng.randn(sample, n_features) * np.sqrt(covariance)
                for (mean, covariance, sample) in zip(
                    self.means_, self.covariances_, n_samples_comp)])
        y = np.concatenate([j * np.ones(sample, dtype=int)
                           for j, sample in enumerate(n_samples_comp)])
        return (X, y)
    def _estimate_weighted_log_prob(self, X):
                return self._estimate_log_prob(X) + self._estimate_log_weights()
    @abstractmethod
    def _estimate_log_weights(self):
                pass
    @abstractmethod
    def _estimate_log_prob(self, X):
                pass
    def _estimate_log_prob_resp(self, X):
                weighted_log_prob = self._estimate_weighted_log_prob(X)
        log_prob_norm = logsumexp(weighted_log_prob, axis=1)
        with np.errstate(under='ignore'):
                        log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
        return log_prob_norm, log_resp
    def _print_verbose_msg_init_beg(self, n_init):
                if self.verbose == 1:
            print("Initialization %d" % n_init)
        elif self.verbose >= 2:
            print("Initialization %d" % n_init)
            self._init_prev_time = time()
            self._iter_prev_time = self._init_prev_time
    def _print_verbose_msg_iter_end(self, n_iter, diff_ll):
                if n_iter % self.verbose_interval == 0:
            if self.verbose == 1:
                print("  Iteration %d" % n_iter)
            elif self.verbose >= 2:
                cur_time = time()
                print("  Iteration %d\t time lapse %.5fs\t ll change %.5f" % (
                    n_iter, cur_time - self._iter_prev_time, diff_ll))
                self._iter_prev_time = cur_time
    def _print_verbose_msg_init_end(self, ll):
                if self.verbose == 1:
            print("Initialization converged: %s" % self.converged_)
        elif self.verbose >= 2:
            print("Initialization converged: %s\t time lapse %.5fs\t ll %.5f" %
                  (self.converged_, time() - self._init_prev_time, ll))
import math
import numpy as np
from scipy.special import betaln, digamma, gammaln
from .base import BaseMixture, _check_shape
from .gaussian_mixture import _check_precision_matrix
from .gaussian_mixture import _check_precision_positivity
from .gaussian_mixture import _compute_log_det_cholesky
from .gaussian_mixture import _compute_precision_cholesky
from .gaussian_mixture import _estimate_gaussian_parameters
from .gaussian_mixture import _estimate_log_gaussian_prob
from ..utils import check_array
from ..utils.validation import check_is_fitted

def _log_dirichlet_norm(dirichlet_concentration):
        return (gammaln(np.sum(dirichlet_concentration)) -
            np.sum(gammaln(dirichlet_concentration)))

def _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):
            return -(degrees_of_freedom * log_det_precisions_chol +
             degrees_of_freedom * n_features * .5 * math.log(2.) +
             np.sum(gammaln(.5 * (degrees_of_freedom -
                                  np.arange(n_features)[:, np.newaxis])), 0))

class BayesianGaussianMixture(BaseMixture):
    
    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',
                 weight_concentration_prior_type='dirichlet_process',
                 weight_concentration_prior=None,
                 mean_precision_prior=None, mean_prior=None,
                 degrees_of_freedom_prior=None, covariance_prior=None,
                 random_state=None, warm_start=False, verbose=0,
                 verbose_interval=10):
        super(BayesianGaussianMixture, self).__init__(
            n_components=n_components, tol=tol, reg_covar=reg_covar,
            max_iter=max_iter, n_init=n_init, init_params=init_params,
            random_state=random_state, warm_start=warm_start,
            verbose=verbose, verbose_interval=verbose_interval)
        self.covariance_type = covariance_type
        self.weight_concentration_prior_type = weight_concentration_prior_type
        self.weight_concentration_prior = weight_concentration_prior
        self.mean_precision_prior = mean_precision_prior
        self.mean_prior = mean_prior
        self.degrees_of_freedom_prior = degrees_of_freedom_prior
        self.covariance_prior = covariance_prior
    def _check_parameters(self, X):
                if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:
            raise ValueError("Invalid value for 'covariance_type': %s "
                             "'covariance_type' should be in "
                             "['spherical', 'tied', 'diag', 'full']"
                             % self.covariance_type)
        if (self.weight_concentration_prior_type not in
                ['dirichlet_process', 'dirichlet_distribution']):
            raise ValueError(
                "Invalid value for 'weight_concentration_prior_type': %s "
                "'weight_concentration_prior_type' should be in "
                "['dirichlet_process', 'dirichlet_distribution']"
                % self.weight_concentration_prior_type)
        self._check_weights_parameters()
        self._check_means_parameters(X)
        self._check_precision_parameters(X)
        self._checkcovariance_prior_parameter(X)
    def _check_weights_parameters(self):
                if self.weight_concentration_prior is None:
            self.weight_concentration_prior_ = 1. / self.n_components
        elif self.weight_concentration_prior > 0.:
            self.weight_concentration_prior_ = (
                self.weight_concentration_prior)
        else:
            raise ValueError("The parameter 'weight_concentration_prior' "
                             "should be greater than 0., but got %.3f."
                             % self.weight_concentration_prior)
    def _check_means_parameters(self, X):
                _, n_features = X.shape
        if self.mean_precision_prior is None:
            self.mean_precision_prior_ = 1.
        elif self.mean_precision_prior > 0.:
            self.mean_precision_prior_ = self.mean_precision_prior
        else:
            raise ValueError("The parameter 'mean_precision_prior' should be "
                             "greater than 0., but got %.3f."
                             % self.mean_precision_prior)
        if self.mean_prior is None:
            self.mean_prior_ = X.mean(axis=0)
        else:
            self.mean_prior_ = check_array(self.mean_prior,
                                           dtype=[np.float64, np.float32],
                                           ensure_2d=False)
            _check_shape(self.mean_prior_, (n_features, ), 'means')
    def _check_precision_parameters(self, X):
                _, n_features = X.shape
        if self.degrees_of_freedom_prior is None:
            self.degrees_of_freedom_prior_ = n_features
        elif self.degrees_of_freedom_prior > n_features - 1.:
            self.degrees_of_freedom_prior_ = self.degrees_of_freedom_prior
        else:
            raise ValueError("The parameter 'degrees_of_freedom_prior' "
                             "should be greater than %d, but got %.3f."
                             % (n_features - 1, self.degrees_of_freedom_prior))
    def _checkcovariance_prior_parameter(self, X):
                _, n_features = X.shape
        if self.covariance_prior is None:
            self.covariance_prior_ = {
                'full': np.atleast_2d(np.cov(X.T)),
                'tied': np.atleast_2d(np.cov(X.T)),
                'diag': np.var(X, axis=0, ddof=1),
                'spherical': np.var(X, axis=0, ddof=1).mean()
            }[self.covariance_type]
        elif self.covariance_type in ['full', 'tied']:
            self.covariance_prior_ = check_array(
                self.covariance_prior, dtype=[np.float64, np.float32],
                ensure_2d=False)
            _check_shape(self.covariance_prior_, (n_features, n_features),
                         '%s covariance_prior' % self.covariance_type)
            _check_precision_matrix(self.covariance_prior_,
                                    self.covariance_type)
        elif self.covariance_type == 'diag':
            self.covariance_prior_ = check_array(
                self.covariance_prior, dtype=[np.float64, np.float32],
                ensure_2d=False)
            _check_shape(self.covariance_prior_, (n_features,),
                         '%s covariance_prior' % self.covariance_type)
            _check_precision_positivity(self.covariance_prior_,
                                        self.covariance_type)
                elif self.covariance_prior > 0.:
            self.covariance_prior_ = self.covariance_prior
        else:
            raise ValueError("The parameter 'spherical covariance_prior' "
                             "should be greater than 0., but got %.3f."
                             % self.covariance_prior)
    def _initialize(self, X, resp):
                nk, xk, sk = _estimate_gaussian_parameters(X, resp, self.reg_covar,
                                                   self.covariance_type)
        self._estimate_weights(nk)
        self._estimate_means(nk, xk)
        self._estimate_precisions(nk, xk, sk)
    def _estimate_weights(self, nk):
                if self.weight_concentration_prior_type == 'dirichlet_process':
                                    self.weight_concentration_ = (
                1. + nk,
                (self.weight_concentration_prior_ +
                 np.hstack((np.cumsum(nk[::-1])[-2::-1], 0))))
        else:
                        self.weight_concentration_ = self.weight_concentration_prior_ + nk
    def _estimate_means(self, nk, xk):
                self.mean_precision_ = self.mean_precision_prior_ + nk
        self.means_ = ((self.mean_precision_prior_ * self.mean_prior_ +
                        nk[:, np.newaxis] * xk) /
                       self.mean_precision_[:, np.newaxis])
    def _estimate_precisions(self, nk, xk, sk):
                {"full": self._estimate_wishart_full,
         "tied": self._estimate_wishart_tied,
         "diag": self._estimate_wishart_diag,
         "spherical": self._estimate_wishart_spherical
         }[self.covariance_type](nk, xk, sk)
        self.precisions_cholesky_ = _compute_precision_cholesky(
            self.covariances_, self.covariance_type)
    def _estimate_wishart_full(self, nk, xk, sk):
                _, n_features = xk.shape
                                self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
        self.covariances_ = np.empty((self.n_components, n_features,
                                      n_features))
        for k in range(self.n_components):
            diff = xk[k] - self.mean_prior_
            self.covariances_[k] = (self.covariance_prior_ + nk[k] * sk[k] +
                                    nk[k] * self.mean_precision_prior_ /
                                    self.mean_precision_[k] * np.outer(diff,
                                                                       diff))
                self.covariances_ /= (
            self.degrees_of_freedom_[:, np.newaxis, np.newaxis])
    def _estimate_wishart_tied(self, nk, xk, sk):
                _, n_features = xk.shape
                                self.degrees_of_freedom_ = (
            self.degrees_of_freedom_prior_ + nk.sum() / self.n_components)
        diff = xk - self.mean_prior_
        self.covariances_ = (
            self.covariance_prior_ + sk * nk.sum() / self.n_components +
            self.mean_precision_prior_ / self.n_components * np.dot(
                (nk / self.mean_precision_) * diff.T, diff))
                self.covariances_ /= self.degrees_of_freedom_
    def _estimate_wishart_diag(self, nk, xk, sk):
                _, n_features = xk.shape
                                self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
        diff = xk - self.mean_prior_
        self.covariances_ = (
            self.covariance_prior_ + nk[:, np.newaxis] * (
                sk + (self.mean_precision_prior_ /
                      self.mean_precision_)[:, np.newaxis] * np.square(diff)))
                self.covariances_ /= self.degrees_of_freedom_[:, np.newaxis]
    def _estimate_wishart_spherical(self, nk, xk, sk):
                _, n_features = xk.shape
                                self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + nk
        diff = xk - self.mean_prior_
        self.covariances_ = (
            self.covariance_prior_ + nk * (
                sk + self.mean_precision_prior_ / self.mean_precision_ *
                np.mean(np.square(diff), 1)))
                self.covariances_ /= self.degrees_of_freedom_
    def _check_is_fitted(self):
        check_is_fitted(self, ['weight_concentration_', 'mean_precision_',
                               'means_', 'degrees_of_freedom_',
                               'covariances_', 'precisions_',
                               'precisions_cholesky_'])
    def _m_step(self, X, log_resp):
                n_samples, _ = X.shape
        nk, xk, sk = _estimate_gaussian_parameters(
            X, np.exp(log_resp), self.reg_covar, self.covariance_type)
        self._estimate_weights(nk)
        self._estimate_means(nk, xk)
        self._estimate_precisions(nk, xk, sk)
    def _estimate_log_weights(self):
        if self.weight_concentration_prior_type == 'dirichlet_process':
            digamma_sum = digamma(self.weight_concentration_[0] +
                                  self.weight_concentration_[1])
            digamma_a = digamma(self.weight_concentration_[0])
            digamma_b = digamma(self.weight_concentration_[1])
            return (digamma_a - digamma_sum +
                    np.hstack((0, np.cumsum(digamma_b - digamma_sum)[:-1])))
        else:
                        return (digamma(self.weight_concentration_) -
                    digamma(np.sum(self.weight_concentration_)))
    def _estimate_log_prob(self, X):
        _, n_features = X.shape
                        log_gauss = (_estimate_log_gaussian_prob(
            X, self.means_, self.precisions_cholesky_, self.covariance_type) -
            .5 * n_features * np.log(self.degrees_of_freedom_))
        log_lambda = n_features * np.log(2.) + np.sum(digamma(
            .5 * (self.degrees_of_freedom_ -
                  np.arange(0, n_features)[:, np.newaxis])), 0)
        return log_gauss + .5 * (log_lambda -
                                 n_features / self.mean_precision_)
    def _compute_lower_bound(self, log_resp, log_prob_norm):
                                n_features, = self.mean_prior_.shape
                        log_det_precisions_chol = (_compute_log_det_cholesky(
            self.precisions_cholesky_, self.covariance_type, n_features) -
            .5 * n_features * np.log(self.degrees_of_freedom_))
        if self.covariance_type == 'tied':
            log_wishart = self.n_components * np.float64(_log_wishart_norm(
                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
        else:
            log_wishart = np.sum(_log_wishart_norm(
                self.degrees_of_freedom_, log_det_precisions_chol, n_features))
        if self.weight_concentration_prior_type == 'dirichlet_process':
            log_norm_weight = -np.sum(betaln(self.weight_concentration_[0],
                                             self.weight_concentration_[1]))
        else:
            log_norm_weight = _log_dirichlet_norm(self.weight_concentration_)
        return (-np.sum(np.exp(log_resp) * log_resp) -
                log_wishart - log_norm_weight -
                0.5 * n_features * np.sum(np.log(self.mean_precision_)))
    def _get_parameters(self):
        return (self.weight_concentration_,
                self.mean_precision_, self.means_,
                self.degrees_of_freedom_, self.covariances_,
                self.precisions_cholesky_)
    def _set_parameters(self, params):
        (self.weight_concentration_, self.mean_precision_, self.means_,
         self.degrees_of_freedom_, self.covariances_,
         self.precisions_cholesky_) = params
                if self.weight_concentration_prior_type == "dirichlet_process":
            weight_dirichlet_sum = (self.weight_concentration_[0] +
                                    self.weight_concentration_[1])
            tmp = self.weight_concentration_[1] / weight_dirichlet_sum
            self.weights_ = (
                self.weight_concentration_[0] / weight_dirichlet_sum *
                np.hstack((1, np.cumprod(tmp[:-1]))))
            self.weights_ /= np.sum(self.weights_)
        else:
            self. weights_ = (self.weight_concentration_ /
                              np.sum(self.weight_concentration_))
                if self.covariance_type == 'full':
            self.precisions_ = np.array([
                np.dot(prec_chol, prec_chol.T)
                for prec_chol in self.precisions_cholesky_])
        elif self.covariance_type == 'tied':
            self.precisions_ = np.dot(self.precisions_cholesky_,
                                      self.precisions_cholesky_.T)
        else:
            self.precisions_ = self.precisions_cholesky_ ** 2
from __future__ import print_function

import numpy as np
from scipy.special import digamma as _digamma, gammaln as _gammaln
from scipy import linalg
from scipy.spatial.distance import cdist
from ..externals.six.moves import xrange
from ..utils import check_random_state, check_array, deprecated
from ..utils.extmath import logsumexp, pinvh, squared_norm, stable_cumsum
from ..utils.validation import check_is_fitted
from .. import cluster
from .gmm import _GMMBase

@deprecated("The function digamma is deprecated in 0.18 and "
            "will be removed in 0.20. Use scipy.special.digamma instead.")
def digamma(x):
    return _digamma(x + np.finfo(np.float32).eps)

@deprecated("The function gammaln is deprecated in 0.18 and "
            "will be removed in 0.20. Use scipy.special.gammaln instead.")
def gammaln(x):
    return _gammaln(x + np.finfo(np.float32).eps)

@deprecated("The function log_normalize is deprecated in 0.18 and "
            "will be removed in 0.20.")
def log_normalize(v, axis=0):
        v = np.rollaxis(v, axis)
    v = v.copy()
    v -= v.max(axis=0)
    out = logsumexp(v)
    v = np.exp(v - out)
    v += np.finfo(np.float32).eps
    v /= np.sum(v, axis=0)
    return np.swapaxes(v, 0, axis)

@deprecated("The function wishart_log_det is deprecated in 0.18 and "
            "will be removed in 0.20.")
def wishart_log_det(a, b, detB, n_features):
        l = np.sum(digamma(0.5 * (a - np.arange(-1, n_features - 1))))
    l += n_features * np.log(2)
    return l + detB

@deprecated("The function wishart_logz is deprecated in 0.18 and "
            "will be removed in 0.20.")
def wishart_logz(v, s, dets, n_features):
    "The logarithm of the normalization constant for the wishart distribution"
    z = 0.
    z += 0.5 * v * n_features * np.log(2)
    z += (0.25 * (n_features * (n_features - 1)) * np.log(np.pi))
    z += 0.5 * v * np.log(dets)
    z += np.sum(gammaln(0.5 * (v - np.arange(n_features) + 1)))
    return z

def _bound_wishart(a, B, detB):
        n_features = B.shape[0]
    logprior = wishart_logz(a, B, detB, n_features)
    logprior -= wishart_logz(n_features,
                             np.identity(n_features),
                             1, n_features)
    logprior += 0.5 * (a - 1) * wishart_log_det(a, B, detB, n_features)
    logprior += 0.5 * a * np.trace(B)
    return logprior


def _sym_quad_form(x, mu, A):
        q = (cdist(x, mu[np.newaxis], "mahalanobis", VI=A) ** 2).reshape(-1)
    return q

def _bound_state_log_lik(X, initial_bound, precs, means, covariance_type):
        n_components, n_features = means.shape
    n_samples = X.shape[0]
    bound = np.empty((n_samples, n_components))
    bound[:] = initial_bound
    if covariance_type in ['diag', 'spherical']:
        for k in range(n_components):
            d = X - means[k]
            bound[:, k] -= 0.5 * np.sum(d * d * precs[k], axis=1)
    elif covariance_type == 'tied':
        for k in range(n_components):
            bound[:, k] -= 0.5 * _sym_quad_form(X, means[k], precs)
    elif covariance_type == 'full':
        for k in range(n_components):
            bound[:, k] -= 0.5 * _sym_quad_form(X, means[k], precs[k])
    return bound

class _DPGMMBase(_GMMBase):
        def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
                 random_state=None, tol=1e-3, verbose=0, min_covar=None,
                 n_iter=10, params='wmc', init_params='wmc'):
        self.alpha = alpha
        super(_DPGMMBase, self).__init__(n_components, covariance_type,
                                         random_state=random_state,
                                         tol=tol, min_covar=min_covar,
                                         n_iter=n_iter, params=params,
                                         init_params=init_params,
                                         verbose=verbose)
    def _get_precisions(self):
                if self.covariance_type == 'full':
            return self.precs_
        elif self.covariance_type in ['diag', 'spherical']:
            return [np.diag(cov) for cov in self.precs_]
        elif self.covariance_type == 'tied':
            return [self.precs_] * self.n_components
    def _get_covars(self):
        return [pinvh(c) for c in self._get_precisions()]
    def _set_covars(self, covars):
        raise NotImplementedError(        check_is_fitted(self, 'gamma_')
        X = check_array(X)
        if X.ndim == 1:
            X = X[:, np.newaxis]
        z = np.zeros((X.shape[0], self.n_components))
        sd = digamma(self.gamma_.T[1] + self.gamma_.T[2])
        dgamma1 = digamma(self.gamma_.T[1]) - sd
        dgamma2 = np.zeros(self.n_components)
        dgamma2[0] = digamma(self.gamma_[0, 2]) - digamma(self.gamma_[0, 1] +
                                                          self.gamma_[0, 2])
        for j in range(1, self.n_components):
            dgamma2[j] = dgamma2[j - 1] + digamma(self.gamma_[j - 1, 2])
            dgamma2[j] -= sd[j - 1]
        dgamma = dgamma1 + dgamma2
                del dgamma1, dgamma2, sd
        if self.covariance_type not in ['full', 'tied', 'diag', 'spherical']:
            raise NotImplementedError("This ctype is not implemented: %s"
                                      % self.covariance_type)
        p = _bound_state_log_lik(X, self._initial_bound + self.bound_prec_,
                                 self.precs_, self.means_,
                                 self.covariance_type)
        z = p + dgamma
        z = log_normalize(z, axis=-1)
        bound = np.sum(z * p, axis=-1)
        return bound, z
    def _update_concentration(self, z):
                n_features = X.shape[1]
        if self.covariance_type == 'spherical':
            self.dof_ = 0.5 * n_features * np.sum(z, axis=0)
            for k in range(self.n_components):
                                sq_diff = np.sum((X - self.means_[k]) ** 2, axis=1)
                self.scale_[k] = 1.
                self.scale_[k] += 0.5 * np.sum(z.T[k] * (sq_diff + n_features))
                self.bound_prec_[k] = (
                    0.5 * n_features * (
                        digamma(self.dof_[k]) - np.log(self.scale_[k])))
            self.precs_ = np.tile(self.dof_ / self.scale_, [n_features, 1]).T
        elif self.covariance_type == 'diag':
            for k in range(self.n_components):
                self.dof_[k].fill(1. + 0.5 * np.sum(z.T[k], axis=0))
                sq_diff = (X - self.means_[k]) ** 2                  self.scale_[k] = np.ones(n_features) + 0.5 * np.dot(
                    z.T[k], (sq_diff + 1))
                self.precs_[k] = self.dof_[k] / self.scale_[k]
                self.bound_prec_[k] = 0.5 * np.sum(digamma(self.dof_[k])
                                                   - np.log(self.scale_[k]))
                self.bound_prec_[k] -= 0.5 * np.sum(self.precs_[k])
        elif self.covariance_type == 'tied':
            self.dof_ = 2 + X.shape[0] + n_features
            self.scale_ = (X.shape[0] + 1) * np.identity(n_features)
            for k in range(self.n_components):
                diff = X - self.means_[k]
                self.scale_ += np.dot(diff.T, z[:, k:k + 1] * diff)
            self.scale_ = pinvh(self.scale_)
            self.precs_ = self.dof_ * self.scale_
            self.det_scale_ = linalg.det(self.scale_)
            self.bound_prec_ = 0.5 * wishart_log_det(
                self.dof_, self.scale_, self.det_scale_, n_features)
            self.bound_prec_ -= 0.5 * self.dof_ * np.trace(self.scale_)
        elif self.covariance_type == 'full':
            for k in range(self.n_components):
                sum_resp = np.sum(z.T[k])
                self.dof_[k] = 2 + sum_resp + n_features
                self.scale_[k] = (sum_resp + 1) * np.identity(n_features)
                diff = X - self.means_[k]
                self.scale_[k] += np.dot(diff.T, z[:, k:k + 1] * diff)
                self.scale_[k] = pinvh(self.scale_[k])
                self.precs_[k] = self.dof_[k] * self.scale_[k]
                self.det_scale_[k] = linalg.det(self.scale_[k])
                self.bound_prec_[k] = 0.5 * wishart_log_det(
                    self.dof_[k], self.scale_[k], self.det_scale_[k],
                    n_features)
                self.bound_prec_[k] -= 0.5 * self.dof_[k] * np.trace(
                    self.scale_[k])
    def _monitor(self, X, z, n, end=False):
                if self.verbose > 0:
            print("Bound after updating %8s: %f" % (n, self.lower_bound(X, z)))
            if end:
                print("Cluster proportions:", self.gamma_.T[1])
                print("covariance_type:", self.covariance_type)
    def _do_mstep(self, X, z, params):
                self._monitor(X, z, "z")
        self._update_concentration(z)
        self._monitor(X, z, "gamma")
        if 'm' in params:
            self._update_means(X, z)
        self._monitor(X, z, "mu")
        if 'c' in params:
            self._update_precisions(X, z)
        self._monitor(X, z, "a and b", end=True)
    def _initialize_gamma(self):
        "Initializes the concentration parameters"
        self.gamma_ = self.alpha * np.ones((self.n_components, 3))
    def _bound_concentration(self):
                logprior = gammaln(self.alpha) * self.n_components
        logprior += np.sum((self.alpha - 1) * (
            digamma(self.gamma_.T[2]) - digamma(self.gamma_.T[1] +
                                                self.gamma_.T[2])))
        logprior += np.sum(- gammaln(self.gamma_.T[1] + self.gamma_.T[2]))
        logprior += np.sum(gammaln(self.gamma_.T[1]) +
                           gammaln(self.gamma_.T[2]))
        logprior -= np.sum((self.gamma_.T[1] - 1) * (
            digamma(self.gamma_.T[1]) - digamma(self.gamma_.T[1] +
                                                self.gamma_.T[2])))
        logprior -= np.sum((self.gamma_.T[2] - 1) * (
            digamma(self.gamma_.T[2]) - digamma(self.gamma_.T[1] +
                                                self.gamma_.T[2])))
        return logprior
    def _bound_means(self):
        "The variational lower bound for the mean parameters"
        logprior = 0.
        logprior -= 0.5 * squared_norm(self.means_)
        logprior -= 0.5 * self.means_.shape[1] * self.n_components
        return logprior
    def _bound_precisions(self):
                logprior = 0.
        if self.covariance_type == 'spherical':
            logprior += np.sum(gammaln(self.dof_))
            logprior -= np.sum(
                (self.dof_ - 1) * digamma(np.maximum(0.5, self.dof_)))
            logprior += np.sum(- np.log(self.scale_) + self.dof_
                               - self.precs_[:, 0])
        elif self.covariance_type == 'diag':
            logprior += np.sum(gammaln(self.dof_))
            logprior -= np.sum(
                (self.dof_ - 1) * digamma(np.maximum(0.5, self.dof_)))
            logprior += np.sum(- np.log(self.scale_) + self.dof_ - self.precs_)
        elif self.covariance_type == 'tied':
            logprior += _bound_wishart(self.dof_, self.scale_, self.det_scale_)
        elif self.covariance_type == 'full':
            for k in range(self.n_components):
                logprior += _bound_wishart(self.dof_[k],
                                           self.scale_[k],
                                           self.det_scale_[k])
        return logprior
    def _bound_proportions(self, z):
                dg12 = digamma(self.gamma_.T[1] + self.gamma_.T[2])
        dg1 = digamma(self.gamma_.T[1]) - dg12
        dg2 = digamma(self.gamma_.T[2]) - dg12
        cz = stable_cumsum(z[:, ::-1], axis=-1)[:, -2::-1]
        logprior = np.sum(cz * dg2[:-1]) + np.sum(z * dg1)
        del cz          z_non_zeros = z[z > np.finfo(np.float32).eps]
        logprior -= np.sum(z_non_zeros * np.log(z_non_zeros))
        return logprior
    def _logprior(self, z):
        logprior = self._bound_concentration()
        logprior += self._bound_means()
        logprior += self._bound_precisions()
        logprior += self._bound_proportions(z)
        return logprior
    def lower_bound(self, X, z):
                check_is_fitted(self, 'means_')
        if self.covariance_type not in ['full', 'tied', 'diag', 'spherical']:
            raise NotImplementedError("This ctype is not implemented: %s"
                                      % self.covariance_type)
        X = np.asarray(X)
        if X.ndim == 1:
            X = X[:, np.newaxis]
        c = np.sum(z * _bound_state_log_lik(X, self._initial_bound +
                                            self.bound_prec_, self.precs_,
                                            self.means_, self.covariance_type))
        return c + self._logprior(z)
    def _set_weights(self):
        for i in xrange(self.n_components):
            self.weights_[i] = self.gamma_[i, 1] / (self.gamma_[i, 1]
                                                    + self.gamma_[i, 2])
        self.weights_ /= np.sum(self.weights_)
    def _fit(self, X, y=None):
                self.random_state_ = check_random_state(self.random_state)
                X = check_array(X)
        if X.ndim == 1:
            X = X[:, np.newaxis]
        n_samples, n_features = X.shape
        z = np.ones((n_samples, self.n_components))
        z /= self.n_components
        self._initial_bound = - 0.5 * n_features * np.log(2 * np.pi)
        self._initial_bound -= np.log(2 * np.pi * np.e)
        if (self.init_params != '') or not hasattr(self, 'gamma_'):
            self._initialize_gamma()
        if 'm' in self.init_params or not hasattr(self, 'means_'):
            self.means_ = cluster.KMeans(
                n_clusters=self.n_components,
                random_state=self.random_state_).fit(X).cluster_centers_[::-1]
        if 'w' in self.init_params or not hasattr(self, 'weights_'):
            self.weights_ = np.tile(1.0 / self.n_components, self.n_components)
        if 'c' in self.init_params or not hasattr(self, 'precs_'):
            if self.covariance_type == 'spherical':
                self.dof_ = np.ones(self.n_components)
                self.scale_ = np.ones(self.n_components)
                self.precs_ = np.ones((self.n_components, n_features))
                self.bound_prec_ = 0.5 * n_features * (
                    digamma(self.dof_) - np.log(self.scale_))
            elif self.covariance_type == 'diag':
                self.dof_ = 1 + 0.5 * n_features
                self.dof_ *= np.ones((self.n_components, n_features))
                self.scale_ = np.ones((self.n_components, n_features))
                self.precs_ = np.ones((self.n_components, n_features))
                self.bound_prec_ = 0.5 * (np.sum(digamma(self.dof_) -
                                                 np.log(self.scale_), 1))
                self.bound_prec_ -= 0.5 * np.sum(self.precs_, 1)
            elif self.covariance_type == 'tied':
                self.dof_ = 1.
                self.scale_ = np.identity(n_features)
                self.precs_ = np.identity(n_features)
                self.det_scale_ = 1.
                self.bound_prec_ = 0.5 * wishart_log_det(
                    self.dof_, self.scale_, self.det_scale_, n_features)
                self.bound_prec_ -= 0.5 * self.dof_ * np.trace(self.scale_)
            elif self.covariance_type == 'full':
                self.dof_ = (1 + self.n_components + n_samples)
                self.dof_ *= np.ones(self.n_components)
                self.scale_ = [2 * np.identity(n_features)
                               for _ in range(self.n_components)]
                self.precs_ = [np.identity(n_features)
                               for _ in range(self.n_components)]
                self.det_scale_ = np.ones(self.n_components)
                self.bound_prec_ = np.zeros(self.n_components)
                for k in range(self.n_components):
                    self.bound_prec_[k] = wishart_log_det(
                        self.dof_[k], self.scale_[k], self.det_scale_[k],
                        n_features)
                    self.bound_prec_[k] -= (self.dof_[k] *
                                            np.trace(self.scale_[k]))
                self.bound_prec_ *= 0.5
                current_log_likelihood = None
                self.converged_ = False
        for i in range(self.n_iter):
            prev_log_likelihood = current_log_likelihood
                        curr_logprob, z = self.score_samples(X)
            current_log_likelihood = (
                curr_logprob.mean() + self._logprior(z) / n_samples)
                        if prev_log_likelihood is not None:
                change = abs(current_log_likelihood - prev_log_likelihood)
                if change < self.tol:
                    self.converged_ = True
                    break
                        self._do_mstep(X, z, self.params)
        if self.n_iter == 0:
                                    z = np.zeros((X.shape[0], self.n_components))
        self._set_weights()
        return z

@deprecated("The `DPGMM` class is not working correctly and it's better "
            "to use `sklearn.mixture.BayesianGaussianMixture` class with "
            "parameter `weight_concentration_prior_type='dirichlet_process'` "
            "instead. DPGMM is deprecated in 0.18 and will be "
            "removed in 0.20.")
class DPGMM(_DPGMMBase):
    
    def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
                 random_state=None, tol=1e-3, verbose=0, min_covar=None,
                 n_iter=10, params='wmc', init_params='wmc'):
        super(DPGMM, self).__init__(
            n_components=n_components, covariance_type=covariance_type,
            alpha=alpha, random_state=random_state, tol=tol, verbose=verbose,
            min_covar=min_covar, n_iter=n_iter, params=params,
            init_params=init_params)

@deprecated("The `VBGMM` class is not working correctly and it's better "
            "to use `sklearn.mixture.BayesianGaussianMixture` class with "
            "parameter `weight_concentration_prior_type="
            "'dirichlet_distribution'` instead. "
            "VBGMM is deprecated in 0.18 and will be removed in 0.20.")
class VBGMM(_DPGMMBase):
    
    def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
                 random_state=None, tol=1e-3, verbose=0,
                 min_covar=None, n_iter=10, params='wmc', init_params='wmc'):
        super(VBGMM, self).__init__(
            n_components, covariance_type, random_state=random_state,
            tol=tol, verbose=verbose, min_covar=min_covar,
            n_iter=n_iter, params=params, init_params=init_params)
        self.alpha = alpha
    def _fit(self, X, y=None):
                self.alpha_ = float(self.alpha) / self.n_components
        return super(VBGMM, self)._fit(X, y)
    def score_samples(self, X):
                check_is_fitted(self, 'gamma_')
        X = check_array(X)
        if X.ndim == 1:
            X = X[:, np.newaxis]
        dg = digamma(self.gamma_) - digamma(np.sum(self.gamma_))
        if self.covariance_type not in ['full', 'tied', 'diag', 'spherical']:
            raise NotImplementedError("This ctype is not implemented: %s"
                                      % self.covariance_type)
        p = _bound_state_log_lik(X, self._initial_bound + self.bound_prec_,
                                 self.precs_, self.means_,
                                 self.covariance_type)
        z = p + dg
        z = log_normalize(z, axis=-1)
        bound = np.sum(z * p, axis=-1)
        return bound, z
    def _update_concentration(self, z):
        for i in range(self.n_components):
            self.gamma_[i] = self.alpha_ + np.sum(z.T[i])
    def _initialize_gamma(self):
        self.gamma_ = self.alpha_ * np.ones(self.n_components)
    def _bound_proportions(self, z):
        logprior = 0.
        dg = digamma(self.gamma_)
        dg -= digamma(np.sum(self.gamma_))
        logprior += np.sum(dg.reshape((-1, 1)) * z.T)
        z_non_zeros = z[z > np.finfo(np.float32).eps]
        logprior -= np.sum(z_non_zeros * np.log(z_non_zeros))
        return logprior
    def _bound_concentration(self):
        logprior = 0.
        logprior = gammaln(np.sum(self.gamma_)) - gammaln(self.n_components
                                                          * self.alpha_)
        logprior -= np.sum(gammaln(self.gamma_) - gammaln(self.alpha_))
        sg = digamma(np.sum(self.gamma_))
        logprior += np.sum((self.gamma_ - self.alpha_)
                           * (digamma(self.gamma_) - sg))
        return logprior
    def _monitor(self, X, z, n, end=False):
                if self.verbose > 0:
            print("Bound after updating %8s: %f" % (n, self.lower_bound(X, z)))
            if end:
                print("Cluster proportions:", self.gamma_)
                print("covariance_type:", self.covariance_type)
    def _set_weights(self):
        self.weights_[:] = self.gamma_
        self.weights_ /= np.sum(self.weights_)

import numpy as np
from scipy import linalg
from .base import BaseMixture, _check_shape
from ..externals.six.moves import zip
from ..utils import check_array
from ..utils.validation import check_is_fitted
from ..utils.extmath import row_norms

def _check_weights(weights, n_components):
        weights = check_array(weights, dtype=[np.float64, np.float32],
                          ensure_2d=False)
    _check_shape(weights, (n_components,), 'weights')
        if (any(np.less(weights, 0.)) or
            any(np.greater(weights, 1.))):
        raise ValueError("The parameter 'weights' should be in the range "
                         "[0, 1], but got max value %.5f, min value %.5f"
                         % (np.min(weights), np.max(weights)))
        if not np.allclose(np.abs(1. - np.sum(weights)), 0.):
        raise ValueError("The parameter 'weights' should be normalized, "
                         "but got sum(weights) = %.5f" % np.sum(weights))
    return weights

def _check_means(means, n_components, n_features):
        means = check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)
    _check_shape(means, (n_components, n_features), 'means')
    return means

def _check_precision_positivity(precision, covariance_type):
        if np.any(np.less_equal(precision, 0.0)):
        raise ValueError("'%s precision' should be "
                         "positive" % covariance_type)

def _check_precision_matrix(precision, covariance_type):
        if not (np.allclose(precision, precision.T) and
            np.all(linalg.eigvalsh(precision) > 0.)):
        raise ValueError("'%s precision' should be symmetric, "
                         "positive-definite" % covariance_type)

def _check_precisions_full(precisions, covariance_type):
        for k, prec in enumerate(precisions):
        prec = _check_precision_matrix(prec, covariance_type)

def _check_precisions(precisions, covariance_type, n_components, n_features):
        precisions = check_array(precisions, dtype=[np.float64, np.float32],
                             ensure_2d=False,
                             allow_nd=covariance_type == 'full')
    precisions_shape = {'full': (n_components, n_features, n_features),
                        'tied': (n_features, n_features),
                        'diag': (n_components, n_features),
                        'spherical': (n_components,)}
    _check_shape(precisions, precisions_shape[covariance_type],
                 '%s precision' % covariance_type)
    _check_precisions = {'full': _check_precisions_full,
                         'tied': _check_precision_matrix,
                         'diag': _check_precision_positivity,
                         'spherical': _check_precision_positivity}
    _check_precisions[covariance_type](precisions, covariance_type)
    return precisions

def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):
        n_components, n_features = means.shape
    covariances = np.empty((n_components, n_features, n_features))
    for k in range(n_components):
        diff = X - means[k]
        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]
        covariances[k].flat[::n_features + 1] += reg_covar
    return covariances

def _estimate_gaussian_covariances_tied(resp, X, nk, means, reg_covar):
        avg_X2 = np.dot(X.T, X)
    avg_means2 = np.dot(nk * means.T, means)
    covariance = avg_X2 - avg_means2
    covariance /= nk.sum()
    covariance.flat[::len(covariance) + 1] += reg_covar
    return covariance

def _estimate_gaussian_covariances_diag(resp, X, nk, means, reg_covar):
        avg_X2 = np.dot(resp.T, X * X) / nk[:, np.newaxis]
    avg_means2 = means ** 2
    avg_X_means = means * np.dot(resp.T, X) / nk[:, np.newaxis]
    return avg_X2 - 2 * avg_X_means + avg_means2 + reg_covar

def _estimate_gaussian_covariances_spherical(resp, X, nk, means, reg_covar):
        return _estimate_gaussian_covariances_diag(resp, X, nk,
                                               means, reg_covar).mean(1)

def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):
        nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps
    means = np.dot(resp.T, X) / nk[:, np.newaxis]
    covariances = {"full": _estimate_gaussian_covariances_full,
                   "tied": _estimate_gaussian_covariances_tied,
                   "diag": _estimate_gaussian_covariances_diag,
                   "spherical": _estimate_gaussian_covariances_spherical
                   }[covariance_type](resp, X, nk, means, reg_covar)
    return nk, means, covariances

def _compute_precision_cholesky(covariances, covariance_type):
        estimate_precision_error_message = (
        "Fitting the mixture model failed because some components have "
        "ill-defined empirical covariance (for instance caused by singleton "
        "or collapsed samples). Try to decrease the number of components, "
        "or increase reg_covar.")
    if covariance_type in 'full':
        n_components, n_features, _ = covariances.shape
        precisions_chol = np.empty((n_components, n_features, n_features))
        for k, covariance in enumerate(covariances):
            try:
                cov_chol = linalg.cholesky(covariance, lower=True)
            except linalg.LinAlgError:
                raise ValueError(estimate_precision_error_message)
            precisions_chol[k] = linalg.solve_triangular(cov_chol,
                                                         np.eye(n_features),
                                                         lower=True).T
    elif covariance_type == 'tied':
        _, n_features = covariances.shape
        try:
            cov_chol = linalg.cholesky(covariances, lower=True)
        except linalg.LinAlgError:
            raise ValueError(estimate_precision_error_message)
        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),
                                                  lower=True).T
    else:
        if np.any(np.less_equal(covariances, 0.0)):
            raise ValueError(estimate_precision_error_message)
        precisions_chol = 1. / np.sqrt(covariances)
    return precisions_chol

def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):
        if covariance_type == 'full':
        n_components, _, _ = matrix_chol.shape
        log_det_chol = (np.sum(np.log(
            matrix_chol.reshape(
                n_components, -1)[:, ::n_features + 1]), 1))
    elif covariance_type == 'tied':
        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))
    elif covariance_type == 'diag':
        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))
    else:
        log_det_chol = n_features * (np.log(matrix_chol))
    return log_det_chol

def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):
        n_samples, n_features = X.shape
    n_components, _ = means.shape
        log_det = _compute_log_det_cholesky(
        precisions_chol, covariance_type, n_features)
    if covariance_type == 'full':
        log_prob = np.empty((n_samples, n_components))
        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):
            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)
            log_prob[:, k] = np.sum(np.square(y), axis=1)
    elif covariance_type == 'tied':
        log_prob = np.empty((n_samples, n_components))
        for k, mu in enumerate(means):
            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)
            log_prob[:, k] = np.sum(np.square(y), axis=1)
    elif covariance_type == 'diag':
        precisions = precisions_chol ** 2
        log_prob = (np.sum((means ** 2 * precisions), 1) -
                    2. * np.dot(X, (means * precisions).T) +
                    np.dot(X ** 2, precisions.T))
    elif covariance_type == 'spherical':
        precisions = precisions_chol ** 2
        log_prob = (np.sum(means ** 2, 1) * precisions -
                    2 * np.dot(X, means.T * precisions) +
                    np.outer(row_norms(X, squared=True), precisions))
    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det

class GaussianMixture(BaseMixture):
    
    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',
                 weights_init=None, means_init=None, precisions_init=None,
                 random_state=None, warm_start=False,
                 verbose=0, verbose_interval=10):
        super(GaussianMixture, self).__init__(
            n_components=n_components, tol=tol, reg_covar=reg_covar,
            max_iter=max_iter, n_init=n_init, init_params=init_params,
            random_state=random_state, warm_start=warm_start,
            verbose=verbose, verbose_interval=verbose_interval)
        self.covariance_type = covariance_type
        self.weights_init = weights_init
        self.means_init = means_init
        self.precisions_init = precisions_init
    def _check_parameters(self, X):
                _, n_features = X.shape
        if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:
            raise ValueError("Invalid value for 'covariance_type': %s "
                             "'covariance_type' should be in "
                             "['spherical', 'tied', 'diag', 'full']"
                             % self.covariance_type)
        if self.weights_init is not None:
            self.weights_init = _check_weights(self.weights_init,
                                               self.n_components)
        if self.means_init is not None:
            self.means_init = _check_means(self.means_init,
                                           self.n_components, n_features)
        if self.precisions_init is not None:
            self.precisions_init = _check_precisions(self.precisions_init,
                                                     self.covariance_type,
                                                     self.n_components,
                                                     n_features)
    def _initialize(self, X, resp):
                n_samples, _ = X.shape
        weights, means, covariances = _estimate_gaussian_parameters(
            X, resp, self.reg_covar, self.covariance_type)
        weights /= n_samples
        self.weights_ = (weights if self.weights_init is None
                         else self.weights_init)
        self.means_ = means if self.means_init is None else self.means_init
        if self.precisions_init is None:
            self.covariances_ = covariances
            self.precisions_cholesky_ = _compute_precision_cholesky(
                covariances, self.covariance_type)
        elif self.covariance_type == 'full':
            self.precisions_cholesky_ = np.array(
                [linalg.cholesky(prec_init, lower=True)
                 for prec_init in self.precisions_init])
        elif self.covariance_type == 'tied':
            self.precisions_cholesky_ = linalg.cholesky(self.precisions_init,
                                                        lower=True)
        else:
            self.precisions_cholesky_ = self.precisions_init
    def _m_step(self, X, log_resp):
                n_samples, _ = X.shape
        self.weights_, self.means_, self.covariances_ = (
            _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,
                                          self.covariance_type))
        self.weights_ /= n_samples
        self.precisions_cholesky_ = _compute_precision_cholesky(
            self.covariances_, self.covariance_type)
    def _estimate_log_prob(self, X):
        return _estimate_log_gaussian_prob(
            X, self.means_, self.precisions_cholesky_, self.covariance_type)
    def _estimate_log_weights(self):
        return np.log(self.weights_)
    def _compute_lower_bound(self, _, log_prob_norm):
        return log_prob_norm
    def _check_is_fitted(self):
        check_is_fitted(self, ['weights_', 'means_', 'precisions_cholesky_'])
    def _get_parameters(self):
        return (self.weights_, self.means_, self.covariances_,
                self.precisions_cholesky_)
    def _set_parameters(self, params):
        (self.weights_, self.means_, self.covariances_,
         self.precisions_cholesky_) = params
                _, n_features = self.means_.shape
        if self.covariance_type == 'full':
            self.precisions_ = np.empty(self.precisions_cholesky_.shape)
            for k, prec_chol in enumerate(self.precisions_cholesky_):
                self.precisions_[k] = np.dot(prec_chol, prec_chol.T)
        elif self.covariance_type == 'tied':
            self.precisions_ = np.dot(self.precisions_cholesky_,
                                      self.precisions_cholesky_.T)
        else:
            self.precisions_ = self.precisions_cholesky_ ** 2
    def _n_parameters(self):
                _, n_features = self.means_.shape
        if self.covariance_type == 'full':
            cov_params = self.n_components * n_features * (n_features + 1) / 2.
        elif self.covariance_type == 'diag':
            cov_params = self.n_components * n_features
        elif self.covariance_type == 'tied':
            cov_params = n_features * (n_features + 1) / 2.
        elif self.covariance_type == 'spherical':
            cov_params = self.n_components
        mean_params = n_features * self.n_components
        return int(cov_params + mean_params + self.n_components - 1)
    def bic(self, X):
                return (-2 * self.score(X) * X.shape[0] +
                self._n_parameters() * np.log(X.shape[0]))
    def aic(self, X):
                return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()

import numpy as np
from scipy import linalg
from time import time
from ..base import BaseEstimator
from ..utils import check_random_state, check_array, deprecated
from ..utils.extmath import logsumexp
from ..utils.validation import check_is_fitted
from .. import cluster
from sklearn.externals.six.moves import zip
EPS = np.finfo(float).eps
@deprecated("The function log_multivariate_normal_density is deprecated in 0.18"
            " and will be removed in 0.20.")
def log_multivariate_normal_density(X, means, covars, covariance_type='diag'):
        log_multivariate_normal_density_dict = {
        'spherical': _log_multivariate_normal_density_spherical,
        'tied': _log_multivariate_normal_density_tied,
        'diag': _log_multivariate_normal_density_diag,
        'full': _log_multivariate_normal_density_full}
    return log_multivariate_normal_density_dict[covariance_type](
        X, means, covars)

@deprecated("The function sample_gaussian is deprecated in 0.18"
            " and will be removed in 0.20."
            " Use numpy.random.multivariate_normal instead.")
def sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,
                    random_state=None):
        _sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,
                     random_state=None)

def _sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,
                     random_state=None):
    rng = check_random_state(random_state)
    n_dim = len(mean)
    rand = rng.randn(n_dim, n_samples)
    if n_samples == 1:
        rand.shape = (n_dim,)
    if covariance_type == 'spherical':
        rand *= np.sqrt(covar)
    elif covariance_type == 'diag':
        rand = np.dot(np.diag(np.sqrt(covar)), rand)
    else:
        s, U = linalg.eigh(covar)
        s.clip(0, out=s)          np.sqrt(s, out=s)
        U *= s
        rand = np.dot(U, rand)
    return (rand.T + mean).T

class _GMMBase(BaseEstimator):
    
    def __init__(self, n_components=1, covariance_type='diag',
                 random_state=None, tol=1e-3, min_covar=1e-3,
                 n_iter=100, n_init=1, params='wmc', init_params='wmc',
                 verbose=0):
        self.n_components = n_components
        self.covariance_type = covariance_type
        self.tol = tol
        self.min_covar = min_covar
        self.random_state = random_state
        self.n_iter = n_iter
        self.n_init = n_init
        self.params = params
        self.init_params = init_params
        self.verbose = verbose
        if covariance_type not in ['spherical', 'tied', 'diag', 'full']:
            raise ValueError('Invalid value for covariance_type: %s' %
                             covariance_type)
        if n_init < 1:
            raise ValueError('GMM estimation requires at least one run')
    def _get_covars(self):
                if self.covariance_type == 'full':
            return self.covars_
        elif self.covariance_type == 'diag':
            return [np.diag(cov) for cov in self.covars_]
        elif self.covariance_type == 'tied':
            return [self.covars_] * self.n_components
        elif self.covariance_type == 'spherical':
            return [np.diag(cov) for cov in self.covars_]
    def _set_covars(self, covars):
                covars = np.asarray(covars)
        _validate_covars(covars, self.covariance_type, self.n_components)
        self.covars_ = covars
    def score_samples(self, X):
                check_is_fitted(self, 'means_')
        X = check_array(X)
        if X.ndim == 1:
            X = X[:, np.newaxis]
        if X.size == 0:
            return np.array([]), np.empty((0, self.n_components))
        if X.shape[1] != self.means_.shape[1]:
            raise ValueError('The shape of X  is not compatible with self')
        lpr = (log_multivariate_normal_density(X, self.means_, self.covars_,
                                               self.covariance_type) +
               np.log(self.weights_))
        logprob = logsumexp(lpr, axis=1)
        responsibilities = np.exp(lpr - logprob[:, np.newaxis])
        return logprob, responsibilities
    def score(self, X, y=None):
                logprob, _ = self.score_samples(X)
        return logprob
    def predict(self, X):
                logprob, responsibilities = self.score_samples(X)
        return responsibilities.argmax(axis=1)
    def predict_proba(self, X):
                logprob, responsibilities = self.score_samples(X)
        return responsibilities
    def sample(self, n_samples=1, random_state=None):
                check_is_fitted(self, 'means_')
        if random_state is None:
            random_state = self.random_state
        random_state = check_random_state(random_state)
        weight_cdf = np.cumsum(self.weights_)
        X = np.empty((n_samples, self.means_.shape[1]))
        rand = random_state.rand(n_samples)
                comps = weight_cdf.searchsorted(rand)
                for comp in range(self.n_components):
                        comp_in_X = (comp == comps)
                        num_comp_in_X = comp_in_X.sum()
            if num_comp_in_X > 0:
                if self.covariance_type == 'tied':
                    cv = self.covars_
                elif self.covariance_type == 'spherical':
                    cv = self.covars_[comp][0]
                else:
                    cv = self.covars_[comp]
                X[comp_in_X] = _sample_gaussian(
                    self.means_[comp], cv, self.covariance_type,
                    num_comp_in_X, random_state=random_state).T
        return X
    def fit_predict(self, X, y=None):
                return self._fit(X, y).argmax(axis=1)
    def _fit(self, X, y=None, do_prediction=False):
        
                X = check_array(X, dtype=np.float64, ensure_min_samples=2,
                        estimator=self)
        if X.shape[0] < self.n_components:
            raise ValueError(
                'GMM estimation with %s components, but got only %s samples' %
                (self.n_components, X.shape[0]))
        max_log_prob = -np.infty
        if self.verbose > 0:
            print('Expectation-maximization algorithm started.')
        for init in range(self.n_init):
            if self.verbose > 0:
                print('Initialization ' + str(init + 1))
                start_init_time = time()
            if 'm' in self.init_params or not hasattr(self, 'means_'):
                self.means_ = cluster.KMeans(
                    n_clusters=self.n_components,
                    random_state=self.random_state).fit(X).cluster_centers_
                if self.verbose > 1:
                    print('\tMeans have been initialized.')
            if 'w' in self.init_params or not hasattr(self, 'weights_'):
                self.weights_ = np.tile(1.0 / self.n_components,
                                        self.n_components)
                if self.verbose > 1:
                    print('\tWeights have been initialized.')
            if 'c' in self.init_params or not hasattr(self, 'covars_'):
                cv = np.cov(X.T) + self.min_covar * np.eye(X.shape[1])
                if not cv.shape:
                    cv.shape = (1, 1)
                self.covars_ = \
                    distribute_covar_matrix_to_match_covariance_type(
                        cv, self.covariance_type, self.n_components)
                if self.verbose > 1:
                    print('\tCovariance matrices have been initialized.')
                        current_log_likelihood = None
                        self.converged_ = False
            for i in range(self.n_iter):
                if self.verbose > 0:
                    print('\tEM iteration ' + str(i + 1))
                    start_iter_time = time()
                prev_log_likelihood = current_log_likelihood
                                log_likelihoods, responsibilities = self.score_samples(X)
                current_log_likelihood = log_likelihoods.mean()
                                if prev_log_likelihood is not None:
                    change = abs(current_log_likelihood - prev_log_likelihood)
                    if self.verbose > 1:
                        print('\t\tChange: ' + str(change))
                    if change < self.tol:
                        self.converged_ = True
                        if self.verbose > 0:
                            print('\t\tEM algorithm converged.')
                        break
                                self._do_mstep(X, responsibilities, self.params,
                               self.min_covar)
                if self.verbose > 1:
                    print('\t\tEM iteration ' + str(i + 1) + ' took {0:.5f}s'.format(
                        time() - start_iter_time))
                        if self.n_iter:
                if current_log_likelihood > max_log_prob:
                    max_log_prob = current_log_likelihood
                    best_params = {'weights': self.weights_,
                                   'means': self.means_,
                                   'covars': self.covars_}
                    if self.verbose > 1:
                        print('\tBetter parameters were found.')
            if self.verbose > 1:
                print('\tInitialization ' + str(init + 1) + ' took {0:.5f}s'.format(
                    time() - start_init_time))
                        if np.isneginf(max_log_prob) and self.n_iter:
            raise RuntimeError(
                "EM algorithm was never able to compute a valid likelihood " +
                "given initial parameters. Try different init parameters " +
                "(or increasing n_init) or check for degenerate data.")
        if self.n_iter:
            self.covars_ = best_params['covars']
            self.means_ = best_params['means']
            self.weights_ = best_params['weights']
        else:                                      responsibilities = np.zeros((X.shape[0], self.n_components))
        return responsibilities
    def fit(self, X, y=None):
                self._fit(X, y)
        return self
    def _do_mstep(self, X, responsibilities, params, min_covar=0):
                weights = responsibilities.sum(axis=0)
        weighted_X_sum = np.dot(responsibilities.T, X)
        inverse_weights = 1.0 / (weights[:, np.newaxis] + 10 * EPS)
        if 'w' in params:
            self.weights_ = (weights / (weights.sum() + 10 * EPS) + EPS)
        if 'm' in params:
            self.means_ = weighted_X_sum * inverse_weights
        if 'c' in params:
            covar_mstep_func = _covar_mstep_funcs[self.covariance_type]
            self.covars_ = covar_mstep_func(
                self, X, responsibilities, weighted_X_sum, inverse_weights,
                min_covar)
        return weights
    def _n_parameters(self):
                ndim = self.means_.shape[1]
        if self.covariance_type == 'full':
            cov_params = self.n_components * ndim * (ndim + 1) / 2.
        elif self.covariance_type == 'diag':
            cov_params = self.n_components * ndim
        elif self.covariance_type == 'tied':
            cov_params = ndim * (ndim + 1) / 2.
        elif self.covariance_type == 'spherical':
            cov_params = self.n_components
        mean_params = ndim * self.n_components
        return int(cov_params + mean_params + self.n_components - 1)
    def bic(self, X):
                return (-2 * self.score(X).sum() +
                self._n_parameters() * np.log(X.shape[0]))
    def aic(self, X):
                return - 2 * self.score(X).sum() + 2 * self._n_parameters()

@deprecated("The class GMM is deprecated in 0.18 and will be "
            " removed in 0.20. Use class GaussianMixture instead.")
class GMM(_GMMBase):
    
    def __init__(self, n_components=1, covariance_type='diag',
                 random_state=None, tol=1e-3, min_covar=1e-3,
                 n_iter=100, n_init=1, params='wmc', init_params='wmc',
                 verbose=0):
        super(GMM, self).__init__(
            n_components=n_components, covariance_type=covariance_type,
            random_state=random_state, tol=tol, min_covar=min_covar,
            n_iter=n_iter, n_init=n_init, params=params,
            init_params=init_params, verbose=verbose)

def _log_multivariate_normal_density_diag(X, means, covars):
        n_samples, n_dim = X.shape
    lpr = -0.5 * (n_dim * np.log(2 * np.pi) + np.sum(np.log(covars), 1)
                  + np.sum((means ** 2) / covars, 1)
                  - 2 * np.dot(X, (means / covars).T)
                  + np.dot(X ** 2, (1.0 / covars).T))
    return lpr

def _log_multivariate_normal_density_spherical(X, means, covars):
        cv = covars.copy()
    if covars.ndim == 1:
        cv = cv[:, np.newaxis]
    if cv.shape[1] == 1:
        cv = np.tile(cv, (1, X.shape[-1]))
    return _log_multivariate_normal_density_diag(X, means, cv)

def _log_multivariate_normal_density_tied(X, means, covars):
        cv = np.tile(covars, (means.shape[0], 1, 1))
    return _log_multivariate_normal_density_full(X, means, cv)

def _log_multivariate_normal_density_full(X, means, covars, min_covar=1.e-7):
        n_samples, n_dim = X.shape
    nmix = len(means)
    log_prob = np.empty((n_samples, nmix))
    for c, (mu, cv) in enumerate(zip(means, covars)):
        try:
            cv_chol = linalg.cholesky(cv, lower=True)
        except linalg.LinAlgError:
                                    try:
                cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),
                                          lower=True)
            except linalg.LinAlgError:
                raise ValueError("'covars' must be symmetric, "
                                 "positive-definite")
        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))
        cv_sol = linalg.solve_triangular(cv_chol, (X - mu).T, lower=True).T
        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) +
                                 n_dim * np.log(2 * np.pi) + cv_log_det)
    return log_prob

def _validate_covars(covars, covariance_type, n_components):
        from scipy import linalg
    if covariance_type == 'spherical':
        if len(covars) != n_components:
            raise ValueError("'spherical' covars have length n_components")
        elif np.any(covars <= 0):
            raise ValueError("'spherical' covars must be non-negative")
    elif covariance_type == 'tied':
        if covars.shape[0] != covars.shape[1]:
            raise ValueError("'tied' covars must have shape (n_dim, n_dim)")
        elif (not np.allclose(covars, covars.T)
              or np.any(linalg.eigvalsh(covars) <= 0)):
            raise ValueError("'tied' covars must be symmetric, "
                             "positive-definite")
    elif covariance_type == 'diag':
        if len(covars.shape) != 2:
            raise ValueError("'diag' covars must have shape "
                             "(n_components, n_dim)")
        elif np.any(covars <= 0):
            raise ValueError("'diag' covars must be non-negative")
    elif covariance_type == 'full':
        if len(covars.shape) != 3:
            raise ValueError("'full' covars must have shape "
                             "(n_components, n_dim, n_dim)")
        elif covars.shape[1] != covars.shape[2]:
            raise ValueError("'full' covars must have shape "
                             "(n_components, n_dim, n_dim)")
        for n, cv in enumerate(covars):
            if (not np.allclose(cv, cv.T)
                    or np.any(linalg.eigvalsh(cv) <= 0)):
                raise ValueError("component %d of 'full' covars must be "
                                 "symmetric, positive-definite" % n)
    else:
        raise ValueError("covariance_type must be one of " +
                         "'spherical', 'tied', 'diag', 'full'")

@deprecated("The functon distribute_covar_matrix_to_match_covariance_type"
            "is deprecated in 0.18 and will be removed in 0.20.")
def distribute_covar_matrix_to_match_covariance_type(
        tied_cv, covariance_type, n_components):
        if covariance_type == 'spherical':
        cv = np.tile(tied_cv.mean() * np.ones(tied_cv.shape[1]),
                     (n_components, 1))
    elif covariance_type == 'tied':
        cv = tied_cv
    elif covariance_type == 'diag':
        cv = np.tile(np.diag(tied_cv), (n_components, 1))
    elif covariance_type == 'full':
        cv = np.tile(tied_cv, (n_components, 1, 1))
    else:
        raise ValueError("covariance_type must be one of " +
                         "'spherical', 'tied', 'diag', 'full'")
    return cv

def _covar_mstep_diag(gmm, X, responsibilities, weighted_X_sum, norm,
                      min_covar):
        avg_X2 = np.dot(responsibilities.T, X * X) * norm
    avg_means2 = gmm.means_ ** 2
    avg_X_means = gmm.means_ * weighted_X_sum * norm
    return avg_X2 - 2 * avg_X_means + avg_means2 + min_covar

def _covar_mstep_spherical(*args):
        cv = _covar_mstep_diag(*args)
    return np.tile(cv.mean(axis=1)[:, np.newaxis], (1, cv.shape[1]))

def _covar_mstep_full(gmm, X, responsibilities, weighted_X_sum, norm,
                      min_covar):
                n_features = X.shape[1]
    cv = np.empty((gmm.n_components, n_features, n_features))
    for c in range(gmm.n_components):
        post = responsibilities[:, c]
        mu = gmm.means_[c]
        diff = X - mu
        with np.errstate(under='ignore'):
                        avg_cv = np.dot(post * diff.T, diff) / (post.sum() + 10 * EPS)
        cv[c] = avg_cv + min_covar * np.eye(n_features)
    return cv

def _covar_mstep_tied(gmm, X, responsibilities, weighted_X_sum, norm,
                      min_covar):
                avg_X2 = np.dot(X.T, X)
    avg_means2 = np.dot(gmm.means_.T, weighted_X_sum)
    out = avg_X2 - avg_means2
    out *= 1. / X.shape[0]
    out.flat[::len(out) + 1] += min_covar
    return out
_covar_mstep_funcs = {'spherical': _covar_mstep_spherical,
                      'diag': _covar_mstep_diag,
                      'tied': _covar_mstep_tied,
                      'full': _covar_mstep_full,
                      }
from .gmm import sample_gaussian, log_multivariate_normal_density
from .gmm import GMM, distribute_covar_matrix_to_match_covariance_type
from .gmm import _validate_covars
from .dpgmm import DPGMM, VBGMM
from .gaussian_mixture import GaussianMixture
from .bayesian_mixture import BayesianGaussianMixture

__all__ = ['DPGMM',
           'GMM',
           'VBGMM',
           '_validate_covars',
           'distribute_covar_matrix_to_match_covariance_type',
           'log_multivariate_normal_density',
           'sample_gaussian',
           'GaussianMixture',
           'BayesianGaussianMixture']
from __future__ import print_function
from __future__ import division

from abc import ABCMeta, abstractmethod
from collections import Mapping, namedtuple, defaultdict, Sequence
from functools import partial, reduce
from itertools import product
import operator
import warnings
import numpy as np
from ..base import BaseEstimator, is_classifier, clone
from ..base import MetaEstimatorMixin
from ._split import check_cv
from ._validation import _fit_and_score
from ..exceptions import NotFittedError
from ..externals.joblib import Parallel, delayed
from ..externals import six
from ..utils import check_random_state
from ..utils.fixes import sp_version
from ..utils.fixes import rankdata
from ..utils.fixes import MaskedArray
from ..utils.random import sample_without_replacement
from ..utils.validation import indexable, check_is_fitted
from ..utils.metaestimators import if_delegate_has_method
from ..metrics.scorer import check_scoring

__all__ = ['GridSearchCV', 'ParameterGrid', 'fit_grid_point',
           'ParameterSampler', 'RandomizedSearchCV']

class ParameterGrid(object):
    
    def __init__(self, param_grid):
        if isinstance(param_grid, Mapping):
                                    param_grid = [param_grid]
        self.param_grid = param_grid
    def __iter__(self):
                for p in self.param_grid:
                        items = sorted(p.items())
            if not items:
                yield {}
            else:
                keys, values = zip(*items)
                for v in product(*values):
                    params = dict(zip(keys, v))
                    yield params
    def __len__(self):
                        product = partial(reduce, operator.mul)
        return sum(product(len(v) for v in p.values()) if p else 1
                   for p in self.param_grid)
    def __getitem__(self, ind):
                                for sub_grid in self.param_grid:
                        if not sub_grid:
                if ind == 0:
                    return {}
                else:
                    ind -= 1
                    continue
                        keys, values_lists = zip(*sorted(sub_grid.items())[::-1])
            sizes = [len(v_list) for v_list in values_lists]
            total = np.product(sizes)
            if ind >= total:
                                ind -= total
            else:
                out = {}
                for key, v_list, n in zip(keys, values_lists, sizes):
                    ind, offset = divmod(ind, n)
                    out[key] = v_list[offset]
                return out
        raise IndexError('ParameterGrid index out of range')

class ParameterSampler(object):
        def __init__(self, param_distributions, n_iter, random_state=None):
        self.param_distributions = param_distributions
        self.n_iter = n_iter
        self.random_state = random_state
    def __iter__(self):
                        all_lists = np.all([not hasattr(v, "rvs")
                            for v in self.param_distributions.values()])
        rnd = check_random_state(self.random_state)
        if all_lists:
                        param_grid = ParameterGrid(self.param_distributions)
            grid_size = len(param_grid)
            if grid_size < self.n_iter:
                raise ValueError(
                    "The total space of parameters %d is smaller "
                    "than n_iter=%d. For exhaustive searches, use "
                    "GridSearchCV." % (grid_size, self.n_iter))
            for i in sample_without_replacement(grid_size, self.n_iter,
                                                random_state=rnd):
                yield param_grid[i]
        else:
                        items = sorted(self.param_distributions.items())
            for _ in six.moves.range(self.n_iter):
                params = dict()
                for k, v in items:
                    if hasattr(v, "rvs"):
                        if sp_version < (0, 16):
                            params[k] = v.rvs()
                        else:
                            params[k] = v.rvs(random_state=rnd)
                    else:
                        params[k] = v[rnd.randint(len(v))]
                yield params
    def __len__(self):
                return self.n_iter

def fit_grid_point(X, y, estimator, parameters, train, test, scorer,
                   verbose, error_score='raise', **fit_params):
        score, n_samples_test, _ = _fit_and_score(estimator, X, y, scorer, train,
                                              test, verbose, parameters,
                                              fit_params=fit_params,
                                              return_n_test_samples=True,
                                              error_score=error_score)
    return score, parameters, n_samples_test

def _check_param_grid(param_grid):
    if hasattr(param_grid, 'items'):
        param_grid = [param_grid]
    for p in param_grid:
        for name, v in p.items():
            if isinstance(v, np.ndarray) and v.ndim > 1:
                raise ValueError("Parameter array should be one-dimensional.")
            if (isinstance(v, six.string_types) or
                    not isinstance(v, (np.ndarray, Sequence))):
                raise ValueError("Parameter values for parameter ({0}) need "
                                 "to be a sequence(but not a string) or"
                                 " np.ndarray.".format(name))
            if len(v) == 0:
                raise ValueError("Parameter values for parameter ({0}) need "
                                 "to be a non-empty sequence.".format(name))

class _CVScoreTuple (namedtuple('_CVScoreTuple',
                                ('parameters',
                                 'mean_validation_score',
                                 'cv_validation_scores'))):
                                    __slots__ = ()
    def __repr__(self):
                return "mean: {0:.5f}, std: {1:.5f}, params: {2}".format(
            self.mean_validation_score,
            np.std(self.cv_validation_scores),
            self.parameters)

class BaseSearchCV(six.with_metaclass(ABCMeta, BaseEstimator,
                                      MetaEstimatorMixin)):
    
    @abstractmethod
    def __init__(self, estimator, scoring=None,
                 fit_params=None, n_jobs=1, iid=True,
                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
                 error_score='raise', return_train_score=True):
        self.scoring = scoring
        self.estimator = estimator
        self.n_jobs = n_jobs
        self.fit_params = fit_params if fit_params is not None else {}
        self.iid = iid
        self.refit = refit
        self.cv = cv
        self.verbose = verbose
        self.pre_dispatch = pre_dispatch
        self.error_score = error_score
        self.return_train_score = return_train_score
    @property
    def _estimator_type(self):
        return self.estimator._estimator_type
    def score(self, X, y=None):
                if self.scorer_ is None:
            raise ValueError("No score function explicitly defined, "
                             "and the estimator doesn't provide one %s"
                             % self.best_estimator_)
        return self.scorer_(self.best_estimator_, X, y)
    def _check_is_fitted(self, method_name):
        if not self.refit:
            raise NotFittedError(('This GridSearchCV instance was initialized '
                                  'with refit=False. %s is '
                                  'available only after refitting on the best '
                                  'parameters. ') % method_name)
        else:
            check_is_fitted(self, 'best_estimator_')
    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
    def predict(self, X):
                self._check_is_fitted('predict')
        return self.best_estimator_.predict(X)
    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
    def predict_proba(self, X):
                self._check_is_fitted('predict_proba')
        return self.best_estimator_.predict_proba(X)
    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
    def predict_log_proba(self, X):
                self._check_is_fitted('predict_log_proba')
        return self.best_estimator_.predict_log_proba(X)
    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
    def decision_function(self, X):
                self._check_is_fitted('decision_function')
        return self.best_estimator_.decision_function(X)
    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
    def transform(self, X):
                self._check_is_fitted('transform')
        return self.best_estimator_.transform(X)
    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
    def inverse_transform(self, Xt):
                self._check_is_fitted('inverse_transform')
        return self.best_estimator_.inverse_transform(Xt)
    @property
    def classes_(self):
        self._check_is_fitted("classes_")
        return self.best_estimator_.classes_
    def fit(self, X, y=None, groups=None, **fit_params):
                if self.fit_params:
            warnings.warn('"fit_params" as a constructor argument was '
                          'deprecated in version 0.19 and will be removed '
                          'in version 0.21. Pass fit parameters to the '
                          '"fit" method instead.', DeprecationWarning)
            if fit_params:
                warnings.warn('Ignoring fit_params passed as a constructor '
                              'argument in favor of keyword arguments to '
                              'the "fit" method.', RuntimeWarning)
            else:
                fit_params = self.fit_params
        estimator = self.estimator
        cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)
        X, y, groups = indexable(X, y, groups)
        n_splits = cv.get_n_splits(X, y, groups)
                candidate_params = list(self._get_param_iterator())
        n_candidates = len(candidate_params)
        if self.verbose > 0:
            print("Fitting {0} folds for each of {1} candidates, totalling"
                  " {2} fits".format(n_splits, n_candidates,
                                     n_candidates * n_splits))
        base_estimator = clone(self.estimator)
        pre_dispatch = self.pre_dispatch
        out = Parallel(
            n_jobs=self.n_jobs, verbose=self.verbose,
            pre_dispatch=pre_dispatch
        )(delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,
                                  train, test, self.verbose, parameters,
                                  fit_params=fit_params,
                                  return_train_score=self.return_train_score,
                                  return_n_test_samples=True,
                                  return_times=True, return_parameters=False,
                                  error_score=self.error_score)
          for train, test in cv.split(X, y, groups)
          for parameters in candidate_params)
                if self.return_train_score:
            (train_scores, test_scores, test_sample_counts, fit_time,
             score_time) = zip(*out)
        else:
            (test_scores, test_sample_counts, fit_time, score_time) = zip(*out)
        results = dict()
        def _store(key_name, array, weights=None, splits=False, rank=False):
                                    array = np.array(array, dtype=np.float64).reshape(n_splits,
                                                              n_candidates).T
            if splits:
                for split_i in range(n_splits):
                    results["split%d_%s"
                            % (split_i, key_name)] = array[:, split_i]
            array_means = np.average(array, axis=1, weights=weights)
            results['mean_%s' % key_name] = array_means
                        array_stds = np.sqrt(np.average((array -
                                             array_means[:, np.newaxis]) ** 2,
                                            axis=1, weights=weights))
            results['std_%s' % key_name] = array_stds
            if rank:
                results["rank_%s" % key_name] = np.asarray(
                    rankdata(-array_means, method='min'), dtype=np.int32)
                        test_sample_counts = np.array(test_sample_counts[::n_candidates],
                                      dtype=np.int)
        _store('test_score', test_scores, splits=True, rank=True,
               weights=test_sample_counts if self.iid else None)
        if self.return_train_score:
            _store('train_score', train_scores, splits=True)
        _store('fit_time', fit_time)
        _store('score_time', score_time)
        best_index = np.flatnonzero(results["rank_test_score"] == 1)[0]
        best_parameters = candidate_params[best_index]
                                param_results = defaultdict(partial(MaskedArray,
                                            np.empty(n_candidates,),
                                            mask=True,
                                            dtype=object))
        for cand_i, params in enumerate(candidate_params):
            for name, value in params.items():
                                                                param_results["param_%s" % name][cand_i] = value
        results.update(param_results)
                results['params'] = candidate_params
        self.cv_results_ = results
        self.best_index_ = best_index
        self.n_splits_ = n_splits
        if self.refit:
                                    best_estimator = clone(base_estimator).set_params(
                **best_parameters)
            if y is not None:
                best_estimator.fit(X, y, **fit_params)
            else:
                best_estimator.fit(X, **fit_params)
            self.best_estimator_ = best_estimator
        return self
    @property
    def best_params_(self):
        check_is_fitted(self, 'cv_results_')
        return self.cv_results_['params'][self.best_index_]
    @property
    def best_score_(self):
        check_is_fitted(self, 'cv_results_')
        return self.cv_results_['mean_test_score'][self.best_index_]
    @property
    def grid_scores_(self):
        warnings.warn(
            "The grid_scores_ attribute was deprecated in version 0.18"
            " in favor of the more elaborate cv_results_ attribute."
            " The grid_scores_ attribute will not be available from 0.20",
            DeprecationWarning)
        check_is_fitted(self, 'cv_results_')
        grid_scores = list()
        for i, (params, mean, std) in enumerate(zip(
                self.cv_results_['params'],
                self.cv_results_['mean_test_score'],
                self.cv_results_['std_test_score'])):
            scores = np.array(list(self.cv_results_['split%d_test_score'
                                                    % s][i]
                                   for s in range(self.n_splits_)),
                              dtype=np.float64)
            grid_scores.append(_CVScoreTuple(params, mean, scores))
        return grid_scores

class GridSearchCV(BaseSearchCV):
    
    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,
                 pre_dispatch='2*n_jobs', error_score='raise',
                 return_train_score=True):
        super(GridSearchCV, self).__init__(
            estimator=estimator, scoring=scoring, fit_params=fit_params,
            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
            pre_dispatch=pre_dispatch, error_score=error_score,
            return_train_score=return_train_score)
        self.param_grid = param_grid
        _check_param_grid(param_grid)
    def _get_param_iterator(self):
                return ParameterGrid(self.param_grid)

class RandomizedSearchCV(BaseSearchCV):
    
    def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
                 fit_params=None, n_jobs=1, iid=True, refit=True, cv=None,
                 verbose=0, pre_dispatch='2*n_jobs', random_state=None,
                 error_score='raise', return_train_score=True):
        self.param_distributions = param_distributions
        self.n_iter = n_iter
        self.random_state = random_state
        super(RandomizedSearchCV, self).__init__(
             estimator=estimator, scoring=scoring, fit_params=fit_params,
             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
             pre_dispatch=pre_dispatch, error_score=error_score,
             return_train_score=return_train_score)
    def _get_param_iterator(self):
                return ParameterSampler(
            self.param_distributions, self.n_iter,
            random_state=self.random_state)

from __future__ import print_function
from __future__ import division
import warnings
from itertools import chain, combinations
from collections import Iterable
from math import ceil, floor
import numbers
from abc import ABCMeta, abstractmethod
import numpy as np
from scipy.misc import comb
from ..utils import indexable, check_random_state, safe_indexing
from ..utils.validation import _num_samples, column_or_1d
from ..utils.validation import check_array
from ..utils.multiclass import type_of_target
from ..externals.six import with_metaclass
from ..externals.six.moves import zip
from ..utils.fixes import bincount
from ..utils.fixes import signature
from ..utils.random import choice
from ..base import _pprint
__all__ = ['BaseCrossValidator',
           'KFold',
           'GroupKFold',
           'LeaveOneGroupOut',
           'LeaveOneOut',
           'LeavePGroupsOut',
           'LeavePOut',
           'RepeatedStratifiedKFold',
           'RepeatedKFold',
           'ShuffleSplit',
           'GroupShuffleSplit',
           'StratifiedKFold',
           'StratifiedShuffleSplit',
           'PredefinedSplit',
           'train_test_split',
           'check_cv']

class BaseCrossValidator(with_metaclass(ABCMeta)):
    
    def __init__(self):
                        pass
    def split(self, X, y=None, groups=None):
                X, y, groups = indexable(X, y, groups)
        indices = np.arange(_num_samples(X))
        for test_index in self._iter_test_masks(X, y, groups):
            train_index = indices[np.logical_not(test_index)]
            test_index = indices[test_index]
            yield train_index, test_index
            def _iter_test_masks(self, X=None, y=None, groups=None):
                for test_index in self._iter_test_indices(X, y, groups):
            test_mask = np.zeros(_num_samples(X), dtype=np.bool)
            test_mask[test_index] = True
            yield test_mask
    def _iter_test_indices(self, X=None, y=None, groups=None):
                raise NotImplementedError
    @abstractmethod
    def get_n_splits(self, X=None, y=None, groups=None):
                if X is None:
            raise ValueError("The X parameter should not be None")
        return _num_samples(X)

class LeavePOut(BaseCrossValidator):
    
    def __init__(self, p):
        self.p = p
    def _iter_test_indices(self, X, y=None, groups=None):
        for combination in combinations(range(_num_samples(X)), self.p):
            yield np.array(combination)
    def get_n_splits(self, X, y=None, groups=None):
                if X is None:
            raise ValueError("The X parameter should not be None")
        return int(comb(_num_samples(X), self.p, exact=True))

class _BaseKFold(with_metaclass(ABCMeta, BaseCrossValidator)):
    
    @abstractmethod
    def __init__(self, n_splits, shuffle, random_state):
        if not isinstance(n_splits, numbers.Integral):
            raise ValueError('The number of folds must be of Integral type. '
                             '%s of type %s was passed.'
                             % (n_splits, type(n_splits)))
        n_splits = int(n_splits)
        if n_splits <= 1:
            raise ValueError(
                "k-fold cross-validation requires at least one"
                " train/test split by setting n_splits=2 or more,"
                " got n_splits={0}.".format(n_splits))
        if not isinstance(shuffle, bool):
            raise TypeError("shuffle must be True or False;"
                            " got {0}".format(shuffle))
        self.n_splits = n_splits
        self.shuffle = shuffle
        self.random_state = random_state
    def split(self, X, y=None, groups=None):
                X, y, groups = indexable(X, y, groups)
        n_samples = _num_samples(X)
        if self.n_splits > n_samples:
            raise ValueError(
                ("Cannot have number of splits n_splits={0} greater"
                 " than the number of samples: {1}.").format(self.n_splits,
                                                             n_samples))
        for train, test in super(_BaseKFold, self).split(X, y, groups):
            yield train, test
    def get_n_splits(self, X=None, y=None, groups=None):
                return self.n_splits

class KFold(_BaseKFold):
    
    def __init__(self, n_splits=3, shuffle=False,
                 random_state=None):
        super(KFold, self).__init__(n_splits, shuffle, random_state)
    def _iter_test_indices(self, X, y=None, groups=None):
        n_samples = _num_samples(X)
        indices = np.arange(n_samples)
        if self.shuffle:
            check_random_state(self.random_state).shuffle(indices)
        n_splits = self.n_splits
        fold_sizes = (n_samples // n_splits) * np.ones(n_splits, dtype=np.int)
        fold_sizes[:n_samples % n_splits] += 1
        current = 0
        for fold_size in fold_sizes:
            start, stop = current, current + fold_size
            yield indices[start:stop]
            current = stop

class GroupKFold(_BaseKFold):
        def __init__(self, n_splits=3):
        super(GroupKFold, self).__init__(n_splits, shuffle=False,
                                         random_state=None)
    def _iter_test_indices(self, X, y, groups):
        if groups is None:
            raise ValueError("The groups parameter should not be None")
        groups = check_array(groups, ensure_2d=False, dtype=None)
        unique_groups, groups = np.unique(groups, return_inverse=True)
        n_groups = len(unique_groups)
        if self.n_splits > n_groups:
            raise ValueError("Cannot have number of splits n_splits=%d greater"
                             " than the number of groups: %d."
                             % (self.n_splits, n_groups))
                n_samples_per_group = np.bincount(groups)
                indices = np.argsort(n_samples_per_group)[::-1]
        n_samples_per_group = n_samples_per_group[indices]
                n_samples_per_fold = np.zeros(self.n_splits)
                group_to_fold = np.zeros(len(unique_groups))
                for group_index, weight in enumerate(n_samples_per_group):
            lightest_fold = np.argmin(n_samples_per_fold)
            n_samples_per_fold[lightest_fold] += weight
            group_to_fold[indices[group_index]] = lightest_fold
        indices = group_to_fold[groups]
        for f in range(self.n_splits):
            yield np.where(indices == f)[0]

class StratifiedKFold(_BaseKFold):
    
    def __init__(self, n_splits=3, shuffle=False, random_state=None):
        super(StratifiedKFold, self).__init__(n_splits, shuffle, random_state)
    def _make_test_folds(self, X, y=None, groups=None):
        if self.shuffle:
            rng = check_random_state(self.random_state)
        else:
            rng = self.random_state
        y = np.asarray(y)
        n_samples = y.shape[0]
        unique_y, y_inversed = np.unique(y, return_inverse=True)
        y_counts = bincount(y_inversed)
        min_groups = np.min(y_counts)
        if np.all(self.n_splits > y_counts):
            raise ValueError("All the n_groups for individual classes"
                             " are less than n_splits=%d."
                             % (self.n_splits))
        if self.n_splits > min_groups:
            warnings.warn(("The least populated class in y has only %d"
                           " members, which is too few. The minimum"
                           " number of groups for any class cannot"
                           " be less than n_splits=%d."
                           % (min_groups, self.n_splits)), Warning)
                                                        per_cls_cvs = [
            KFold(self.n_splits, shuffle=self.shuffle,
                  random_state=rng).split(np.zeros(max(count, self.n_splits)))
            for count in y_counts]
        test_folds = np.zeros(n_samples, dtype=np.int)
        for test_fold_indices, per_cls_splits in enumerate(zip(*per_cls_cvs)):
            for cls, (_, test_split) in zip(unique_y, per_cls_splits):
                cls_test_folds = test_folds[y == cls]
                                                                                                test_split = test_split[test_split < len(cls_test_folds)]
                cls_test_folds[test_split] = test_fold_indices
                test_folds[y == cls] = cls_test_folds
        return test_folds
    def _iter_test_masks(self, X, y=None, groups=None):
        test_folds = self._make_test_folds(X, y)
        for i in range(self.n_splits):
            yield test_folds == i
    def split(self, X, y, groups=None):
                y = check_array(y, ensure_2d=False, dtype=None)
        return super(StratifiedKFold, self).split(X, y, groups)

class TimeSeriesSplit(_BaseKFold):
        def __init__(self, n_splits=3):
        super(TimeSeriesSplit, self).__init__(n_splits,
                                              shuffle=False,
                                              random_state=None)
    def split(self, X, y=None, groups=None):
                X, y, groups = indexable(X, y, groups)
        n_samples = _num_samples(X)
        n_splits = self.n_splits
        n_folds = n_splits + 1
        if n_folds > n_samples:
            raise ValueError(
                ("Cannot have number of folds ={0} greater"
                 " than the number of samples: {1}.").format(n_folds,
                                                             n_samples))
        indices = np.arange(n_samples)
        test_size = (n_samples // n_folds)
        test_starts = range(test_size + n_samples % n_folds,
                            n_samples, test_size)
        for test_start in test_starts:
            yield (indices[:test_start],
                   indices[test_start:test_start + test_size])

class LeaveOneGroupOut(BaseCrossValidator):
    
    def _iter_test_masks(self, X, y, groups):
        if groups is None:
            raise ValueError("The groups parameter should not be None")
                groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)
        unique_groups = np.unique(groups)
        if len(unique_groups) <= 1:
            raise ValueError(
                "The groups parameter contains fewer than 2 unique groups "
                "(%s). LeaveOneGroupOut expects at least 2." % unique_groups)
        for i in unique_groups:
            yield groups == i
    def get_n_splits(self, X, y, groups):
                if groups is None:
            raise ValueError("The groups parameter should not be None")
        return len(np.unique(groups))

class LeavePGroupsOut(BaseCrossValidator):
    
    def __init__(self, n_groups):
        self.n_groups = n_groups
    def _iter_test_masks(self, X, y, groups):
        if groups is None:
            raise ValueError("The groups parameter should not be None")
        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)
        unique_groups = np.unique(groups)
        if self.n_groups >= len(unique_groups):
            raise ValueError(
                "The groups parameter contains fewer than (or equal to) "
                "n_groups (%d) numbers of unique groups (%s). LeavePGroupsOut "
                "expects that at least n_groups + 1 (%d) unique groups be "
                "present" % (self.n_groups, unique_groups, self.n_groups + 1))
        combi = combinations(range(len(unique_groups)), self.n_groups)
        for indices in combi:
            test_index = np.zeros(_num_samples(X), dtype=np.bool)
            for l in unique_groups[np.array(indices)]:
                test_index[groups == l] = True
            yield test_index
    def get_n_splits(self, X, y, groups):
                if groups is None:
            raise ValueError("The groups parameter should not be None")
        groups = check_array(groups, ensure_2d=False, dtype=None)
        X, y, groups = indexable(X, y, groups)
        return int(comb(len(np.unique(groups)), self.n_groups, exact=True))

class _RepeatedSplits(with_metaclass(ABCMeta)):
        def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):
        if not isinstance(n_repeats, (np.integer, numbers.Integral)):
            raise ValueError("Number of repetitions must be of Integral type.")
        if n_repeats <= 1:
            raise ValueError("Number of repetitions must be greater than 1.")
        if any(key in cvargs for key in ('random_state', 'shuffle')):
            raise ValueError(
                "cvargs must not contain random_state or shuffle.")
        self.cv = cv
        self.n_repeats = n_repeats
        self.random_state = random_state
        self.cvargs = cvargs
    def split(self, X, y=None, groups=None):
                n_repeats = self.n_repeats
        rng = check_random_state(self.random_state)
        for idx in range(n_repeats):
            cv = self.cv(random_state=rng, shuffle=True,
                         **self.cvargs)
            for train_index, test_index in cv.split(X, y, groups):
                yield train_index, test_index

class RepeatedKFold(_RepeatedSplits):
        def __init__(self, n_splits=5, n_repeats=10, random_state=None):
        super(RepeatedKFold, self).__init__(
            KFold, n_repeats, random_state, n_splits=n_splits)

class RepeatedStratifiedKFold(_RepeatedSplits):
        def __init__(self, n_splits=5, n_repeats=10, random_state=None):
        super(RepeatedStratifiedKFold, self).__init__(
            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)

class BaseShuffleSplit(with_metaclass(ABCMeta)):
    
    def __init__(self, n_splits=10, test_size=0.1, train_size=None,
                 random_state=None):
        _validate_shuffle_split_init(test_size, train_size)
        self.n_splits = n_splits
        self.test_size = test_size
        self.train_size = train_size
        self.random_state = random_state
    def split(self, X, y=None, groups=None):
                X, y, groups = indexable(X, y, groups)
        for train, test in self._iter_indices(X, y, groups):
            yield train, test
    @abstractmethod
    def _iter_indices(self, X, y=None, groups=None):
        
    def get_n_splits(self, X=None, y=None, groups=None):
                return self.n_splits
    def __repr__(self):
        return _build_repr(self)

class ShuffleSplit(BaseShuffleSplit):
    
    def _iter_indices(self, X, y=None, groups=None):
        n_samples = _num_samples(X)
        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,
                                                  self.train_size)
        rng = check_random_state(self.random_state)
        for i in range(self.n_splits):
                        permutation = rng.permutation(n_samples)
            ind_test = permutation[:n_test]
            ind_train = permutation[n_test:(n_test + n_train)]
            yield ind_train, ind_test

class GroupShuffleSplit(ShuffleSplit):
    '''Shuffle-Group(s)-Out cross-validation iterator
    Provides randomized train/test indices to split data according to a
    third-party provided group. This group information can be used to encode
    arbitrary domain specific stratifications of the samples as integers.
    For instance the groups could be the year of collection of the samples
    and thus allow for cross-validation against time-based splits.
    The difference between LeavePGroupsOut and GroupShuffleSplit is that
    the former generates splits using all subsets of size ``p`` unique groups,
    whereas GroupShuffleSplit generates a user-determined number of random
    test splits, each with a user-determined fraction of unique groups.
    For example, a less computationally intensive alternative to
    ``LeavePGroupsOut(p=10)`` would be
    ``GroupShuffleSplit(test_size=10, n_splits=100)``.
    Note: The parameters ``test_size`` and ``train_size`` refer to groups, and
    not to samples, as in ShuffleSplit.

    Parameters
    ----------
    n_splits : int (default 5)
        Number of re-shuffling & splitting iterations.
    test_size : float (default 0.2), int, or None
        If float, should be between 0.0 and 1.0 and represent the
        proportion of the groups to include in the test split. If
        int, represents the absolute number of test groups. If None,
        the value is automatically set to the complement of the train size.
    train_size : float, int, or None (default is None)
        If float, should be between 0.0 and 1.0 and represent the
        proportion of the groups to include in the train split. If
        int, represents the absolute number of train groups. If None,
        the value is automatically set to the complement of the test size.
    random_state : int or RandomState
        Pseudo-random number generator state used for random sampling.
    '''
    def __init__(self, n_splits=5, test_size=0.2, train_size=None,
                 random_state=None):
        super(GroupShuffleSplit, self).__init__(
            n_splits=n_splits,
            test_size=test_size,
            train_size=train_size,
            random_state=random_state)
    def _iter_indices(self, X, y, groups):
        if groups is None:
            raise ValueError("The groups parameter should not be None")
        groups = check_array(groups, ensure_2d=False, dtype=None)
        classes, group_indices = np.unique(groups, return_inverse=True)
        for group_train, group_test in super(
                GroupShuffleSplit, self)._iter_indices(X=classes):
                        
            train = np.flatnonzero(np.in1d(group_indices, group_train))
            test = np.flatnonzero(np.in1d(group_indices, group_test))
            yield train, test

def _approximate_mode(class_counts, n_draws, rng):
                continuous = n_draws * class_counts / class_counts.sum()
        floored = np.floor(continuous)
            need_to_add = int(n_draws - floored.sum())
    if need_to_add > 0:
        remainder = continuous - floored
        values = np.sort(np.unique(remainder))[::-1]
                        for value in values:
            inds, = np.where(remainder == value)
                                                            add_now = min(len(inds), need_to_add)
            inds = choice(inds, size=add_now, replace=False, random_state=rng)
            floored[inds] += 1
            need_to_add -= add_now
            if need_to_add == 0:
                break
    return floored.astype(np.int)

class StratifiedShuffleSplit(BaseShuffleSplit):
    
    def __init__(self, n_splits=10, test_size=0.1, train_size=None,
                 random_state=None):
        super(StratifiedShuffleSplit, self).__init__(
            n_splits, test_size, train_size, random_state)
    def _iter_indices(self, X, y, groups=None):
        n_samples = _num_samples(X)
        y = check_array(y, ensure_2d=False, dtype=None)
        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,
                                                  self.train_size)
        classes, y_indices = np.unique(y, return_inverse=True)
        n_classes = classes.shape[0]
        class_counts = bincount(y_indices)
        if np.min(class_counts) < 2:
            raise ValueError("The least populated class in y has only 1"
                             " member, which is too few. The minimum"
                             " number of groups for any class cannot"
                             " be less than 2.")
        if n_train < n_classes:
            raise ValueError('The train_size = %d should be greater or '
                             'equal to the number of classes = %d' %
                             (n_train, n_classes))
        if n_test < n_classes:
            raise ValueError('The test_size = %d should be greater or '
                             'equal to the number of classes = %d' %
                             (n_test, n_classes))
        rng = check_random_state(self.random_state)
        for _ in range(self.n_splits):
                                    n_i = _approximate_mode(class_counts, n_train, rng)
            class_counts_remaining = class_counts - n_i
            t_i = _approximate_mode(class_counts_remaining, n_test, rng)
            train = []
            test = []
            for i, class_i in enumerate(classes):
                permutation = rng.permutation(class_counts[i])
                perm_indices_class_i = np.where((y == class_i))[0][permutation]
                train.extend(perm_indices_class_i[:n_i[i]])
                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])
            train = rng.permutation(train)
            test = rng.permutation(test)
            yield train, test
    def split(self, X, y, groups=None):
                y = check_array(y, ensure_2d=False, dtype=None)
        return super(StratifiedShuffleSplit, self).split(X, y, groups)

def _validate_shuffle_split_init(test_size, train_size):
        if test_size is None and train_size is None:
        raise ValueError('test_size and train_size can not both be None')
    if test_size is not None:
        if np.asarray(test_size).dtype.kind == 'f':
            if test_size >= 1.:
                raise ValueError(
                    'test_size=%f should be smaller '
                    'than 1.0 or be an integer' % test_size)
        elif np.asarray(test_size).dtype.kind != 'i':
                        raise ValueError("Invalid value for test_size: %r" % test_size)
    if train_size is not None:
        if np.asarray(train_size).dtype.kind == 'f':
            if train_size >= 1.:
                raise ValueError("train_size=%f should be smaller "
                                 "than 1.0 or be an integer" % train_size)
            elif (np.asarray(test_size).dtype.kind == 'f' and
                    (train_size + test_size) > 1.):
                raise ValueError('The sum of test_size and train_size = %f, '
                                 'should be smaller than 1.0. Reduce '
                                 'test_size and/or train_size.' %
                                 (train_size + test_size))
        elif np.asarray(train_size).dtype.kind != 'i':
                        raise ValueError("Invalid value for train_size: %r" % train_size)

def _validate_shuffle_split(n_samples, test_size, train_size):
        if (test_size is not None and np.asarray(test_size).dtype.kind == 'i' and
            test_size >= n_samples):
        raise ValueError('test_size=%d should be smaller than the number of '
                         'samples %d' % (test_size, n_samples))
    if (train_size is not None and np.asarray(train_size).dtype.kind == 'i' and
            train_size >= n_samples):
        raise ValueError("train_size=%d should be smaller than the number of"
                         " samples %d" % (train_size, n_samples))
    if np.asarray(test_size).dtype.kind == 'f':
        n_test = ceil(test_size * n_samples)
    elif np.asarray(test_size).dtype.kind == 'i':
        n_test = float(test_size)
    if train_size is None:
        n_train = n_samples - n_test
    elif np.asarray(train_size).dtype.kind == 'f':
        n_train = floor(train_size * n_samples)
    else:
        n_train = float(train_size)
    if test_size is None:
        n_test = n_samples - n_train
    if n_train + n_test > n_samples:
        raise ValueError('The sum of train_size and test_size = %d, '
                         'should be smaller than the number of '
                         'samples %d. Reduce test_size and/or '
                         'train_size.' % (n_train + n_test, n_samples))
    return int(n_train), int(n_test)

class PredefinedSplit(BaseCrossValidator):
    
    def __init__(self, test_fold):
        self.test_fold = np.array(test_fold, dtype=np.int)
        self.test_fold = column_or_1d(self.test_fold)
        self.unique_folds = np.unique(self.test_fold)
        self.unique_folds = self.unique_folds[self.unique_folds != -1]
    def split(self, X=None, y=None, groups=None):
                ind = np.arange(len(self.test_fold))
        for test_index in self._iter_test_masks():
            train_index = ind[np.logical_not(test_index)]
            test_index = ind[test_index]
            yield train_index, test_index
    def _iter_test_masks(self):
                for f in self.unique_folds:
            test_index = np.where(self.test_fold == f)[0]
            test_mask = np.zeros(len(self.test_fold), dtype=np.bool)
            test_mask[test_index] = True
            yield test_mask
    def get_n_splits(self, X=None, y=None, groups=None):
                return len(self.unique_folds)

class _CVIterableWrapper(BaseCrossValidator):
        def __init__(self, cv):
        self.cv = list(cv)
    def get_n_splits(self, X=None, y=None, groups=None):
                return len(self.cv)
    def split(self, X=None, y=None, groups=None):
                for train, test in self.cv:
            yield train, test

def check_cv(cv=3, y=None, classifier=False):
        if cv is None:
        cv = 3
    if isinstance(cv, numbers.Integral):
        if (classifier and (y is not None) and
                (type_of_target(y) in ('binary', 'multiclass'))):
            return StratifiedKFold(cv)
        else:
            return KFold(cv)
    if not hasattr(cv, 'split') or isinstance(cv, str):
        if not isinstance(cv, Iterable) or isinstance(cv, str):
            raise ValueError("Expected cv as an integer, cross-validation "
                             "object (from sklearn.model_selection) "
                             "or an iterable. Got %s." % cv)
        return _CVIterableWrapper(cv)
    return cv  
def train_test_split(*arrays, **options):
        n_arrays = len(arrays)
    if n_arrays == 0:
        raise ValueError("At least one array required as input")
    test_size = options.pop('test_size', None)
    train_size = options.pop('train_size', None)
    random_state = options.pop('random_state', None)
    stratify = options.pop('stratify', None)
    if options:
        raise TypeError("Invalid parameters passed: %s" % str(options))
    if test_size is None and train_size is None:
        test_size = 0.25
    arrays = indexable(*arrays)
    if stratify is not None:
        CVClass = StratifiedShuffleSplit
    else:
        CVClass = ShuffleSplit
    cv = CVClass(test_size=test_size,
                 train_size=train_size,
                 random_state=random_state)
    train, test = next(cv.split(X=arrays[0], y=stratify))
    return list(chain.from_iterable((safe_indexing(a, train),
                                     safe_indexing(a, test)) for a in arrays))

train_test_split.__test__ = False  
def _build_repr(self):
        cls = self.__class__
    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
        init_signature = signature(init)
        if init is object.__init__:
        args = []
    else:
        args = sorted([p.name for p in init_signature.parameters.values()
                       if p.name != 'self' and p.kind != p.VAR_KEYWORD])
    class_name = self.__class__.__name__
    params = dict()
    for key in args:
                                        warnings.simplefilter("always", DeprecationWarning)
        try:
            with warnings.catch_warnings(record=True) as w:
                value = getattr(self, key, None)
            if len(w) and w[0].category == DeprecationWarning:
                                continue
        finally:
            warnings.filters.pop(0)
        params[key] = value
    return '%s(%s)' % (class_name, _pprint(params, offset=len(class_name)))

from __future__ import print_function
from __future__ import division
import warnings
import numbers
import time
import numpy as np
import scipy.sparse as sp
from ..base import is_classifier, clone
from ..utils import indexable, check_random_state, safe_indexing
from ..utils.fixes import astype
from ..utils.validation import _is_arraylike, _num_samples
from ..utils.metaestimators import _safe_split
from ..externals.joblib import Parallel, delayed, logger
from ..metrics.scorer import check_scoring
from ..exceptions import FitFailedWarning
from ._split import check_cv
from ..preprocessing import LabelEncoder
__all__ = ['cross_val_score', 'cross_val_predict', 'permutation_test_score',
           'learning_curve', 'validation_curve']

def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
                    n_jobs=1, verbose=0, fit_params=None,
                    pre_dispatch='2*n_jobs'):
        X, y, groups = indexable(X, y, groups)
    cv = check_cv(cv, y, classifier=is_classifier(estimator))
    scorer = check_scoring(estimator, scoring=scoring)
            parallel = Parallel(n_jobs=n_jobs, verbose=verbose,
                        pre_dispatch=pre_dispatch)
    scores = parallel(delayed(_fit_and_score)(clone(estimator), X, y, scorer,
                                              train, test, verbose, None,
                                              fit_params)
                      for train, test in cv.split(X, y, groups))
    return np.array(scores)[:, 0]

def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
                   parameters, fit_params, return_train_score=False,
                   return_parameters=False, return_n_test_samples=False,
                   return_times=False, error_score='raise'):
        if verbose > 1:
        if parameters is None:
            msg = ''
        else:
            msg = '%s' % (', '.join('%s=%s' % (k, v)
                          for k, v in parameters.items()))
        print("[CV] %s %s" % (msg, (64 - len(msg)) * '.'))
        fit_params = fit_params if fit_params is not None else {}
    fit_params = dict([(k, _index_param_value(X, v, train))
                      for k, v in fit_params.items()])
    if parameters is not None:
        estimator.set_params(**parameters)
    start_time = time.time()
    X_train, y_train = _safe_split(estimator, X, y, train)
    X_test, y_test = _safe_split(estimator, X, y, test, train)
    try:
        if y_train is None:
            estimator.fit(X_train, **fit_params)
        else:
            estimator.fit(X_train, y_train, **fit_params)
    except Exception as e:
                fit_time = time.time() - start_time
        score_time = 0.0
        if error_score == 'raise':
            raise
        elif isinstance(error_score, numbers.Number):
            test_score = error_score
            if return_train_score:
                train_score = error_score
            warnings.warn("Classifier fit failed. The score on this train-test"
                          " partition for these parameters will be set to %f. "
                          "Details: \n%r" % (error_score, e), FitFailedWarning)
        else:
            raise ValueError("error_score must be the string 'raise' or a"
                             " numeric value. (Hint: if using 'raise', please"
                             " make sure that it has been spelled correctly.)")
    else:
        fit_time = time.time() - start_time
        test_score = _score(estimator, X_test, y_test, scorer)
        score_time = time.time() - start_time - fit_time
        if return_train_score:
            train_score = _score(estimator, X_train, y_train, scorer)
    if verbose > 2:
        msg += ", score=%f" % test_score
    if verbose > 1:
        total_time = score_time + fit_time
        end_msg = "%s, total=%s" % (msg, logger.short_format_time(total_time))
        print("[CV] %s %s" % ((64 - len(end_msg)) * '.', end_msg))
    ret = [train_score, test_score] if return_train_score else [test_score]
    if return_n_test_samples:
        ret.append(_num_samples(X_test))
    if return_times:
        ret.extend([fit_time, score_time])
    if return_parameters:
        ret.append(parameters)
    return ret

def _score(estimator, X_test, y_test, scorer):
        if y_test is None:
        score = scorer(estimator, X_test)
    else:
        score = scorer(estimator, X_test, y_test)
    if hasattr(score, 'item'):
        try:
                        score = score.item()
        except ValueError:
                        pass
    if not isinstance(score, numbers.Number):
        raise ValueError("scoring must return a number, got %s (%s) instead."
                         % (str(score), type(score)))
    return score

def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
                      verbose=0, fit_params=None, pre_dispatch='2*n_jobs',
                      method='predict'):
        X, y, groups = indexable(X, y, groups)
    cv = check_cv(cv, y, classifier=is_classifier(estimator))
        if not callable(getattr(estimator, method)):
        raise AttributeError('{} not implemented in estimator'
                             .format(method))
    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:
        le = LabelEncoder()
        y = le.fit_transform(y)
            parallel = Parallel(n_jobs=n_jobs, verbose=verbose,
                        pre_dispatch=pre_dispatch)
    prediction_blocks = parallel(delayed(_fit_and_predict)(
        clone(estimator), X, y, train, test, verbose, fit_params, method)
        for train, test in cv.split(X, y, groups))
        predictions = [pred_block_i for pred_block_i, _ in prediction_blocks]
    test_indices = np.concatenate([indices_i
                                   for _, indices_i in prediction_blocks])
    if not _check_is_permutation(test_indices, _num_samples(X)):
        raise ValueError('cross_val_predict only works for partitions')
    inv_test_indices = np.empty(len(test_indices), dtype=int)
    inv_test_indices[test_indices] = np.arange(len(test_indices))
        if sp.issparse(predictions[0]):
        predictions = sp.vstack(predictions, format=predictions[0].format)
    else:
        predictions = np.concatenate(predictions)
    return predictions[inv_test_indices]

def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params,
                     method):
            fit_params = fit_params if fit_params is not None else {}
    fit_params = dict([(k, _index_param_value(X, v, train))
                      for k, v in fit_params.items()])
    X_train, y_train = _safe_split(estimator, X, y, train)
    X_test, _ = _safe_split(estimator, X, y, test, train)
    if y_train is None:
        estimator.fit(X_train, **fit_params)
    else:
        estimator.fit(X_train, y_train, **fit_params)
    func = getattr(estimator, method)
    predictions = func(X_test)
    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:
        n_classes = len(set(y))
        predictions_ = np.zeros((X_test.shape[0], n_classes))
        if method == 'decision_function' and len(estimator.classes_) == 2:
            predictions_[:, estimator.classes_[-1]] = predictions
        else:
            predictions_[:, estimator.classes_] = predictions
        predictions = predictions_
    return predictions, test

def _check_is_permutation(indices, n_samples):
        if len(indices) != n_samples:
        return False
    hit = np.zeros(n_samples, dtype=bool)
    hit[indices] = True
    if not np.all(hit):
        return False
    return True

def _index_param_value(X, v, indices):
        if not _is_arraylike(v) or _num_samples(v) != _num_samples(X):
                return v
    if sp.issparse(v):
        v = v.tocsr()
    return safe_indexing(v, indices)

def permutation_test_score(estimator, X, y, groups=None, cv=None,
                           n_permutations=100, n_jobs=1, random_state=0,
                           verbose=0, scoring=None):
        X, y, groups = indexable(X, y, groups)
    cv = check_cv(cv, y, classifier=is_classifier(estimator))
    scorer = check_scoring(estimator, scoring=scoring)
    random_state = check_random_state(random_state)
            score = _permutation_test_score(clone(estimator), X, y, groups, cv, scorer)
    permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(
        delayed(_permutation_test_score)(
            clone(estimator), X, _shuffle(y, groups, random_state),
            groups, cv, scorer)
        for _ in range(n_permutations))
    permutation_scores = np.array(permutation_scores)
    pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)
    return score, permutation_scores, pvalue

permutation_test_score.__test__ = False  
def _permutation_test_score(estimator, X, y, groups, cv, scorer):
        avg_score = []
    for train, test in cv.split(X, y, groups):
        X_train, y_train = _safe_split(estimator, X, y, train)
        X_test, y_test = _safe_split(estimator, X, y, test, train)
        estimator.fit(X_train, y_train)
        avg_score.append(scorer(estimator, X_test, y_test))
    return np.mean(avg_score)

def _shuffle(y, groups, random_state):
        if groups is None:
        indices = random_state.permutation(len(y))
    else:
        indices = np.arange(len(groups))
        for group in np.unique(groups):
            this_mask = (groups == group)
            indices[this_mask] = random_state.permutation(indices[this_mask])
    return safe_indexing(y, indices)

def learning_curve(estimator, X, y, groups=None,
                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None, scoring=None,
                   exploit_incremental_learning=False, n_jobs=1,
                   pre_dispatch="all", verbose=0, shuffle=False,
                   random_state=None):
        if exploit_incremental_learning and not hasattr(estimator, "partial_fit"):
        raise ValueError("An estimator must support the partial_fit interface "
                         "to exploit incremental learning")
    X, y, groups = indexable(X, y, groups)
    cv = check_cv(cv, y, classifier=is_classifier(estimator))
        cv_iter = list(cv.split(X, y, groups))
    scorer = check_scoring(estimator, scoring=scoring)
    n_max_training_samples = len(cv_iter[0][0])
                train_sizes_abs = _translate_train_sizes(train_sizes,
                                             n_max_training_samples)
    n_unique_ticks = train_sizes_abs.shape[0]
    if verbose > 0:
        print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,
                        verbose=verbose)
    if shuffle:
        rng = check_random_state(random_state)
        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)
    if exploit_incremental_learning:
        classes = np.unique(y) if is_classifier(estimator) else None
        out = parallel(delayed(_incremental_fit_estimator)(
            clone(estimator), X, y, classes, train, test, train_sizes_abs,
            scorer, verbose) for train, test in cv_iter)
    else:
        train_test_proportions = []
        for train, test in cv_iter:
            for n_train_samples in train_sizes_abs:
                train_test_proportions.append((train[:n_train_samples], test))
        out = parallel(delayed(_fit_and_score)(
            clone(estimator), X, y, scorer, train, test,
            verbose, parameters=None, fit_params=None, return_train_score=True)
            for train, test in train_test_proportions)
        out = np.array(out)
        n_cv_folds = out.shape[0] // n_unique_ticks
        out = out.reshape(n_cv_folds, n_unique_ticks, 2)
    out = np.asarray(out).transpose((2, 1, 0))
    return train_sizes_abs, out[0], out[1]

def _translate_train_sizes(train_sizes, n_max_training_samples):
        train_sizes_abs = np.asarray(train_sizes)
    n_ticks = train_sizes_abs.shape[0]
    n_min_required_samples = np.min(train_sizes_abs)
    n_max_required_samples = np.max(train_sizes_abs)
    if np.issubdtype(train_sizes_abs.dtype, np.float):
        if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:
            raise ValueError("train_sizes has been interpreted as fractions "
                             "of the maximum number of training samples and "
                             "must be within (0, 1], but is within [%f, %f]."
                             % (n_min_required_samples,
                                n_max_required_samples))
        train_sizes_abs = astype(train_sizes_abs * n_max_training_samples,
                                 dtype=np.int, copy=False)
        train_sizes_abs = np.clip(train_sizes_abs, 1,
                                  n_max_training_samples)
    else:
        if (n_min_required_samples <= 0 or
                n_max_required_samples > n_max_training_samples):
            raise ValueError("train_sizes has been interpreted as absolute "
                             "numbers of training samples and must be within "
                             "(0, %d], but is within [%d, %d]."
                             % (n_max_training_samples,
                                n_min_required_samples,
                                n_max_required_samples))
    train_sizes_abs = np.unique(train_sizes_abs)
    if n_ticks > train_sizes_abs.shape[0]:
        warnings.warn("Removed duplicate entries from 'train_sizes'. Number "
                      "of ticks will be less than the size of "
                      "'train_sizes' %d instead of %d)."
                      % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)
    return train_sizes_abs

def _incremental_fit_estimator(estimator, X, y, classes, train, test,
                               train_sizes, scorer, verbose):
        train_scores, test_scores = [], []
    partitions = zip(train_sizes, np.split(train, train_sizes)[:-1])
    for n_train_samples, partial_train in partitions:
        train_subset = train[:n_train_samples]
        X_train, y_train = _safe_split(estimator, X, y, train_subset)
        X_partial_train, y_partial_train = _safe_split(estimator, X, y,
                                                       partial_train)
        X_test, y_test = _safe_split(estimator, X, y, test, train_subset)
        if y_partial_train is None:
            estimator.partial_fit(X_partial_train, classes=classes)
        else:
            estimator.partial_fit(X_partial_train, y_partial_train,
                                  classes=classes)
        train_scores.append(_score(estimator, X_train, y_train, scorer))
        test_scores.append(_score(estimator, X_test, y_test, scorer))
    return np.array((train_scores, test_scores)).T

def validation_curve(estimator, X, y, param_name, param_range, groups=None,
                     cv=None, scoring=None, n_jobs=1, pre_dispatch="all",
                     verbose=0):
        X, y, groups = indexable(X, y, groups)
    cv = check_cv(cv, y, classifier=is_classifier(estimator))
    scorer = check_scoring(estimator, scoring=scoring)
    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,
                        verbose=verbose)
    out = parallel(delayed(_fit_and_score)(
        estimator, X, y, scorer, train, test, verbose,
        parameters={param_name: v}, fit_params=None, return_train_score=True)
                for train, test in cv.split(X, y, groups) for v in param_range)
    out = np.asarray(out)
    n_params = len(param_range)
    n_cv_folds = out.shape[0] // n_params
    out = out.reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0))
    return out[0], out[1]
from ._split import BaseCrossValidator
from ._split import KFold
from ._split import GroupKFold
from ._split import StratifiedKFold
from ._split import TimeSeriesSplit
from ._split import LeaveOneGroupOut
from ._split import LeaveOneOut
from ._split import LeavePGroupsOut
from ._split import LeavePOut
from ._split import RepeatedKFold
from ._split import RepeatedStratifiedKFold
from ._split import ShuffleSplit
from ._split import GroupShuffleSplit
from ._split import StratifiedShuffleSplit
from ._split import PredefinedSplit
from ._split import train_test_split
from ._split import check_cv
from ._validation import cross_val_score
from ._validation import cross_val_predict
from ._validation import learning_curve
from ._validation import permutation_test_score
from ._validation import validation_curve
from ._search import GridSearchCV
from ._search import RandomizedSearchCV
from ._search import ParameterGrid
from ._search import ParameterSampler
from ._search import fit_grid_point
__all__ = ('BaseCrossValidator',
           'GridSearchCV',
           'TimeSeriesSplit',
           'KFold',
           'GroupKFold',
           'GroupShuffleSplit',
           'LeaveOneGroupOut',
           'LeaveOneOut',
           'LeavePGroupsOut',
           'LeavePOut',
           'RepeatedKFold',
           'RepeatedStratifiedKFold',
           'ParameterGrid',
           'ParameterSampler',
           'PredefinedSplit',
           'RandomizedSearchCV',
           'ShuffleSplit',
           'StratifiedKFold',
           'StratifiedShuffleSplit',
           'check_cv',
           'cross_val_predict',
           'cross_val_score',
           'fit_grid_point',
           'learning_curve',
           'permutation_test_score',
           'train_test_split',
           'validation_curve')
import numpy as np
from sklearn.model_selection import KFold

class OneTimeSplitter:
    
import numpy as np
import warnings
from scipy import sparse
from .base import KNeighborsMixin, RadiusNeighborsMixin
from ..base import BaseEstimator
from ..utils.validation import check_array
from ..utils import check_random_state
from ..metrics.pairwise import pairwise_distances
from ..random_projection import GaussianRandomProjection
__all__ = ["LSHForest"]
HASH_DTYPE = '>u4'
MAX_HASH_SIZE = np.dtype(HASH_DTYPE).itemsize * 8

def _find_matching_indices(tree, bin_X, left_mask, right_mask):
        left_index = np.searchsorted(tree, bin_X & left_mask)
    right_index = np.searchsorted(tree, bin_X | right_mask,
                                  side='right')
    return left_index, right_index

def _find_longest_prefix_match(tree, bin_X, hash_size,
                               left_masks, right_masks):
        hi = np.empty_like(bin_X, dtype=np.intp)
    hi.fill(hash_size)
    lo = np.zeros_like(bin_X, dtype=np.intp)
    res = np.empty_like(bin_X, dtype=np.intp)
    left_idx, right_idx = _find_matching_indices(tree, bin_X,
                                                 left_masks[hi],
                                                 right_masks[hi])
    found = right_idx > left_idx
    res[found] = lo[found] = hash_size
    r = np.arange(bin_X.shape[0])
    kept = r[lo < hi]      while kept.shape[0]:
        mid = (lo.take(kept) + hi.take(kept)) // 2
        left_idx, right_idx = _find_matching_indices(tree,
                                                     bin_X.take(kept),
                                                     left_masks[mid],
                                                     right_masks[mid])
        found = right_idx > left_idx
        mid_found = mid[found]
        lo[kept[found]] = mid_found + 1
        res[kept[found]] = mid_found
        hi[kept[~found]] = mid[~found]
        kept = r[lo < hi]
    return res

class ProjectionToHashMixin(object):
        @staticmethod
    def _to_hash(projected):
        if projected.shape[1] % 8 != 0:
            raise ValueError('Require reduced dimensionality to be a multiple '
                             'of 8 for hashing')
                out = np.packbits((projected > 0).astype(int)).view(dtype=HASH_DTYPE)
        return out.reshape(projected.shape[0], -1)
    def fit_transform(self, X, y=None):
        self.fit(X)
        return self.transform(X)
    def transform(self, X, y=None):
        return self._to_hash(super(ProjectionToHashMixin, self).transform(X))

class GaussianRandomProjectionHash(ProjectionToHashMixin,
                                   GaussianRandomProjection):
        def __init__(self,
                 n_components=8,
                 random_state=None):
        super(GaussianRandomProjectionHash, self).__init__(
            n_components=n_components,
            random_state=random_state)

def _array_of_arrays(list_of_arrays):
        out = np.empty(len(list_of_arrays), dtype=object)
    out[:] = list_of_arrays
    return out

class LSHForest(BaseEstimator, KNeighborsMixin, RadiusNeighborsMixin):
    
    def __init__(self, n_estimators=10, radius=1.0, n_candidates=50,
                 n_neighbors=5, min_hash_match=4, radius_cutoff_ratio=.9,
                 random_state=None):
        self.n_estimators = n_estimators
        self.radius = radius
        self.random_state = random_state
        self.n_candidates = n_candidates
        self.n_neighbors = n_neighbors
        self.min_hash_match = min_hash_match
        self.radius_cutoff_ratio = radius_cutoff_ratio
    def _compute_distances(self, query, candidates):
                if candidates.shape == (0,):
                        return np.empty(0, dtype=np.int), np.empty(0, dtype=float)
        if sparse.issparse(self._fit_X):
            candidate_X = self._fit_X[candidates]
        else:
            candidate_X = self._fit_X.take(candidates, axis=0, mode='clip')
        distances = pairwise_distances(query, candidate_X,
                                       metric='cosine')[0]
        distance_positions = np.argsort(distances)
        distances = distances.take(distance_positions, mode='clip', axis=0)
        return distance_positions, distances
    def _generate_masks(self):
                tri_size = MAX_HASH_SIZE + 1
                left_mask = np.tril(np.ones((tri_size, tri_size), dtype=int))[:, 1:]
        right_mask = left_mask[::-1, ::-1]
        self._left_mask = np.packbits(left_mask).view(dtype=HASH_DTYPE)
        self._right_mask = np.packbits(right_mask).view(dtype=HASH_DTYPE)
    def _get_candidates(self, query, max_depth, bin_queries, n_neighbors):
                index_size = self._fit_X.shape[0]
                                n_candidates = 0
        candidate_set = set()
        min_candidates = self.n_candidates * self.n_estimators
        while (max_depth > self.min_hash_match and
               (n_candidates < min_candidates or
                len(candidate_set) < n_neighbors)):
            left_mask = self._left_mask[max_depth]
            right_mask = self._right_mask[max_depth]
            for i in range(self.n_estimators):
                start, stop = _find_matching_indices(self.trees_[i],
                                                     bin_queries[i],
                                                     left_mask, right_mask)
                n_candidates += stop - start
                candidate_set.update(
                    self.original_indices_[i][start:stop].tolist())
            max_depth -= 1
        candidates = np.fromiter(candidate_set, count=len(candidate_set),
                                 dtype=np.intp)
                        if candidates.shape[0] < n_neighbors:
            warnings.warn(
                "Number of candidates is not sufficient to retrieve"
                " %i neighbors with"
                " min_hash_match = %i. Candidates are filled up"
                " uniformly from unselected"
                " indices." % (n_neighbors, self.min_hash_match))
            remaining = np.setdiff1d(np.arange(0, index_size), candidates)
            to_fill = n_neighbors - candidates.shape[0]
            candidates = np.concatenate((candidates, remaining[:to_fill]))
        ranks, distances = self._compute_distances(query,
                                                   candidates.astype(int))
        return (candidates[ranks[:n_neighbors]],
                distances[:n_neighbors])
    def _get_radius_neighbors(self, query, max_depth, bin_queries, radius):
                ratio_within_radius = 1
        threshold = 1 - self.radius_cutoff_ratio
        total_candidates = np.array([], dtype=int)
        total_neighbors = np.array([], dtype=int)
        total_distances = np.array([], dtype=float)
        while (max_depth > self.min_hash_match and
               ratio_within_radius > threshold):
            left_mask = self._left_mask[max_depth]
            right_mask = self._right_mask[max_depth]
            candidates = []
            for i in range(self.n_estimators):
                start, stop = _find_matching_indices(self.trees_[i],
                                                     bin_queries[i],
                                                     left_mask, right_mask)
                candidates.extend(
                    self.original_indices_[i][start:stop].tolist())
            candidates = np.setdiff1d(candidates, total_candidates)
            total_candidates = np.append(total_candidates, candidates)
            ranks, distances = self._compute_distances(query, candidates)
            m = np.searchsorted(distances, radius, side='right')
            positions = np.searchsorted(total_distances, distances[:m])
            total_neighbors = np.insert(total_neighbors, positions,
                                        candidates[ranks[:m]])
            total_distances = np.insert(total_distances, positions,
                                        distances[:m])
            ratio_within_radius = (total_neighbors.shape[0] /
                                   float(total_candidates.shape[0]))
            max_depth = max_depth - 1
        return total_neighbors, total_distances
    def fit(self, X, y=None):
        
        self._fit_X = check_array(X, accept_sparse='csr')
                self.hash_functions_ = []
        self.trees_ = []
        self.original_indices_ = []
        rng = check_random_state(self.random_state)
        int_max = np.iinfo(np.int32).max
        for i in range(self.n_estimators):
                                                            hasher = GaussianRandomProjectionHash(MAX_HASH_SIZE,
                                                  rng.randint(0, int_max))
            hashes = hasher.fit_transform(self._fit_X)[:, 0]
            original_index = np.argsort(hashes)
            bin_hashes = hashes[original_index]
            self.original_indices_.append(original_index)
            self.trees_.append(bin_hashes)
            self.hash_functions_.append(hasher)
        self._generate_masks()
        return self
    def _query(self, X):
                        bin_queries = np.asarray([hasher.transform(X)[:, 0]
                                  for hasher in self.hash_functions_])
        bin_queries = np.rollaxis(bin_queries, 1)
                depths = [_find_longest_prefix_match(tree, tree_queries, MAX_HASH_SIZE,
                                             self._left_mask, self._right_mask)
                  for tree, tree_queries in zip(self.trees_,
                                                np.rollaxis(bin_queries, 1))]
        return bin_queries, np.max(depths, axis=0)
    def kneighbors(self, X, n_neighbors=None, return_distance=True):
                if not hasattr(self, 'hash_functions_'):
            raise ValueError("estimator should be fitted.")
        if n_neighbors is None:
            n_neighbors = self.n_neighbors
        X = check_array(X, accept_sparse='csr')
        neighbors, distances = [], []
        bin_queries, max_depth = self._query(X)
        for i in range(X.shape[0]):
            neighs, dists = self._get_candidates(X[[i]], max_depth[i],
                                                 bin_queries[i],
                                                 n_neighbors)
            neighbors.append(neighs)
            distances.append(dists)
        if return_distance:
            return np.array(distances), np.array(neighbors)
        else:
            return np.array(neighbors)
    def radius_neighbors(self, X, radius=None, return_distance=True):
                if not hasattr(self, 'hash_functions_'):
            raise ValueError("estimator should be fitted.")
        if radius is None:
            radius = self.radius
        X = check_array(X, accept_sparse='csr')
        neighbors, distances = [], []
        bin_queries, max_depth = self._query(X)
        for i in range(X.shape[0]):
            neighs, dists = self._get_radius_neighbors(X[[i]], max_depth[i],
                                                       bin_queries[i], radius)
            neighbors.append(neighs)
            distances.append(dists)
        if return_distance:
            return _array_of_arrays(distances), _array_of_arrays(neighbors)
        else:
            return _array_of_arrays(neighbors)
    def partial_fit(self, X, y=None):
                X = check_array(X, accept_sparse='csr')
        if not hasattr(self, 'hash_functions_'):
            return self.fit(X)
        if X.shape[1] != self._fit_X.shape[1]:
            raise ValueError("Number of features in X and"
                             " fitted array does not match.")
        n_samples = X.shape[0]
        n_indexed = self._fit_X.shape[0]
        for i in range(self.n_estimators):
            bin_X = self.hash_functions_[i].transform(X)[:, 0]
                        positions = self.trees_[i].searchsorted(bin_X)
                        self.trees_[i] = np.insert(self.trees_[i],
                                       positions, bin_X)
                        self.original_indices_[i] = np.insert(self.original_indices_[i],
                                                  positions,
                                                  np.arange(n_indexed,
                                                            n_indexed +
                                                            n_samples))
                if sparse.issparse(X) or sparse.issparse(self._fit_X):
            self._fit_X = sparse.vstack((self._fit_X, X))
        else:
            self._fit_X = np.row_stack((self._fit_X, X))
        return self

__all__ = ['BallTree']
DOC_DICT = {'BinaryTree': 'BallTree', 'binary_tree': 'ball_tree'}
VALID_METRICS = ['EuclideanDistance', 'SEuclideanDistance',
                 'ManhattanDistance', 'ChebyshevDistance',
                 'MinkowskiDistance', 'WMinkowskiDistance',
                 'MahalanobisDistance', 'HammingDistance',
                 'CanberraDistance', 'BrayCurtisDistance',
                 'JaccardDistance', 'MatchingDistance',
                 'DiceDistance', 'KulsinskiDistance',
                 'RogersTanimotoDistance', 'RussellRaoDistance',
                 'SokalMichenerDistance', 'SokalSneathDistance',
                 'PyFuncDistance', 'HaversineDistance']

include "binary_tree.pxi"
cdef class BallTree(BinaryTree):
    __doc__ = CLASS_DOC.format(**DOC_DICT)
    pass

cdef int allocate_data(BinaryTree tree, ITYPE_t n_nodes,
                       ITYPE_t n_features) except -1:
        tree.node_bounds_arr = np.zeros((1, n_nodes, n_features), dtype=DTYPE)
    tree.node_bounds = get_memview_DTYPE_3D(tree.node_bounds_arr)
    return 0

cdef int init_node(BinaryTree tree, ITYPE_t i_node,
                   ITYPE_t idx_start, ITYPE_t idx_end) except -1:
        cdef ITYPE_t n_features = tree.data.shape[1]
    cdef ITYPE_t n_points = idx_end - idx_start
    cdef ITYPE_t i, j
    cdef DTYPE_t radius
    cdef DTYPE_t *this_pt
    cdef ITYPE_t* idx_array = &tree.idx_array[0]
    cdef DTYPE_t* data = &tree.data[0, 0]
    cdef DTYPE_t* centroid = &tree.node_bounds[0, i_node, 0]
        for j in range(n_features):
        centroid[j] = 0
    for i in range(idx_start, idx_end):
        this_pt = data + n_features * idx_array[i]
        for j from 0 <= j < n_features:
            centroid[j] += this_pt[j]
    for j in range(n_features):
        centroid[j] /= n_points
        radius = 0
    for i in range(idx_start, idx_end):
        radius = fmax(radius,
                      tree.rdist(centroid,
                                 data + n_features * idx_array[i],
                                 n_features))
    tree.node_data[i_node].radius = tree.dist_metric._rdist_to_dist(radius)
    tree.node_data[i_node].idx_start = idx_start
    tree.node_data[i_node].idx_end = idx_end
    return 0

cdef inline DTYPE_t min_dist(BinaryTree tree, ITYPE_t i_node,
                             DTYPE_t* pt) nogil except -1:
        cdef DTYPE_t dist_pt = tree.dist(pt, &tree.node_bounds[0, i_node, 0],
                                     tree.data.shape[1])
    return fmax(0, dist_pt - tree.node_data[i_node].radius)

cdef inline DTYPE_t max_dist(BinaryTree tree, ITYPE_t i_node,
                             DTYPE_t* pt) except -1:
        cdef DTYPE_t dist_pt = tree.dist(pt, &tree.node_bounds[0, i_node, 0],
                                     tree.data.shape[1])
    return dist_pt + tree.node_data[i_node].radius

cdef inline int min_max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt,
                             DTYPE_t* min_dist, DTYPE_t* max_dist) except -1:
        cdef DTYPE_t dist_pt = tree.dist(pt, &tree.node_bounds[0, i_node, 0],
                                     tree.data.shape[1])
    cdef DTYPE_t rad = tree.node_data[i_node].radius
    min_dist[0] = fmax(0, dist_pt - rad)
    max_dist[0] = dist_pt + rad
    return 0

cdef inline DTYPE_t min_rdist(BinaryTree tree, ITYPE_t i_node,
                              DTYPE_t* pt) nogil except -1:
        if tree.euclidean:
        return euclidean_dist_to_rdist(min_dist(tree, i_node, pt))
    else:
        return tree.dist_metric._dist_to_rdist(min_dist(tree, i_node, pt))

cdef inline DTYPE_t max_rdist(BinaryTree tree, ITYPE_t i_node,
                              DTYPE_t* pt) except -1:
        if tree.euclidean:
        return euclidean_dist_to_rdist(max_dist(tree, i_node, pt))
    else:
        return tree.dist_metric._dist_to_rdist(max_dist(tree, i_node, pt))

cdef inline DTYPE_t min_dist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                  BinaryTree tree2, ITYPE_t i_node2) except -1:
        cdef DTYPE_t dist_pt = tree1.dist(&tree2.node_bounds[0, i_node2, 0],
                                      &tree1.node_bounds[0, i_node1, 0],
                                      tree1.data.shape[1])
    return fmax(0, (dist_pt - tree1.node_data[i_node1].radius
                    - tree2.node_data[i_node2].radius))

cdef inline DTYPE_t max_dist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                  BinaryTree tree2, ITYPE_t i_node2) except -1:
        cdef DTYPE_t dist_pt = tree1.dist(&tree2.node_bounds[0, i_node2, 0],
                                      &tree1.node_bounds[0, i_node1, 0],
                                      tree1.data.shape[1])
    return (dist_pt + tree1.node_data[i_node1].radius
            + tree2.node_data[i_node2].radius)

cdef inline DTYPE_t min_rdist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                   BinaryTree tree2, ITYPE_t i_node2) except -1:
        if tree1.euclidean:
        return euclidean_dist_to_rdist(min_dist_dual(tree1, i_node1,
                                                     tree2, i_node2))
    else:
        return tree1.dist_metric._dist_to_rdist(min_dist_dual(tree1, i_node1,
                                                              tree2, i_node2))

cdef inline DTYPE_t max_rdist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                   BinaryTree tree2, ITYPE_t i_node2) except -1:
        if tree1.euclidean:
        return euclidean_dist_to_rdist(max_dist_dual(tree1, i_node1,
                                                     tree2, i_node2))
    else:
        return tree1.dist_metric._dist_to_rdist(max_dist_dual(tree1, i_node1,
                                                              tree2, i_node2))
import warnings
from abc import ABCMeta, abstractmethod
import numpy as np
from scipy.sparse import csr_matrix, issparse
from .ball_tree import BallTree
from .kd_tree import KDTree
from ..base import BaseEstimator
from ..metrics import pairwise_distances
from ..metrics.pairwise import PAIRWISE_DISTANCE_FUNCTIONS
from ..utils import check_X_y, check_array, _get_n_jobs, gen_even_slices
from ..utils.fixes import argpartition
from ..utils.multiclass import check_classification_targets
from ..externals import six
from ..externals.joblib import Parallel, delayed
from ..exceptions import NotFittedError
from ..exceptions import DataConversionWarning
VALID_METRICS = dict(ball_tree=BallTree.valid_metrics,
                     kd_tree=KDTree.valid_metrics,
                                                               brute=(list(PAIRWISE_DISTANCE_FUNCTIONS.keys()) +
                            ['braycurtis', 'canberra', 'chebyshev',
                             'correlation', 'cosine', 'dice', 'hamming',
                             'jaccard', 'kulsinski', 'mahalanobis',
                             'matching', 'minkowski', 'rogerstanimoto',
                             'russellrao', 'seuclidean', 'sokalmichener',
                             'sokalsneath', 'sqeuclidean',
                             'yule', 'wminkowski']))

VALID_METRICS_SPARSE = dict(ball_tree=[],
                            kd_tree=[],
                            brute=PAIRWISE_DISTANCE_FUNCTIONS.keys())

def _check_weights(weights):
        if weights in (None, 'uniform', 'distance'):
        return weights
    elif callable(weights):
        return weights
    else:
        raise ValueError("weights not recognized: should be 'uniform', "
                         "'distance', or a callable function")

def _get_weights(dist, weights):
        if weights in (None, 'uniform'):
        return None
    elif weights == 'distance':
                                if dist.dtype is np.dtype(object):
            for point_dist_i, point_dist in enumerate(dist):
                                                                if hasattr(point_dist, '__contains__') and 0. in point_dist:
                    dist[point_dist_i] = point_dist == 0.
                else:
                    dist[point_dist_i] = 1. / point_dist
        else:
            with np.errstate(divide='ignore'):
                dist = 1. / dist
            inf_mask = np.isinf(dist)
            inf_row = np.any(inf_mask, axis=1)
            dist[inf_row] = inf_mask[inf_row]
        return dist
    elif callable(weights):
        return weights(dist)
    else:
        raise ValueError("weights not recognized: should be 'uniform', "
                         "'distance', or a callable function")

class NeighborsBase(six.with_metaclass(ABCMeta, BaseEstimator)):
    
    @abstractmethod
    def __init__(self):
        pass
    def _init_params(self, n_neighbors=None, radius=None,
                     algorithm='auto', leaf_size=30, metric='minkowski',
                     p=2, metric_params=None, n_jobs=1):
        self.n_neighbors = n_neighbors
        self.radius = radius
        self.algorithm = algorithm
        self.leaf_size = leaf_size
        self.metric = metric
        self.metric_params = metric_params
        self.p = p
        self.n_jobs = n_jobs
        if algorithm not in ['auto', 'brute',
                             'kd_tree', 'ball_tree']:
            raise ValueError("unrecognized algorithm: '%s'" % algorithm)
        if algorithm == 'auto':
            if metric == 'precomputed':
                alg_check = 'brute'
            else:
                alg_check = 'ball_tree'
        else:
            alg_check = algorithm
        if callable(metric):
            if algorithm == 'kd_tree':
                                raise ValueError(
                    "kd_tree algorithm does not support callable metric '%s'"
                    % metric)
        elif metric not in VALID_METRICS[alg_check]:
            raise ValueError("Metric '%s' not valid for algorithm '%s'"
                             % (metric, algorithm))
        if self.metric_params is not None and 'p' in self.metric_params:
            warnings.warn("Parameter p is found in metric_params. "
                          "The corresponding parameter from __init__ "
                          "is ignored.", SyntaxWarning, stacklevel=3)
            effective_p = metric_params['p']
        else:
            effective_p = self.p
        if self.metric in ['wminkowski', 'minkowski'] and effective_p < 1:
            raise ValueError("p must be greater than one for minkowski metric")
        self._fit_X = None
        self._tree = None
        self._fit_method = None
    def _fit(self, X):
        if self.metric_params is None:
            self.effective_metric_params_ = {}
        else:
            self.effective_metric_params_ = self.metric_params.copy()
        effective_p = self.effective_metric_params_.get('p', self.p)
        if self.metric in ['wminkowski', 'minkowski']:
            self.effective_metric_params_['p'] = effective_p
        self.effective_metric_ = self.metric
                if self.metric == 'minkowski':
            p = self.effective_metric_params_.pop('p', 2)
            if p < 1:
                raise ValueError("p must be greater than one "
                                 "for minkowski metric")
            elif p == 1:
                self.effective_metric_ = 'manhattan'
            elif p == 2:
                self.effective_metric_ = 'euclidean'
            elif p == np.inf:
                self.effective_metric_ = 'chebyshev'
            else:
                self.effective_metric_params_['p'] = p
        if isinstance(X, NeighborsBase):
            self._fit_X = X._fit_X
            self._tree = X._tree
            self._fit_method = X._fit_method
            return self
        elif isinstance(X, BallTree):
            self._fit_X = X.data
            self._tree = X
            self._fit_method = 'ball_tree'
            return self
        elif isinstance(X, KDTree):
            self._fit_X = X.data
            self._tree = X
            self._fit_method = 'kd_tree'
            return self
        X = check_array(X, accept_sparse='csr')
        n_samples = X.shape[0]
        if n_samples == 0:
            raise ValueError("n_samples must be greater than 0")
        if issparse(X):
            if self.algorithm not in ('auto', 'brute'):
                warnings.warn("cannot use tree with sparse input: "
                              "using brute force")
            if self.effective_metric_ not in VALID_METRICS_SPARSE['brute']:
                raise ValueError("metric '%s' not valid for sparse input"
                                 % self.effective_metric_)
            self._fit_X = X.copy()
            self._tree = None
            self._fit_method = 'brute'
            return self
        self._fit_method = self.algorithm
        self._fit_X = X
        if self._fit_method == 'auto':
                                    if ((self.n_neighbors is None or
                 self.n_neighbors < self._fit_X.shape[0] // 2) and
                    self.metric != 'precomputed'):
                if self.effective_metric_ in VALID_METRICS['kd_tree']:
                    self._fit_method = 'kd_tree'
                else:
                    self._fit_method = 'ball_tree'
            else:
                self._fit_method = 'brute'
        if self._fit_method == 'ball_tree':
            self._tree = BallTree(X, self.leaf_size,
                                  metric=self.effective_metric_,
                                  **self.effective_metric_params_)
        elif self._fit_method == 'kd_tree':
            self._tree = KDTree(X, self.leaf_size,
                                metric=self.effective_metric_,
                                **self.effective_metric_params_)
        elif self._fit_method == 'brute':
            self._tree = None
        else:
            raise ValueError("algorithm = '%s' not recognized"
                             % self.algorithm)
        if self.n_neighbors is not None:
            if self.n_neighbors <= 0:
                raise ValueError(
                    "Expected n_neighbors > 0. Got %d" %
                    self.n_neighbors
                )
        return self
    @property
    def _pairwise(self):
                return self.metric == 'precomputed'

class KNeighborsMixin(object):
    
    def kneighbors(self, X=None, n_neighbors=None, return_distance=True):
                if self._fit_method is None:
            raise NotFittedError("Must fit neighbors before querying.")
        if n_neighbors is None:
            n_neighbors = self.n_neighbors
        if X is not None:
            query_is_train = False
            X = check_array(X, accept_sparse='csr')
        else:
            query_is_train = True
            X = self._fit_X
                                    n_neighbors += 1
        train_size = self._fit_X.shape[0]
        if n_neighbors > train_size:
            raise ValueError(
                "Expected n_neighbors <= n_samples, "
                " but n_samples = %d, n_neighbors = %d" %
                (train_size, n_neighbors)
            )
        n_samples, _ = X.shape
        sample_range = np.arange(n_samples)[:, None]
        n_jobs = _get_n_jobs(self.n_jobs)
        if self._fit_method == 'brute':
                        if self.effective_metric_ == 'euclidean':
                dist = pairwise_distances(X, self._fit_X, 'euclidean',
                                          n_jobs=n_jobs, squared=True)
            else:
                dist = pairwise_distances(
                    X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,
                    **self.effective_metric_params_)
            neigh_ind = argpartition(dist, n_neighbors - 1, axis=1)
            neigh_ind = neigh_ind[:, :n_neighbors]
                        neigh_ind = neigh_ind[
                sample_range, np.argsort(dist[sample_range, neigh_ind])]
            if return_distance:
                if self.effective_metric_ == 'euclidean':
                    result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind
                else:
                    result = dist[sample_range, neigh_ind], neigh_ind
            else:
                result = neigh_ind
        elif self._fit_method in ['ball_tree', 'kd_tree']:
            if issparse(X):
                raise ValueError(
                    "%s does not work with sparse matrices. Densify the data, "
                    "or set algorithm='brute'" % self._fit_method)
            result = Parallel(n_jobs, backend='threading')(
                delayed(self._tree.query, check_pickle=False)(
                    X[s], n_neighbors, return_distance)
                for s in gen_even_slices(X.shape[0], n_jobs)
            )
            if return_distance:
                dist, neigh_ind = tuple(zip(*result))
                result = np.vstack(dist), np.vstack(neigh_ind)
            else:
                result = np.vstack(result)
        else:
            raise ValueError("internal: _fit_method not recognized")
        if not query_is_train:
            return result
        else:
                                                if return_distance:
                dist, neigh_ind = result
            else:
                neigh_ind = result
            sample_mask = neigh_ind != sample_range
                                                            dup_gr_nbrs = np.all(sample_mask, axis=1)
            sample_mask[:, 0][dup_gr_nbrs] = False
            neigh_ind = np.reshape(
                neigh_ind[sample_mask], (n_samples, n_neighbors - 1))
            if return_distance:
                dist = np.reshape(
                    dist[sample_mask], (n_samples, n_neighbors - 1))
                return dist, neigh_ind
            return neigh_ind
    def kneighbors_graph(self, X=None, n_neighbors=None,
                         mode='connectivity'):
                if n_neighbors is None:
            n_neighbors = self.n_neighbors
                if X is not None:
            X = check_array(X, accept_sparse='csr')
            n_samples1 = X.shape[0]
        else:
            n_samples1 = self._fit_X.shape[0]
        n_samples2 = self._fit_X.shape[0]
        n_nonzero = n_samples1 * n_neighbors
        A_indptr = np.arange(0, n_nonzero + 1, n_neighbors)
                if mode == 'connectivity':
            A_data = np.ones(n_samples1 * n_neighbors)
            A_ind = self.kneighbors(X, n_neighbors, return_distance=False)
        elif mode == 'distance':
            A_data, A_ind = self.kneighbors(
                X, n_neighbors, return_distance=True)
            A_data = np.ravel(A_data)
        else:
            raise ValueError(
                'Unsupported mode, must be one of "connectivity" '
                'or "distance" but got "%s" instead' % mode)
        kneighbors_graph = csr_matrix((A_data, A_ind.ravel(), A_indptr),
                                      shape=(n_samples1, n_samples2))
        return kneighbors_graph

class RadiusNeighborsMixin(object):
    
    def radius_neighbors(self, X=None, radius=None, return_distance=True):
                if self._fit_method is None:
            raise NotFittedError("Must fit neighbors before querying.")
        if X is not None:
            query_is_train = False
            X = check_array(X, accept_sparse='csr')
        else:
            query_is_train = True
            X = self._fit_X
        if radius is None:
            radius = self.radius
        n_samples = X.shape[0]
        if self._fit_method == 'brute':
                        if self.effective_metric_ == 'euclidean':
                dist = pairwise_distances(X, self._fit_X, 'euclidean',
                                          n_jobs=self.n_jobs, squared=True)
                radius *= radius
            else:
                dist = pairwise_distances(X, self._fit_X,
                                          self.effective_metric_,
                                          n_jobs=self.n_jobs,
                                          **self.effective_metric_params_)
            neigh_ind_list = [np.where(d <= radius)[0] for d in dist]
                                    neigh_ind = np.empty(n_samples, dtype='object')
            neigh_ind[:] = neigh_ind_list
            if return_distance:
                dist_array = np.empty(n_samples, dtype='object')
                if self.effective_metric_ == 'euclidean':
                    dist_list = [np.sqrt(d[neigh_ind[i]])
                                 for i, d in enumerate(dist)]
                else:
                    dist_list = [d[neigh_ind[i]]
                                 for i, d in enumerate(dist)]
                dist_array[:] = dist_list
                results = dist_array, neigh_ind
            else:
                results = neigh_ind
        elif self._fit_method in ['ball_tree', 'kd_tree']:
            if issparse(X):
                raise ValueError(
                    "%s does not work with sparse matrices. Densify the data, "
                    "or set algorithm='brute'" % self._fit_method)
            results = self._tree.query_radius(X, radius,
                                              return_distance=return_distance)
            if return_distance:
                results = results[::-1]
        else:
            raise ValueError("internal: _fit_method not recognized")
        if not query_is_train:
            return results
        else:
                                                if return_distance:
                dist, neigh_ind = results
            else:
                neigh_ind = results
            for ind, ind_neighbor in enumerate(neigh_ind):
                mask = ind_neighbor != ind
                neigh_ind[ind] = ind_neighbor[mask]
                if return_distance:
                    dist[ind] = dist[ind][mask]
            if return_distance:
                return dist, neigh_ind
            return neigh_ind
    def radius_neighbors_graph(self, X=None, radius=None, mode='connectivity'):
                if X is not None:
            X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])
        n_samples2 = self._fit_X.shape[0]
        if radius is None:
            radius = self.radius
                if mode == 'connectivity':
            A_ind = self.radius_neighbors(X, radius,
                                          return_distance=False)
            A_data = None
        elif mode == 'distance':
            dist, A_ind = self.radius_neighbors(X, radius,
                                                return_distance=True)
            A_data = np.concatenate(list(dist))
        else:
            raise ValueError(
                'Unsupported mode, must be one of "connectivity", '
                'or "distance" but got %s instead' % mode)
        n_samples1 = A_ind.shape[0]
        n_neighbors = np.array([len(a) for a in A_ind])
        A_ind = np.concatenate(list(A_ind))
        if A_data is None:
            A_data = np.ones(len(A_ind))
        A_indptr = np.concatenate((np.zeros(1, dtype=int),
                                   np.cumsum(n_neighbors)))
        return csr_matrix((A_data, A_ind, A_indptr),
                          shape=(n_samples1, n_samples2))

class SupervisedFloatMixin(object):
    def fit(self, X, y):
                if not isinstance(X, (KDTree, BallTree)):
            X, y = check_X_y(X, y, "csr", multi_output=True)
        self._y = y
        return self._fit(X)

class SupervisedIntegerMixin(object):
    def fit(self, X, y):
                if not isinstance(X, (KDTree, BallTree)):
            X, y = check_X_y(X, y, "csr", multi_output=True)
        if y.ndim == 1 or y.ndim == 2 and y.shape[1] == 1:
            if y.ndim != 1:
                warnings.warn("A column-vector y was passed when a 1d array "
                              "was expected. Please change the shape of y to "
                              "(n_samples, ), for example using ravel().",
                              DataConversionWarning, stacklevel=2)
            self.outputs_2d_ = False
            y = y.reshape((-1, 1))
        else:
            self.outputs_2d_ = True
        check_classification_targets(y)
        self.classes_ = []
        self._y = np.empty(y.shape, dtype=np.int)
        for k in range(self._y.shape[1]):
            classes, self._y[:, k] = np.unique(y[:, k], return_inverse=True)
            self.classes_.append(classes)
        if not self.outputs_2d_:
            self.classes_ = self.classes_[0]
            self._y = self._y.ravel()
        return self._fit(X)

class UnsupervisedMixin(object):
    def fit(self, X, y=None):
                return self._fit(X)

import numpy as np
from scipy import stats
from ..utils.extmath import weighted_mode
from .base import \
    _check_weights, _get_weights, \
    NeighborsBase, KNeighborsMixin,\
    RadiusNeighborsMixin, SupervisedIntegerMixin
from ..base import ClassifierMixin
from ..utils import check_array

class KNeighborsClassifier(NeighborsBase, KNeighborsMixin,
                           SupervisedIntegerMixin, ClassifierMixin):
    
    def __init__(self, n_neighbors=5,
                 weights='uniform', algorithm='auto', leaf_size=30,
                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
                 **kwargs):
        self._init_params(n_neighbors=n_neighbors,
                          algorithm=algorithm,
                          leaf_size=leaf_size, metric=metric, p=p,
                          metric_params=metric_params, n_jobs=n_jobs, **kwargs)
        self.weights = _check_weights(weights)
    def predict(self, X):
                X = check_array(X, accept_sparse='csr')
        neigh_dist, neigh_ind = self.kneighbors(X)
        classes_ = self.classes_
        _y = self._y
        if not self.outputs_2d_:
            _y = self._y.reshape((-1, 1))
            classes_ = [self.classes_]
        n_outputs = len(classes_)
        n_samples = X.shape[0]
        weights = _get_weights(neigh_dist, self.weights)
        y_pred = np.empty((n_samples, n_outputs), dtype=classes_[0].dtype)
        for k, classes_k in enumerate(classes_):
            if weights is None:
                mode, _ = stats.mode(_y[neigh_ind, k], axis=1)
            else:
                mode, _ = weighted_mode(_y[neigh_ind, k], weights, axis=1)
            mode = np.asarray(mode.ravel(), dtype=np.intp)
            y_pred[:, k] = classes_k.take(mode)
        if not self.outputs_2d_:
            y_pred = y_pred.ravel()
        return y_pred
    def predict_proba(self, X):
                X = check_array(X, accept_sparse='csr')
        neigh_dist, neigh_ind = self.kneighbors(X)
        classes_ = self.classes_
        _y = self._y
        if not self.outputs_2d_:
            _y = self._y.reshape((-1, 1))
            classes_ = [self.classes_]
        n_samples = X.shape[0]
        weights = _get_weights(neigh_dist, self.weights)
        if weights is None:
            weights = np.ones_like(neigh_ind)
        all_rows = np.arange(X.shape[0])
        probabilities = []
        for k, classes_k in enumerate(classes_):
            pred_labels = _y[:, k][neigh_ind]
            proba_k = np.zeros((n_samples, classes_k.size))
                        for i, idx in enumerate(pred_labels.T):                  proba_k[all_rows, idx] += weights[:, i]
                        normalizer = proba_k.sum(axis=1)[:, np.newaxis]
            normalizer[normalizer == 0.0] = 1.0
            proba_k /= normalizer
            probabilities.append(proba_k)
        if not self.outputs_2d_:
            probabilities = probabilities[0]
        return probabilities

class RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin,
                                SupervisedIntegerMixin, ClassifierMixin):
    
    def __init__(self, radius=1.0, weights='uniform',
                 algorithm='auto', leaf_size=30, p=2, metric='minkowski',
                 outlier_label=None, metric_params=None, **kwargs):
        self._init_params(radius=radius,
                          algorithm=algorithm,
                          leaf_size=leaf_size,
                          metric=metric, p=p, metric_params=metric_params,
                          **kwargs)
        self.weights = _check_weights(weights)
        self.outlier_label = outlier_label
    def predict(self, X):
                X = check_array(X, accept_sparse='csr')
        n_samples = X.shape[0]
        neigh_dist, neigh_ind = self.radius_neighbors(X)
        inliers = [i for i, nind in enumerate(neigh_ind) if len(nind) != 0]
        outliers = [i for i, nind in enumerate(neigh_ind) if len(nind) == 0]
        classes_ = self.classes_
        _y = self._y
        if not self.outputs_2d_:
            _y = self._y.reshape((-1, 1))
            classes_ = [self.classes_]
        n_outputs = len(classes_)
        if self.outlier_label is not None:
            neigh_dist[outliers] = 1e-6
        elif outliers:
            raise ValueError('No neighbors found for test samples %r, '
                             'you can try using larger radius, '
                             'give a label for outliers, '
                             'or consider removing them from your dataset.'
                             % outliers)
        weights = _get_weights(neigh_dist, self.weights)
        y_pred = np.empty((n_samples, n_outputs), dtype=classes_[0].dtype)
        for k, classes_k in enumerate(classes_):
            pred_labels = np.array([_y[ind, k] for ind in neigh_ind],
                                   dtype=object)
            if weights is None:
                mode = np.array([stats.mode(pl)[0]
                                 for pl in pred_labels[inliers]], dtype=np.int)
            else:
                mode = np.array([weighted_mode(pl, w)[0]
                                 for (pl, w)
                                 in zip(pred_labels[inliers], weights[inliers])],
                                dtype=np.int)
            mode = mode.ravel()
            y_pred[inliers, k] = classes_k.take(mode)
        if outliers:
            y_pred[outliers, :] = self.outlier_label
        if not self.outputs_2d_:
            y_pred = y_pred.ravel()
        return y_pred

import numpy as np
cimport numpy as np
np.import_array()  
cdef DTYPE_t[:, ::1] get_memview_DTYPE_2D(
                               np.ndarray[DTYPE_t, ndim=2, mode='c'] X):
    return <DTYPE_t[:X.shape[0],:X.shape[1]:1]> (<DTYPE_t*> X.data)

cdef DTYPE_t* get_vec_ptr(np.ndarray[DTYPE_t, ndim=1, mode='c'] vec):
    return &vec[0]

cdef DTYPE_t* get_mat_ptr(np.ndarray[DTYPE_t, ndim=2, mode='c'] mat):
    return &mat[0, 0]

cdef extern from "arrayobject.h":
    object PyArray_SimpleNewFromData(int nd, np.npy_intp* dims,
                                     int typenum, void* data)

cdef inline np.ndarray _buffer_to_ndarray(DTYPE_t* x, np.npy_intp n):
                
        return PyArray_SimpleNewFromData(1, &n, DTYPECODE, <void*>x)

from libc.math cimport fabs, sqrt, exp, pow, cos, sin, asin
cdef DTYPE_t INF = np.inf
from typedefs cimport DTYPE_t, ITYPE_t, DITYPE_t, DTYPECODE
from typedefs import DTYPE, ITYPE

def newObj(obj):
    return obj.__new__(obj)

METRIC_MAPPING = {'euclidean': EuclideanDistance,
                  'l2': EuclideanDistance,
                  'minkowski': MinkowskiDistance,
                  'p': MinkowskiDistance,
                  'manhattan': ManhattanDistance,
                  'cityblock': ManhattanDistance,
                  'l1': ManhattanDistance,
                  'chebyshev': ChebyshevDistance,
                  'infinity': ChebyshevDistance,
                  'seuclidean': SEuclideanDistance,
                  'mahalanobis': MahalanobisDistance,
                  'wminkowski': WMinkowskiDistance,
                  'hamming': HammingDistance,
                  'canberra': CanberraDistance,
                  'braycurtis': BrayCurtisDistance,
                  'matching': MatchingDistance,
                  'jaccard': JaccardDistance,
                  'dice': DiceDistance,
                  'kulsinski': KulsinskiDistance,
                  'rogerstanimoto': RogersTanimotoDistance,
                  'russellrao': RussellRaoDistance,
                  'sokalmichener': SokalMichenerDistance,
                  'sokalsneath': SokalSneathDistance,
                  'haversine': HaversineDistance,
                  'pyfunc': PyFuncDistance}

def get_valid_metric_ids(L):
        return [key for (key, val) in METRIC_MAPPING.items()
            if (val.__name__ in L) or (val in L)]

cdef class DistanceMetric:
        def __cinit__(self):
        self.p = 2
        self.vec = np.zeros(1, dtype=DTYPE, order='c')
        self.mat = np.zeros((1, 1), dtype=DTYPE, order='c')
        self.vec_ptr = get_vec_ptr(self.vec)
        self.mat_ptr = get_mat_ptr(self.mat)
        self.size = 1
    def __reduce__(self):
                return (newObj, (self.__class__,), self.__getstate__())
    def __getstate__(self):
                if self.__class__.__name__ == "PyFuncDistance":
            return (float(self.p), self.vec, self.mat, self.func, self.kwargs)
        return (float(self.p), self.vec, self.mat)
    def __setstate__(self, state):
                self.p = state[0]
        self.vec = state[1]
        self.mat = state[2]
        if self.__class__.__name__ == "PyFuncDistance":
            self.func = state[3]
            self.kwargs = state[4]
        self.vec_ptr = get_vec_ptr(self.vec)
        self.mat_ptr = get_mat_ptr(self.mat)
        self.size = self.vec.shape[0]
    @classmethod
    def get_metric(cls, metric, **kwargs):
                if isinstance(metric, DistanceMetric):
            return metric
        if callable(metric):
            return PyFuncDistance(metric, **kwargs)
                if isinstance(metric, type) and issubclass(metric, DistanceMetric):
            pass
        else:
            try:
                metric = METRIC_MAPPING[metric]
            except:
                raise ValueError("Unrecognized metric '%s'" % metric)
                if metric is MinkowskiDistance:
            p = kwargs.pop('p', 2)
            if p == 1:
                return ManhattanDistance(**kwargs)
            elif p == 2:
                return EuclideanDistance(**kwargs)
            elif np.isinf(p):
                return ChebyshevDistance(**kwargs)
            else:
                return MinkowskiDistance(p, **kwargs)
        else:
            return metric(**kwargs)
    def __init__(self):
        if self.__class__ is DistanceMetric:
            raise NotImplementedError("DistanceMetric is an abstract class")
    cdef DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                      ITYPE_t size) nogil except -1:
                return -999
    cdef DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,
                       ITYPE_t size) nogil except -1:
                return self.dist(x1, x2, size)
    cdef int pdist(self, DTYPE_t[:, ::1] X, DTYPE_t[:, ::1] D) except -1:
                cdef ITYPE_t i1, i2
        for i1 in range(X.shape[0]):
            for i2 in range(i1, X.shape[0]):
                D[i1, i2] = self.dist(&X[i1, 0], &X[i2, 0], X.shape[1])
                D[i2, i1] = D[i1, i2]
        return 0
    cdef int cdist(self, DTYPE_t[:, ::1] X, DTYPE_t[:, ::1] Y,
                   DTYPE_t[:, ::1] D) except -1:
                cdef ITYPE_t i1, i2
        if X.shape[1] != Y.shape[1]:
            raise ValueError('X and Y must have the same second dimension')
        for i1 in range(X.shape[0]):
            for i2 in range(Y.shape[0]):
                D[i1, i2] = self.dist(&X[i1, 0], &Y[i2, 0], X.shape[1])
        return 0
    cdef DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
                return rdist
    cdef DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
                return dist
    def rdist_to_dist(self, rdist):
                return rdist
    def dist_to_rdist(self, dist):
                return dist
    def pairwise(self, X, Y=None):
                cdef np.ndarray[DTYPE_t, ndim=2, mode='c'] Xarr
        cdef np.ndarray[DTYPE_t, ndim=2, mode='c'] Yarr
        cdef np.ndarray[DTYPE_t, ndim=2, mode='c'] Darr
        Xarr = np.asarray(X, dtype=DTYPE, order='C')
        if Y is None:
            Darr = np.zeros((Xarr.shape[0], Xarr.shape[0]),
                         dtype=DTYPE, order='C')
            self.pdist(get_memview_DTYPE_2D(Xarr),
                       get_memview_DTYPE_2D(Darr))
        else:
            Yarr = np.asarray(Y, dtype=DTYPE, order='C')
            Darr = np.zeros((Xarr.shape[0], Yarr.shape[0]),
                         dtype=DTYPE, order='C')
            self.cdist(get_memview_DTYPE_2D(Xarr),
                       get_memview_DTYPE_2D(Yarr),
                       get_memview_DTYPE_2D(Darr))
        return Darr

cdef class EuclideanDistance(DistanceMetric):
        def __init__(self):
        self.p = 2
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        return euclidean_dist(x1, x2, size)
    cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,
                              ITYPE_t size) nogil except -1:
        return euclidean_rdist(x1, x2, size)
    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
        return sqrt(rdist)
    cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
        return dist * dist
    def rdist_to_dist(self, rdist):
        return np.sqrt(rdist)
    def dist_to_rdist(self, dist):
        return dist ** 2

cdef class SEuclideanDistance(DistanceMetric):
        def __init__(self, V):
        self.vec = np.asarray(V, dtype=DTYPE)
        self.vec_ptr = get_vec_ptr(self.vec)
        self.size = self.vec.shape[0]
        self.p = 2
    cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,
                              ITYPE_t size) nogil except -1:
        if size != self.size:
            with gil:
                raise ValueError('SEuclidean dist: size of V does not match')
        cdef DTYPE_t tmp, d=0
        cdef np.intp_t j
        for j in range(size):
            tmp = x1[j] - x2[j]
            d += tmp * tmp / self.vec_ptr[j]
        return d
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        return sqrt(self.rdist(x1, x2, size))
    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
        return sqrt(rdist)
    cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
        return dist * dist
    def rdist_to_dist(self, rdist):
        return np.sqrt(rdist)
    def dist_to_rdist(self, dist):
        return dist ** 2

cdef class ManhattanDistance(DistanceMetric):
        def __init__(self):
        self.p = 1
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef DTYPE_t d = 0
        cdef np.intp_t j
        for j in range(size):
            d += fabs(x1[j] - x2[j])
        return d

cdef class ChebyshevDistance(DistanceMetric):
        def __init__(self):
        self.p = INF
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef DTYPE_t d = 0
        cdef np.intp_t j
        for j in range(size):
            d = fmax(d, fabs(x1[j] - x2[j]))
        return d

cdef class MinkowskiDistance(DistanceMetric):
        def __init__(self, p):
        if p < 1:
            raise ValueError("p must be greater than 1")
        elif np.isinf(p):
            raise ValueError("MinkowskiDistance requires finite p. "
                             "For p=inf, use ChebyshevDistance.")
        self.p = p
    cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,
                              ITYPE_t size) nogil except -1:
        cdef DTYPE_t d=0
        cdef np.intp_t j
        for j in range(size):
            d += pow(fabs(x1[j] - x2[j]), self.p)
        return d
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        return pow(self.rdist(x1, x2, size), 1. / self.p)
    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
        return pow(rdist, 1. / self.p)
    cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
        return pow(dist, self.p)
    def rdist_to_dist(self, rdist):
        return rdist ** (1. / self.p)
    def dist_to_rdist(self, dist):
        return dist ** self.p

cdef class WMinkowskiDistance(DistanceMetric):
        def __init__(self, p, w):
        if p < 1:
            raise ValueError("p must be greater than 1")
        elif np.isinf(p):
            raise ValueError("WMinkowskiDistance requires finite p. "
                             "For p=inf, use ChebyshevDistance.")
        self.p = p
        self.vec = np.asarray(w, dtype=DTYPE)
        self.vec_ptr = get_vec_ptr(self.vec)
        self.size = self.vec.shape[0]
    cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,
                              ITYPE_t size) nogil except -1:
        if size != self.size:
            with gil:
                raise ValueError('WMinkowskiDistance dist: '
                                 'size of w does not match')
        cdef DTYPE_t d=0
        cdef np.intp_t j
        for j in range(size):
            d += pow(self.vec_ptr[j] * fabs(x1[j] - x2[j]), self.p)
        return d
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        return pow(self.rdist(x1, x2, size), 1. / self.p)
    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
        return pow(rdist, 1. / self.p)
    cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
        return pow(dist, self.p)
    def rdist_to_dist(self, rdist):
        return rdist ** (1. / self.p)
    def dist_to_rdist(self, dist):
        return dist ** self.p

cdef class MahalanobisDistance(DistanceMetric):
        def __init__(self, V=None, VI=None):
        if VI is None:
            if V is None:
                raise ValueError("Must provide either V or VI "
                                 "for Mahalanobis distance")
            VI = np.linalg.inv(V)
        if VI.ndim != 2 or VI.shape[0] != VI.shape[1]:
            raise ValueError("V/VI must be square")
        self.mat = np.asarray(VI, dtype=float, order='C')
        self.mat_ptr = get_mat_ptr(self.mat)
        self.size = self.mat.shape[0]
                self.vec = np.zeros(self.size, dtype=DTYPE)
        self.vec_ptr = get_vec_ptr(self.vec)
    cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,
                              ITYPE_t size) nogil except -1:
        if size != self.size:
            with gil:
                raise ValueError('Mahalanobis dist: size of V does not match')
        cdef DTYPE_t tmp, d = 0
        cdef np.intp_t i, j
                for i in range(size):
            self.vec_ptr[i] = x1[i] - x2[i]
        for i in range(size):
            tmp = 0
            for j in range(size):
                tmp += self.mat_ptr[i * size + j] * self.vec_ptr[j]
            d += tmp * self.vec_ptr[i]
        return d
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        return sqrt(self.rdist(x1, x2, size))
    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
        return sqrt(rdist)
    cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
        return dist * dist
    def rdist_to_dist(self, rdist):
        return np.sqrt(rdist)
    def dist_to_rdist(self, dist):
        return dist ** 2

cdef class HammingDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int n_unequal = 0
        cdef np.intp_t j
        for j in range(size):
            if x1[j] != x2[j]:
                n_unequal += 1
        return float(n_unequal) / size

cdef class CanberraDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef DTYPE_t denom, d = 0
        cdef np.intp_t j
        for j in range(size):
            denom = fabs(x1[j]) + fabs(x2[j])
            if denom > 0:
                d += fabs(x1[j] - x2[j]) / denom
        return d

cdef class BrayCurtisDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef DTYPE_t num = 0, denom = 0
        cdef np.intp_t j
        for j in range(size):
            num += fabs(x1[j] - x2[j])
            denom += fabs(x1[j]) + fabs(x2[j])
        if denom > 0:
            return num / denom
        else:
            return 0.0

cdef class JaccardDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, n_eq = 0, nnz = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            nnz += (tf1 or tf2)
            n_eq += (tf1 and tf2)
        return (nnz - n_eq) * 1.0 / nnz

cdef class MatchingDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, n_neq = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            n_neq += (tf1 != tf2)
        return n_neq * 1. / size

cdef class DiceDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, n_neq = 0, ntt = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            ntt += (tf1 and tf2)
            n_neq += (tf1 != tf2)
        return n_neq / (2.0 * ntt + n_neq)

cdef class KulsinskiDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, ntt = 0, n_neq = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            n_neq += (tf1 != tf2)
            ntt += (tf1 and tf2)
        return (n_neq - ntt + size) * 1.0 / (n_neq + size)

cdef class RogersTanimotoDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, n_neq = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            n_neq += (tf1 != tf2)
        return (2.0 * n_neq) / (size + n_neq)

cdef class RussellRaoDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, ntt = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            ntt += (tf1 and tf2)
        return (size - ntt) * 1. / size

cdef class SokalMichenerDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, n_neq = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            n_neq += (tf1 != tf2)
        return (2.0 * n_neq) / (size + n_neq)

cdef class SokalSneathDistance(DistanceMetric):
        cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) nogil except -1:
        cdef int tf1, tf2, ntt = 0, n_neq = 0
        cdef np.intp_t j
        for j in range(size):
            tf1 = x1[j] != 0
            tf2 = x2[j] != 0
            n_neq += (tf1 != tf2)
            ntt += (tf1 and tf2)
        return n_neq / (0.5 * ntt + n_neq)

cdef class HaversineDistance(DistanceMetric):
        cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,
                              ITYPE_t size) nogil except -1:
        if size != 2:
            with gil:
                raise ValueError("Haversine distance only valid "
                                 "in 2 dimensions")
        cdef DTYPE_t sin_0 = sin(0.5 * (x1[0] - x2[0]))
        cdef DTYPE_t sin_1 = sin(0.5 * (x1[1] - x2[1]))
        return (sin_0 * sin_0 + cos(x1[0]) * cos(x2[0]) * sin_1 * sin_1)
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                              ITYPE_t size) nogil except -1:
        if size != 2:
            with gil:
                raise ValueError("Haversine distance only valid in 2 dimensions")
        cdef DTYPE_t sin_0 = sin(0.5 * (x1[0] - x2[0]))
        cdef DTYPE_t sin_1 = sin(0.5 * (x1[1] - x2[1]))
        return 2 * asin(sqrt(sin_0 * sin_0
                             + cos(x1[0]) * cos(x2[0]) * sin_1 * sin_1))
    cdef inline DTYPE_t _rdist_to_dist(self, DTYPE_t rdist) except -1:
        return 2 * asin(sqrt(rdist))
    cdef inline DTYPE_t _dist_to_rdist(self, DTYPE_t dist) nogil except -1:
        cdef DTYPE_t tmp = sin(0.5 * dist)
        return tmp * tmp
    def rdist_to_dist(self, rdist):
        return 2 * np.arcsin(np.sqrt(rdist))
    def dist_to_rdist(self, dist):
        tmp = np.sin(0.5 * dist)
        return tmp * tmp




cdef class PyFuncDistance(DistanceMetric):
        def __init__(self, func, **kwargs):
        self.func = func
        self.kwargs = kwargs
    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,
                             ITYPE_t size) except -1 with gil:
        cdef np.ndarray x1arr
        cdef np.ndarray x2arr
        with gil:
            x1arr = _buffer_to_ndarray(x1, size)
            x2arr = _buffer_to_ndarray(x2, size)
            d = self.func(x1arr, x2arr, **self.kwargs)
            try:
                                                return d
            except TypeError:
                raise TypeError("Custom distance function must accept two "
                                "vectors and return a float.")
            

cdef inline double fmax(double a, double b) nogil:
    return max(a, b)

from .base import KNeighborsMixin, RadiusNeighborsMixin
from .unsupervised import NearestNeighbors

def _check_params(X, metric, p, metric_params):
        params = zip(['metric', 'p', 'metric_params'],
                 [metric, p, metric_params])
    est_params = X.get_params()
    for param_name, func_param in params:
        if func_param != est_params[param_name]:
            raise ValueError(
                "Got %s for %s, while the estimator has %s for "
                "the same parameter." % (
                    func_param, param_name, est_params[param_name]))

def _query_include_self(X, include_self):
        if include_self:
        query = X._fit_X
    else:
        query = None
    return query

def kneighbors_graph(X, n_neighbors, mode='connectivity', metric='minkowski',
                     p=2, metric_params=None, include_self=False, n_jobs=1):
        if not isinstance(X, KNeighborsMixin):
        X = NearestNeighbors(n_neighbors, metric=metric, p=p,
                             metric_params=metric_params, n_jobs=n_jobs).fit(X)
    else:
        _check_params(X, metric, p, metric_params)
    query = _query_include_self(X, include_self)
    return X.kneighbors_graph(X=query, n_neighbors=n_neighbors, mode=mode)

def radius_neighbors_graph(X, radius, mode='connectivity', metric='minkowski',
                           p=2, metric_params=None, include_self=False, n_jobs=1):
        if not isinstance(X, RadiusNeighborsMixin):
        X = NearestNeighbors(radius=radius, metric=metric, p=p,
                             metric_params=metric_params, n_jobs=n_jobs).fit(X)
    else:
        _check_params(X, metric, p, metric_params)
    query = _query_include_self(X, include_self)
    return X.radius_neighbors_graph(query, radius, mode)
import numpy as np
from scipy.special import gammainc
from ..base import BaseEstimator
from ..utils import check_array, check_random_state
from ..utils.extmath import row_norms
from .ball_tree import BallTree, DTYPE
from .kd_tree import KDTree

VALID_KERNELS = ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',
                 'cosine']
TREE_DICT = {'ball_tree': BallTree, 'kd_tree': KDTree}

class KernelDensity(BaseEstimator):
        def __init__(self, bandwidth=1.0, algorithm='auto',
                 kernel='gaussian', metric="euclidean", atol=0, rtol=0,
                 breadth_first=True, leaf_size=40, metric_params=None):
        self.algorithm = algorithm
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.metric = metric
        self.atol = atol
        self.rtol = rtol
        self.breadth_first = breadth_first
        self.leaf_size = leaf_size
        self.metric_params = metric_params
                                self._choose_algorithm(self.algorithm, self.metric)
        if bandwidth <= 0:
            raise ValueError("bandwidth must be positive")
        if kernel not in VALID_KERNELS:
            raise ValueError("invalid kernel: '{0}'".format(kernel))
    def _choose_algorithm(self, algorithm, metric):
                        if algorithm == 'auto':
                        if metric in KDTree.valid_metrics:
                return 'kd_tree'
            elif metric in BallTree.valid_metrics:
                return 'ball_tree'
            else:
                raise ValueError("invalid metric: '{0}'".format(metric))
        elif algorithm in TREE_DICT:
            if metric not in TREE_DICT[algorithm].valid_metrics:
                raise ValueError("invalid metric for {0}: "
                                 "'{1}'".format(TREE_DICT[algorithm],
                                                metric))
            return algorithm
        else:
            raise ValueError("invalid algorithm: '{0}'".format(algorithm))
    def fit(self, X, y=None):
                algorithm = self._choose_algorithm(self.algorithm, self.metric)
        X = check_array(X, order='C', dtype=DTYPE)
        kwargs = self.metric_params
        if kwargs is None:
            kwargs = {}
        self.tree_ = TREE_DICT[algorithm](X, metric=self.metric,
                                          leaf_size=self.leaf_size,
                                          **kwargs)
        return self
    def score_samples(self, X):
                                        X = check_array(X, order='C', dtype=DTYPE)
        N = self.tree_.data.shape[0]
        atol_N = self.atol * N
        log_density = self.tree_.kernel_density(
            X, h=self.bandwidth, kernel=self.kernel, atol=atol_N,
            rtol=self.rtol, breadth_first=self.breadth_first, return_log=True)
        log_density -= np.log(N)
        return log_density
    def score(self, X, y=None):
                return np.sum(self.score_samples(X))
    def sample(self, n_samples=1, random_state=None):
                        if self.kernel not in ['gaussian', 'tophat']:
            raise NotImplementedError()
        data = np.asarray(self.tree_.data)
        rng = check_random_state(random_state)
        i = rng.randint(data.shape[0], size=n_samples)
        if self.kernel == 'gaussian':
            return np.atleast_2d(rng.normal(data[i], self.bandwidth))
        elif self.kernel == 'tophat':
                                                dim = data.shape[1]
            X = rng.normal(size=(n_samples, dim))
            s_sq = row_norms(X, squared=True)
            correction = (gammainc(0.5 * dim, 0.5 * s_sq) ** (1. / dim)
                          * self.bandwidth / np.sqrt(s_sq))
            return data[i] + X * correction[:, np.newaxis]

__all__ = ['KDTree']
DOC_DICT = {'BinaryTree': 'KDTree', 'binary_tree': 'kd_tree'}
VALID_METRICS = ['EuclideanDistance', 'ManhattanDistance',
                 'ChebyshevDistance', 'MinkowskiDistance']

include "binary_tree.pxi"
cdef class KDTree(BinaryTree):
    __doc__ = CLASS_DOC.format(**DOC_DICT)
    pass

cdef int allocate_data(BinaryTree tree, ITYPE_t n_nodes,
                       ITYPE_t n_features) except -1:
        tree.node_bounds_arr = np.zeros((2, n_nodes, n_features), dtype=DTYPE)
    tree.node_bounds = get_memview_DTYPE_3D(tree.node_bounds_arr)
    return 0

cdef int init_node(BinaryTree tree, ITYPE_t i_node,
                   ITYPE_t idx_start, ITYPE_t idx_end) except -1:
        cdef ITYPE_t n_features = tree.data.shape[1]
    cdef ITYPE_t i, j
    cdef DTYPE_t rad = 0
    cdef DTYPE_t* lower_bounds = &tree.node_bounds[0, i_node, 0]
    cdef DTYPE_t* upper_bounds = &tree.node_bounds[1, i_node, 0]
    cdef DTYPE_t* data = &tree.data[0, 0]
    cdef ITYPE_t* idx_array = &tree.idx_array[0]
    cdef DTYPE_t* data_row
        for j in range(n_features):
        lower_bounds[j] = INF
        upper_bounds[j] = -INF
                for i in range(idx_start, idx_end):
        data_row = data + idx_array[i] * n_features
        for j in range(n_features):
            lower_bounds[j] = fmin(lower_bounds[j], data_row[j])
            upper_bounds[j] = fmax(upper_bounds[j], data_row[j])
        if tree.dist_metric.p == INF:
            rad = fmax(rad, 0.5 * (upper_bounds[j] - lower_bounds[j]))
        else:
            rad += pow(0.5 * abs(upper_bounds[j] - lower_bounds[j]),
                       tree.dist_metric.p)
    tree.node_data[i_node].idx_start = idx_start
    tree.node_data[i_node].idx_end = idx_end
                tree.node_data[i_node].radius = pow(rad, 1. / tree.dist_metric.p)
    return 0

cdef DTYPE_t min_rdist(BinaryTree tree, ITYPE_t i_node,
                       DTYPE_t* pt) nogil except -1:
        cdef ITYPE_t n_features = tree.data.shape[1]
    cdef DTYPE_t d, d_lo, d_hi, rdist=0.0
    cdef ITYPE_t j
    if tree.dist_metric.p == INF:
        for j in range(n_features):
            d_lo = tree.node_bounds[0, i_node, j] - pt[j]
            d_hi = pt[j] - tree.node_bounds[1, i_node, j]
            d = (d_lo + fabs(d_lo)) + (d_hi + fabs(d_hi))
            rdist = fmax(rdist, 0.5 * d)
    else:
                for j in range(n_features):
            d_lo = tree.node_bounds[0, i_node, j] - pt[j]
            d_hi = pt[j] - tree.node_bounds[1, i_node, j]
            d = (d_lo + fabs(d_lo)) + (d_hi + fabs(d_hi))
            rdist += pow(0.5 * d, tree.dist_metric.p)
    return rdist

cdef DTYPE_t min_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt) except -1:
        if tree.dist_metric.p == INF:
        return min_rdist(tree, i_node, pt)
    else:
        return pow(min_rdist(tree, i_node, pt), 1. / tree.dist_metric.p)

cdef DTYPE_t max_rdist(BinaryTree tree,
                       ITYPE_t i_node, DTYPE_t* pt) except -1:
        cdef ITYPE_t n_features = tree.data.shape[1]
    cdef DTYPE_t d, d_lo, d_hi, rdist=0.0
    cdef ITYPE_t j
    if tree.dist_metric.p == INF:
        for j in range(n_features):
            rdist = fmax(rdist, fabs(pt[j] - tree.node_bounds[0, i_node, j]))
            rdist = fmax(rdist, fabs(pt[j] - tree.node_bounds[1, i_node, j]))
    else:
        for j in range(n_features):
            d_lo = fabs(pt[j] - tree.node_bounds[0, i_node, j])
            d_hi = fabs(pt[j] - tree.node_bounds[1, i_node, j])
            rdist += pow(fmax(d_lo, d_hi), tree.dist_metric.p)
    return rdist

cdef DTYPE_t max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt) except -1:
        if tree.dist_metric.p == INF:
        return max_rdist(tree, i_node, pt)
    else:
        return pow(max_rdist(tree, i_node, pt), 1. / tree.dist_metric.p)

cdef inline int min_max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt,
                             DTYPE_t* min_dist, DTYPE_t* max_dist) except -1:
        cdef ITYPE_t n_features = tree.data.shape[1]
    cdef DTYPE_t d, d_lo, d_hi
    cdef ITYPE_t j
    min_dist[0] = 0.0
    max_dist[0] = 0.0
    if tree.dist_metric.p == INF:
        for j in range(n_features):
            d_lo = tree.node_bounds[0, i_node, j] - pt[j]
            d_hi = pt[j] - tree.node_bounds[1, i_node, j]
            d = (d_lo + fabs(d_lo)) + (d_hi + fabs(d_hi))
            min_dist[0] = fmax(min_dist[0], 0.5 * d)
            max_dist[0] = fmax(max_dist[0],
                               fabs(pt[j] - tree.node_bounds[0, i_node, j]))
            max_dist[0] = fmax(max_dist[0],
                               fabs(pt[j] - tree.node_bounds[1, i_node, j]))
    else:
                for j in range(n_features):
            d_lo = tree.node_bounds[0, i_node, j] - pt[j]
            d_hi = pt[j] - tree.node_bounds[1, i_node, j]
            d = (d_lo + fabs(d_lo)) + (d_hi + fabs(d_hi))
            min_dist[0] += pow(0.5 * d, tree.dist_metric.p)
            max_dist[0] += pow(fmax(fabs(d_lo), fabs(d_hi)),
                               tree.dist_metric.p)
        min_dist[0] = pow(min_dist[0], 1. / tree.dist_metric.p)
        max_dist[0] = pow(max_dist[0], 1. / tree.dist_metric.p)
    return 0

cdef inline DTYPE_t min_rdist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                   BinaryTree tree2, ITYPE_t i_node2) except -1:
        cdef ITYPE_t n_features = tree1.data.shape[1]
    cdef DTYPE_t d, d1, d2, rdist=0.0
    cdef DTYPE_t zero = 0.0
    cdef ITYPE_t j
    if tree1.dist_metric.p == INF:
        for j in range(n_features):
            d1 = (tree1.node_bounds[0, i_node1, j]
                  - tree2.node_bounds[1, i_node2, j])
            d2 = (tree2.node_bounds[0, i_node2, j]
                  - tree1.node_bounds[1, i_node1, j])
            d = (d1 + fabs(d1)) + (d2 + fabs(d2))
            rdist = fmax(rdist, 0.5 * d)
    else:
                for j in range(n_features):
            d1 = (tree1.node_bounds[0, i_node1, j]
                  - tree2.node_bounds[1, i_node2, j])
            d2 = (tree2.node_bounds[0, i_node2, j]
                  - tree1.node_bounds[1, i_node1, j])
            d = (d1 + fabs(d1)) + (d2 + fabs(d2))
            rdist += pow(0.5 * d, tree1.dist_metric.p)
    return rdist

cdef inline DTYPE_t min_dist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                  BinaryTree tree2, ITYPE_t i_node2) except -1:
        return tree1.dist_metric._rdist_to_dist(min_rdist_dual(tree1, i_node1,
                                                           tree2, i_node2))

cdef inline DTYPE_t max_rdist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                   BinaryTree tree2, ITYPE_t i_node2) except -1:
        cdef ITYPE_t n_features = tree1.data.shape[1]
    cdef DTYPE_t d, d1, d2, rdist=0.0
    cdef DTYPE_t zero = 0.0
    cdef ITYPE_t j
    if tree1.dist_metric.p == INF:
        for j in range(n_features):
            rdist = fmax(rdist, fabs(tree1.node_bounds[0, i_node1, j]
                                     - tree2.node_bounds[1, i_node2, j]))
            rdist = fmax(rdist, fabs(tree1.node_bounds[1, i_node1, j]
                                     - tree2.node_bounds[0, i_node2, j]))
    else:
        for j in range(n_features):
            d1 = fabs(tree1.node_bounds[0, i_node1, j]
                      - tree2.node_bounds[1, i_node2, j])
            d2 = fabs(tree1.node_bounds[1, i_node1, j]
                      - tree2.node_bounds[0, i_node2, j])
            rdist += pow(fmax(d1, d2), tree1.dist_metric.p)
    return rdist

cdef inline DTYPE_t max_dist_dual(BinaryTree tree1, ITYPE_t i_node1,
                                  BinaryTree tree2, ITYPE_t i_node2) except -1:
        return tree1.dist_metric._rdist_to_dist(max_rdist_dual(tree1, i_node1,
                                                           tree2, i_node2))
import numpy as np
from warnings import warn
from scipy.stats import scoreatpercentile
from .base import NeighborsBase
from .base import KNeighborsMixin
from .base import UnsupervisedMixin
from ..utils.validation import check_is_fitted
from ..utils import check_array
__all__ = ["LocalOutlierFactor"]

class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin):
        def __init__(self, n_neighbors=20, algorithm='auto', leaf_size=30,
                 metric='minkowski', p=2, metric_params=None,
                 contamination=0.1, n_jobs=1):
        self._init_params(n_neighbors=n_neighbors,
                          algorithm=algorithm,
                          leaf_size=leaf_size, metric=metric, p=p,
                          metric_params=metric_params, n_jobs=n_jobs)
        self.contamination = contamination
    def fit_predict(self, X, y=None):
        
        return self.fit(X)._predict()
    def fit(self, X, y=None):
                if not (0. < self.contamination <= .5):
            raise ValueError("contamination must be in (0, 0.5]")
        super(LocalOutlierFactor, self).fit(X)
        n_samples = self._fit_X.shape[0]
        if self.n_neighbors > n_samples:
            warn("n_neighbors (%s) is greater than the "
                 "total number of samples (%s). n_neighbors "
                 "will be set to (n_samples - 1) for estimation."
                 % (self.n_neighbors, n_samples))
        self.n_neighbors_ = max(1, min(self.n_neighbors, n_samples - 1))
        self._distances_fit_X_, _neighbors_indices_fit_X_ = (
            self.kneighbors(None, n_neighbors=self.n_neighbors_))
        self._lrd = self._local_reachability_density(
            self._distances_fit_X_, _neighbors_indices_fit_X_)
                lrd_ratios_array = (self._lrd[_neighbors_indices_fit_X_] /
                            self._lrd[:, np.newaxis])
        self.negative_outlier_factor_ = -np.mean(lrd_ratios_array, axis=1)
        self.threshold_ = -scoreatpercentile(
            -self.negative_outlier_factor_, 100. * (1. - self.contamination))
        return self
    def _predict(self, X=None):
                check_is_fitted(self, ["threshold_", "negative_outlier_factor_",
                               "n_neighbors_", "_distances_fit_X_"])
        if X is not None:
            X = check_array(X, accept_sparse='csr')
            is_inlier = np.ones(X.shape[0], dtype=int)
            is_inlier[self._decision_function(X) <= self.threshold_] = -1
        else:
            is_inlier = np.ones(self._fit_X.shape[0], dtype=int)
            is_inlier[self.negative_outlier_factor_ <= self.threshold_] = -1
        return is_inlier
    def _decision_function(self, X):
                check_is_fitted(self, ["threshold_", "negative_outlier_factor_",
                               "_distances_fit_X_"])
        X = check_array(X, accept_sparse='csr')
        distances_X, neighbors_indices_X = (
            self.kneighbors(X, n_neighbors=self.n_neighbors_))
        X_lrd = self._local_reachability_density(distances_X,
                                                 neighbors_indices_X)
        lrd_ratios_array = (self._lrd[neighbors_indices_X] /
                            X_lrd[:, np.newaxis])
                return -np.mean(lrd_ratios_array, axis=1)
    def _local_reachability_density(self, distances_X, neighbors_indices):
                dist_k = self._distances_fit_X_[neighbors_indices,
                                        self.n_neighbors_ - 1]
        reach_dist_array = np.maximum(distances_X, dist_k)
                return 1. / (np.mean(reach_dist_array, axis=1) + 1e-10)

import warnings
import numpy as np
from scipy import sparse as sp
from ..base import BaseEstimator, ClassifierMixin
from ..metrics.pairwise import pairwise_distances
from ..preprocessing import LabelEncoder
from ..utils.validation import check_array, check_X_y, check_is_fitted
from ..utils.sparsefuncs import csc_median_axis_0
from ..utils.multiclass import check_classification_targets
class NearestCentroid(BaseEstimator, ClassifierMixin):
    
    def __init__(self, metric='euclidean', shrink_threshold=None):
        self.metric = metric
        self.shrink_threshold = shrink_threshold
    def fit(self, X, y):
                                if self.metric == 'manhattan':
            X, y = check_X_y(X, y, ['csc'])
        else:
            X, y = check_X_y(X, y, ['csr', 'csc'])
        is_X_sparse = sp.issparse(X)
        if is_X_sparse and self.shrink_threshold:
            raise ValueError("threshold shrinking not supported"
                             " for sparse input")
        check_classification_targets(y)
        n_samples, n_features = X.shape
        le = LabelEncoder()
        y_ind = le.fit_transform(y)
        self.classes_ = classes = le.classes_
        n_classes = classes.size
        if n_classes < 2:
            raise ValueError('y has less than 2 classes')
                self.centroids_ = np.empty((n_classes, n_features), dtype=np.float64)
                nk = np.zeros(n_classes)
        for cur_class in range(n_classes):
            center_mask = y_ind == cur_class
            nk[cur_class] = np.sum(center_mask)
            if is_X_sparse:
                center_mask = np.where(center_mask)[0]
                        if self.metric == "manhattan":
                                if not is_X_sparse:
                    self.centroids_[cur_class] = np.median(X[center_mask], axis=0)
                else:
                    self.centroids_[cur_class] = csc_median_axis_0(X[center_mask])
            else:
                if self.metric != 'euclidean':
                    warnings.warn("Averaging for metrics other than "
                                  "euclidean and manhattan not supported. "
                                  "The average is set to be the mean."
                                  )
                self.centroids_[cur_class] = X[center_mask].mean(axis=0)
        if self.shrink_threshold:
            dataset_centroid_ = np.mean(X, axis=0)
                        m = np.sqrt((1. / nk) + (1. / n_samples))
                        variance = (X - self.centroids_[y_ind]) ** 2
            variance = variance.sum(axis=0)
            s = np.sqrt(variance / (n_samples - n_classes))
            s += np.median(s)              mm = m.reshape(len(m), 1)              ms = mm * s
            deviation = ((self.centroids_ - dataset_centroid_) / ms)
                                    signs = np.sign(deviation)
            deviation = (np.abs(deviation) - self.shrink_threshold)
            deviation[deviation < 0] = 0
            deviation *= signs
                        msd = ms * deviation
            self.centroids_ = dataset_centroid_[np.newaxis, :] + msd
        return self
    def predict(self, X):
                check_is_fitted(self, 'centroids_')
        X = check_array(X, accept_sparse='csr')
        return self.classes_[pairwise_distances(
            X, self.centroids_, metric=self.metric).argmin(axis=1)]

import numpy as np
from .base import _get_weights, _check_weights, NeighborsBase, KNeighborsMixin
from .base import RadiusNeighborsMixin, SupervisedFloatMixin
from ..base import RegressorMixin
from ..utils import check_array

class KNeighborsRegressor(NeighborsBase, KNeighborsMixin,
                          SupervisedFloatMixin,
                          RegressorMixin):
    
    def __init__(self, n_neighbors=5, weights='uniform',
                 algorithm='auto', leaf_size=30,
                 p=2, metric='minkowski', metric_params=None, n_jobs=1,
                 **kwargs):
        self._init_params(n_neighbors=n_neighbors,
                          algorithm=algorithm,
                          leaf_size=leaf_size, metric=metric, p=p,
                          metric_params=metric_params, n_jobs=n_jobs, **kwargs)
        self.weights = _check_weights(weights)
    def predict(self, X):
                X = check_array(X, accept_sparse='csr')
        neigh_dist, neigh_ind = self.kneighbors(X)
        weights = _get_weights(neigh_dist, self.weights)
        _y = self._y
        if _y.ndim == 1:
            _y = _y.reshape((-1, 1))
        if weights is None:
            y_pred = np.mean(_y[neigh_ind], axis=1)
        else:
            y_pred = np.empty((X.shape[0], _y.shape[1]), dtype=np.float64)
            denom = np.sum(weights, axis=1)
            for j in range(_y.shape[1]):
                num = np.sum(_y[neigh_ind, j] * weights, axis=1)
                y_pred[:, j] = num / denom
        if self._y.ndim == 1:
            y_pred = y_pred.ravel()
        return y_pred

class RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin,
                               SupervisedFloatMixin,
                               RegressorMixin):
    
    def __init__(self, radius=1.0, weights='uniform',
                 algorithm='auto', leaf_size=30,
                 p=2, metric='minkowski', metric_params=None, **kwargs):
        self._init_params(radius=radius,
                          algorithm=algorithm,
                          leaf_size=leaf_size,
                          p=p, metric=metric, metric_params=metric_params,
                          **kwargs)
        self.weights = _check_weights(weights)
    def predict(self, X):
                X = check_array(X, accept_sparse='csr')
        neigh_dist, neigh_ind = self.radius_neighbors(X)
        weights = _get_weights(neigh_dist, self.weights)
        _y = self._y
        if _y.ndim == 1:
            _y = _y.reshape((-1, 1))
        if weights is None:
            y_pred = np.array([np.mean(_y[ind, :], axis=0)
                               for ind in neigh_ind])
        else:
            y_pred = np.array([(np.average(_y[ind, :], axis=0,
                                           weights=weights[i]))
                               for (i, ind) in enumerate(neigh_ind)])
        if self._y.ndim == 1:
            y_pred = y_pred.ravel()
        return y_pred
import os

def configuration(parent_package='', top_path=None):
    import numpy
    from numpy.distutils.misc_util import Configuration
    config = Configuration('neighbors', parent_package, top_path)
    libraries = []
    if os.name == 'posix':
        libraries.append('m')
    config.add_extension('ball_tree',
                         sources=['ball_tree.pyx'],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_extension('kd_tree',
                         sources=['kd_tree.pyx'],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_extension('dist_metrics',
                         sources=['dist_metrics.pyx'],
                         include_dirs=[numpy.get_include(),
                                       os.path.join(numpy.get_include(),
                                                    'numpy')],
                         libraries=libraries)
    config.add_extension('typedefs',
                         sources=['typedefs.pyx'],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_subpackage('tests')
    return config
import numpy as np
cimport numpy as np
from libc.math cimport sqrt
ITYPE = np.intp  
DTYPE = np.float64  
cdef DTYPE_t INF = np.inf
cdef DTYPE_t PI = np.pi
cdef DTYPE_t ROOT_2PI = sqrt(2 * PI)
The :mod:`sklearn.neighbors` module implements the k-nearest neighbors
algorithm.

import numpy as np
from abc import ABCMeta, abstractmethod
from scipy.optimize import fmin_l_bfgs_b
import warnings
from ..base import BaseEstimator, ClassifierMixin, RegressorMixin
from ._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS
from ._stochastic_optimizers import SGDOptimizer, AdamOptimizer
from ..model_selection import train_test_split
from ..externals import six
from ..preprocessing import LabelBinarizer
from ..utils import gen_batches, check_random_state
from ..utils import shuffle
from ..utils import check_array, check_X_y, column_or_1d
from ..exceptions import ConvergenceWarning
from ..utils.extmath import safe_sparse_dot
from ..utils.validation import check_is_fitted
from ..utils.multiclass import _check_partial_fit_first_call, unique_labels
from ..utils.multiclass import type_of_target

_STOCHASTIC_SOLVERS = ['sgd', 'adam']

def _pack(coefs_, intercepts_):
        return np.hstack([l.ravel() for l in coefs_ + intercepts_])

class BaseMultilayerPerceptron(six.with_metaclass(ABCMeta, BaseEstimator)):
    
    @abstractmethod
    def __init__(self, hidden_layer_sizes, activation, solver,
                 alpha, batch_size, learning_rate, learning_rate_init, power_t,
                 max_iter, loss, shuffle, random_state, tol, verbose,
                 warm_start, momentum, nesterovs_momentum, early_stopping,
                 validation_fraction, beta_1, beta_2, epsilon):
        self.activation = activation
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.loss = loss
        self.hidden_layer_sizes = hidden_layer_sizes
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
    def _unpack(self, packed_parameters):
                for i in range(self.n_layers_ - 1):
            start, end, shape = self._coef_indptr[i]
            self.coefs_[i] = np.reshape(packed_parameters[start:end], shape)
            start, end = self._intercept_indptr[i]
            self.intercepts_[i] = packed_parameters[start:end]
    def _forward_pass(self, activations):
                hidden_activation = ACTIVATIONS[self.activation]
                for i in range(self.n_layers_ - 1):
            activations[i + 1] = safe_sparse_dot(activations[i],
                                                 self.coefs_[i])
            activations[i + 1] += self.intercepts_[i]
                        if (i + 1) != (self.n_layers_ - 1):
                activations[i + 1] = hidden_activation(activations[i + 1])
                output_activation = ACTIVATIONS[self.out_activation_]
        activations[i + 1] = output_activation(activations[i + 1])
        return activations
    def _compute_loss_grad(self, layer, n_samples, activations, deltas,
                           coef_grads, intercept_grads):
                coef_grads[layer] = safe_sparse_dot(activations[layer].T,
                                            deltas[layer])
        coef_grads[layer] += (self.alpha * self.coefs_[layer])
        coef_grads[layer] /= n_samples
        intercept_grads[layer] = np.mean(deltas[layer], 0)
        return coef_grads, intercept_grads
    def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
                         coef_grads, intercept_grads):
                self._unpack(packed_coef_inter)
        loss, coef_grads, intercept_grads = self._backprop(
            X, y, activations, deltas, coef_grads, intercept_grads)
        self.n_iter_ += 1
        grad = _pack(coef_grads, intercept_grads)
        return loss, grad
    def _backprop(self, X, y, activations, deltas, coef_grads,
                  intercept_grads):
                n_samples = X.shape[0]
                activations = self._forward_pass(activations)
                loss_func_name = self.loss
        if loss_func_name == 'log_loss' and self.out_activation_ == 'logistic':
            loss_func_name = 'binary_log_loss'
        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])
                values = np.sum(
            np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))
        loss += (0.5 * self.alpha) * values / n_samples
                last = self.n_layers_ - 2
                                        deltas[last] = activations[-1] - y
                coef_grads, intercept_grads = self._compute_loss_grad(
            last, n_samples, activations, deltas, coef_grads, intercept_grads)
                for i in range(self.n_layers_ - 2, 0, -1):
            deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)
            inplace_derivative = DERIVATIVES[self.activation]
            inplace_derivative(activations[i], deltas[i - 1])
            coef_grads, intercept_grads = self._compute_loss_grad(
                i - 1, n_samples, activations, deltas, coef_grads,
                intercept_grads)
        return loss, coef_grads, intercept_grads
    def _initialize(self, y, layer_units):
                        self.n_iter_ = 0
        self.t_ = 0
        self.n_outputs_ = y.shape[1]
                self.n_layers_ = len(layer_units)
                if not isinstance(self, ClassifierMixin):
            self.out_activation_ = 'identity'
                elif self._label_binarizer.y_type_ == 'multiclass':
            self.out_activation_ = 'softmax'
                else:
            self.out_activation_ = 'logistic'
                self.coefs_ = []
        self.intercepts_ = []
        for i in range(self.n_layers_ - 1):
            coef_init, intercept_init = self._init_coef(layer_units[i],
                                                        layer_units[i + 1])
            self.coefs_.append(coef_init)
            self.intercepts_.append(intercept_init)
        if self.solver in _STOCHASTIC_SOLVERS:
            self.loss_curve_ = []
            self._no_improvement_count = 0
            if self.early_stopping:
                self.validation_scores_ = []
                self.best_validation_score_ = -np.inf
            else:
                self.best_loss_ = np.inf
    def _init_coef(self, fan_in, fan_out):
        if self.activation == 'logistic':
                                    init_bound = np.sqrt(2. / (fan_in + fan_out))
        elif self.activation in ('identity', 'tanh', 'relu'):
            init_bound = np.sqrt(6. / (fan_in + fan_out))
        else:
                        raise ValueError("Unknown activation function %s" %
                             self.activation)
        coef_init = self._random_state.uniform(-init_bound, init_bound,
                                               (fan_in, fan_out))
        intercept_init = self._random_state.uniform(-init_bound, init_bound,
                                                    fan_out)
        return coef_init, intercept_init
    def _fit(self, X, y, incremental=False):
                hidden_layer_sizes = self.hidden_layer_sizes
        if not hasattr(hidden_layer_sizes, "__iter__"):
            hidden_layer_sizes = [hidden_layer_sizes]
        hidden_layer_sizes = list(hidden_layer_sizes)
                self._validate_hyperparameters()
        if np.any(np.array(hidden_layer_sizes) <= 0):
            raise ValueError("hidden_layer_sizes must be > 0, got %s." %
                             hidden_layer_sizes)
        X, y = self._validate_input(X, y, incremental)
        n_samples, n_features = X.shape
                if y.ndim == 1:
            y = y.reshape((-1, 1))
        self.n_outputs_ = y.shape[1]
        layer_units = ([n_features] + hidden_layer_sizes +
                       [self.n_outputs_])
                self._random_state = check_random_state(self.random_state)
        if not hasattr(self, 'coefs_') or (not self.warm_start and not
                                           incremental):
                        self._initialize(y, layer_units)
                if self.solver == 'lbfgs':
            batch_size = n_samples
        elif self.batch_size == 'auto':
            batch_size = min(200, n_samples)
        else:
            if self.batch_size < 1 or self.batch_size > n_samples:
                warnings.warn("Got `batch_size` less than 1 or larger than "
                              "sample size. It is going to be clipped")
            batch_size = np.clip(self.batch_size, 1, n_samples)
                activations = [X]
        activations.extend(np.empty((batch_size, n_fan_out))
                           for n_fan_out in layer_units[1:])
        deltas = [np.empty_like(a_layer) for a_layer in activations]
        coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,
                      n_fan_out_ in zip(layer_units[:-1],
                                        layer_units[1:])]
        intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in
                           layer_units[1:]]
                if self.solver in _STOCHASTIC_SOLVERS:
            self._fit_stochastic(X, y, activations, deltas, coef_grads,
                                 intercept_grads, layer_units, incremental)
                elif self.solver == 'lbfgs':
            self._fit_lbfgs(X, y, activations, deltas, coef_grads,
                            intercept_grads, layer_units)
        return self
    def _validate_hyperparameters(self):
        if not isinstance(self.shuffle, bool):
            raise ValueError("shuffle must be either True or False, got %s." %
                             self.shuffle)
        if self.max_iter <= 0:
            raise ValueError("max_iter must be > 0, got %s." % self.max_iter)
        if self.alpha < 0.0:
            raise ValueError("alpha must be >= 0, got %s." % self.alpha)
        if (self.learning_rate in ["constant", "invscaling", "adaptive"] and
                self.learning_rate_init <= 0.0):
            raise ValueError("learning_rate_init must be > 0, got %s." %
                             self.learning_rate)
        if self.momentum > 1 or self.momentum < 0:
            raise ValueError("momentum must be >= 0 and <= 1, got %s" %
                             self.momentum)
        if not isinstance(self.nesterovs_momentum, bool):
            raise ValueError("nesterovs_momentum must be either True or False,"
                             " got %s." % self.nesterovs_momentum)
        if not isinstance(self.early_stopping, bool):
            raise ValueError("early_stopping must be either True or False,"
                             " got %s." % self.early_stopping)
        if self.validation_fraction < 0 or self.validation_fraction >= 1:
            raise ValueError("validation_fraction must be >= 0 and < 1, "
                             "got %s" % self.validation_fraction)
        if self.beta_1 < 0 or self.beta_1 >= 1:
            raise ValueError("beta_1 must be >= 0 and < 1, got %s" %
                             self.beta_1)
        if self.beta_2 < 0 or self.beta_2 >= 1:
            raise ValueError("beta_2 must be >= 0 and < 1, got %s" %
                             self.beta_2)
        if self.epsilon <= 0.0:
            raise ValueError("epsilon must be > 0, got %s." % self.epsilon)
                supported_activations = ('identity', 'logistic', 'tanh', 'relu')
        if self.activation not in supported_activations:
            raise ValueError("The activation '%s' is not supported. Supported "
                             "activations are %s." % (self.activation,
                                                      supported_activations))
        if self.learning_rate not in ["constant", "invscaling", "adaptive"]:
            raise ValueError("learning rate %s is not supported. " %
                             self.learning_rate)
        supported_solvers = _STOCHASTIC_SOLVERS + ["lbfgs"]
        if self.solver not in supported_solvers:
            raise ValueError("The solver %s is not supported. "
                             " Expected one of: %s" %
                             (self.solver, ", ".join(supported_solvers)))
    def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
                   intercept_grads, layer_units):
                self._coef_indptr = []
        self._intercept_indptr = []
        start = 0
                for i in range(self.n_layers_ - 1):
            n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]
            end = start + (n_fan_in * n_fan_out)
            self._coef_indptr.append((start, end, (n_fan_in, n_fan_out)))
            start = end
                for i in range(self.n_layers_ - 1):
            end = start + layer_units[i + 1]
            self._intercept_indptr.append((start, end))
            start = end
                packed_coef_inter = _pack(self.coefs_,
                                  self.intercepts_)
        if self.verbose is True or self.verbose >= 1:
            iprint = 1
        else:
            iprint = -1
        optimal_parameters, self.loss_, d = fmin_l_bfgs_b(
            x0=packed_coef_inter,
            func=self._loss_grad_lbfgs,
            maxfun=self.max_iter,
            iprint=iprint,
            pgtol=self.tol,
            args=(X, y, activations, deltas, coef_grads, intercept_grads))
        self._unpack(optimal_parameters)
    def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
                        intercept_grads, layer_units, incremental):
        if not incremental or not hasattr(self, '_optimizer'):
            params = self.coefs_ + self.intercepts_
            if self.solver == 'sgd':
                self._optimizer = SGDOptimizer(
                    params, self.learning_rate_init, self.learning_rate,
                    self.momentum, self.nesterovs_momentum, self.power_t)
            elif self.solver == 'adam':
                self._optimizer = AdamOptimizer(
                    params, self.learning_rate_init, self.beta_1, self.beta_2,
                    self.epsilon)
                early_stopping = self.early_stopping and not incremental
        if early_stopping:
            X, X_val, y, y_val = train_test_split(
                X, y, random_state=self._random_state,
                test_size=self.validation_fraction)
            if isinstance(self, ClassifierMixin):
                y_val = self._label_binarizer.inverse_transform(y_val)
        else:
            X_val = None
            y_val = None
        n_samples = X.shape[0]
        if self.batch_size == 'auto':
            batch_size = min(200, n_samples)
        else:
            batch_size = np.clip(self.batch_size, 1, n_samples)
        try:
            for it in range(self.max_iter):
                X, y = shuffle(X, y, random_state=self._random_state)
                accumulated_loss = 0.0
                for batch_slice in gen_batches(n_samples, batch_size):
                    activations[0] = X[batch_slice]
                    batch_loss, coef_grads, intercept_grads = self._backprop(
                        X[batch_slice], y[batch_slice], activations, deltas,
                        coef_grads, intercept_grads)
                    accumulated_loss += batch_loss * (batch_slice.stop -
                                                      batch_slice.start)
                                        grads = coef_grads + intercept_grads
                    self._optimizer.update_params(grads)
                self.n_iter_ += 1
                self.loss_ = accumulated_loss / X.shape[0]
                self.t_ += n_samples
                self.loss_curve_.append(self.loss_)
                if self.verbose:
                    print("Iteration %d, loss = %.8f" % (self.n_iter_,
                                                         self.loss_))
                                                self._update_no_improvement_count(early_stopping, X_val, y_val)
                                self._optimizer.iteration_ends(self.t_)
                if self._no_improvement_count > 2:
                                                            if early_stopping:
                        msg = ("Validation score did not improve more than "
                               "tol=%f for two consecutive epochs." % self.tol)
                    else:
                        msg = ("Training loss did not improve more than tol=%f"
                               " for two consecutive epochs." % self.tol)
                    is_stopping = self._optimizer.trigger_stopping(
                        msg, self.verbose)
                    if is_stopping:
                        break
                    else:
                        self._no_improvement_count = 0
                if incremental:
                    break
                if self.n_iter_ == self.max_iter:
                    warnings.warn('Stochastic Optimizer: Maximum iterations'
                                  ' reached and the optimization hasn\'t '
                                  'converged yet.'
                                  % (), ConvergenceWarning)
        except KeyboardInterrupt:
            warnings.warn("Training interrupted by user.")
        if early_stopping:
                        self.coefs_ = self._best_coefs
            self.intercepts_ = self._best_intercepts
    def _update_no_improvement_count(self, early_stopping, X_val, y_val):
        if early_stopping:
                        self.validation_scores_.append(self.score(X_val, y_val))
            if self.verbose:
                print("Validation score: %f" % self.validation_scores_[-1])
                                                last_valid_score = self.validation_scores_[-1]
            if last_valid_score < (self.best_validation_score_ +
                                   self.tol):
                self._no_improvement_count += 1
            else:
                self._no_improvement_count = 0
            if last_valid_score > self.best_validation_score_:
                self.best_validation_score_ = last_valid_score
                self._best_coefs = [c.copy() for c in self.coefs_]
                self._best_intercepts = [i.copy()
                                         for i in self.intercepts_]
        else:
            if self.loss_curve_[-1] > self.best_loss_ - self.tol:
                self._no_improvement_count += 1
            else:
                self._no_improvement_count = 0
            if self.loss_curve_[-1] < self.best_loss_:
                self.best_loss_ = self.loss_curve_[-1]
    def fit(self, X, y):
                return self._fit(X, y, incremental=False)
    @property
    def partial_fit(self):
                if self.solver not in _STOCHASTIC_SOLVERS:
            raise AttributeError("partial_fit is only available for stochastic"
                                 " optimizers. %s is not stochastic."
                                 % self.solver)
        return self._partial_fit
    def _partial_fit(self, X, y, classes=None):
        return self._fit(X, y, incremental=True)
    def _predict(self, X):
                X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])
                hidden_layer_sizes = self.hidden_layer_sizes
        if not hasattr(hidden_layer_sizes, "__iter__"):
            hidden_layer_sizes = [hidden_layer_sizes]
        hidden_layer_sizes = list(hidden_layer_sizes)
        layer_units = [X.shape[1]] + hidden_layer_sizes + \
            [self.n_outputs_]
                activations = [X]
        for i in range(self.n_layers_ - 1):
            activations.append(np.empty((X.shape[0],
                                         layer_units[i + 1])))
                self._forward_pass(activations)
        y_pred = activations[-1]
        return y_pred

class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
        def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                 solver='adam', alpha=0.0001,
                 batch_size='auto', learning_rate="constant",
                 learning_rate_init=0.001, power_t=0.5, max_iter=200,
                 shuffle=True, random_state=None, tol=1e-4,
                 verbose=False, warm_start=False, momentum=0.9,
                 nesterovs_momentum=True, early_stopping=False,
                 validation_fraction=0.1, beta_1=0.9, beta_2=0.999,
                 epsilon=1e-8):
        sup = super(MLPClassifier, self)
        sup.__init__(hidden_layer_sizes=hidden_layer_sizes,
                     activation=activation, solver=solver, alpha=alpha,
                     batch_size=batch_size, learning_rate=learning_rate,
                     learning_rate_init=learning_rate_init, power_t=power_t,
                     max_iter=max_iter, loss='log_loss', shuffle=shuffle,
                     random_state=random_state, tol=tol, verbose=verbose,
                     warm_start=warm_start, momentum=momentum,
                     nesterovs_momentum=nesterovs_momentum,
                     early_stopping=early_stopping,
                     validation_fraction=validation_fraction,
                     beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)
    def _validate_input(self, X, y, incremental):
        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],
                         multi_output=True)
        if y.ndim == 2 and y.shape[1] == 1:
            y = column_or_1d(y, warn=True)
        if not incremental:
            self._label_binarizer = LabelBinarizer()
            self._label_binarizer.fit(y)
            self.classes_ = self._label_binarizer.classes_
        elif self.warm_start:
            classes = unique_labels(y)
            if set(classes) != set(self.classes_):
                raise ValueError("warm_start can only be used where `y` has "
                                 "the same classes as in the previous "
                                 "call to fit. Previously got %s, `y` has %s" %
                                 (self.classes_, classes))
        else:
            classes = unique_labels(y)
            if np.setdiff1d(classes, self.classes_, assume_unique=True):
                raise ValueError("`y` has classes not in `self.classes_`."
                                 " `self.classes_` has %s. 'y' has %s." %
                                 (self.classes_, classes))
        y = self._label_binarizer.transform(y)
        return X, y
    def predict(self, X):
                check_is_fitted(self, "coefs_")
        y_pred = self._predict(X)
        if self.n_outputs_ == 1:
            y_pred = y_pred.ravel()
        return self._label_binarizer.inverse_transform(y_pred)
    def fit(self, X, y):
                return self._fit(X, y, incremental=(self.warm_start and
                                            hasattr(self, "classes_")))
    @property
    def partial_fit(self):
                if self.solver not in _STOCHASTIC_SOLVERS:
            raise AttributeError("partial_fit is only available for stochastic"
                                 " optimizer. %s is not stochastic"
                                 % self.solver)
        return self._partial_fit
    def _partial_fit(self, X, y, classes=None):
        if _check_partial_fit_first_call(self, classes):
            self._label_binarizer = LabelBinarizer()
            if type_of_target(y).startswith('multilabel'):
                self._label_binarizer.fit(y)
            else:
                self._label_binarizer.fit(classes)
        super(MLPClassifier, self)._partial_fit(X, y)
        return self
    def predict_log_proba(self, X):
                y_prob = self.predict_proba(X)
        return np.log(y_prob, out=y_prob)
    def predict_proba(self, X):
                check_is_fitted(self, "coefs_")
        y_pred = self._predict(X)
        if self.n_outputs_ == 1:
            y_pred = y_pred.ravel()
        if y_pred.ndim == 1:
            return np.vstack([1 - y_pred, y_pred]).T
        else:
            return y_pred

class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
        def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                 solver='adam', alpha=0.0001,
                 batch_size='auto', learning_rate="constant",
                 learning_rate_init=0.001,
                 power_t=0.5, max_iter=200, shuffle=True,
                 random_state=None, tol=1e-4,
                 verbose=False, warm_start=False, momentum=0.9,
                 nesterovs_momentum=True, early_stopping=False,
                 validation_fraction=0.1, beta_1=0.9, beta_2=0.999,
                 epsilon=1e-8):
        sup = super(MLPRegressor, self)
        sup.__init__(hidden_layer_sizes=hidden_layer_sizes,
                     activation=activation, solver=solver, alpha=alpha,
                     batch_size=batch_size, learning_rate=learning_rate,
                     learning_rate_init=learning_rate_init, power_t=power_t,
                     max_iter=max_iter, loss='squared_loss', shuffle=shuffle,
                     random_state=random_state, tol=tol, verbose=verbose,
                     warm_start=warm_start, momentum=momentum,
                     nesterovs_momentum=nesterovs_momentum,
                     early_stopping=early_stopping,
                     validation_fraction=validation_fraction,
                     beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)
    def predict(self, X):
                check_is_fitted(self, "coefs_")
        y_pred = self._predict(X)
        if y_pred.shape[1] == 1:
            return y_pred.ravel()
        return y_pred
    def _validate_input(self, X, y, incremental):
        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],
                         multi_output=True, y_numeric=True)
        if y.ndim == 2 and y.shape[1] == 1:
            y = column_or_1d(y, warn=True)
        return X, y

import time
import numpy as np
import scipy.sparse as sp
from ..base import BaseEstimator
from ..base import TransformerMixin
from ..externals.six.moves import xrange
from ..utils import check_array
from ..utils import check_random_state
from ..utils import gen_even_slices
from ..utils import issparse
from ..utils.extmath import safe_sparse_dot
from ..utils.extmath import log_logistic
from ..utils.fixes import expit             from ..utils.validation import check_is_fitted

class BernoulliRBM(BaseEstimator, TransformerMixin):
        def __init__(self, n_components=256, learning_rate=0.1, batch_size=10,
                 n_iter=10, verbose=0, random_state=None):
        self.n_components = n_components
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.n_iter = n_iter
        self.verbose = verbose
        self.random_state = random_state
    def transform(self, X):
                check_is_fitted(self, "components_")
        X = check_array(X, accept_sparse='csr', dtype=np.float64)
        return self._mean_hiddens(X)
    def _mean_hiddens(self, v):
                p = safe_sparse_dot(v, self.components_.T)
        p += self.intercept_hidden_
        return expit(p, out=p)
    def _sample_hiddens(self, v, rng):
                p = self._mean_hiddens(v)
        return (rng.random_sample(size=p.shape) < p)
    def _sample_visibles(self, h, rng):
                p = np.dot(h, self.components_)
        p += self.intercept_visible_
        expit(p, out=p)
        return (rng.random_sample(size=p.shape) < p)
    def _free_energy(self, v):
                return (- safe_sparse_dot(v, self.intercept_visible_)
                - np.logaddexp(0, safe_sparse_dot(v, self.components_.T)
                               + self.intercept_hidden_).sum(axis=1))
    def gibbs(self, v):
                check_is_fitted(self, "components_")
        if not hasattr(self, "random_state_"):
            self.random_state_ = check_random_state(self.random_state)
        h_ = self._sample_hiddens(v, self.random_state_)
        v_ = self._sample_visibles(h_, self.random_state_)
        return v_
    def partial_fit(self, X, y=None):
                X = check_array(X, accept_sparse='csr', dtype=np.float64)
        if not hasattr(self, 'random_state_'):
            self.random_state_ = check_random_state(self.random_state)
        if not hasattr(self, 'components_'):
            self.components_ = np.asarray(
                self.random_state_.normal(
                    0,
                    0.01,
                    (self.n_components, X.shape[1])
                ),
                order='F')
        if not hasattr(self, 'intercept_hidden_'):
            self.intercept_hidden_ = np.zeros(self.n_components, )
        if not hasattr(self, 'intercept_visible_'):
            self.intercept_visible_ = np.zeros(X.shape[1], )
        if not hasattr(self, 'h_samples_'):
            self.h_samples_ = np.zeros((self.batch_size, self.n_components))
        self._fit(X, self.random_state_)
    def _fit(self, v_pos, rng):
                h_pos = self._mean_hiddens(v_pos)
        v_neg = self._sample_visibles(self.h_samples_, rng)
        h_neg = self._mean_hiddens(v_neg)
        lr = float(self.learning_rate) / v_pos.shape[0]
        update = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T
        update -= np.dot(h_neg.T, v_neg)
        self.components_ += lr * update
        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))
        self.intercept_visible_ += lr * (np.asarray(
                                         v_pos.sum(axis=0)).squeeze() -
                                         v_neg.sum(axis=0))
        h_neg[rng.uniform(size=h_neg.shape) < h_neg] = 1.0          self.h_samples_ = np.floor(h_neg, h_neg)
    def score_samples(self, X):
                check_is_fitted(self, "components_")
        v = check_array(X, accept_sparse='csr')
        rng = check_random_state(self.random_state)
                ind = (np.arange(v.shape[0]),
               rng.randint(0, v.shape[1], v.shape[0]))
        if issparse(v):
            data = -2 * v[ind] + 1
            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)
        else:
            v_ = v.copy()
            v_[ind] = 1 - v_[ind]
        fe = self._free_energy(v)
        fe_ = self._free_energy(v_)
        return v.shape[1] * log_logistic(fe_ - fe)
    def fit(self, X, y=None):
                X = check_array(X, accept_sparse='csr', dtype=np.float64)
        n_samples = X.shape[0]
        rng = check_random_state(self.random_state)
        self.components_ = np.asarray(
            rng.normal(0, 0.01, (self.n_components, X.shape[1])),
            order='F')
        self.intercept_hidden_ = np.zeros(self.n_components, )
        self.intercept_visible_ = np.zeros(X.shape[1], )
        self.h_samples_ = np.zeros((self.batch_size, self.n_components))
        n_batches = int(np.ceil(float(n_samples) / self.batch_size))
        batch_slices = list(gen_even_slices(n_batches * self.batch_size,
                                            n_batches, n_samples))
        verbose = self.verbose
        begin = time.time()
        for iteration in xrange(1, self.n_iter + 1):
            for batch_slice in batch_slices:
                self._fit(X[batch_slice], rng)
            if verbose:
                end = time.time()
                print("[%s] Iteration %d, pseudo-likelihood = %.2f,"
                      " time = %.2fs"
                      % (type(self).__name__, iteration,
                         self.score_samples(X).mean(), end - begin))
                begin = end
        return self

import numpy as np
from ..utils.fixes import expit as logistic_sigmoid

def identity(X):
        return X

def logistic(X):
        return logistic_sigmoid(X, out=X)

def tanh(X):
        return np.tanh(X, out=X)

def relu(X):
        np.clip(X, 0, np.finfo(X.dtype).max, out=X)
    return X

def softmax(X):
        tmp = X - X.max(axis=1)[:, np.newaxis]
    np.exp(tmp, out=X)
    X /= X.sum(axis=1)[:, np.newaxis]
    return X

ACTIVATIONS = {'identity': identity, 'tanh': tanh, 'logistic': logistic,
               'relu': relu, 'softmax': softmax}

def inplace_identity_derivative(Z, delta):
        
def inplace_logistic_derivative(Z, delta):
        delta *= Z
    delta *= (1 - Z)

def inplace_tanh_derivative(Z, delta):
        delta *= (1 - Z ** 2)

def inplace_relu_derivative(Z, delta):
        delta[Z == 0] = 0

DERIVATIVES = {'identity': inplace_identity_derivative,
               'tanh': inplace_tanh_derivative,
               'logistic': inplace_logistic_derivative,
               'relu': inplace_relu_derivative}

def squared_loss(y_true, y_pred):
        return ((y_true - y_pred) ** 2).mean() / 2

def log_loss(y_true, y_prob):
        y_prob = np.clip(y_prob, 1e-10, 1 - 1e-10)
    if y_prob.shape[1] == 1:
        y_prob = np.append(1 - y_prob, y_prob, axis=1)
    if y_true.shape[1] == 1:
        y_true = np.append(1 - y_true, y_true, axis=1)
    return -np.sum(y_true * np.log(y_prob)) / y_prob.shape[0]

def binary_log_loss(y_true, y_prob):
        y_prob = np.clip(y_prob, 1e-10, 1 - 1e-10)
    return -np.sum(y_true * np.log(y_prob) +
                   (1 - y_true) * np.log(1 - y_prob)) / y_prob.shape[0]

LOSS_FUNCTIONS = {'squared_loss': squared_loss, 'log_loss': log_loss,
                  'binary_log_loss': binary_log_loss}

import numpy as np

class BaseOptimizer(object):
    
    def __init__(self, params, learning_rate_init=0.1):
        self.params = [param for param in params]
        self.learning_rate_init = learning_rate_init
        self.learning_rate = float(learning_rate_init)
    def update_params(self, grads):
                updates = self._get_updates(grads)
        for param, update in zip(self.params, updates):
            param += update
    def iteration_ends(self, time_step):
                pass
    def trigger_stopping(self, msg, verbose):
                if verbose:
            print(msg + " Stopping.")
        return True

class SGDOptimizer(BaseOptimizer):
    
    def __init__(self, params, learning_rate_init=0.1, lr_schedule='constant',
                 momentum=0.9, nesterov=True, power_t=0.5):
        super(SGDOptimizer, self).__init__(params, learning_rate_init)
        self.lr_schedule = lr_schedule
        self.momentum = momentum
        self.nesterov = nesterov
        self.power_t = power_t
        self.velocities = [np.zeros_like(param) for param in params]
    def iteration_ends(self, time_step):
                if self.lr_schedule == 'invscaling':
            self.learning_rate = (float(self.learning_rate_init) /
                                  (time_step + 1) ** self.power_t)
    def trigger_stopping(self, msg, verbose):
        if self.lr_schedule == 'adaptive':
            if self.learning_rate > 1e-6:
                self.learning_rate /= 5.
                if verbose:
                    print(msg + " Setting learning rate to %f" %
                          self.learning_rate)
                return False
            else:
                if verbose:
                    print(msg + " Learning rate too small. Stopping.")
                return True
        else:
            if verbose:
                print(msg + " Stopping.")
            return True
    def _get_updates(self, grads):
                updates = [self.momentum * velocity - self.learning_rate * grad
                   for velocity, grad in zip(self.velocities, grads)]
        self.velocities = updates
        if self.nesterov:
            updates = [self.momentum * velocity - self.learning_rate * grad
                       for velocity, grad in zip(self.velocities, grads)]
        return updates

class AdamOptimizer(BaseOptimizer):
    
    def __init__(self, params, learning_rate_init=0.001, beta_1=0.9,
                 beta_2=0.999, epsilon=1e-8):
        super(AdamOptimizer, self).__init__(params, learning_rate_init)
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.t = 0
        self.ms = [np.zeros_like(param) for param in params]
        self.vs = [np.zeros_like(param) for param in params]
    def _get_updates(self, grads):
                self.t += 1
        self.ms = [self.beta_1 * m + (1 - self.beta_1) * grad
                   for m, grad in zip(self.ms, grads)]
        self.vs = [self.beta_2 * v + (1 - self.beta_2) * (grad ** 2)
                   for v, grad in zip(self.vs, grads)]
        self.learning_rate = (self.learning_rate_init *
                              np.sqrt(1 - self.beta_2 ** self.t) /
                              (1 - self.beta_1 ** self.t))
        updates = [-self.learning_rate * m / (np.sqrt(v) + self.epsilon)
                   for m, v in zip(self.ms, self.vs)]
        return updates

from .rbm import BernoulliRBM
from .multilayer_perceptron import MLPClassifier
from .multilayer_perceptron import MLPRegressor
__all__ = ["BernoulliRBM",
           "MLPClassifier",
           "MLPRegressor"]
from itertools import chain, combinations
import numbers
import warnings
from itertools import combinations_with_replacement as combinations_w_r
import numpy as np
from scipy import sparse
from ..base import BaseEstimator, TransformerMixin
from ..externals import six
from ..utils import check_array
from ..utils.extmath import row_norms
from ..utils.extmath import _incremental_mean_and_var
from ..utils.fixes import bincount
from ..utils.sparsefuncs_fast import (inplace_csr_row_normalize_l1,
                                      inplace_csr_row_normalize_l2)
from ..utils.sparsefuncs import (inplace_column_scale,
                                 mean_variance_axis, incr_mean_variance_axis,
                                 min_max_axis)
from ..utils.validation import check_is_fitted, FLOAT_DTYPES

zip = six.moves.zip
map = six.moves.map
range = six.moves.range
__all__ = [
    'Binarizer',
    'KernelCenterer',
    'MinMaxScaler',
    'MaxAbsScaler',
    'Normalizer',
    'OneHotEncoder',
    'RobustScaler',
    'StandardScaler',
    'add_dummy_feature',
    'binarize',
    'normalize',
    'scale',
    'robust_scale',
    'maxabs_scale',
    'minmax_scale',
]

def _handle_zeros_in_scale(scale, copy=True):
    ''' Makes sure that whenever scale is zero, we handle it correctly.
    This happens in most scalers when we have constant features.'''
        if np.isscalar(scale):
        if scale == .0:
            scale = 1.
        return scale
    elif isinstance(scale, np.ndarray):
        if copy:
                        scale = scale.copy()
        scale[scale == 0.0] = 1.0
        return scale

def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
    
    def __init__(self, feature_range=(0, 1), copy=True):
        self.feature_range = feature_range
        self.copy = copy
    def _reset(self):
        
                        if hasattr(self, 'scale_'):
            del self.scale_
            del self.min_
            del self.n_samples_seen_
            del self.data_min_
            del self.data_max_
            del self.data_range_
    def fit(self, X, y=None):
        
                self._reset()
        return self.partial_fit(X, y)
    def partial_fit(self, X, y=None):
                feature_range = self.feature_range
        if feature_range[0] >= feature_range[1]:
            raise ValueError("Minimum of desired feature range must be smaller"
                             " than maximum. Got %s." % str(feature_range))
        if sparse.issparse(X):
            raise TypeError("MinMaxScaler does no support sparse input. "
                            "You may consider to use MaxAbsScaler instead.")
        X = check_array(X, copy=self.copy, warn_on_dtype=True,
                        estimator=self, dtype=FLOAT_DTYPES)
        data_min = np.min(X, axis=0)
        data_max = np.max(X, axis=0)
                if not hasattr(self, 'n_samples_seen_'):
            self.n_samples_seen_ = X.shape[0]
                else:
            data_min = np.minimum(self.data_min_, data_min)
            data_max = np.maximum(self.data_max_, data_max)
            self.n_samples_seen_ += X.shape[0]
        data_range = data_max - data_min
        self.scale_ = ((feature_range[1] - feature_range[0]) /
                       _handle_zeros_in_scale(data_range))
        self.min_ = feature_range[0] - data_min * self.scale_
        self.data_min_ = data_min
        self.data_max_ = data_max
        self.data_range_ = data_range
        return self
    def transform(self, X):
                check_is_fitted(self, 'scale_')
        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)
        X *= self.scale_
        X += self.min_
        return X
    def inverse_transform(self, X):
                check_is_fitted(self, 'scale_')
        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)
        X -= self.min_
        X /= self.scale_
        return X

def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):
    
                        if hasattr(self, 'scale_'):
            del self.scale_
            del self.n_samples_seen_
            del self.mean_
            del self.var_
    def fit(self, X, y=None):
        
                self._reset()
        return self.partial_fit(X, y)
    def partial_fit(self, X, y=None):
                X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)
                        
        if sparse.issparse(X):
            if self.with_mean:
                raise ValueError(
                    "Cannot center sparse matrices: pass `with_mean=False` "
                    "instead. See docstring for motivation and alternatives.")
            if self.with_std:
                                if not hasattr(self, 'n_samples_seen_'):
                    self.mean_, self.var_ = mean_variance_axis(X, axis=0)
                    self.n_samples_seen_ = X.shape[0]
                                else:
                    self.mean_, self.var_, self.n_samples_seen_ = \
                        incr_mean_variance_axis(X, axis=0,
                                                last_mean=self.mean_,
                                                last_var=self.var_,
                                                last_n=self.n_samples_seen_)
            else:
                self.mean_ = None
                self.var_ = None
        else:
                        if not hasattr(self, 'n_samples_seen_'):
                self.mean_ = .0
                self.n_samples_seen_ = 0
                if self.with_std:
                    self.var_ = .0
                else:
                    self.var_ = None
            self.mean_, self.var_, self.n_samples_seen_ = \
                _incremental_mean_and_var(X, self.mean_, self.var_,
                                          self.n_samples_seen_)
        if self.with_std:
            self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))
        else:
            self.scale_ = None
        return self
    def transform(self, X, y=None, copy=None):
                check_is_fitted(self, 'scale_')
        copy = copy if copy is not None else self.copy
        X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,
                        estimator=self, dtype=FLOAT_DTYPES)
        if sparse.issparse(X):
            if self.with_mean:
                raise ValueError(
                    "Cannot center sparse matrices: pass `with_mean=False` "
                    "instead. See docstring for motivation and alternatives.")
            if self.scale_ is not None:
                inplace_column_scale(X, 1 / self.scale_)
        else:
            if self.with_mean:
                X -= self.mean_
            if self.with_std:
                X /= self.scale_
        return X
    def inverse_transform(self, X, copy=None):
                check_is_fitted(self, 'scale_')
        copy = copy if copy is not None else self.copy
        if sparse.issparse(X):
            if self.with_mean:
                raise ValueError(
                    "Cannot uncenter sparse matrices: pass `with_mean=False` "
                    "instead See docstring for motivation and alternatives.")
            if not sparse.isspmatrix_csr(X):
                X = X.tocsr()
                copy = False
            if copy:
                X = X.copy()
            if self.scale_ is not None:
                inplace_column_scale(X, self.scale_)
        else:
            X = np.asarray(X)
            if copy:
                X = X.copy()
            if self.with_std:
                X *= self.scale_
            if self.with_mean:
                X += self.mean_
        return X

class MaxAbsScaler(BaseEstimator, TransformerMixin):
    
    def __init__(self, copy=True):
        self.copy = copy
    def _reset(self):
        
                        if hasattr(self, 'scale_'):
            del self.scale_
            del self.n_samples_seen_
            del self.max_abs_
    def fit(self, X, y=None):
        
                self._reset()
        return self.partial_fit(X, y)
    def partial_fit(self, X, y=None):
                X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
                        estimator=self, dtype=FLOAT_DTYPES)
        if sparse.issparse(X):
            mins, maxs = min_max_axis(X, axis=0)
            max_abs = np.maximum(np.abs(mins), np.abs(maxs))
        else:
            max_abs = np.abs(X).max(axis=0)
                if not hasattr(self, 'n_samples_seen_'):
            self.n_samples_seen_ = X.shape[0]
                else:
            max_abs = np.maximum(self.max_abs_, max_abs)
            self.n_samples_seen_ += X.shape[0]
        self.max_abs_ = max_abs
        self.scale_ = _handle_zeros_in_scale(max_abs)
        return self
    def transform(self, X, y=None):
                check_is_fitted(self, 'scale_')
        X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
                        estimator=self, dtype=FLOAT_DTYPES)
        if sparse.issparse(X):
            inplace_column_scale(X, 1.0 / self.scale_)
        else:
            X /= self.scale_
        return X
    def inverse_transform(self, X):
                check_is_fitted(self, 'scale_')
        X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
                        estimator=self, dtype=FLOAT_DTYPES)
        if sparse.issparse(X):
            inplace_column_scale(X, self.scale_)
        else:
            X *= self.scale_
        return X

def maxabs_scale(X, axis=0, copy=True):
    
    def __init__(self, with_centering=True, with_scaling=True,
                 quantile_range=(25.0, 75.0), copy=True):
        self.with_centering = with_centering
        self.with_scaling = with_scaling
        self.quantile_range = quantile_range
        self.copy = copy
    def _check_array(self, X, copy):
                X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,
                        estimator=self, dtype=FLOAT_DTYPES)
        if sparse.issparse(X):
            if self.with_centering:
                raise ValueError(
                    "Cannot center sparse matrices: use `with_centering=False`"
                    " instead. See docstring for motivation and alternatives.")
        return X
    def fit(self, X, y=None):
                if sparse.issparse(X):
            raise TypeError("RobustScaler cannot be fitted on sparse inputs")
        X = self._check_array(X, self.copy)
        if self.with_centering:
            self.center_ = np.median(X, axis=0)
        if self.with_scaling:
            q_min, q_max = self.quantile_range
            if not 0 <= q_min <= q_max <= 100:
                raise ValueError("Invalid quantile range: %s" %
                                 str(self.quantile_range))
            q = np.percentile(X, self.quantile_range, axis=0)
            self.scale_ = (q[1] - q[0])
            self.scale_ = _handle_zeros_in_scale(self.scale_, copy=False)
        return self
    def transform(self, X, y=None):
                if self.with_centering:
            check_is_fitted(self, 'center_')
        if self.with_scaling:
            check_is_fitted(self, 'scale_')
        X = self._check_array(X, self.copy)
        if sparse.issparse(X):
            if self.with_scaling:
                inplace_column_scale(X, 1.0 / self.scale_)
        else:
            if self.with_centering:
                X -= self.center_
            if self.with_scaling:
                X /= self.scale_
        return X
    def inverse_transform(self, X):
                if self.with_centering:
            check_is_fitted(self, 'center_')
        if self.with_scaling:
            check_is_fitted(self, 'scale_')
        X = self._check_array(X, self.copy)
        if sparse.issparse(X):
            if self.with_scaling:
                inplace_column_scale(X, self.scale_)
        else:
            if self.with_scaling:
                X *= self.scale_
            if self.with_centering:
                X += self.center_
        return X

def robust_scale(X, axis=0, with_centering=True, with_scaling=True,
                 quantile_range=(25.0, 75.0), copy=True):
        s = RobustScaler(with_centering=with_centering, with_scaling=with_scaling,
                     quantile_range=quantile_range, copy=copy)
    if axis == 0:
        return s.fit_transform(X)
    else:
        return s.fit_transform(X.T).T

class PolynomialFeatures(BaseEstimator, TransformerMixin):
        def __init__(self, degree=2, interaction_only=False, include_bias=True):
        self.degree = degree
        self.interaction_only = interaction_only
        self.include_bias = include_bias
    @staticmethod
    def _combinations(n_features, degree, interaction_only, include_bias):
        comb = (combinations if interaction_only else combinations_w_r)
        start = int(not include_bias)
        return chain.from_iterable(comb(range(n_features), i)
                                   for i in range(start, degree + 1))
    @property
    def powers_(self):
        check_is_fitted(self, 'n_input_features_')
        combinations = self._combinations(self.n_input_features_, self.degree,
                                          self.interaction_only,
                                          self.include_bias)
        return np.vstack(bincount(c, minlength=self.n_input_features_)
                         for c in combinations)
    def get_feature_names(self, input_features=None):
                powers = self.powers_
        if input_features is None:
            input_features = ['x%d' % i for i in range(powers.shape[1])]
        feature_names = []
        for row in powers:
            inds = np.where(row)[0]
            if len(inds):
                name = " ".join("%s^%d" % (input_features[ind], exp)
                                if exp != 1 else input_features[ind]
                                for ind, exp in zip(inds, row[inds]))
            else:
                name = "1"
            feature_names.append(name)
        return feature_names
    def fit(self, X, y=None):
                n_samples, n_features = check_array(X).shape
        combinations = self._combinations(n_features, self.degree,
                                          self.interaction_only,
                                          self.include_bias)
        self.n_input_features_ = n_features
        self.n_output_features_ = sum(1 for _ in combinations)
        return self
    def transform(self, X, y=None):
                check_is_fitted(self, ['n_input_features_', 'n_output_features_'])
        X = check_array(X, dtype=FLOAT_DTYPES)
        n_samples, n_features = X.shape
        if n_features != self.n_input_features_:
            raise ValueError("X shape does not match training shape")
                XP = np.empty((n_samples, self.n_output_features_), dtype=X.dtype)
        combinations = self._combinations(n_features, self.degree,
                                          self.interaction_only,
                                          self.include_bias)
        for i, c in enumerate(combinations):
            XP[:, i] = X[:, c].prod(1)
        return XP

def normalize(X, norm='l2', axis=1, copy=True, return_norm=False):
        if norm not in ('l1', 'l2', 'max'):
        raise ValueError("'%s' is not a supported norm" % norm)
    if axis == 0:
        sparse_format = 'csc'
    elif axis == 1:
        sparse_format = 'csr'
    else:
        raise ValueError("'%d' is not a supported axis" % axis)
    X = check_array(X, sparse_format, copy=copy, warn_on_dtype=True,
                    estimator='the normalize function', dtype=FLOAT_DTYPES)
    if axis == 0:
        X = X.T
    if sparse.issparse(X):
        if return_norm and norm in ('l1', 'l2'):
            raise NotImplementedError("return_norm=True is not implemented "
                                      "for sparse matrices with norm 'l1' "
                                      "or norm 'l2'")
        if norm == 'l1':
            inplace_csr_row_normalize_l1(X)
        elif norm == 'l2':
            inplace_csr_row_normalize_l2(X)
        elif norm == 'max':
            _, norms = min_max_axis(X, 1)
            norms_elementwise = norms.repeat(np.diff(X.indptr))
            mask = norms_elementwise != 0
            X.data[mask] /= norms_elementwise[mask]
    else:
        if norm == 'l1':
            norms = np.abs(X).sum(axis=1)
        elif norm == 'l2':
            norms = row_norms(X)
        elif norm == 'max':
            norms = np.max(X, axis=1)
        norms = _handle_zeros_in_scale(norms, copy=False)
        X /= norms[:, np.newaxis]
    if axis == 0:
        X = X.T
    if return_norm:
        return X, norms
    else:
        return X

class Normalizer(BaseEstimator, TransformerMixin):
    
    def __init__(self, norm='l2', copy=True):
        self.norm = norm
        self.copy = copy
    def fit(self, X, y=None):
                X = check_array(X, accept_sparse='csr')
        return self
    def transform(self, X, y=None, copy=None):
                copy = copy if copy is not None else self.copy
        X = check_array(X, accept_sparse='csr')
        return normalize(X, norm=self.norm, axis=1, copy=copy)

def binarize(X, threshold=0.0, copy=True):
        X = check_array(X, accept_sparse=['csr', 'csc'], copy=copy)
    if sparse.issparse(X):
        if threshold < 0:
            raise ValueError('Cannot binarize a sparse matrix with threshold '
                             '< 0')
        cond = X.data > threshold
        not_cond = np.logical_not(cond)
        X.data[cond] = 1
        X.data[not_cond] = 0
        X.eliminate_zeros()
    else:
        cond = X > threshold
        not_cond = np.logical_not(cond)
        X[cond] = 1
        X[not_cond] = 0
    return X

class Binarizer(BaseEstimator, TransformerMixin):
    
    def __init__(self, threshold=0.0, copy=True):
        self.threshold = threshold
        self.copy = copy
    def fit(self, X, y=None):
                check_array(X, accept_sparse='csr')
        return self
    def transform(self, X, y=None, copy=None):
                copy = copy if copy is not None else self.copy
        return binarize(X, threshold=self.threshold, copy=copy)

class KernelCenterer(BaseEstimator, TransformerMixin):
    
    def fit(self, K, y=None):
                K = check_array(K, dtype=FLOAT_DTYPES)
        n_samples = K.shape[0]
        self.K_fit_rows_ = np.sum(K, axis=0) / n_samples
        self.K_fit_all_ = self.K_fit_rows_.sum() / n_samples
        return self
    def transform(self, K, y=None, copy=True):
                check_is_fitted(self, 'K_fit_all_')
        K = check_array(K, copy=copy, dtype=FLOAT_DTYPES)
        K_pred_cols = (np.sum(K, axis=1) /
                       self.K_fit_rows_.shape[0])[:, np.newaxis]
        K -= self.K_fit_rows_
        K -= K_pred_cols
        K += self.K_fit_all_
        return K
    @property
    def _pairwise(self):
        return True

def add_dummy_feature(X, value=1.0):
        X = check_array(X, accept_sparse=['csc', 'csr', 'coo'], dtype=FLOAT_DTYPES)
    n_samples, n_features = X.shape
    shape = (n_samples, n_features + 1)
    if sparse.issparse(X):
        if sparse.isspmatrix_coo(X):
                        col = X.col + 1
                        col = np.concatenate((np.zeros(n_samples), col))
                        row = np.concatenate((np.arange(n_samples), X.row))
                        data = np.concatenate((np.ones(n_samples) * value, X.data))
            return sparse.coo_matrix((data, (row, col)), shape)
        elif sparse.isspmatrix_csc(X):
                        indptr = X.indptr + n_samples
                        indptr = np.concatenate((np.array([0]), indptr))
                        indices = np.concatenate((np.arange(n_samples), X.indices))
                        data = np.concatenate((np.ones(n_samples) * value, X.data))
            return sparse.csc_matrix((data, indices, indptr), shape)
        else:
            klass = X.__class__
            return klass(add_dummy_feature(X.tocoo(), value))
    else:
        return np.hstack((np.ones((n_samples, 1)) * value, X))

def _transform_selected(X, transform, selected="all", copy=True):
        X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
    if isinstance(selected, six.string_types) and selected == "all":
        return transform(X)
    if len(selected) == 0:
        return X
    n_features = X.shape[1]
    ind = np.arange(n_features)
    sel = np.zeros(n_features, dtype=bool)
    sel[np.asarray(selected)] = True
    not_sel = np.logical_not(sel)
    n_selected = np.sum(sel)
    if n_selected == 0:
                return X
    elif n_selected == n_features:
                return transform(X)
    else:
        X_sel = transform(X[:, ind[sel]])
        X_not_sel = X[:, ind[not_sel]]
        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):
            return sparse.hstack((X_sel, X_not_sel))
        else:
            return np.hstack((X_sel, X_not_sel))

class OneHotEncoder(BaseEstimator, TransformerMixin):
        def __init__(self, n_values="auto", categorical_features="all",
                 dtype=np.float64, sparse=True, handle_unknown='error'):
        self.n_values = n_values
        self.categorical_features = categorical_features
        self.dtype = dtype
        self.sparse = sparse
        self.handle_unknown = handle_unknown
    def fit(self, X, y=None):
                self.fit_transform(X)
        return self
    def _fit_transform(self, X):
                X = check_array(X, dtype=np.int)
        if np.any(X < 0):
            raise ValueError("X needs to contain only non-negative integers.")
        n_samples, n_features = X.shape
        if (isinstance(self.n_values, six.string_types) and
                self.n_values == 'auto'):
            n_values = np.max(X, axis=0) + 1
        elif isinstance(self.n_values, numbers.Integral):
            if (np.max(X, axis=0) >= self.n_values).any():
                raise ValueError("Feature out of bounds for n_values=%d"
                                 % self.n_values)
            n_values = np.empty(n_features, dtype=np.int)
            n_values.fill(self.n_values)
        else:
            try:
                n_values = np.asarray(self.n_values, dtype=int)
            except (ValueError, TypeError):
                raise TypeError("Wrong type for parameter `n_values`. Expected"
                                " 'auto', int or array of ints, got %r"
                                % type(X))
            if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:
                raise ValueError("Shape mismatch: if n_values is an array,"
                                 " it has to be of shape (n_features,).")
        self.n_values_ = n_values
        n_values = np.hstack([[0], n_values])
        indices = np.cumsum(n_values)
        self.feature_indices_ = indices
        column_indices = (X + indices[:-1]).ravel()
        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
                                n_features)
        data = np.ones(n_samples * n_features)
        out = sparse.coo_matrix((data, (row_indices, column_indices)),
                                shape=(n_samples, indices[-1]),
                                dtype=self.dtype).tocsr()
        if (isinstance(self.n_values, six.string_types) and
                self.n_values == 'auto'):
            mask = np.array(out.sum(axis=0)).ravel() != 0
            active_features = np.where(mask)[0]
            out = out[:, active_features]
            self.active_features_ = active_features
        return out if self.sparse else out.toarray()
    def fit_transform(self, X, y=None):
                return _transform_selected(X, self._fit_transform,
                                   self.categorical_features, copy=True)
    def _transform(self, X):
                X = check_array(X, dtype=np.int)
        if np.any(X < 0):
            raise ValueError("X needs to contain only non-negative integers.")
        n_samples, n_features = X.shape
        indices = self.feature_indices_
        if n_features != indices.shape[0] - 1:
            raise ValueError("X has different shape than during fitting."
                             " Expected %d, got %d."
                             % (indices.shape[0] - 1, n_features))
                                                mask = (X < self.n_values_).ravel()
        if np.any(~mask):
            if self.handle_unknown not in ['error', 'ignore']:
                raise ValueError("handle_unknown should be either error or "
                                 "unknown got %s" % self.handle_unknown)
            if self.handle_unknown == 'error':
                raise ValueError("unknown categorical feature present %s "
                                 "during transform." % X.ravel()[~mask])
        column_indices = (X + indices[:-1]).ravel()[mask]
        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
                                n_features)[mask]
        data = np.ones(np.sum(mask))
        out = sparse.coo_matrix((data, (row_indices, column_indices)),
                                shape=(n_samples, indices[-1]),
                                dtype=self.dtype).tocsr()
        if (isinstance(self.n_values, six.string_types) and
                self.n_values == 'auto'):
            out = out[:, self.active_features_]
        return out if self.sparse else out.toarray()
    def transform(self, X):
                return _transform_selected(X, self._transform,
                                   self.categorical_features, copy=True)
import warnings
import numpy as np
import numpy.ma as ma
from scipy import sparse
from scipy import stats
from ..base import BaseEstimator, TransformerMixin
from ..utils import check_array
from ..utils.fixes import astype
from ..utils.sparsefuncs import _get_median
from ..utils.validation import check_is_fitted
from ..utils.validation import FLOAT_DTYPES
from ..externals import six
zip = six.moves.zip
map = six.moves.map
__all__ = [
    'Imputer',
]

def _get_mask(X, value_to_mask):
        if value_to_mask == "NaN" or np.isnan(value_to_mask):
        return np.isnan(X)
    else:
        return X == value_to_mask

def _most_frequent(array, extra_value, n_repeat):
            if array.size > 0:
        mode = stats.mode(array)
        most_frequent_value = mode[0][0]
        most_frequent_count = mode[1][0]
    else:
        most_frequent_value = 0
        most_frequent_count = 0
        if most_frequent_count == 0 and n_repeat == 0:
        return np.nan
    elif most_frequent_count < n_repeat:
        return extra_value
    elif most_frequent_count > n_repeat:
        return most_frequent_value
    elif most_frequent_count == n_repeat:
                if most_frequent_value < extra_value:
            return most_frequent_value
        else:
            return extra_value

class Imputer(BaseEstimator, TransformerMixin):
        def __init__(self, missing_values="NaN", strategy="mean",
                 axis=0, verbose=0, copy=True):
        self.missing_values = missing_values
        self.strategy = strategy
        self.axis = axis
        self.verbose = verbose
        self.copy = copy
    def fit(self, X, y=None):
                        allowed_strategies = ["mean", "median", "most_frequent"]
        if self.strategy not in allowed_strategies:
            raise ValueError("Can only use these strategies: {0} "
                             " got strategy={1}".format(allowed_strategies,
                                                        self.strategy))
        if self.axis not in [0, 1]:
            raise ValueError("Can only impute missing values on axis 0 and 1, "
                             " got axis={0}".format(self.axis))
                                if self.axis == 0:
            X = check_array(X, accept_sparse='csc', dtype=np.float64,
                            force_all_finite=False)
            if sparse.issparse(X):
                self.statistics_ = self._sparse_fit(X,
                                                    self.strategy,
                                                    self.missing_values,
                                                    self.axis)
            else:
                self.statistics_ = self._dense_fit(X,
                                                   self.strategy,
                                                   self.missing_values,
                                                   self.axis)
        return self
    def _sparse_fit(self, X, strategy, missing_values, axis):
                                if axis == 1:
            X = X.tocsr()
        else:
            X = X.tocsc()
                if missing_values == 0:
            n_zeros_axis = np.zeros(X.shape[not axis], dtype=int)
        else:
            n_zeros_axis = X.shape[axis] - np.diff(X.indptr)
                if strategy == "mean":
            if missing_values != 0:
                n_non_missing = n_zeros_axis
                                mask_missing_values = _get_mask(X.data, missing_values)
                mask_valids = np.logical_not(mask_missing_values)
                                new_data = X.data.copy()
                new_data[mask_missing_values] = 0
                X = sparse.csc_matrix((new_data, X.indices, X.indptr),
                                      copy=False)
                sums = X.sum(axis=0)
                                mask_non_zeros = sparse.csc_matrix(
                    (mask_valids.astype(np.float64),
                     X.indices,
                     X.indptr), copy=False)
                s = mask_non_zeros.sum(axis=0)
                n_non_missing = np.add(n_non_missing, s)
            else:
                sums = X.sum(axis=axis)
                n_non_missing = np.diff(X.indptr)
                                                with np.errstate(all="ignore"):
                return np.ravel(sums) / np.ravel(n_non_missing)
                else:
                        columns_all = np.hsplit(X.data, X.indptr[1:-1])
            mask_missing_values = _get_mask(X.data, missing_values)
            mask_valids = np.hsplit(np.logical_not(mask_missing_values),
                                    X.indptr[1:-1])
                        columns = [col[astype(mask, bool, copy=False)]
                       for col, mask in zip(columns_all, mask_valids)]
                        if strategy == "median":
                median = np.empty(len(columns))
                for i, column in enumerate(columns):
                    median[i] = _get_median(column, n_zeros_axis[i])
                return median
                        elif strategy == "most_frequent":
                most_frequent = np.empty(len(columns))
                for i, column in enumerate(columns):
                    most_frequent[i] = _most_frequent(column,
                                                      0,
                                                      n_zeros_axis[i])
                return most_frequent
    def _dense_fit(self, X, strategy, missing_values, axis):
                X = check_array(X, force_all_finite=False)
        mask = _get_mask(X, missing_values)
        masked_X = ma.masked_array(X, mask=mask)
                if strategy == "mean":
            mean_masked = np.ma.mean(masked_X, axis=axis)
                        mean = np.ma.getdata(mean_masked)
            mean[np.ma.getmask(mean_masked)] = np.nan
            return mean
                elif strategy == "median":
            if tuple(int(v) for v in np.__version__.split('.')[:2]) < (1, 5):
                                                                masked_X.mask = np.logical_or(masked_X.mask,
                                              np.isnan(X))
            median_masked = np.ma.median(masked_X, axis=axis)
                        median = np.ma.getdata(median_masked)
            median[np.ma.getmaskarray(median_masked)] = np.nan
            return median
                elif strategy == "most_frequent":
                                                
                        if axis == 0:
                X = X.transpose()
                mask = mask.transpose()
            most_frequent = np.empty(X.shape[0])
            for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):
                row_mask = np.logical_not(row_mask).astype(np.bool)
                row = row[row_mask]
                most_frequent[i] = _most_frequent(row, np.nan, 0)
            return most_frequent
    def transform(self, X):
                if self.axis == 0:
            check_is_fitted(self, 'statistics_')
            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
                            force_all_finite=False, copy=self.copy)
            statistics = self.statistics_
            if X.shape[1] != statistics.shape[0]:
                raise ValueError("X has %d features per sample, expected %d"
                                 % (X.shape[1], self.statistics_.shape[0]))
                                else:
            X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,
                            force_all_finite=False, copy=self.copy)
            if sparse.issparse(X):
                statistics = self._sparse_fit(X,
                                              self.strategy,
                                              self.missing_values,
                                              self.axis)
            else:
                statistics = self._dense_fit(X,
                                             self.strategy,
                                             self.missing_values,
                                             self.axis)
                invalid_mask = np.isnan(statistics)
        valid_mask = np.logical_not(invalid_mask)
        valid_statistics = statistics[valid_mask]
        valid_statistics_indexes = np.where(valid_mask)[0]
        missing = np.arange(X.shape[not self.axis])[invalid_mask]
        if self.axis == 0 and invalid_mask.any():
            if self.verbose:
                warnings.warn("Deleting features without "
                              "observed values: %s" % missing)
            X = X[:, valid_statistics_indexes]
        elif self.axis == 1 and invalid_mask.any():
            raise ValueError("Some rows only contain "
                             "missing values: %s" % missing)
                if sparse.issparse(X) and self.missing_values != 0:
            mask = _get_mask(X.data, self.missing_values)
            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),
                                np.diff(X.indptr))[mask]
            X.data[mask] = astype(valid_statistics[indexes], X.dtype,
                                  copy=False)
        else:
            if sparse.issparse(X):
                X = X.toarray()
            mask = _get_mask(X, self.missing_values)
            n_missing = np.sum(mask, axis=self.axis)
            values = np.repeat(valid_statistics, n_missing)
            if self.axis == 0:
                coordinates = np.where(mask.transpose())[::-1]
            else:
                coordinates = mask
            X[coordinates] = values
        return X
from collections import defaultdict
import itertools
import array
import numpy as np
import scipy.sparse as sp
from ..base import BaseEstimator, TransformerMixin
from ..utils.fixes import np_version
from ..utils.fixes import sparse_min_max
from ..utils.fixes import astype
from ..utils.fixes import in1d
from ..utils import column_or_1d
from ..utils.validation import check_array
from ..utils.validation import check_is_fitted
from ..utils.validation import _num_samples
from ..utils.multiclass import unique_labels
from ..utils.multiclass import type_of_target
from ..externals import six
zip = six.moves.zip
map = six.moves.map
__all__ = [
    'label_binarize',
    'LabelBinarizer',
    'LabelEncoder',
    'MultiLabelBinarizer',
]

def _check_numpy_unicode_bug(labels):
        if np_version[:3] < (1, 7, 0) and labels.dtype.kind == 'U':
        raise RuntimeError("NumPy < 1.7.0 does not implement searchsorted"
                           " on unicode data correctly. Please upgrade"
                           " NumPy to use LabelEncoder with unicode inputs.")

class LabelEncoder(BaseEstimator, TransformerMixin):
    
    def fit(self, y):
                y = column_or_1d(y, warn=True)
        _check_numpy_unicode_bug(y)
        self.classes_ = np.unique(y)
        return self
    def fit_transform(self, y):
                y = column_or_1d(y, warn=True)
        _check_numpy_unicode_bug(y)
        self.classes_, y = np.unique(y, return_inverse=True)
        return y
    def transform(self, y):
                check_is_fitted(self, 'classes_')
        y = column_or_1d(y, warn=True)
        classes = np.unique(y)
        _check_numpy_unicode_bug(classes)
        if len(np.intersect1d(classes, self.classes_)) < len(classes):
            diff = np.setdiff1d(classes, self.classes_)
            raise ValueError("y contains new labels: %s" % str(diff))
        return np.searchsorted(self.classes_, y)
    def inverse_transform(self, y):
                check_is_fitted(self, 'classes_')
        diff = np.setdiff1d(y, np.arange(len(self.classes_)))
        if diff:
            raise ValueError("y contains new labels: %s" % str(diff))
        y = np.asarray(y)
        return self.classes_[y]

class LabelBinarizer(BaseEstimator, TransformerMixin):
    
    def __init__(self, neg_label=0, pos_label=1, sparse_output=False):
        if neg_label >= pos_label:
            raise ValueError("neg_label={0} must be strictly less than "
                             "pos_label={1}.".format(neg_label, pos_label))
        if sparse_output and (pos_label == 0 or neg_label != 0):
            raise ValueError("Sparse binarization is only supported with non "
                             "zero pos_label and zero neg_label, got "
                             "pos_label={0} and neg_label={1}"
                             "".format(pos_label, neg_label))
        self.neg_label = neg_label
        self.pos_label = pos_label
        self.sparse_output = sparse_output
    def fit(self, y):
                self.y_type_ = type_of_target(y)
        if 'multioutput' in self.y_type_:
            raise ValueError("Multioutput target data is not supported with "
                             "label binarization")
        if _num_samples(y) == 0:
            raise ValueError('y has 0 samples: %r' % y)
        self.sparse_input_ = sp.issparse(y)
        self.classes_ = unique_labels(y)
        return self
    def fit_transform(self, y):
                return self.fit(y).transform(y)
    def transform(self, y):
                check_is_fitted(self, 'classes_')
        y_is_multilabel = type_of_target(y).startswith('multilabel')
        if y_is_multilabel and not self.y_type_.startswith('multilabel'):
            raise ValueError("The object was not fitted with multilabel"
                             " input.")
        return label_binarize(y, self.classes_,
                              pos_label=self.pos_label,
                              neg_label=self.neg_label,
                              sparse_output=self.sparse_output)
    def inverse_transform(self, Y, threshold=None):
                check_is_fitted(self, 'classes_')
        if threshold is None:
            threshold = (self.pos_label + self.neg_label) / 2.
        if self.y_type_ == "multiclass":
            y_inv = _inverse_binarize_multiclass(Y, self.classes_)
        else:
            y_inv = _inverse_binarize_thresholding(Y, self.y_type_,
                                                   self.classes_, threshold)
        if self.sparse_input_:
            y_inv = sp.csr_matrix(y_inv)
        elif sp.issparse(y_inv):
            y_inv = y_inv.toarray()
        return y_inv

def label_binarize(y, classes, neg_label=0, pos_label=1, sparse_output=False):
        if not isinstance(y, list):
                        y = check_array(y, accept_sparse='csr', ensure_2d=False, dtype=None)
    else:
        if _num_samples(y) == 0:
            raise ValueError('y has 0 samples: %r' % y)
    if neg_label >= pos_label:
        raise ValueError("neg_label={0} must be strictly less than "
                         "pos_label={1}.".format(neg_label, pos_label))
    if (sparse_output and (pos_label == 0 or neg_label != 0)):
        raise ValueError("Sparse binarization is only supported with non "
                         "zero pos_label and zero neg_label, got "
                         "pos_label={0} and neg_label={1}"
                         "".format(pos_label, neg_label))
        pos_switch = pos_label == 0
    if pos_switch:
        pos_label = -neg_label
    y_type = type_of_target(y)
    if 'multioutput' in y_type:
        raise ValueError("Multioutput target data is not supported with label "
                         "binarization")
    if y_type == 'unknown':
        raise ValueError("The type of target data is not known")
    n_samples = y.shape[0] if sp.issparse(y) else len(y)
    n_classes = len(classes)
    classes = np.asarray(classes)
    if y_type == "binary":
        if n_classes == 1:
            if sparse_output:
                return sp.csr_matrix((n_samples, 1), dtype=int)
            else:
                Y = np.zeros((len(y), 1), dtype=np.int)
                Y += neg_label
                return Y
        elif len(classes) >= 3:
            y_type = "multiclass"
    sorted_class = np.sort(classes)
    if (y_type == "multilabel-indicator" and classes.size != y.shape[1]):
        raise ValueError("classes {0} missmatch with the labels {1}"
                         "found in the data".format(classes, unique_labels(y)))
    if y_type in ("binary", "multiclass"):
        y = column_or_1d(y)
                y_in_classes = in1d(y, classes)
        y_seen = y[y_in_classes]
        indices = np.searchsorted(sorted_class, y_seen)
        indptr = np.hstack((0, np.cumsum(y_in_classes)))
        data = np.empty_like(indices)
        data.fill(pos_label)
        Y = sp.csr_matrix((data, indices, indptr),
                          shape=(n_samples, n_classes))
    elif y_type == "multilabel-indicator":
        Y = sp.csr_matrix(y)
        if pos_label != 1:
            data = np.empty_like(Y.data)
            data.fill(pos_label)
            Y.data = data
    else:
        raise ValueError("%s target data is not supported with label "
                         "binarization" % y_type)
    if not sparse_output:
        Y = Y.toarray()
        Y = astype(Y, int, copy=False)
        if neg_label != 0:
            Y[Y == 0] = neg_label
        if pos_switch:
            Y[Y == pos_label] = 0
    else:
        Y.data = astype(Y.data, int, copy=False)
        if np.any(classes != sorted_class):
        indices = np.searchsorted(sorted_class, classes)
        Y = Y[:, indices]
    if y_type == "binary":
        if sparse_output:
            Y = Y.getcol(-1)
        else:
            Y = Y[:, -1].reshape((-1, 1))
    return Y

def _inverse_binarize_multiclass(y, classes):
        classes = np.asarray(classes)
    if sp.issparse(y):
        
        y = y.tocsr()
        n_samples, n_outputs = y.shape
        outputs = np.arange(n_outputs)
        row_max = sparse_min_max(y, 1)[1]
        row_nnz = np.diff(y.indptr)
        y_data_repeated_max = np.repeat(row_max, row_nnz)
                y_i_all_argmax = np.flatnonzero(y_data_repeated_max == y.data)
                if row_max[-1] == 0:
            y_i_all_argmax = np.append(y_i_all_argmax, [len(y.data)])
                index_first_argmax = np.searchsorted(y_i_all_argmax, y.indptr[:-1])
                y_ind_ext = np.append(y.indices, [0])
        y_i_argmax = y_ind_ext[y_i_all_argmax[index_first_argmax]]
                y_i_argmax[np.where(row_nnz == 0)[0]] = 0
                samples = np.arange(n_samples)[(row_nnz > 0) &
                                       (row_max.ravel() == 0)]
        for i in samples:
            ind = y.indices[y.indptr[i]:y.indptr[i + 1]]
            y_i_argmax[i] = classes[np.setdiff1d(outputs, ind)][0]
        return classes[y_i_argmax]
    else:
        return classes.take(y.argmax(axis=1), mode="clip")

def _inverse_binarize_thresholding(y, output_type, classes, threshold):
    
    if output_type == "binary" and y.ndim == 2 and y.shape[1] > 2:
        raise ValueError("output_type='binary', but y.shape = {0}".
                         format(y.shape))
    if output_type != "binary" and y.shape[1] != len(classes):
        raise ValueError("The number of class is not equal to the number of "
                         "dimension of y.")
    classes = np.asarray(classes)
        if sp.issparse(y):
        if threshold > 0:
            if y.format not in ('csr', 'csc'):
                y = y.tocsr()
            y.data = np.array(y.data > threshold, dtype=np.int)
            y.eliminate_zeros()
        else:
            y = np.array(y.toarray() > threshold, dtype=np.int)
    else:
        y = np.array(y > threshold, dtype=np.int)
        if output_type == "binary":
        if sp.issparse(y):
            y = y.toarray()
        if y.ndim == 2 and y.shape[1] == 2:
            return classes[y[:, 1]]
        else:
            if len(classes) == 1:
                return np.repeat(classes[0], len(y))
            else:
                return classes[y.ravel()]
    elif output_type == "multilabel-indicator":
        return y
    else:
        raise ValueError("{0} format is not supported".format(output_type))

class MultiLabelBinarizer(BaseEstimator, TransformerMixin):
        def __init__(self, classes=None, sparse_output=False):
        self.classes = classes
        self.sparse_output = sparse_output
    def fit(self, y):
                if self.classes is None:
            classes = sorted(set(itertools.chain.from_iterable(y)))
        else:
            classes = self.classes
        dtype = np.int if all(isinstance(c, int) for c in classes) else object
        self.classes_ = np.empty(len(classes), dtype=dtype)
        self.classes_[:] = classes
        return self
    def fit_transform(self, y):
                if self.classes is not None:
            return self.fit(y).transform(y)
                class_mapping = defaultdict(int)
        class_mapping.default_factory = class_mapping.__len__
        yt = self._transform(y, class_mapping)
                tmp = sorted(class_mapping, key=class_mapping.get)
                dtype = np.int if all(isinstance(c, int) for c in tmp) else object
        class_mapping = np.empty(len(tmp), dtype=dtype)
        class_mapping[:] = tmp
        self.classes_, inverse = np.unique(class_mapping, return_inverse=True)
                yt.indices = np.array(inverse[yt.indices], dtype=yt.indices.dtype,
                              copy=False)
        if not self.sparse_output:
            yt = yt.toarray()
        return yt
    def transform(self, y):
                check_is_fitted(self, 'classes_')
        class_to_index = dict(zip(self.classes_, range(len(self.classes_))))
        yt = self._transform(y, class_to_index)
        if not self.sparse_output:
            yt = yt.toarray()
        return yt
    def _transform(self, y, class_mapping):
                indices = array.array('i')
        indptr = array.array('i', [0])
        for labels in y:
            indices.extend(set(class_mapping[label] for label in labels))
            indptr.append(len(indices))
        data = np.ones(len(indices), dtype=int)
        return sp.csr_matrix((data, indices, indptr),
                             shape=(len(indptr) - 1, len(class_mapping)))
    def inverse_transform(self, yt):
                check_is_fitted(self, 'classes_')
        if yt.shape[1] != len(self.classes_):
            raise ValueError('Expected indicator for {0} classes, but got {1}'
                             .format(len(self.classes_), yt.shape[1]))
        if sp.issparse(yt):
            yt = yt.tocsr()
            if len(yt.data) != 0 and len(np.setdiff1d(yt.data, [0, 1])) > 0:
                raise ValueError('Expected only 0s and 1s in label indicator.')
            return [tuple(self.classes_.take(yt.indices[start:end]))
                    for start, end in zip(yt.indptr[:-1], yt.indptr[1:])]
        else:
            unexpected = np.setdiff1d(yt, [0, 1])
            if len(unexpected) > 0:
                raise ValueError('Expected only 0s and 1s in label indicator. '
                                 'Also got {0}'.format(unexpected))
            return [tuple(self.classes_.compress(indicators)) for indicators
                    in yt]
from ..base import BaseEstimator, TransformerMixin
from ..utils import check_array

def _identity(X):
        return X

class FunctionTransformer(BaseEstimator, TransformerMixin):
        def __init__(self, func=None, inverse_func=None, validate=True,
                 accept_sparse=False, pass_y=False,
                 kw_args=None, inv_kw_args=None):
        self.func = func
        self.inverse_func = inverse_func
        self.validate = validate
        self.accept_sparse = accept_sparse
        self.pass_y = pass_y
        self.kw_args = kw_args
        self.inv_kw_args = inv_kw_args
    def fit(self, X, y=None):
        if self.validate:
            check_array(X, self.accept_sparse)
        return self
    def transform(self, X, y=None):
        return self._transform(X, y, self.func, self.kw_args)
    def inverse_transform(self, X, y=None):
        return self._transform(X, y, self.inverse_func, self.inv_kw_args)
    def _transform(self, X, y=None, func=None, kw_args=None):
        if self.validate:
            X = check_array(X, self.accept_sparse)
        if func is None:
            func = _identity
        return func(X, *((y,) if self.pass_y else ()),
                    **(kw_args if kw_args else {}))
from ._function_transformer import FunctionTransformer
from .data import Binarizer
from .data import KernelCenterer
from .data import MinMaxScaler
from .data import MaxAbsScaler
from .data import Normalizer
from .data import RobustScaler
from .data import StandardScaler
from .data import add_dummy_feature
from .data import binarize
from .data import normalize
from .data import scale
from .data import robust_scale
from .data import maxabs_scale
from .data import minmax_scale
from .data import OneHotEncoder
from .data import PolynomialFeatures
from .label import label_binarize
from .label import LabelBinarizer
from .label import LabelEncoder
from .label import MultiLabelBinarizer
from .imputation import Imputer

__all__ = [
    'Binarizer',
    'FunctionTransformer',
    'Imputer',
    'KernelCenterer',
    'LabelBinarizer',
    'LabelEncoder',
    'MultiLabelBinarizer',
    'MinMaxScaler',
    'MaxAbsScaler',
    'Normalizer',
    'OneHotEncoder',
    'RobustScaler',
    'StandardScaler',
    'add_dummy_feature',
    'PolynomialFeatures',
    'binarize',
    'normalize',
    'scale',
    'robust_scale',
    'maxabs_scale',
    'minmax_scale',
    'label_binarize',
]
from abc import ABCMeta, abstractmethod
import numpy as np
from scipy import sparse
from ..base import BaseEstimator, ClassifierMixin
from ..externals import six
from ..metrics.pairwise import rbf_kernel
from ..neighbors.unsupervised import NearestNeighbors
from ..utils.extmath import safe_sparse_dot
from ..utils.graph import graph_laplacian
from ..utils.multiclass import check_classification_targets
from ..utils.validation import check_X_y, check_is_fitted, check_array

def _not_converged(y_truth, y_prediction, tol=1e-3):
        return np.abs(y_truth - y_prediction).sum() > tol

class BaseLabelPropagation(six.with_metaclass(ABCMeta, BaseEstimator,
                                              ClassifierMixin)):
    
    def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,
                 alpha=1, max_iter=30, tol=1e-3, n_jobs=1):
        self.max_iter = max_iter
        self.tol = tol
                self.kernel = kernel
        self.gamma = gamma
        self.n_neighbors = n_neighbors
                self.alpha = alpha
        self.n_jobs = n_jobs
    def _get_kernel(self, X, y=None):
        if self.kernel == "rbf":
            if y is None:
                return rbf_kernel(X, X, gamma=self.gamma)
            else:
                return rbf_kernel(X, y, gamma=self.gamma)
        elif self.kernel == "knn":
            if self.nn_fit is None:
                self.nn_fit = NearestNeighbors(self.n_neighbors,
                                               n_jobs=self.n_jobs).fit(X)
            if y is None:
                return self.nn_fit.kneighbors_graph(self.nn_fit._fit_X,
                                                    self.n_neighbors,
                                                    mode='connectivity')
            else:
                return self.nn_fit.kneighbors(y, return_distance=False)
        elif callable(self.kernel):
            if y is None:
                return self.kernel(X, X)
            else:
                return self.kernel(X, y)
        else:
            raise ValueError("%s is not a valid kernel. Only rbf and knn"
                             " or an explicit function "
                             " are supported at this time." % self.kernel)
    @abstractmethod
    def _build_graph(self):
        raise NotImplementedError("Graph construction must be implemented"
                                  " to fit a label propagation model.")
    def predict(self, X):
                probas = self.predict_proba(X)
        return self.classes_[np.argmax(probas, axis=1)].ravel()
    def predict_proba(self, X):
                check_is_fitted(self, 'X_')
        X_2d = check_array(X, accept_sparse=['csc', 'csr', 'coo', 'dok',
                                             'bsr', 'lil', 'dia'])
        weight_matrices = self._get_kernel(self.X_, X_2d)
        if self.kernel == 'knn':
            probabilities = []
            for weight_matrix in weight_matrices:
                ine = np.sum(self.label_distributions_[weight_matrix], axis=0)
                probabilities.append(ine)
            probabilities = np.array(probabilities)
        else:
            weight_matrices = weight_matrices.T
            probabilities = np.dot(weight_matrices, self.label_distributions_)
        normalizer = np.atleast_2d(np.sum(probabilities, axis=1)).T
        probabilities /= normalizer
        return probabilities
    def fit(self, X, y):
                X, y = check_X_y(X, y)
        self.X_ = X
        check_classification_targets(y)
                graph_matrix = self._build_graph()
                        classes = np.unique(y)
        classes = (classes[classes != -1])
        self.classes_ = classes
        n_samples, n_classes = len(y), len(classes)
        y = np.asarray(y)
        unlabeled = y == -1
        clamp_weights = np.ones((n_samples, 1))
        clamp_weights[unlabeled, 0] = self.alpha
                self.label_distributions_ = np.zeros((n_samples, n_classes))
        for label in classes:
            self.label_distributions_[y == label, classes == label] = 1
        y_static = np.copy(self.label_distributions_)
        if self.alpha > 0.:
            y_static *= 1 - self.alpha
        y_static[unlabeled] = 0
        l_previous = np.zeros((self.X_.shape[0], n_classes))
        remaining_iter = self.max_iter
        if sparse.isspmatrix(graph_matrix):
            graph_matrix = graph_matrix.tocsr()
        while (_not_converged(self.label_distributions_, l_previous, self.tol)
               and remaining_iter > 1):
            l_previous = self.label_distributions_
            self.label_distributions_ = safe_sparse_dot(
                graph_matrix, self.label_distributions_)
                        self.label_distributions_ = np.multiply(
                clamp_weights, self.label_distributions_) + y_static
            remaining_iter -= 1
        normalizer = np.sum(self.label_distributions_, axis=1)[:, np.newaxis]
        self.label_distributions_ /= normalizer
                transduction = self.classes_[np.argmax(self.label_distributions_,
                                               axis=1)]
        self.transduction_ = transduction.ravel()
        self.n_iter_ = self.max_iter - remaining_iter
        return self

class LabelPropagation(BaseLabelPropagation):
    
    def _build_graph(self):
                if self.kernel == 'knn':
            self.nn_fit = None
        affinity_matrix = self._get_kernel(self.X_)
        normalizer = affinity_matrix.sum(axis=0)
        if sparse.isspmatrix(affinity_matrix):
            affinity_matrix.data /= np.diag(np.array(normalizer))
        else:
            affinity_matrix /= normalizer[:, np.newaxis]
        return affinity_matrix

class LabelSpreading(BaseLabelPropagation):
    
    def __init__(self, kernel='rbf', gamma=20, n_neighbors=7, alpha=0.2,
                 max_iter=30, tol=1e-3, n_jobs=1):
                super(LabelSpreading, self).__init__(kernel=kernel, gamma=gamma,
                                             n_neighbors=n_neighbors,
                                             alpha=alpha, max_iter=max_iter,
                                             tol=tol,
                                             n_jobs=n_jobs)
    def _build_graph(self):
                        if self.kernel == 'knn':
            self.nn_fit = None
        n_samples = self.X_.shape[0]
        affinity_matrix = self._get_kernel(self.X_)
        laplacian = graph_laplacian(affinity_matrix, normed=True)
        laplacian = -laplacian
        if sparse.isspmatrix(laplacian):
            diag_mask = (laplacian.row == laplacian.col)
            laplacian.data[diag_mask] = 0.0
        else:
            laplacian.flat[::n_samples + 1] = 0.0          return laplacian
from .label_propagation import LabelPropagation, LabelSpreading
__all__ = ['LabelPropagation', 'LabelSpreading']
from __future__ import print_function
import numpy as np
import scipy.sparse as sp
import warnings
from abc import ABCMeta, abstractmethod
from . import libsvm, liblinear
from . import libsvm_sparse
from ..base import BaseEstimator, ClassifierMixin
from ..preprocessing import LabelEncoder
from ..utils.multiclass import _ovr_decision_function
from ..utils import check_array, check_consistent_length, check_random_state
from ..utils import column_or_1d, check_X_y
from ..utils import compute_class_weight
from ..utils.extmath import safe_sparse_dot
from ..utils.validation import check_is_fitted
from ..utils.multiclass import check_classification_targets
from ..externals import six
from ..exceptions import ConvergenceWarning
from ..exceptions import NotFittedError

LIBSVM_IMPL = ['c_svc', 'nu_svc', 'one_class', 'epsilon_svr', 'nu_svr']

def _one_vs_one_coef(dual_coef, n_support, support_vectors):
    
                    n_class = dual_coef.shape[0] + 1
            coef = []
    sv_locs = np.cumsum(np.hstack([[0], n_support]))
    for class1 in range(n_class):
                sv1 = support_vectors[sv_locs[class1]:sv_locs[class1 + 1], :]
        for class2 in range(class1 + 1, n_class):
                        sv2 = support_vectors[sv_locs[class2]:sv_locs[class2 + 1], :]
                        alpha1 = dual_coef[class2 - 1, sv_locs[class1]:sv_locs[class1 + 1]]
                        alpha2 = dual_coef[class1, sv_locs[class2]:sv_locs[class2 + 1]]
            
            coef.append(safe_sparse_dot(alpha1, sv1)
                        + safe_sparse_dot(alpha2, sv2))
    return coef

class BaseLibSVM(six.with_metaclass(ABCMeta, BaseEstimator)):
    
                _sparse_kernels = ["linear", "poly", "rbf", "sigmoid", "precomputed"]
    @abstractmethod
    def __init__(self, impl, kernel, degree, gamma, coef0,
                 tol, C, nu, epsilon, shrinking, probability, cache_size,
                 class_weight, verbose, max_iter, random_state):
        if impl not in LIBSVM_IMPL:              raise ValueError("impl should be one of %s, %s was given" % (
                LIBSVM_IMPL, impl))
        if gamma == 0:
            msg = ("The gamma value of 0.0 is invalid. Use 'auto' to set"
                   " gamma to a value of 1 / n_features.")
            raise ValueError(msg)
        self._impl = impl
        self.kernel = kernel
        self.degree = degree
        self.gamma = gamma
        self.coef0 = coef0
        self.tol = tol
        self.C = C
        self.nu = nu
        self.epsilon = epsilon
        self.shrinking = shrinking
        self.probability = probability
        self.cache_size = cache_size
        self.class_weight = class_weight
        self.verbose = verbose
        self.max_iter = max_iter
        self.random_state = random_state
    @property
    def _pairwise(self):
                return self.kernel == "precomputed"
    def fit(self, X, y, sample_weight=None):
        
        rnd = check_random_state(self.random_state)
        sparse = sp.isspmatrix(X)
        if sparse and self.kernel == "precomputed":
            raise TypeError("Sparse precomputed kernels are not supported.")
        self._sparse = sparse and not callable(self.kernel)
        X, y = check_X_y(X, y, dtype=np.float64, order='C', accept_sparse='csr')
        y = self._validate_targets(y)
        sample_weight = np.asarray([]
                                   if sample_weight is None
                                   else sample_weight, dtype=np.float64)
        solver_type = LIBSVM_IMPL.index(self._impl)
                if solver_type != 2 and X.shape[0] != y.shape[0]:
            raise ValueError("X and y have incompatible shapes.\n" +
                             "X has %s samples, but y has %s." %
                             (X.shape[0], y.shape[0]))
        if self.kernel == "precomputed" and X.shape[0] != X.shape[1]:
            raise ValueError("X.shape[0] should be equal to X.shape[1]")
        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:
            raise ValueError("sample_weight and X have incompatible shapes: "
                             "%r vs %r\n"
                             "Note: Sparse matrices cannot be indexed w/"
                             "boolean masks (use `indices=True` in CV)."
                             % (sample_weight.shape, X.shape))
        if self.gamma == 'auto':
            self._gamma = 1.0 / X.shape[1]
        else:
            self._gamma = self.gamma
        kernel = self.kernel
        if callable(kernel):
            kernel = 'precomputed'
        fit = self._sparse_fit if self._sparse else self._dense_fit
        if self.verbose:              print('[LibSVM]', end='')
        seed = rnd.randint(np.iinfo('i').max)
        fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)
        
        self.shape_fit_ = X.shape
                        self._intercept_ = self.intercept_.copy()
        self._dual_coef_ = self.dual_coef_
        if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:
            self.intercept_ *= -1
            self.dual_coef_ = -self.dual_coef_
        return self
    def _validate_targets(self, y):
                                self.class_weight_ = np.empty(0)
        return column_or_1d(y, warn=True).astype(np.float64)
    def _warn_from_fit_status(self):
        assert self.fit_status_ in (0, 1)
        if self.fit_status_ == 1:
            warnings.warn('Solver terminated early (max_iter=%i).'
                          '  Consider pre-processing your data with'
                          ' StandardScaler or MinMaxScaler.'
                          % self.max_iter, ConvergenceWarning)
    def _dense_fit(self, X, y, sample_weight, solver_type, kernel,
                   random_seed):
        if callable(self.kernel):
                                    self.__Xfit = X
            X = self._compute_kernel(X)
            if X.shape[0] != X.shape[1]:
                raise ValueError("X.shape[0] should be equal to X.shape[1]")
        libsvm.set_verbosity_wrap(self.verbose)
        if six.PY2:
                        if isinstance(kernel, six.types.UnicodeType):
                kernel = str(kernel)
        if six.PY3:
                        if isinstance(kernel, bytes):
                kernel = str(kernel, 'utf8')
                        self.support_, self.support_vectors_, self.n_support_, \
            self.dual_coef_, self.intercept_, self.probA_, \
            self.probB_, self.fit_status_ = libsvm.fit(
                X, y,
                svm_type=solver_type, sample_weight=sample_weight,
                class_weight=self.class_weight_, kernel=kernel, C=self.C,
                nu=self.nu, probability=self.probability, degree=self.degree,
                shrinking=self.shrinking, tol=self.tol,
                cache_size=self.cache_size, coef0=self.coef0,
                gamma=self._gamma, epsilon=self.epsilon,
                max_iter=self.max_iter, random_seed=random_seed)
        self._warn_from_fit_status()
    def _sparse_fit(self, X, y, sample_weight, solver_type, kernel,
                    random_seed):
        X.data = np.asarray(X.data, dtype=np.float64, order='C')
        X.sort_indices()
        kernel_type = self._sparse_kernels.index(kernel)
        libsvm_sparse.set_verbosity_wrap(self.verbose)
        self.support_, self.support_vectors_, dual_coef_data, \
            self.intercept_, self.n_support_, \
            self.probA_, self.probB_, self.fit_status_ = \
            libsvm_sparse.libsvm_sparse_train(
                X.shape[1], X.data, X.indices, X.indptr, y, solver_type,
                kernel_type, self.degree, self._gamma, self.coef0, self.tol,
                self.C, self.class_weight_,
                sample_weight, self.nu, self.cache_size, self.epsilon,
                int(self.shrinking), int(self.probability), self.max_iter,
                random_seed)
        self._warn_from_fit_status()
        if hasattr(self, "classes_"):
            n_class = len(self.classes_) - 1
        else:               n_class = 1
        n_SV = self.support_vectors_.shape[0]
        dual_coef_indices = np.tile(np.arange(n_SV), n_class)
        dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,
                                     dual_coef_indices.size / n_class)
        self.dual_coef_ = sp.csr_matrix(
            (dual_coef_data, dual_coef_indices, dual_coef_indptr),
            (n_class, n_SV))
    def predict(self, X):
                X = self._validate_for_predict(X)
        predict = self._sparse_predict if self._sparse else self._dense_predict
        return predict(X)
    def _dense_predict(self, X):
        n_samples, n_features = X.shape
        X = self._compute_kernel(X)
        if X.ndim == 1:
            X = check_array(X, order='C')
        kernel = self.kernel
        if callable(self.kernel):
            kernel = 'precomputed'
            if X.shape[1] != self.shape_fit_[0]:
                raise ValueError("X.shape[1] = %d should be equal to %d, "
                                 "the number of samples at training time" %
                                 (X.shape[1], self.shape_fit_[0]))
        svm_type = LIBSVM_IMPL.index(self._impl)
        return libsvm.predict(
            X, self.support_, self.support_vectors_, self.n_support_,
            self._dual_coef_, self._intercept_,
            self.probA_, self.probB_, svm_type=svm_type, kernel=kernel,
            degree=self.degree, coef0=self.coef0, gamma=self._gamma,
            cache_size=self.cache_size)
    def _sparse_predict(self, X):
                kernel = self.kernel
        if callable(kernel):
            kernel = 'precomputed'
        kernel_type = self._sparse_kernels.index(kernel)
        C = 0.0  
        return libsvm_sparse.libsvm_sparse_predict(
            X.data, X.indices, X.indptr,
            self.support_vectors_.data,
            self.support_vectors_.indices,
            self.support_vectors_.indptr,
            self._dual_coef_.data, self._intercept_,
            LIBSVM_IMPL.index(self._impl), kernel_type,
            self.degree, self._gamma, self.coef0, self.tol,
            C, self.class_weight_,
            self.nu, self.epsilon, self.shrinking,
            self.probability, self.n_support_,
            self.probA_, self.probB_)
    def _compute_kernel(self, X):
                if callable(self.kernel):
                                    kernel = self.kernel(X, self.__Xfit)
            if sp.issparse(kernel):
                kernel = kernel.toarray()
            X = np.asarray(kernel, dtype=np.float64, order='C')
        return X
    def _decision_function(self, X):
                                X = self._validate_for_predict(X)
        X = self._compute_kernel(X)
        if self._sparse:
            dec_func = self._sparse_decision_function(X)
        else:
            dec_func = self._dense_decision_function(X)
                        if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:
            return -dec_func.ravel()
        return dec_func
    def _dense_decision_function(self, X):
        X = check_array(X, dtype=np.float64, order="C")
        kernel = self.kernel
        if callable(kernel):
            kernel = 'precomputed'
        return libsvm.decision_function(
            X, self.support_, self.support_vectors_, self.n_support_,
            self._dual_coef_, self._intercept_,
            self.probA_, self.probB_,
            svm_type=LIBSVM_IMPL.index(self._impl),
            kernel=kernel, degree=self.degree, cache_size=self.cache_size,
            coef0=self.coef0, gamma=self._gamma)
    def _sparse_decision_function(self, X):
        X.data = np.asarray(X.data, dtype=np.float64, order='C')
        kernel = self.kernel
        if hasattr(kernel, '__call__'):
            kernel = 'precomputed'
        kernel_type = self._sparse_kernels.index(kernel)
        return libsvm_sparse.libsvm_sparse_decision_function(
            X.data, X.indices, X.indptr,
            self.support_vectors_.data,
            self.support_vectors_.indices,
            self.support_vectors_.indptr,
            self._dual_coef_.data, self._intercept_,
            LIBSVM_IMPL.index(self._impl), kernel_type,
            self.degree, self._gamma, self.coef0, self.tol,
            self.C, self.class_weight_,
            self.nu, self.epsilon, self.shrinking,
            self.probability, self.n_support_,
            self.probA_, self.probB_)
    def _validate_for_predict(self, X):
        check_is_fitted(self, 'support_')
        X = check_array(X, accept_sparse='csr', dtype=np.float64, order="C")
        if self._sparse and not sp.isspmatrix(X):
            X = sp.csr_matrix(X)
        if self._sparse:
            X.sort_indices()
        if sp.issparse(X) and not self._sparse and not callable(self.kernel):
            raise ValueError(
                "cannot use sparse input in %r trained on dense data"
                % type(self).__name__)
        n_samples, n_features = X.shape
        if self.kernel == "precomputed":
            if X.shape[1] != self.shape_fit_[0]:
                raise ValueError("X.shape[1] = %d should be equal to %d, "
                                 "the number of samples at training time" %
                                 (X.shape[1], self.shape_fit_[0]))
        elif n_features != self.shape_fit_[1]:
            raise ValueError("X.shape[1] = %d should be equal to %d, "
                             "the number of features at training time" %
                             (n_features, self.shape_fit_[1]))
        return X
    @property
    def coef_(self):
        if self.kernel != 'linear':
            raise AttributeError('coef_ is only available when using a '
                                 'linear kernel')
        coef = self._get_coef()
                        if sp.issparse(coef):
                        coef.data.flags.writeable = False
        else:
                        coef.flags.writeable = False
        return coef
    def _get_coef(self):
        return safe_sparse_dot(self._dual_coef_, self.support_vectors_)

class BaseSVC(six.with_metaclass(ABCMeta, BaseLibSVM, ClassifierMixin)):
        @abstractmethod
    def __init__(self, impl, kernel, degree, gamma, coef0, tol, C, nu,
                 shrinking, probability, cache_size, class_weight, verbose,
                 max_iter, decision_function_shape, random_state):
        self.decision_function_shape = decision_function_shape
        super(BaseSVC, self).__init__(
            impl=impl, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0,
            tol=tol, C=C, nu=nu, epsilon=0., shrinking=shrinking,
            probability=probability, cache_size=cache_size,
            class_weight=class_weight, verbose=verbose, max_iter=max_iter,
            random_state=random_state)
    def _validate_targets(self, y):
        y_ = column_or_1d(y, warn=True)
        check_classification_targets(y)
        cls, y = np.unique(y_, return_inverse=True)
        self.class_weight_ = compute_class_weight(self.class_weight, cls, y_)
        if len(cls) < 2:
            raise ValueError(
                "The number of classes has to be greater than one; got %d"
                % len(cls))
        self.classes_ = cls
        return np.asarray(y, dtype=np.float64, order='C')
    def decision_function(self, X):
                dec = self._decision_function(X)
        if self.decision_function_shape == 'ovr' and len(self.classes_) > 2:
            return _ovr_decision_function(dec < 0, -dec, len(self.classes_))
        return dec
    def predict(self, X):
                y = super(BaseSVC, self).predict(X)
        return self.classes_.take(np.asarray(y, dtype=np.intp))
                    def _check_proba(self):
        if not self.probability:
            raise AttributeError("predict_proba is not available when "
                                 " probability=False")
        if self._impl not in ('c_svc', 'nu_svc'):
            raise AttributeError("predict_proba only implemented for SVC"
                                 " and NuSVC")
    @property
    def predict_proba(self):
                self._check_proba()
        return self._predict_proba
    def _predict_proba(self, X):
        X = self._validate_for_predict(X)
        if self.probA_.size == 0 or self.probB_.size == 0:
            raise NotFittedError("predict_proba is not available when fitted "
                                 "with probability=False")
        pred_proba = (self._sparse_predict_proba
                      if self._sparse else self._dense_predict_proba)
        return pred_proba(X)
    @property
    def predict_log_proba(self):
                self._check_proba()
        return self._predict_log_proba
    def _predict_log_proba(self, X):
        return np.log(self.predict_proba(X))
    def _dense_predict_proba(self, X):
        X = self._compute_kernel(X)
        kernel = self.kernel
        if callable(kernel):
            kernel = 'precomputed'
        svm_type = LIBSVM_IMPL.index(self._impl)
        pprob = libsvm.predict_proba(
            X, self.support_, self.support_vectors_, self.n_support_,
            self._dual_coef_, self._intercept_,
            self.probA_, self.probB_,
            svm_type=svm_type, kernel=kernel, degree=self.degree,
            cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma)
        return pprob
    def _sparse_predict_proba(self, X):
        X.data = np.asarray(X.data, dtype=np.float64, order='C')
        kernel = self.kernel
        if callable(kernel):
            kernel = 'precomputed'
        kernel_type = self._sparse_kernels.index(kernel)
        return libsvm_sparse.libsvm_sparse_predict_proba(
            X.data, X.indices, X.indptr,
            self.support_vectors_.data,
            self.support_vectors_.indices,
            self.support_vectors_.indptr,
            self._dual_coef_.data, self._intercept_,
            LIBSVM_IMPL.index(self._impl), kernel_type,
            self.degree, self._gamma, self.coef0, self.tol,
            self.C, self.class_weight_,
            self.nu, self.epsilon, self.shrinking,
            self.probability, self.n_support_,
            self.probA_, self.probB_)
    def _get_coef(self):
        if self.dual_coef_.shape[0] == 1:
                        coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)
        else:
                        coef = _one_vs_one_coef(self.dual_coef_, self.n_support_,
                                    self.support_vectors_)
            if sp.issparse(coef[0]):
                coef = sp.vstack(coef).tocsr()
            else:
                coef = np.vstack(coef)
        return coef

def _get_liblinear_solver_type(multi_class, penalty, loss, dual):
                        _solver_type_dict = {
        'logistic_regression': {
            'l1': {False: 6},
            'l2': {False: 0, True: 7}},
        'hinge': {
            'l2': {True: 3}},
        'squared_hinge': {
            'l1': {False: 5},
            'l2': {False: 2, True: 1}},
        'epsilon_insensitive': {
            'l2': {True: 13}},
        'squared_epsilon_insensitive': {
            'l2': {False: 11, True: 12}},
        'crammer_singer': 4
    }
    if multi_class == 'crammer_singer':
        return _solver_type_dict[multi_class]
    elif multi_class != 'ovr':
        raise ValueError("`multi_class` must be one of `ovr`, "
                         "`crammer_singer`, got %r" % multi_class)
    _solver_pen = _solver_type_dict.get(loss, None)
    if _solver_pen is None:
        error_string = ("loss='%s' is not supported" % loss)
    else:
        _solver_dual = _solver_pen.get(penalty, None)
        if _solver_dual is None:
            error_string = ("The combination of penalty='%s' "
                            "and loss='%s' is not supported"
                            % (penalty, loss))
        else:
            solver_num = _solver_dual.get(dual, None)
            if solver_num is None:
                error_string = ("The combination of penalty='%s' and "
                                "loss='%s' are not supported when dual=%s"
                                % (penalty, loss, dual))
            else:
                return solver_num
    raise ValueError('Unsupported set of arguments: %s, '
                     'Parameters: penalty=%r, loss=%r, dual=%r'
                     % (error_string, penalty, loss, dual))

def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,
                   penalty, dual, verbose, max_iter, tol,
                   random_state=None, multi_class='ovr',
                   loss='logistic_regression', epsilon=0.1,
                   sample_weight=None):
        if loss not in ['epsilon_insensitive', 'squared_epsilon_insensitive']:
        enc = LabelEncoder()
        y_ind = enc.fit_transform(y)
        classes_ = enc.classes_
        if len(classes_) < 2:
            raise ValueError("This solver needs samples of at least 2 classes"
                             " in the data, but the data contains only one"
                             " class: %r" % classes_[0])
        class_weight_ = compute_class_weight(class_weight, classes_, y)
    else:
        class_weight_ = np.empty(0, dtype=np.float64)
        y_ind = y
    liblinear.set_verbosity_wrap(verbose)
    rnd = check_random_state(random_state)
    if verbose:
        print('[LibLinear]', end='')
        bias = -1.0
    if fit_intercept:
        if intercept_scaling <= 0:
            raise ValueError("Intercept scaling is %r but needs to be greater than 0."
                             " To disable fitting an intercept,"
                             " set fit_intercept=False." % intercept_scaling)
        else:
            bias = intercept_scaling
    libsvm.set_verbosity_wrap(verbose)
    libsvm_sparse.set_verbosity_wrap(verbose)
    liblinear.set_verbosity_wrap(verbose)
        y_ind = np.asarray(y_ind, dtype=np.float64).ravel()
    if sample_weight is None:
        sample_weight = np.ones(X.shape[0])
    else:
        sample_weight = np.array(sample_weight, dtype=np.float64, order='C')
        check_consistent_length(sample_weight, X)
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
    raw_coef_, n_iter_ = liblinear.train_wrap(
        X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C,
        class_weight_, max_iter, rnd.randint(np.iinfo('i').max),
        epsilon, sample_weight)
                    n_iter_ = max(n_iter_)
    if n_iter_ >= max_iter and verbose > 0:
        warnings.warn("Liblinear failed to converge, increase "
                      "the number of iterations.", ConvergenceWarning)
    if fit_intercept:
        coef_ = raw_coef_[:, :-1]
        intercept_ = intercept_scaling * raw_coef_[:, -1]
    else:
        coef_ = raw_coef_
        intercept_ = 0.
    return coef_, intercept_, n_iter_
from warnings import warn
import numpy as np
from ..preprocessing import LabelBinarizer
from ..utils.validation import check_consistent_length, check_array
from ..utils.extmath import safe_sparse_dot

def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,
             intercept_scaling=1.0):
        if loss not in ('squared_hinge', 'log'):
        raise ValueError('loss type not in ("squared_hinge", "log", "l2")')
    X = check_array(X, accept_sparse='csc')
    check_consistent_length(X, y)
    Y = LabelBinarizer(neg_label=-1).fit_transform(y).T
        den = np.max(np.abs(safe_sparse_dot(Y, X)))
    if fit_intercept:
        bias = intercept_scaling * np.ones((np.size(y), 1))
        den = max(den, abs(np.dot(Y, bias)).max())
    if den == 0.0:
        raise ValueError('Ill-posed l1_min_c calculation: l1 will always '
                         'select zero coefficients for this data')
    if loss == 'squared_hinge':
        return 0.5 / den
    else:          return 2.0 / den
import warnings
import numpy as np
from .base import _fit_liblinear, BaseSVC, BaseLibSVM
from ..base import BaseEstimator, RegressorMixin
from ..linear_model.base import LinearClassifierMixin, SparseCoefMixin, \
    LinearModel
from ..utils import check_X_y
from ..utils.validation import _num_samples
from ..utils.multiclass import check_classification_targets

class LinearSVC(BaseEstimator, LinearClassifierMixin,
                SparseCoefMixin):
    
    def __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=1e-4,
                 C=1.0, multi_class='ovr', fit_intercept=True,
                 intercept_scaling=1, class_weight=None, verbose=0,
                 random_state=None, max_iter=1000):
        self.dual = dual
        self.tol = tol
        self.C = C
        self.multi_class = multi_class
        self.fit_intercept = fit_intercept
        self.intercept_scaling = intercept_scaling
        self.class_weight = class_weight
        self.verbose = verbose
        self.random_state = random_state
        self.max_iter = max_iter
        self.penalty = penalty
        self.loss = loss
    def fit(self, X, y, sample_weight=None):
                        msg = ("loss='%s' has been deprecated in favor of "
               "loss='%s' as of 0.16. Backward compatibility"
               " for the loss='%s' will be removed in %s")
        if self.loss in ('l1', 'l2'):
            old_loss = self.loss
            self.loss = {'l1': 'hinge', 'l2': 'squared_hinge'}.get(self.loss)
            warnings.warn(msg % (old_loss, self.loss, old_loss, '1.0'),
                          DeprecationWarning)
        
        if self.C < 0:
            raise ValueError("Penalty term must be positive; got (C=%r)"
                             % self.C)
        X, y = check_X_y(X, y, accept_sparse='csr',
                         dtype=np.float64, order="C")
        check_classification_targets(y)
        self.classes_ = np.unique(y)
        self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(
            X, y, self.C, self.fit_intercept, self.intercept_scaling,
            self.class_weight, self.penalty, self.dual, self.verbose,
            self.max_iter, self.tol, self.random_state, self.multi_class,
            self.loss, sample_weight=sample_weight)
        if self.multi_class == "crammer_singer" and len(self.classes_) == 2:
            self.coef_ = (self.coef_[1] - self.coef_[0]).reshape(1, -1)
            if self.fit_intercept:
                intercept = self.intercept_[1] - self.intercept_[0]
                self.intercept_ = np.array([intercept])
        return self

class LinearSVR(LinearModel, RegressorMixin):
    
    def __init__(self, epsilon=0.0, tol=1e-4, C=1.0,
                 loss='epsilon_insensitive', fit_intercept=True,
                 intercept_scaling=1., dual=True, verbose=0,
                 random_state=None, max_iter=1000):
        self.tol = tol
        self.C = C
        self.epsilon = epsilon
        self.fit_intercept = fit_intercept
        self.intercept_scaling = intercept_scaling
        self.verbose = verbose
        self.random_state = random_state
        self.max_iter = max_iter
        self.dual = dual
        self.loss = loss
    def fit(self, X, y, sample_weight=None):
                        msg = ("loss='%s' has been deprecated in favor of "
               "loss='%s' as of 0.16. Backward compatibility"
               " for the loss='%s' will be removed in %s")
        if self.loss in ('l1', 'l2'):
            old_loss = self.loss
            self.loss = {'l1': 'epsilon_insensitive',
                         'l2': 'squared_epsilon_insensitive'
                         }.get(self.loss)
            warnings.warn(msg % (old_loss, self.loss, old_loss, '1.0'),
                          DeprecationWarning)
        
        if self.C < 0:
            raise ValueError("Penalty term must be positive; got (C=%r)"
                             % self.C)
        X, y = check_X_y(X, y, accept_sparse='csr',
                         dtype=np.float64, order="C")
        penalty = 'l2'          self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(
            X, y, self.C, self.fit_intercept, self.intercept_scaling,
            None, penalty, self.dual, self.verbose,
            self.max_iter, self.tol, self.random_state, loss=self.loss,
            epsilon=self.epsilon, sample_weight=sample_weight)
        self.coef_ = self.coef_.ravel()
        return self

class SVC(BaseSVC):
    
    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto',
                 coef0=0.0, shrinking=True, probability=False,
                 tol=1e-3, cache_size=200, class_weight=None,
                 verbose=False, max_iter=-1, decision_function_shape='ovr',
                 random_state=None):
        super(SVC, self).__init__(
            impl='c_svc', kernel=kernel, degree=degree, gamma=gamma,
            coef0=coef0, tol=tol, C=C, nu=0., shrinking=shrinking,
            probability=probability, cache_size=cache_size,
            class_weight=class_weight, verbose=verbose, max_iter=max_iter,
            decision_function_shape=decision_function_shape,
            random_state=random_state)

class NuSVC(BaseSVC):
    
    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0,
                 shrinking=True, probability=False, tol=1e-3, cache_size=200,
                 class_weight=None, verbose=False, max_iter=-1,
                 decision_function_shape='ovr', random_state=None):
        super(NuSVC, self).__init__(
            impl='nu_svc', kernel=kernel, degree=degree, gamma=gamma,
            coef0=coef0, tol=tol, C=0., nu=nu, shrinking=shrinking,
            probability=probability, cache_size=cache_size,
            class_weight=class_weight, verbose=verbose, max_iter=max_iter,
            decision_function_shape=decision_function_shape,
            random_state=random_state)

class SVR(BaseLibSVM, RegressorMixin):
        def __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0,
                 tol=1e-3, C=1.0, epsilon=0.1, shrinking=True,
                 cache_size=200, verbose=False, max_iter=-1):
        super(SVR, self).__init__(
            'epsilon_svr', kernel=kernel, degree=degree, gamma=gamma,
            coef0=coef0, tol=tol, C=C, nu=0., epsilon=epsilon, verbose=verbose,
            shrinking=shrinking, probability=False, cache_size=cache_size,
            class_weight=None, max_iter=max_iter, random_state=None)

class NuSVR(BaseLibSVM, RegressorMixin):
    
    def __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3,
                 gamma='auto', coef0=0.0, shrinking=True, tol=1e-3,
                 cache_size=200, verbose=False, max_iter=-1):
        super(NuSVR, self).__init__(
            'nu_svr', kernel=kernel, degree=degree, gamma=gamma, coef0=coef0,
            tol=tol, C=C, nu=nu, epsilon=0., shrinking=shrinking,
            probability=False, cache_size=cache_size, class_weight=None,
            verbose=verbose, max_iter=max_iter, random_state=None)

class OneClassSVM(BaseLibSVM):
        def __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0,
                 tol=1e-3, nu=0.5, shrinking=True, cache_size=200,
                 verbose=False, max_iter=-1, random_state=None):
        super(OneClassSVM, self).__init__(
            'one_class', kernel, degree, gamma, coef0, tol, 0., nu, 0.,
            shrinking, False, cache_size, None, verbose, max_iter,
            random_state)
    def fit(self, X, y=None, sample_weight=None, **params):
                super(OneClassSVM, self).fit(X, np.ones(_num_samples(X)),
                                     sample_weight=sample_weight, **params)
        return self
    def decision_function(self, X):
                dec = self._decision_function(X)
        return dec
import  numpy as np
cimport numpy as np
cimport liblinear
np.import_array()

def train_wrap(X, np.ndarray[np.float64_t, ndim=1, mode='c'] Y,
               bint is_sparse, int solver_type, double eps, double bias,
               double C, np.ndarray[np.float64_t, ndim=1] class_weight,
               int max_iter, unsigned random_seed, double epsilon,
               np.ndarray[np.float64_t, ndim=1, mode='c'] sample_weight):
    cdef parameter *param
    cdef problem *problem
    cdef model *model
    cdef char_const_ptr error_msg
    cdef int len_w
    if is_sparse:
        problem = csr_set_problem(
                (<np.ndarray[np.float64_t, ndim=1, mode='c']>X.data).data,
                (<np.ndarray[np.int32_t,   ndim=1, mode='c']>X.indices).shape,
                (<np.ndarray[np.int32_t,   ndim=1, mode='c']>X.indices).data,
                (<np.ndarray[np.int32_t,   ndim=1, mode='c']>X.indptr).shape,
                (<np.ndarray[np.int32_t,   ndim=1, mode='c']>X.indptr).data,
                Y.data, (<np.int32_t>X.shape[1]), bias,
                sample_weight.data)
    else:
        problem = set_problem(
                (<np.ndarray[np.float64_t, ndim=2, mode='c']>X).data,
                Y.data,
                (<np.ndarray[np.float64_t, ndim=2, mode='c']>X).shape,
                bias, sample_weight.data)
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.intc)
    param = set_parameter(solver_type, eps, C, class_weight.shape[0],
                          class_weight_label.data, class_weight.data,
                          max_iter, random_seed, epsilon)
    error_msg = check_parameter(problem, param)
    if error_msg:
        free_problem(problem)
        free_parameter(param)
        raise ValueError(error_msg)
        with nogil:
        model = train(problem, param)
        cdef np.ndarray[np.float64_t, ndim=2, mode='fortran'] w
    cdef int nr_class = get_nr_class(model)
    cdef int labels_ = nr_class
    if nr_class == 2:
        labels_ = 1
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] n_iter = np.zeros(labels_, dtype=np.intc)
    get_n_iter(model, <int *>n_iter.data)
    cdef int nr_feature = get_nr_feature(model)
    if bias > 0: nr_feature = nr_feature + 1
    if nr_class == 2 and solver_type != 4:          w = np.empty((1, nr_feature),order='F')
        copy_w(w.data, model, nr_feature)
    else:
        len_w = (nr_class) * nr_feature
        w = np.empty((nr_class, nr_feature),order='F')
        copy_w(w.data, model, len_w)
        free_and_destroy_model(&model)
    free_problem(problem)
    free_parameter(param)
    
    return w, n_iter

def set_verbosity_wrap(int verbosity):
        set_verbosity(verbosity)
import warnings
import  numpy as np
cimport numpy as np
cimport libsvm
from libc.stdlib cimport free
cdef extern from *:
    ctypedef struct svm_parameter:
        pass
np.import_array()

LIBSVM_KERNEL_TYPES = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']

def fit(
    np.ndarray[np.float64_t, ndim=2, mode='c'] X,
    np.ndarray[np.float64_t, ndim=1, mode='c'] Y,
    int svm_type=0, str kernel='rbf', int degree=3,
    double gamma=0.1, double coef0=0., double tol=1e-3,
    double C=1., double nu=0.5, double epsilon=0.1,
    np.ndarray[np.float64_t, ndim=1, mode='c']
        class_weight=np.empty(0),
    np.ndarray[np.float64_t, ndim=1, mode='c']
        sample_weight=np.empty(0),
    int shrinking=1, int probability=0,
    double cache_size=100.,
    int max_iter=-1,
    int random_seed=0):
    
    cdef svm_parameter param
    cdef svm_problem problem
    cdef svm_model *model
    cdef const char *error_msg
    cdef np.npy_intp SV_len
    cdef np.npy_intp nr

    if len(sample_weight) == 0:
        sample_weight = np.ones(X.shape[0], dtype=np.float64)
    else:
        assert sample_weight.shape[0] == X.shape[0], \
               "sample_weight and X have incompatible shapes: " + \
               "sample_weight has %s samples while X has %s" % \
               (sample_weight.shape[0], X.shape[0])
    kernel_index = LIBSVM_KERNEL_TYPES.index(kernel)
    set_problem(
        &problem, X.data, Y.data, sample_weight.data, X.shape, kernel_index)
    if problem.x == NULL:
        raise MemoryError("Seems we've run out of memory")
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
    set_parameter(
        &param, svm_type, kernel_index, degree, gamma, coef0, nu, cache_size,
        C, tol, epsilon, shrinking, probability, <int> class_weight.shape[0],
        class_weight_label.data, class_weight.data, max_iter, random_seed)
    error_msg = svm_check_parameter(&problem, &param)
    if error_msg:
                error_repl = error_msg.decode('utf-8').replace("p < 0", "epsilon < 0")
        raise ValueError(error_repl)
        cdef int fit_status = 0
    with nogil:
        model = svm_train(&problem, &param, &fit_status)
            SV_len  = get_l(model)
    n_class = get_nr(model)
    cdef np.ndarray[np.float64_t, ndim=2, mode='c'] sv_coef
    sv_coef = np.empty((n_class-1, SV_len), dtype=np.float64)
    copy_sv_coef (sv_coef.data, model)
        cdef np.ndarray[np.float64_t, ndim=1, mode='c'] intercept
    intercept = np.empty(int((n_class*(n_class-1))/2), dtype=np.float64)
    copy_intercept (intercept.data, model, intercept.shape)
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] support
    support = np.empty (SV_len, dtype=np.int32)
    copy_support (support.data, model)
        cdef np.ndarray[np.float64_t, ndim=2, mode='c'] support_vectors
    if kernel_index == 4:
                support_vectors = np.empty((0, 0), dtype=np.float64)
    else:
        support_vectors = np.empty((SV_len, X.shape[1]), dtype=np.float64)
        copy_SV(support_vectors.data, model, support_vectors.shape)
        cdef np.ndarray[np.int32_t, ndim=1, mode='c'] n_class_SV
    n_class_SV = np.empty(n_class, dtype=np.int32)
    copy_nSV(n_class_SV.data, model)
    cdef np.ndarray[np.float64_t, ndim=1, mode='c'] probA
    cdef np.ndarray[np.float64_t, ndim=1, mode='c'] probB
    if probability != 0:
        if svm_type < 2:             probA = np.empty(int(n_class*(n_class-1)/2), dtype=np.float64)
            probB = np.empty(int(n_class*(n_class-1)/2), dtype=np.float64)
            copy_probB(probB.data, model, probB.shape)
        else:
            probA = np.empty(1, dtype=np.float64)
            probB = np.empty(0, dtype=np.float64)
        copy_probA(probA.data, model, probA.shape)
    else:
        probA = np.empty(0, dtype=np.float64)
        probB = np.empty(0, dtype=np.float64)
    svm_free_and_destroy_model(&model)
    free(problem.x)
    return (support, support_vectors, n_class_SV, sv_coef, intercept,
           probA, probB, fit_status)

cdef void set_predict_params(
    svm_parameter *param, int svm_type, kernel, int degree, double gamma,
    double coef0, double cache_size, int probability, int nr_weight,
    char *weight_label, char *weight) except *:
    
        cdef double C = .0
    cdef double epsilon = .1
    cdef int max_iter = 0
    cdef double nu = .5
    cdef int shrinking = 0
    cdef double tol = .1
    cdef int random_seed = -1
    kernel_index = LIBSVM_KERNEL_TYPES.index(kernel)
    set_parameter(param, svm_type, kernel_index, degree, gamma, coef0, nu,
                         cache_size, C, tol, epsilon, shrinking, probability,
                         nr_weight, weight_label, weight, max_iter, random_seed)

def predict(np.ndarray[np.float64_t, ndim=2, mode='c'] X,
            np.ndarray[np.int32_t, ndim=1, mode='c'] support,
            np.ndarray[np.float64_t, ndim=2, mode='c'] SV,
            np.ndarray[np.int32_t, ndim=1, mode='c'] nSV,
            np.ndarray[np.float64_t, ndim=2, mode='c'] sv_coef,
            np.ndarray[np.float64_t, ndim=1, mode='c'] intercept,
            np.ndarray[np.float64_t, ndim=1, mode='c'] probA=np.empty(0),
            np.ndarray[np.float64_t, ndim=1, mode='c'] probB=np.empty(0),
            int svm_type=0, kernel='rbf', int degree=3,
            double gamma=0.1, double coef0=0.,
            np.ndarray[np.float64_t, ndim=1, mode='c']
                class_weight=np.empty(0),
            np.ndarray[np.float64_t, ndim=1, mode='c']
                sample_weight=np.empty(0),
            double cache_size=100.):
        cdef np.ndarray[np.float64_t, ndim=1, mode='c'] dec_values
    cdef svm_parameter param
    cdef svm_model *model
    cdef int rv
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
    set_predict_params(&param, svm_type, kernel, degree, gamma, coef0,
                       cache_size, 0, <int>class_weight.shape[0],
                       class_weight_label.data, class_weight.data)
    model = set_model(&param, <int> nSV.shape[0], SV.data, SV.shape,
                      support.data, support.shape, sv_coef.strides,
                      sv_coef.data, intercept.data, nSV.data, probA.data, probB.data)
        try:
        dec_values = np.empty(X.shape[0])
        with nogil:
            rv = copy_predict(X.data, model, X.shape, dec_values.data)
        if rv < 0:
            raise MemoryError("We've run out of memory")
    finally:
        free_model(model)
    return dec_values

def predict_proba(
    np.ndarray[np.float64_t, ndim=2, mode='c'] X,
    np.ndarray[np.int32_t, ndim=1, mode='c'] support,
    np.ndarray[np.float64_t, ndim=2, mode='c'] SV,
    np.ndarray[np.int32_t, ndim=1, mode='c'] nSV,
    np.ndarray[np.float64_t, ndim=2, mode='c'] sv_coef,
    np.ndarray[np.float64_t, ndim=1, mode='c'] intercept,
    np.ndarray[np.float64_t, ndim=1, mode='c'] probA=np.empty(0),
    np.ndarray[np.float64_t, ndim=1, mode='c'] probB=np.empty(0),
    int svm_type=0, str kernel='rbf', int degree=3,
    double gamma=0.1, double coef0=0.,
    np.ndarray[np.float64_t, ndim=1, mode='c']
        class_weight=np.empty(0),
    np.ndarray[np.float64_t, ndim=1, mode='c']
        sample_weight=np.empty(0),
    double cache_size=100.):
        cdef np.ndarray[np.float64_t, ndim=2, mode='c'] dec_values
    cdef svm_parameter param
    cdef svm_model *model
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
    cdef int rv
    set_predict_params(&param, svm_type, kernel, degree, gamma, coef0,
                       cache_size, 1, <int>class_weight.shape[0],
                       class_weight_label.data, class_weight.data)
    model = set_model(&param, <int> nSV.shape[0], SV.data, SV.shape,
                      support.data, support.shape, sv_coef.strides,
                      sv_coef.data, intercept.data, nSV.data,
                      probA.data, probB.data)
    cdef np.npy_intp n_class = get_nr(model)
    try:
        dec_values = np.empty((X.shape[0], n_class), dtype=np.float64)
        with nogil:
            rv = copy_predict_proba(X.data, model, X.shape, dec_values.data)
        if rv < 0:
            raise MemoryError("We've run out of memory")
    finally:
        free_model(model)
    return dec_values

def decision_function(
    np.ndarray[np.float64_t, ndim=2, mode='c'] X,
    np.ndarray[np.int32_t, ndim=1, mode='c'] support,
    np.ndarray[np.float64_t, ndim=2, mode='c'] SV,
    np.ndarray[np.int32_t, ndim=1, mode='c'] nSV,
    np.ndarray[np.float64_t, ndim=2, mode='c'] sv_coef,
    np.ndarray[np.float64_t, ndim=1, mode='c'] intercept,
    np.ndarray[np.float64_t, ndim=1, mode='c'] probA=np.empty(0),
    np.ndarray[np.float64_t, ndim=1, mode='c'] probB=np.empty(0),
    int svm_type=0, kernel='rbf', int degree=3,
    double gamma=0.1, double coef0=0.,
    np.ndarray[np.float64_t, ndim=1, mode='c']
        class_weight=np.empty(0),
    np.ndarray[np.float64_t, ndim=1, mode='c']
         sample_weight=np.empty(0),
    double cache_size=100.):
        cdef np.ndarray[np.float64_t, ndim=2, mode='c'] dec_values
    cdef svm_parameter param
    cdef svm_model *model
    cdef np.npy_intp n_class
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
    cdef int rv
    set_predict_params(&param, svm_type, kernel, degree, gamma, coef0,
                       cache_size, 0, <int>class_weight.shape[0],
                       class_weight_label.data, class_weight.data)
    model = set_model(&param, <int> nSV.shape[0], SV.data, SV.shape,
                      support.data, support.shape, sv_coef.strides,
                      sv_coef.data, intercept.data, nSV.data,
                      probA.data, probB.data)
    if svm_type > 1:
        n_class = 1
    else:
        n_class = get_nr(model)
        n_class = n_class * (n_class - 1) / 2
    try:
        dec_values = np.empty((X.shape[0], n_class), dtype=np.float64)
        with nogil:
            rv = copy_predict_values(X.data, model, X.shape, dec_values.data, n_class)
        if rv < 0:
            raise MemoryError("We've run out of memory")
    finally:
        free_model(model)
    return dec_values

def cross_validation(
    np.ndarray[np.float64_t, ndim=2, mode='c'] X,
    np.ndarray[np.float64_t, ndim=1, mode='c'] Y,
    int n_fold, svm_type=0, str kernel='rbf', int degree=3,
    double gamma=0.1, double coef0=0., double tol=1e-3,
    double C=1., double nu=0.5, double epsilon=0.1,
    np.ndarray[np.float64_t, ndim=1, mode='c']
        class_weight=np.empty(0),
    np.ndarray[np.float64_t, ndim=1, mode='c']
        sample_weight=np.empty(0),
    int shrinking=0, int probability=0, double cache_size=100.,
    int max_iter=-1,
    int random_seed=0):
    
    cdef svm_parameter param
    cdef svm_problem problem
    cdef svm_model *model
    cdef const char *error_msg
    cdef np.npy_intp SV_len
    cdef np.npy_intp nr
    if len(sample_weight) == 0:
        sample_weight = np.ones(X.shape[0], dtype=np.float64)
    else:
        assert sample_weight.shape[0] == X.shape[0], \
               "sample_weight and X have incompatible shapes: " + \
               "sample_weight has %s samples while X has %s" % \
               (sample_weight.shape[0], X.shape[0])
    if X.shape[0] < n_fold:
        raise ValueError("Number of samples is less than number of folds")
        kernel_index = LIBSVM_KERNEL_TYPES.index(kernel)
    set_problem(
        &problem, X.data, Y.data, sample_weight.data, X.shape, kernel_index)
    if problem.x == NULL:
        raise MemoryError("Seems we've run out of memory")
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
        set_parameter(
        &param, svm_type, kernel_index, degree, gamma, coef0, nu, cache_size,
        C, tol, tol, shrinking, probability, <int>
        class_weight.shape[0], class_weight_label.data,
        class_weight.data, max_iter, random_seed)
    error_msg = svm_check_parameter(&problem, &param);
    if error_msg:
        raise ValueError(error_msg)
    cdef np.ndarray[np.float64_t, ndim=1, mode='c'] target
    try:
        target = np.empty((X.shape[0]), dtype=np.float64)
        with nogil:
            svm_cross_validation(&problem, &param, n_fold, <double *> target.data)
    finally:
        free(problem.x)
    return target

def set_verbosity_wrap(int verbosity):
        set_verbosity(verbosity)
import warnings
import  numpy as np
cimport numpy as np
from scipy import sparse
from ..exceptions import ConvergenceWarning
cdef extern from *:
    ctypedef char* const_char_p "const char*"

cdef extern from "svm.h":
    cdef struct svm_csr_node
    cdef struct svm_csr_model
    cdef struct svm_parameter
    cdef struct svm_csr_problem
    char *svm_csr_check_parameter(svm_csr_problem *, svm_parameter *)
    svm_csr_model *svm_csr_train(svm_csr_problem *, svm_parameter *, int *) nogil
    void svm_csr_free_and_destroy_model(svm_csr_model** model_ptr_ptr)
cdef extern from "libsvm_sparse_helper.c":
        svm_csr_problem * csr_set_problem (char *, np.npy_intp *,
         char *, np.npy_intp *, char *, char *, char *, int )
    svm_csr_model *csr_set_model(svm_parameter *param, int nr_class,
                            char *SV_data, np.npy_intp *SV_indices_dims,
                            char *SV_indices, np.npy_intp *SV_intptr_dims,
                            char *SV_intptr,
                            char *sv_coef, char *rho, char *nSV,
                            char *probA, char *probB)
    svm_parameter *set_parameter (int , int , int , double, double ,
                                  double , double , double , double,
                                  double, int, int, int, char *, char *, int,
                                  int)
    void copy_sv_coef   (char *, svm_csr_model *)
    void copy_support   (char *, svm_csr_model *)
    void copy_intercept (char *, svm_csr_model *, np.npy_intp *)
    int copy_predict (char *, svm_csr_model *, np.npy_intp *, char *)
    int csr_copy_predict_values (np.npy_intp *data_size, char *data, np.npy_intp *index_size,
        	char *index, np.npy_intp *intptr_size, char *size,
                svm_csr_model *model, char *dec_values, int nr_class)
    int csr_copy_predict (np.npy_intp *data_size, char *data, np.npy_intp *index_size,
        	char *index, np.npy_intp *intptr_size, char *size,
                svm_csr_model *model, char *dec_values) nogil
    int csr_copy_predict_proba (np.npy_intp *data_size, char *data, np.npy_intp *index_size,
        	char *index, np.npy_intp *intptr_size, char *size,
                svm_csr_model *model, char *dec_values) nogil
    int  copy_predict_values(char *, svm_csr_model *, np.npy_intp *, char *, int)
    int  csr_copy_SV (char *values, np.npy_intp *n_indices,
        	char *indices, np.npy_intp *n_indptr, char *indptr,
                svm_csr_model *model, int n_features)
    np.npy_intp get_nonzero_SV ( svm_csr_model *)
    void copy_nSV     (char *, svm_csr_model *)
    void copy_probA   (char *, svm_csr_model *, np.npy_intp *)
    void copy_probB   (char *, svm_csr_model *, np.npy_intp *)
    np.npy_intp  get_l  (svm_csr_model *)
    np.npy_intp  get_nr (svm_csr_model *)
    int  free_problem   (svm_csr_problem *)
    int  free_model     (svm_csr_model *)
    int  free_param     (svm_parameter *)
    int free_model_SV(svm_csr_model *model)
    void set_verbosity(int)

np.import_array()

def libsvm_sparse_train ( int n_features,
                     np.ndarray[np.float64_t, ndim=1, mode='c'] values,
                     np.ndarray[np.int32_t,   ndim=1, mode='c'] indices,
                     np.ndarray[np.int32_t,   ndim=1, mode='c'] indptr,
                     np.ndarray[np.float64_t, ndim=1, mode='c'] Y,
                     int svm_type, int kernel_type, int degree, double gamma,
                     double coef0, double eps, double C,
                     np.ndarray[np.float64_t, ndim=1, mode='c'] class_weight,
                     np.ndarray[np.float64_t, ndim=1, mode='c'] sample_weight,
                     double nu, double cache_size, double p, int
                     shrinking, int probability, int max_iter,
                     int random_seed):
    
    cdef svm_parameter *param
    cdef svm_csr_problem *problem
    cdef svm_csr_model *model
    cdef const_char_p error_msg
    if len(sample_weight) == 0:
        sample_weight = np.ones(Y.shape[0], dtype=np.float64)
    else:
        assert sample_weight.shape[0] == indptr.shape[0] - 1, \
               "sample_weight and X have incompatible shapes: " + \
               "sample_weight has %s samples while X has %s" % \
               (sample_weight.shape[0], indptr.shape[0] - 1)
            assert(kernel_type != 4)
        problem = csr_set_problem(values.data, indices.shape, indices.data,
                              indptr.shape, indptr.data, Y.data,
                              sample_weight.data, kernel_type)
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
        param = set_parameter(svm_type, kernel_type, degree, gamma, coef0,
                          nu, cache_size, C, eps, p, shrinking,
                          probability, <int> class_weight.shape[0],
                          class_weight_label.data, class_weight.data, max_iter,
                          random_seed)
        if (param == NULL or problem == NULL):
        raise MemoryError("Seems we've run out of memory")
    error_msg = svm_csr_check_parameter(problem, param);
    if error_msg:
        free_problem(problem)
        free_param(param)
        raise ValueError(error_msg)
        cdef int fit_status = 0
    with nogil:
        model = svm_csr_train(problem, param, &fit_status)
    cdef np.npy_intp SV_len = get_l(model)
    cdef np.npy_intp n_class = get_nr(model)
                cdef np.ndarray sv_coef_data
    sv_coef_data = np.empty((n_class-1)*SV_len, dtype=np.float64)
    copy_sv_coef (sv_coef_data.data, model)
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] support
    support = np.empty(SV_len, dtype=np.int32)
    copy_support(support.data, model)
            cdef np.ndarray intercept
    intercept = np.empty(n_class*(n_class-1)/2, dtype=np.float64)
    copy_intercept (intercept.data, model, intercept.shape)
                cdef np.npy_intp nonzero_SV
    nonzero_SV = get_nonzero_SV (model)
    cdef np.ndarray SV_data, SV_indices, SV_indptr
    SV_data = np.empty(nonzero_SV, dtype=np.float64)
    SV_indices = np.empty(nonzero_SV, dtype=np.int32)
    SV_indptr = np.empty(<np.npy_intp>SV_len + 1, dtype=np.int32)
    csr_copy_SV(SV_data.data, SV_indices.shape, SV_indices.data,
                SV_indptr.shape, SV_indptr.data, model, n_features)
    support_vectors_ = sparse.csr_matrix(
	(SV_data, SV_indices, SV_indptr), (SV_len, n_features))
            cdef np.ndarray n_class_SV 
    n_class_SV = np.empty(n_class, dtype=np.int32)
    copy_nSV(n_class_SV.data, model)
        cdef np.ndarray probA, probB
    if probability != 0:
        if svm_type < 2:             probA = np.empty(n_class*(n_class-1)/2, dtype=np.float64)
            probB = np.empty(n_class*(n_class-1)/2, dtype=np.float64)
            copy_probB(probB.data, model, probB.shape)
        else:
            probA = np.empty(1, dtype=np.float64)
            probB = np.empty(0, dtype=np.float64)
        copy_probA(probA.data, model, probA.shape)
    else:
        probA = np.empty(0, dtype=np.float64)
        probB = np.empty(0, dtype=np.float64)
    svm_csr_free_and_destroy_model (&model)
    free_problem(problem)
    free_param(param)
    return (support, support_vectors_, sv_coef_data, intercept, n_class_SV,
            probA, probB, fit_status)

def libsvm_sparse_predict (np.ndarray[np.float64_t, ndim=1, mode='c'] T_data,
                            np.ndarray[np.int32_t,   ndim=1, mode='c'] T_indices,
                            np.ndarray[np.int32_t,   ndim=1, mode='c'] T_indptr,
                            np.ndarray[np.float64_t, ndim=1, mode='c'] SV_data,
                            np.ndarray[np.int32_t,   ndim=1, mode='c'] SV_indices,
                            np.ndarray[np.int32_t,   ndim=1, mode='c'] SV_indptr,
                            np.ndarray[np.float64_t, ndim=1, mode='c'] sv_coef,
                            np.ndarray[np.float64_t, ndim=1, mode='c']
                            intercept, int svm_type, int kernel_type, int
                            degree, double gamma, double coef0, double
                            eps, double C,
                            np.ndarray[np.float64_t, ndim=1] class_weight,
                            double nu, double p, int
                            shrinking, int probability,
                            np.ndarray[np.int32_t, ndim=1, mode='c'] nSV,
                            np.ndarray[np.float64_t, ndim=1, mode='c'] probA,
                            np.ndarray[np.float64_t, ndim=1, mode='c'] probB):
        cdef np.ndarray[np.float64_t, ndim=1, mode='c'] dec_values
    cdef svm_parameter *param
    cdef svm_csr_model *model
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
    cdef int rv
    param = set_parameter(svm_type, kernel_type, degree, gamma,
                          coef0, nu,
                          100.,                           C, eps, p, shrinking,
                          probability, <int> class_weight.shape[0], class_weight_label.data,
                          class_weight.data, -1,
                          -1) 
    model = csr_set_model(param, <int> nSV.shape[0], SV_data.data,
                          SV_indices.shape, SV_indices.data,
                          SV_indptr.shape, SV_indptr.data,
                          sv_coef.data, intercept.data,
                          nSV.data, probA.data, probB.data)
        dec_values = np.empty(T_indptr.shape[0]-1)
    with nogil:
        rv = csr_copy_predict(T_data.shape, T_data.data,
                              T_indices.shape, T_indices.data,
                              T_indptr.shape, T_indptr.data,
                              model, dec_values.data)
    if rv < 0:
        raise MemoryError("We've run out of memory")
        free_model_SV(model)
    free_model(model)
    free_param(param)
    return dec_values

def libsvm_sparse_predict_proba(
    np.ndarray[np.float64_t, ndim=1, mode='c'] T_data,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] T_indices,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] T_indptr,
    np.ndarray[np.float64_t, ndim=1, mode='c'] SV_data,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] SV_indices,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] SV_indptr,
    np.ndarray[np.float64_t, ndim=1, mode='c'] sv_coef,
    np.ndarray[np.float64_t, ndim=1, mode='c']
    intercept, int svm_type, int kernel_type, int
    degree, double gamma, double coef0, double
    eps, double C,
    np.ndarray[np.float64_t, ndim=1] class_weight,
    double nu, double p, int shrinking, int probability,
    np.ndarray[np.int32_t, ndim=1, mode='c'] nSV,
    np.ndarray[np.float64_t, ndim=1, mode='c'] probA,
    np.ndarray[np.float64_t, ndim=1, mode='c'] probB):
        cdef np.ndarray[np.float64_t, ndim=2, mode='c'] dec_values
    cdef svm_parameter *param
    cdef svm_csr_model *model
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
    param = set_parameter(svm_type, kernel_type, degree, gamma,
                          coef0, nu,
                          100.,                           C, eps, p, shrinking,
                          probability, <int> class_weight.shape[0], class_weight_label.data,
                          class_weight.data, -1,
                          -1) 
    model = csr_set_model(param, <int> nSV.shape[0], SV_data.data,
                          SV_indices.shape, SV_indices.data,
                          SV_indptr.shape, SV_indptr.data,
                          sv_coef.data, intercept.data,
                          nSV.data, probA.data, probB.data)
        cdef np.npy_intp n_class = get_nr(model)
    cdef int rv
    dec_values = np.empty((T_indptr.shape[0]-1, n_class), dtype=np.float64)
    with nogil:
        rv = csr_copy_predict_proba(T_data.shape, T_data.data,
                                    T_indices.shape, T_indices.data,
                                    T_indptr.shape, T_indptr.data,
                                    model, dec_values.data)
    if rv < 0:
        raise MemoryError("We've run out of memory")
        free_model_SV(model)
    free_model(model)
    free_param(param)
    return dec_values


def libsvm_sparse_decision_function(
    np.ndarray[np.float64_t, ndim=1, mode='c'] T_data,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] T_indices,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] T_indptr,
    np.ndarray[np.float64_t, ndim=1, mode='c'] SV_data,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] SV_indices,
    np.ndarray[np.int32_t,   ndim=1, mode='c'] SV_indptr,
    np.ndarray[np.float64_t, ndim=1, mode='c'] sv_coef,
    np.ndarray[np.float64_t, ndim=1, mode='c']
    intercept, int svm_type, int kernel_type, int
    degree, double gamma, double coef0, double
    eps, double C,
    np.ndarray[np.float64_t, ndim=1] class_weight,
    double nu, double p, int shrinking, int probability,
    np.ndarray[np.int32_t, ndim=1, mode='c'] nSV,
    np.ndarray[np.float64_t, ndim=1, mode='c'] probA,
    np.ndarray[np.float64_t, ndim=1, mode='c'] probB):
        cdef np.ndarray[np.float64_t, ndim=2, mode='c'] dec_values
    cdef svm_parameter *param
    cdef np.npy_intp n_class
    cdef svm_csr_model *model
    cdef np.ndarray[np.int32_t, ndim=1, mode='c'] \
        class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
    param = set_parameter(svm_type, kernel_type, degree, gamma,
                          coef0, nu,
                          100.,                           C, eps, p, shrinking,
                          probability, <int> class_weight.shape[0],
                          class_weight_label.data, class_weight.data, -1, -1)
    model = csr_set_model(param, <int> nSV.shape[0], SV_data.data,
                          SV_indices.shape, SV_indices.data,
                          SV_indptr.shape, SV_indptr.data,
                          sv_coef.data, intercept.data,
                          nSV.data, probA.data, probB.data)
    if svm_type > 1:
        n_class = 1
    else:
        n_class = get_nr(model)
        n_class = n_class * (n_class - 1) / 2
    dec_values = np.empty((T_indptr.shape[0] - 1, n_class), dtype=np.float64)
    if csr_copy_predict_values(T_data.shape, T_data.data,
                        T_indices.shape, T_indices.data,
                        T_indptr.shape, T_indptr.data,
                        model, dec_values.data, n_class) < 0:
        raise MemoryError("We've run out of memory")
        free_model_SV(model)
    free_model(model)
    free_param(param)
    return dec_values

def set_verbosity_wrap(int verbosity):
        set_verbosity(verbosity)
import os
from os.path import join
import numpy
from sklearn._build_utils import get_blas_info

def configuration(parent_package='', top_path=None):
    from numpy.distutils.misc_util import Configuration
    config = Configuration('svm', parent_package, top_path)
    config.add_subpackage('tests')
    
        config.add_library('libsvm-skl',
                       sources=[join('src', 'libsvm', 'libsvm_template.cpp')],
                       depends=[join('src', 'libsvm', 'svm.cpp'),
                                join('src', 'libsvm', 'svm.h')],
                                                                     extra_link_args=['-lstdc++'],
                       )
    libsvm_sources = ['libsvm.pyx']
    libsvm_depends = [join('src', 'libsvm', 'libsvm_helper.c'),
                      join('src', 'libsvm', 'libsvm_template.cpp'),
                      join('src', 'libsvm', 'svm.cpp'),
                      join('src', 'libsvm', 'svm.h')]
    config.add_extension('libsvm',
                         sources=libsvm_sources,
                         include_dirs=[numpy.get_include(),
                                       join('src', 'libsvm')],
                         libraries=['libsvm-skl'],
                         depends=libsvm_depends,
                         )
        cblas_libs, blas_info = get_blas_info()
    if os.name == 'posix':
        cblas_libs.append('m')
    liblinear_sources = ['liblinear.pyx',
                         join('src', 'liblinear', '*.cpp')]
    liblinear_depends = [join('src', 'liblinear', '*.h'),
                         join('src', 'liblinear', 'liblinear_helper.c')]
    config.add_extension('liblinear',
                         sources=liblinear_sources,
                         libraries=cblas_libs,
                         include_dirs=[join('..', 'src', 'cblas'),
                                       numpy.get_include(),
                                       blas_info.pop('include_dirs', [])],
                         extra_compile_args=blas_info.pop('extra_compile_args',
                                                          []),
                         depends=liblinear_depends,
                                                  ** blas_info)
    
        libsvm_sparse_sources = ['libsvm_sparse.pyx']
    config.add_extension('libsvm_sparse', libraries=['libsvm-skl'],
                         sources=libsvm_sparse_sources,
                         include_dirs=[numpy.get_include(),
                                       join("src", "libsvm")],
                         depends=[join("src", "libsvm", "svm.h"),
                                  join("src", "libsvm",
                                       "libsvm_sparse_helper.c")])
    return config

if __name__ == '__main__':
    from numpy.distutils.core import setup
    setup(**configuration(top_path='').todict())

from .classes import SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC, \
        LinearSVR
from .bounds import l1_min_c
from . import libsvm, liblinear, libsvm_sparse
__all__ = ['LinearSVC',
           'LinearSVR',
           'NuSVC',
           'NuSVR',
           'OneClassSVM',
           'SVC',
           'SVR',
           'l1_min_c',
           'liblinear',
           'libsvm',
           'libsvm_sparse']

import numpy as np
import warnings
from ..externals import six
from . import _criterion
from . import _tree

def _color_brew(n):
        color_list = []
        s, v = 0.75, 0.9
    c = s * v
    m = v - c
    for h in np.arange(25, 385, 360. / n).astype(int):
                h_bar = h / 60.
        x = c * (1 - abs((h_bar % 2) - 1))
                rgb = [(c, x, 0),
               (x, c, 0),
               (0, c, x),
               (0, x, c),
               (x, 0, c),
               (c, 0, x),
               (c, x, 0)]
        r, g, b = rgb[int(h_bar)]
                rgb = [(int(255 * (r + m))),
               (int(255 * (g + m))),
               (int(255 * (b + m)))]
        color_list.append(rgb)
    return color_list

class Sentinel(object):
    def __repr__():
        return '"tree.dot"'
SENTINEL = Sentinel()

def export_graphviz(decision_tree, out_file=SENTINEL, max_depth=None,
                    feature_names=None, class_names=None, label='all',
                    filled=False, leaves_parallel=False, impurity=True,
                    node_ids=False, proportion=False, rotate=False,
                    rounded=False, special_characters=False):
    
    def get_color(value):
                if colors['bounds'] is None:
                        color = list(colors['rgb'][np.argmax(value)])
            sorted_values = sorted(value, reverse=True)
            if len(sorted_values) == 1:
                alpha = 0
            else:
                alpha = int(np.round(255 * (sorted_values[0] - sorted_values[1]) /
                                           (1 - sorted_values[1]), 0))
        else:
                        color = list(colors['rgb'][0])
            alpha = int(np.round(255 * ((value - colors['bounds'][0]) /
                                        (colors['bounds'][1] -
                                         colors['bounds'][0])), 0))
                color.append(alpha)
        hex_codes = [str(i) for i in range(10)]
        hex_codes.extend(['a', 'b', 'c', 'd', 'e', 'f'])
        color = [hex_codes[c // 16] + hex_codes[c % 16] for c in color]
        return '
    def node_to_str(tree, node_id, criterion):
                if tree.n_outputs == 1:
            value = tree.value[node_id][0, :]
        else:
            value = tree.value[node_id]
                labels = (label == 'root' and node_id == 0) or label == 'all'
                if special_characters:
            characters = ['&            node_string = '<'
        else:
            characters = ['            node_string = '"'
                if node_ids:
            if labels:
                node_string += 'node '
            node_string += characters[0] + str(node_id) + characters[4]
                if tree.children_left[node_id] != _tree.TREE_LEAF:
                        if feature_names is not None:
                feature = feature_names[tree.feature[node_id]]
            else:
                feature = "X%s%s%s" % (characters[1],
                                       tree.feature[node_id],
                                       characters[2])
            node_string += '%s %s %s%s' % (feature,
                                           characters[3],
                                           round(tree.threshold[node_id], 4),
                                           characters[4])
                if impurity:
            if isinstance(criterion, _criterion.FriedmanMSE):
                criterion = "friedman_mse"
            elif not isinstance(criterion, six.string_types):
                criterion = "impurity"
            if labels:
                node_string += '%s = ' % criterion
            node_string += (str(round(tree.impurity[node_id], 4)) +
                            characters[4])
                if labels:
            node_string += 'samples = '
        if proportion:
            percent = (100. * tree.n_node_samples[node_id] /
                       float(tree.n_node_samples[0]))
            node_string += (str(round(percent, 1)) + '%' +
                            characters[4])
        else:
            node_string += (str(tree.n_node_samples[node_id]) +
                            characters[4])
                if proportion and tree.n_classes[0] != 1:
                        value = value / tree.weighted_n_node_samples[node_id]
        if labels:
            node_string += 'value = '
        if tree.n_classes[0] == 1:
                        value_text = np.around(value, 4)
        elif proportion:
                        value_text = np.around(value, 2)
        elif np.all(np.equal(np.mod(value, 1), 0)):
                        value_text = value.astype(int)
        else:
                        value_text = np.around(value, 4)
                value_text = str(value_text.astype('S32')).replace("b'", "'")
        value_text = value_text.replace("' '", ", ").replace("'", "")
        if tree.n_classes[0] == 1 and tree.n_outputs == 1:
            value_text = value_text.replace("[", "").replace("]", "")
        value_text = value_text.replace("\n ", characters[4])
        node_string += value_text + characters[4]
                if (class_names is not None and
                tree.n_classes[0] != 1 and
                tree.n_outputs == 1):
                        if labels:
                node_string += 'class = '
            if class_names is not True:
                class_name = class_names[np.argmax(value)]
            else:
                class_name = "y%s%s%s" % (characters[1],
                                          np.argmax(value),
                                          characters[2])
            node_string += class_name
                if node_string[-2:] == '\\n':
            node_string = node_string[:-2]
        if node_string[-5:] == '<br/>':
            node_string = node_string[:-5]
        return node_string + characters[5]
    def recurse(tree, node_id, criterion, parent=None, depth=0):
        if node_id == _tree.TREE_LEAF:
            raise ValueError("Invalid node_id %s" % _tree.TREE_LEAF)
        left_child = tree.children_left[node_id]
        right_child = tree.children_right[node_id]
                if max_depth is None or depth <= max_depth:
                        if left_child == _tree.TREE_LEAF:
                ranks['leaves'].append(str(node_id))
            elif str(depth) not in ranks:
                ranks[str(depth)] = [str(node_id)]
            else:
                ranks[str(depth)].append(str(node_id))
            out_file.write('%d [label=%s'
                           % (node_id,
                              node_to_str(tree, node_id, criterion)))
            if filled:
                                if 'rgb' not in colors:
                                        colors['rgb'] = _color_brew(tree.n_classes[0])
                    if tree.n_outputs != 1:
                                                colors['bounds'] = (np.min(-tree.impurity),
                                            np.max(-tree.impurity))
                    elif tree.n_classes[0] == 1 and len(np.unique(tree.value)) != 1:
                                                colors['bounds'] = (np.min(tree.value),
                                            np.max(tree.value))
                if tree.n_outputs == 1:
                    node_val = (tree.value[node_id][0, :] /
                                tree.weighted_n_node_samples[node_id])
                    if tree.n_classes[0] == 1:
                                                node_val = tree.value[node_id][0, :]
                else:
                                        node_val = -tree.impurity[node_id]
                out_file.write(', fillcolor="%s"' % get_color(node_val))
            out_file.write('] ;\n')
            if parent is not None:
                                out_file.write('%d -> %d' % (parent, node_id))
                if parent == 0:
                                        angles = np.array([45, -45]) * ((rotate - .5) * -2)
                    out_file.write(' [labeldistance=2.5, labelangle=')
                    if node_id == 1:
                        out_file.write('%d, headlabel="True"]' % angles[0])
                    else:
                        out_file.write('%d, headlabel="False"]' % angles[1])
                out_file.write(' ;\n')
            if left_child != _tree.TREE_LEAF:
                recurse(tree, left_child, criterion=criterion, parent=node_id,
                        depth=depth + 1)
                recurse(tree, right_child, criterion=criterion, parent=node_id,
                        depth=depth + 1)
        else:
            ranks['leaves'].append(str(node_id))
            out_file.write('%d [label="(...)"' % node_id)
            if filled:
                                out_file.write(', fillcolor="            out_file.write('] ;\n' % node_id)
            if parent is not None:
                                out_file.write('%d -> %d ;\n' % (parent, node_id))
    own_file = False
    return_string = False
    try:
        if out_file == SENTINEL:
            warnings.warn("out_file can be set to None starting from 0.18. "
                          "This will be the default in 0.20.",
                          DeprecationWarning)
            out_file = "tree.dot"
        if isinstance(out_file, six.string_types):
            if six.PY3:
                out_file = open(out_file, "w", encoding="utf-8")
            else:
                out_file = open(out_file, "wb")
            own_file = True
        if out_file is None:
            return_string = True
            out_file = six.StringIO()
                ranks = {'leaves': []}
                colors = {'bounds': None}
        out_file.write('digraph Tree {\n')
                out_file.write('node [shape=box')
        rounded_filled = []
        if filled:
            rounded_filled.append('filled')
        if rounded:
            rounded_filled.append('rounded')
        if len(rounded_filled) > 0:
            out_file.write(', style="%s", color="black"'
                           % ", ".join(rounded_filled))
        if rounded:
            out_file.write(', fontname=helvetica')
        out_file.write('] ;\n')
                if leaves_parallel:
            out_file.write('graph [ranksep=equally, splines=polyline] ;\n')
        if rounded:
            out_file.write('edge [fontname=helvetica] ;\n')
        if rotate:
            out_file.write('rankdir=LR ;\n')
                if isinstance(decision_tree, _tree.Tree):
            recurse(decision_tree, 0, criterion="impurity")
        else:
            recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)
                if leaves_parallel:
            for rank in sorted(ranks):
                out_file.write("{rank=same ; " +
                               "; ".join(r for r in ranks[rank]) + "} ;\n")
        out_file.write("}")
        if return_string:
            return out_file.getvalue()
    finally:
        if own_file:
            out_file.close()
import os
import numpy
from numpy.distutils.misc_util import Configuration

def configuration(parent_package="", top_path=None):
    config = Configuration("tree", parent_package, top_path)
    libraries = []
    if os.name == 'posix':
        libraries.append('m')
    config.add_extension("_tree",
                         sources=["_tree.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries,
                         extra_compile_args=["-O3"])
    config.add_extension("_splitter",
                         sources=["_splitter.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries,
                         extra_compile_args=["-O3"])
    config.add_extension("_criterion",
                         sources=["_criterion.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries,
                         extra_compile_args=["-O3"])
    config.add_extension("_utils",
                         sources=["_utils.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries,
                         extra_compile_args=["-O3"])
    config.add_subpackage("tests")
    return config
if __name__ == "__main__":
    from numpy.distutils.core import setup
    setup(**configuration().todict())

from __future__ import division

import numbers
from abc import ABCMeta
from abc import abstractmethod
from math import ceil
import numpy as np
from scipy.sparse import issparse
from ..base import BaseEstimator
from ..base import ClassifierMixin
from ..base import RegressorMixin
from ..externals import six
from ..utils import check_array
from ..utils import check_random_state
from ..utils import compute_sample_weight
from ..utils.multiclass import check_classification_targets
from ..utils.validation import check_is_fitted
from ..exceptions import NotFittedError
from ._criterion import Criterion
from ._splitter import Splitter
from ._tree import DepthFirstTreeBuilder
from ._tree import BestFirstTreeBuilder
from ._tree import Tree
from . import _tree, _splitter, _criterion
__all__ = ["DecisionTreeClassifier",
           "DecisionTreeRegressor",
           "ExtraTreeClassifier",
           "ExtraTreeRegressor"]

DTYPE = _tree.DTYPE
DOUBLE = _tree.DOUBLE
CRITERIA_CLF = {"gini": _criterion.Gini, "entropy": _criterion.Entropy}
CRITERIA_REG = {"mse": _criterion.MSE, "friedman_mse": _criterion.FriedmanMSE,
                "mae": _criterion.MAE}
DENSE_SPLITTERS = {"best": _splitter.BestSplitter,
                   "random": _splitter.RandomSplitter}
SPARSE_SPLITTERS = {"best": _splitter.BestSparseSplitter,
                    "random": _splitter.RandomSparseSplitter}

class BaseDecisionTree(six.with_metaclass(ABCMeta, BaseEstimator)):
    
    @abstractmethod
    def __init__(self,
                 criterion,
                 splitter,
                 max_depth,
                 min_samples_split,
                 min_samples_leaf,
                 min_weight_fraction_leaf,
                 max_features,
                 max_leaf_nodes,
                 random_state,
                 min_impurity_split,
                 class_weight=None,
                 presort=False):
        self.criterion = criterion
        self.splitter = splitter
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_weight_fraction_leaf = min_weight_fraction_leaf
        self.max_features = max_features
        self.random_state = random_state
        self.max_leaf_nodes = max_leaf_nodes
        self.min_impurity_split = min_impurity_split
        self.class_weight = class_weight
        self.presort = presort
    def fit(self, X, y, sample_weight=None, check_input=True,
            X_idx_sorted=None):
        random_state = check_random_state(self.random_state)
        if check_input:
            X = check_array(X, dtype=DTYPE, accept_sparse="csc")
            y = check_array(y, ensure_2d=False, dtype=None)
            if issparse(X):
                X.sort_indices()
                if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:
                    raise ValueError("No support for np.int64 index based "
                                     "sparse matrices")
                n_samples, self.n_features_ = X.shape
        is_classification = isinstance(self, ClassifierMixin)
        y = np.atleast_1d(y)
        expanded_class_weight = None
        if y.ndim == 1:
                                    y = np.reshape(y, (-1, 1))
        self.n_outputs_ = y.shape[1]
        if is_classification:
            check_classification_targets(y)
            y = np.copy(y)
            self.classes_ = []
            self.n_classes_ = []
            if self.class_weight is not None:
                y_original = np.copy(y)
            y_encoded = np.zeros(y.shape, dtype=np.int)
            for k in range(self.n_outputs_):
                classes_k, y_encoded[:, k] = np.unique(y[:, k],
                                                       return_inverse=True)
                self.classes_.append(classes_k)
                self.n_classes_.append(classes_k.shape[0])
            y = y_encoded
            if self.class_weight is not None:
                expanded_class_weight = compute_sample_weight(
                    self.class_weight, y_original)
        else:
            self.classes_ = [None] * self.n_outputs_
            self.n_classes_ = [1] * self.n_outputs_
        self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)
        if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
            y = np.ascontiguousarray(y, dtype=DOUBLE)
                max_depth = ((2 ** 31) - 1 if self.max_depth is None
                     else self.max_depth)
        max_leaf_nodes = (-1 if self.max_leaf_nodes is None
                          else self.max_leaf_nodes)
        if isinstance(self.min_samples_leaf, (numbers.Integral, np.integer)):
            if not 1 <= self.min_samples_leaf:
                raise ValueError("min_samples_leaf must be at least 1 "
                                 "or in (0, 0.5], got %s"
                                 % self.min_samples_leaf)
            min_samples_leaf = self.min_samples_leaf
        else:              if not 0. < self.min_samples_leaf <= 0.5:
                raise ValueError("min_samples_leaf must be at least 1 "
                                 "or in (0, 0.5], got %s"
                                 % self.min_samples_leaf)
            min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))
        if isinstance(self.min_samples_split, (numbers.Integral, np.integer)):
            if not 2 <= self.min_samples_split:
                raise ValueError("min_samples_split must be an integer "
                                 "greater than 1 or a float in (0.0, 1.0]; "
                                 "got the integer %s"
                                 % self.min_samples_split)
            min_samples_split = self.min_samples_split
        else:              if not 0. < self.min_samples_split <= 1.:
                raise ValueError("min_samples_split must be an integer "
                                 "greater than 1 or a float in (0.0, 1.0]; "
                                 "got the float %s"
                                 % self.min_samples_split)
            min_samples_split = int(ceil(self.min_samples_split * n_samples))
            min_samples_split = max(2, min_samples_split)
        min_samples_split = max(min_samples_split, 2 * min_samples_leaf)
        if isinstance(self.max_features, six.string_types):
            if self.max_features == "auto":
                if is_classification:
                    max_features = max(1, int(np.sqrt(self.n_features_)))
                else:
                    max_features = self.n_features_
            elif self.max_features == "sqrt":
                max_features = max(1, int(np.sqrt(self.n_features_)))
            elif self.max_features == "log2":
                max_features = max(1, int(np.log2(self.n_features_)))
            else:
                raise ValueError(
                    'Invalid value for max_features. Allowed string '
                    'values are "auto", "sqrt" or "log2".')
        elif self.max_features is None:
            max_features = self.n_features_
        elif isinstance(self.max_features, (numbers.Integral, np.integer)):
            max_features = self.max_features
        else:              if self.max_features > 0.0:
                max_features = max(1,
                                   int(self.max_features * self.n_features_))
            else:
                max_features = 0
        self.max_features_ = max_features
        if len(y) != n_samples:
            raise ValueError("Number of labels=%d does not match "
                             "number of samples=%d" % (len(y), n_samples))
        if not 0 <= self.min_weight_fraction_leaf <= 0.5:
            raise ValueError("min_weight_fraction_leaf must in [0, 0.5]")
        if max_depth <= 0:
            raise ValueError("max_depth must be greater than zero. ")
        if not (0 < max_features <= self.n_features_):
            raise ValueError("max_features must be in (0, n_features]")
        if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):
            raise ValueError("max_leaf_nodes must be integral number but was "
                             "%r" % max_leaf_nodes)
        if -1 < max_leaf_nodes < 2:
            raise ValueError(("max_leaf_nodes {0} must be either smaller than "
                              "0 or larger than 1").format(max_leaf_nodes))
        if sample_weight is not None:
            if (getattr(sample_weight, "dtype", None) != DOUBLE or
                    not sample_weight.flags.contiguous):
                sample_weight = np.ascontiguousarray(
                    sample_weight, dtype=DOUBLE)
            if len(sample_weight.shape) > 1:
                raise ValueError("Sample weights array has more "
                                 "than one dimension: %d" %
                                 len(sample_weight.shape))
            if len(sample_weight) != n_samples:
                raise ValueError("Number of weights=%d does not match "
                                 "number of samples=%d" %
                                 (len(sample_weight), n_samples))
        if expanded_class_weight is not None:
            if sample_weight is not None:
                sample_weight = sample_weight * expanded_class_weight
            else:
                sample_weight = expanded_class_weight
                if sample_weight is None:
            min_weight_leaf = (self.min_weight_fraction_leaf *
                               n_samples)
        else:
            min_weight_leaf = (self.min_weight_fraction_leaf *
                               np.sum(sample_weight))
        if self.min_impurity_split < 0.:
            raise ValueError("min_impurity_split must be greater than "
                             "or equal to 0")
        presort = self.presort
                        if self.presort == 'auto' and issparse(X):
            presort = False
        elif self.presort == 'auto':
            presort = True
        if presort is True and issparse(X):
            raise ValueError("Presorting is not supported for sparse "
                             "matrices.")
                                                if X_idx_sorted is None and presort:
            X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
                                             dtype=np.int32)
        if presort and X_idx_sorted.shape != X.shape:
            raise ValueError("The shape of X (X.shape = {}) doesn't match "
                             "the shape of X_idx_sorted (X_idx_sorted"
                             ".shape = {})".format(X.shape,
                                                   X_idx_sorted.shape))
                criterion = self.criterion
        if not isinstance(criterion, Criterion):
            if is_classification:
                criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,
                                                         self.n_classes_)
            else:
                criterion = CRITERIA_REG[self.criterion](self.n_outputs_,
                                                         n_samples)
        SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS
        splitter = self.splitter
        if not isinstance(self.splitter, Splitter):
            splitter = SPLITTERS[self.splitter](criterion,
                                                self.max_features_,
                                                min_samples_leaf,
                                                min_weight_leaf,
                                                random_state,
                                                self.presort)
        self.tree_ = Tree(self.n_features_, self.n_classes_, self.n_outputs_)
                if max_leaf_nodes < 0:
            builder = DepthFirstTreeBuilder(splitter, min_samples_split,
                                            min_samples_leaf,
                                            min_weight_leaf,
                                            max_depth, self.min_impurity_split)
        else:
            builder = BestFirstTreeBuilder(splitter, min_samples_split,
                                           min_samples_leaf,
                                           min_weight_leaf,
                                           max_depth,
                                           max_leaf_nodes,
                                           self.min_impurity_split)
        builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)
        if self.n_outputs_ == 1:
            self.n_classes_ = self.n_classes_[0]
            self.classes_ = self.classes_[0]
        return self
    def _validate_X_predict(self, X, check_input):
                if self.tree_ is None:
            raise NotFittedError("Estimator not fitted, "
                                 "call `fit` before exploiting the model.")
        if check_input:
            X = check_array(X, dtype=DTYPE, accept_sparse="csr")
            if issparse(X) and (X.indices.dtype != np.intc or
                                X.indptr.dtype != np.intc):
                raise ValueError("No support for np.int64 index based "
                                 "sparse matrices")
        n_features = X.shape[1]
        if self.n_features_ != n_features:
            raise ValueError("Number of features of the model must "
                             "match the input. Model n_features is %s and "
                             "input n_features is %s "
                             % (self.n_features_, n_features))
        return X
    def predict(self, X, check_input=True):
                check_is_fitted(self, 'tree_')
        X = self._validate_X_predict(X, check_input)
        proba = self.tree_.predict(X)
        n_samples = X.shape[0]
                if isinstance(self, ClassifierMixin):
            if self.n_outputs_ == 1:
                return self.classes_.take(np.argmax(proba, axis=1), axis=0)
            else:
                predictions = np.zeros((n_samples, self.n_outputs_))
                for k in range(self.n_outputs_):
                    predictions[:, k] = self.classes_[k].take(
                        np.argmax(proba[:, k], axis=1),
                        axis=0)
                return predictions
                else:
            if self.n_outputs_ == 1:
                return proba[:, 0]
            else:
                return proba[:, :, 0]
    def apply(self, X, check_input=True):
                check_is_fitted(self, 'tree_')
        X = self._validate_X_predict(X, check_input)
        return self.tree_.apply(X)
    def decision_path(self, X, check_input=True):
                X = self._validate_X_predict(X, check_input)
        return self.tree_.decision_path(X)
    @property
    def feature_importances_(self):
                check_is_fitted(self, 'tree_')
        return self.tree_.compute_feature_importances()

class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):
        def __init__(self,
                 criterion="gini",
                 splitter="best",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features=None,
                 random_state=None,
                 max_leaf_nodes=None,
                 min_impurity_split=1e-7,
                 class_weight=None,
                 presort=False):
        super(DecisionTreeClassifier, self).__init__(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            class_weight=class_weight,
            random_state=random_state,
            min_impurity_split=min_impurity_split,
            presort=presort)
    def fit(self, X, y, sample_weight=None, check_input=True,
            X_idx_sorted=None):
        
        super(DecisionTreeClassifier, self).fit(
            X, y,
            sample_weight=sample_weight,
            check_input=check_input,
            X_idx_sorted=X_idx_sorted)
        return self
    def predict_proba(self, X, check_input=True):
                check_is_fitted(self, 'tree_')
        X = self._validate_X_predict(X, check_input)
        proba = self.tree_.predict(X)
        if self.n_outputs_ == 1:
            proba = proba[:, :self.n_classes_]
            normalizer = proba.sum(axis=1)[:, np.newaxis]
            normalizer[normalizer == 0.0] = 1.0
            proba /= normalizer
            return proba
        else:
            all_proba = []
            for k in range(self.n_outputs_):
                proba_k = proba[:, k, :self.n_classes_[k]]
                normalizer = proba_k.sum(axis=1)[:, np.newaxis]
                normalizer[normalizer == 0.0] = 1.0
                proba_k /= normalizer
                all_proba.append(proba_k)
            return all_proba
    def predict_log_proba(self, X):
                proba = self.predict_proba(X)
        if self.n_outputs_ == 1:
            return np.log(proba)
        else:
            for k in range(self.n_outputs_):
                proba[k] = np.log(proba[k])
            return proba

class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):
        def __init__(self,
                 criterion="mse",
                 splitter="best",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features=None,
                 random_state=None,
                 max_leaf_nodes=None,
                 min_impurity_split=1e-7,
                 presort=False):
        super(DecisionTreeRegressor, self).__init__(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            random_state=random_state,
            min_impurity_split=min_impurity_split,
            presort=presort)
    def fit(self, X, y, sample_weight=None, check_input=True,
            X_idx_sorted=None):
        
        super(DecisionTreeRegressor, self).fit(
            X, y,
            sample_weight=sample_weight,
            check_input=check_input,
            X_idx_sorted=X_idx_sorted)
        return self

class ExtraTreeClassifier(DecisionTreeClassifier):
        def __init__(self,
                 criterion="gini",
                 splitter="random",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 random_state=None,
                 max_leaf_nodes=None,
                 min_impurity_split=1e-7,
                 class_weight=None):
        super(ExtraTreeClassifier, self).__init__(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            class_weight=class_weight,
            min_impurity_split=min_impurity_split,
            random_state=random_state)

class ExtraTreeRegressor(DecisionTreeRegressor):
        def __init__(self,
                 criterion="mse",
                 splitter="random",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features="auto",
                 random_state=None,
                 min_impurity_split=1e-7,
                 max_leaf_nodes=None):
        super(ExtraTreeRegressor, self).__init__(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_split=min_impurity_split,
            random_state=random_state)

from libc.stdlib cimport calloc
from libc.stdlib cimport free
from libc.string cimport memcpy
from libc.string cimport memset
from libc.math cimport fabs
import numpy as np
cimport numpy as np
np.import_array()
from ._utils cimport log
from ._utils cimport safe_realloc
from ._utils cimport sizet_ptr_to_ndarray
from ._utils cimport WeightedMedianCalculator
cdef class Criterion:
    
    def __dealloc__(self):
        
        free(self.sum_total)
        free(self.sum_left)
        free(self.sum_right)
    def __getstate__(self):
        return {}
    def __setstate__(self, d):
        pass
    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,
                  SIZE_t end) nogil except -1:
        
        pass
    cdef int reset(self) nogil except -1:
        
        pass
    cdef int reverse_reset(self) nogil except -1:
                pass
    cdef int update(self, SIZE_t new_pos) nogil except -1:
        
        pass
    cdef double node_impurity(self) nogil:
        
        pass
    cdef void children_impurity(self, double* impurity_left,
                                double* impurity_right) nogil:
        
        pass
    cdef void node_value(self, double* dest) nogil:
        
        pass
    cdef double proxy_impurity_improvement(self) nogil:
                cdef double impurity_left
        cdef double impurity_right
        self.children_impurity(&impurity_left, &impurity_right)
        return (- self.weighted_n_right * impurity_right
                - self.weighted_n_left * impurity_left)
    cdef double impurity_improvement(self, double impurity) nogil:
        
        cdef double impurity_left
        cdef double impurity_right
        self.children_impurity(&impurity_left, &impurity_right)
        return ((self.weighted_n_node_samples / self.weighted_n_samples) *
                (impurity - (self.weighted_n_right / 
                             self.weighted_n_node_samples * impurity_right)
                          - (self.weighted_n_left / 
                             self.weighted_n_node_samples * impurity_left)))

cdef class ClassificationCriterion(Criterion):
    
    cdef SIZE_t* n_classes
    cdef SIZE_t sum_stride
    def __cinit__(self, SIZE_t n_outputs,
                  np.ndarray[SIZE_t, ndim=1] n_classes):
        
        self.y = NULL
        self.y_stride = 0
        self.sample_weight = NULL
        self.samples = NULL
        self.start = 0
        self.pos = 0
        self.end = 0
        self.n_outputs = n_outputs
        self.n_samples = 0
        self.n_node_samples = 0
        self.weighted_n_node_samples = 0.0
        self.weighted_n_left = 0.0
        self.weighted_n_right = 0.0
                self.sum_total = NULL
        self.sum_left = NULL
        self.sum_right = NULL
        self.n_classes = NULL
        safe_realloc(&self.n_classes, n_outputs)
        cdef SIZE_t k = 0
        cdef SIZE_t sum_stride = 0
                        for k in range(n_outputs):
            self.n_classes[k] = n_classes[k]
            if n_classes[k] > sum_stride:
                sum_stride = n_classes[k]
        self.sum_stride = sum_stride
        cdef SIZE_t n_elements = n_outputs * sum_stride
        self.sum_total = <double*> calloc(n_elements, sizeof(double))
        self.sum_left = <double*> calloc(n_elements, sizeof(double))
        self.sum_right = <double*> calloc(n_elements, sizeof(double))
        if (self.sum_total == NULL or
                self.sum_left == NULL or
                self.sum_right == NULL):
            raise MemoryError()
    def __dealloc__(self):
                free(self.n_classes)
    def __reduce__(self):
        return (type(self),
                (self.n_outputs,
                 sizet_ptr_to_ndarray(self.n_classes, self.n_outputs)),
                self.__getstate__())
    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride,
                  DOUBLE_t* sample_weight, double weighted_n_samples,
                  SIZE_t* samples, SIZE_t start, SIZE_t end) nogil except -1:
        
        self.y = y
        self.y_stride = y_stride
        self.sample_weight = sample_weight
        self.samples = samples
        self.start = start
        self.end = end
        self.n_node_samples = end - start
        self.weighted_n_samples = weighted_n_samples
        self.weighted_n_node_samples = 0.0
        cdef SIZE_t* n_classes = self.n_classes
        cdef double* sum_total = self.sum_total
        cdef SIZE_t i
        cdef SIZE_t p
        cdef SIZE_t k
        cdef SIZE_t c
        cdef DOUBLE_t w = 1.0
        cdef SIZE_t offset = 0
        for k in range(self.n_outputs):
            memset(sum_total + offset, 0, n_classes[k] * sizeof(double))
            offset += self.sum_stride
        for p in range(start, end):
            i = samples[p]
                                    if sample_weight != NULL:
                w = sample_weight[i]
                        for k in range(self.n_outputs):
                c = <SIZE_t> y[i * y_stride + k]
                sum_total[k * self.sum_stride + c] += w
            self.weighted_n_node_samples += w
                self.reset()
        return 0
    cdef int reset(self) nogil except -1:
                self.pos = self.start
        self.weighted_n_left = 0.0
        self.weighted_n_right = self.weighted_n_node_samples
        cdef double* sum_total = self.sum_total
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef SIZE_t* n_classes = self.n_classes
        cdef SIZE_t k
        for k in range(self.n_outputs):
            memset(sum_left, 0, n_classes[k] * sizeof(double))
            memcpy(sum_right, sum_total, n_classes[k] * sizeof(double))
            sum_total += self.sum_stride
            sum_left += self.sum_stride
            sum_right += self.sum_stride
        return 0
    cdef int reverse_reset(self) nogil except -1:
                self.pos = self.end
        self.weighted_n_left = self.weighted_n_node_samples
        self.weighted_n_right = 0.0
        cdef double* sum_total = self.sum_total
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef SIZE_t* n_classes = self.n_classes
        cdef SIZE_t k
        for k in range(self.n_outputs):
            memset(sum_right, 0, n_classes[k] * sizeof(double))
            memcpy(sum_left, sum_total, n_classes[k] * sizeof(double))
            sum_total += self.sum_stride
            sum_left += self.sum_stride
            sum_right += self.sum_stride
        return 0
    cdef int update(self, SIZE_t new_pos) nogil except -1:
                cdef DOUBLE_t* y = self.y
        cdef SIZE_t pos = self.pos
        cdef SIZE_t end = self.end
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef double* sum_total = self.sum_total
        cdef SIZE_t* n_classes = self.n_classes
        cdef SIZE_t* samples = self.samples
        cdef DOUBLE_t* sample_weight = self.sample_weight
        cdef SIZE_t i
        cdef SIZE_t p
        cdef SIZE_t k
        cdef SIZE_t c
        cdef SIZE_t label_index
        cdef DOUBLE_t w = 1.0
                                                        
        if (new_pos - pos) <= (end - new_pos):
            for p in range(pos, new_pos):
                i = samples[p]
                if sample_weight != NULL:
                    w = sample_weight[i]
                for k in range(self.n_outputs):
                    label_index = (k * self.sum_stride +
                                   <SIZE_t> y[i * self.y_stride + k])
                    sum_left[label_index] += w
                self.weighted_n_left += w
        else:
            self.reverse_reset()
            for p in range(end - 1, new_pos - 1, -1):
                i = samples[p]
                if sample_weight != NULL:
                    w = sample_weight[i]
                for k in range(self.n_outputs):
                    label_index = (k * self.sum_stride +
                                   <SIZE_t> y[i * self.y_stride + k])
                    sum_left[label_index] -= w
                self.weighted_n_left -= w
                self.weighted_n_right = self.weighted_n_node_samples - self.weighted_n_left
        for k in range(self.n_outputs):
            for c in range(n_classes[k]):
                sum_right[c] = sum_total[c] - sum_left[c]
            sum_right += self.sum_stride
            sum_left += self.sum_stride
            sum_total += self.sum_stride
        self.pos = new_pos
        return 0
    cdef double node_impurity(self) nogil:
        pass
    cdef void children_impurity(self, double* impurity_left,
                                double* impurity_right) nogil:
        pass
    cdef void node_value(self, double* dest) nogil:
        
        cdef double* sum_total = self.sum_total
        cdef SIZE_t* n_classes = self.n_classes
        cdef SIZE_t k
        for k in range(self.n_outputs):
            memcpy(dest, sum_total, n_classes[k] * sizeof(double))
            dest += self.sum_stride
            sum_total += self.sum_stride

cdef class Entropy(ClassificationCriterion):
    
    cdef double node_impurity(self) nogil:
        
        cdef SIZE_t* n_classes = self.n_classes
        cdef double* sum_total = self.sum_total
        cdef double entropy = 0.0
        cdef double count_k
        cdef SIZE_t k
        cdef SIZE_t c
        for k in range(self.n_outputs):
            for c in range(n_classes[k]):
                count_k = sum_total[c]
                if count_k > 0.0:
                    count_k /= self.weighted_n_node_samples
                    entropy -= count_k * log(count_k)
            sum_total += self.sum_stride
        return entropy / self.n_outputs
    cdef void children_impurity(self, double* impurity_left,
                                double* impurity_right) nogil:
        
        cdef SIZE_t* n_classes = self.n_classes
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef double entropy_left = 0.0
        cdef double entropy_right = 0.0
        cdef double count_k
        cdef SIZE_t k
        cdef SIZE_t c
        for k in range(self.n_outputs):
            for c in range(n_classes[k]):
                count_k = sum_left[c]
                if count_k > 0.0:
                    count_k /= self.weighted_n_left
                    entropy_left -= count_k * log(count_k)
                count_k = sum_right[c]
                if count_k > 0.0:
                    count_k /= self.weighted_n_right
                    entropy_right -= count_k * log(count_k)
            sum_left += self.sum_stride
            sum_right += self.sum_stride
        impurity_left[0] = entropy_left / self.n_outputs
        impurity_right[0] = entropy_right / self.n_outputs

cdef class Gini(ClassificationCriterion):
    
    cdef double node_impurity(self) nogil:
        
        cdef SIZE_t* n_classes = self.n_classes
        cdef double* sum_total = self.sum_total
        cdef double gini = 0.0
        cdef double sq_count
        cdef double count_k
        cdef SIZE_t k
        cdef SIZE_t c
        for k in range(self.n_outputs):
            sq_count = 0.0
            for c in range(n_classes[k]):
                count_k = sum_total[c]
                sq_count += count_k * count_k
            gini += 1.0 - sq_count / (self.weighted_n_node_samples *
                                      self.weighted_n_node_samples)
            sum_total += self.sum_stride
        return gini / self.n_outputs
    cdef void children_impurity(self, double* impurity_left,
                                double* impurity_right) nogil:
        
        cdef SIZE_t* n_classes = self.n_classes
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef double gini_left = 0.0
        cdef double gini_right = 0.0
        cdef double sq_count_left
        cdef double sq_count_right
        cdef double count_k
        cdef SIZE_t k
        cdef SIZE_t c
        for k in range(self.n_outputs):
            sq_count_left = 0.0
            sq_count_right = 0.0
            for c in range(n_classes[k]):
                count_k = sum_left[c]
                sq_count_left += count_k * count_k
                count_k = sum_right[c]
                sq_count_right += count_k * count_k
            gini_left += 1.0 - sq_count_left / (self.weighted_n_left *
                                                self.weighted_n_left)
            gini_right += 1.0 - sq_count_right / (self.weighted_n_right *
                                                  self.weighted_n_right)
            sum_left += self.sum_stride
            sum_right += self.sum_stride
        impurity_left[0] = gini_left / self.n_outputs
        impurity_right[0] = gini_right / self.n_outputs

cdef class RegressionCriterion(Criterion):
    
    cdef double sq_sum_total
    def __cinit__(self, SIZE_t n_outputs, SIZE_t n_samples):
        
                self.y = NULL
        self.y_stride = 0
        self.sample_weight = NULL
        self.samples = NULL
        self.start = 0
        self.pos = 0
        self.end = 0
        self.n_outputs = n_outputs
        self.n_samples = n_samples
        self.n_node_samples = 0
        self.weighted_n_node_samples = 0.0
        self.weighted_n_left = 0.0
        self.weighted_n_right = 0.0
        self.sq_sum_total = 0.0
                        self.sum_total = NULL
        self.sum_left = NULL
        self.sum_right = NULL
                self.sum_total = <double*> calloc(n_outputs, sizeof(double))
        self.sum_left = <double*> calloc(n_outputs, sizeof(double))
        self.sum_right = <double*> calloc(n_outputs, sizeof(double))
        if (self.sum_total == NULL or 
                self.sum_left == NULL or
                self.sum_right == NULL):
            raise MemoryError()
    def __reduce__(self):
        return (type(self), (self.n_outputs, self.n_samples), self.__getstate__())
    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,
                  SIZE_t end) nogil except -1:
                        self.y = y
        self.y_stride = y_stride
        self.sample_weight = sample_weight
        self.samples = samples
        self.start = start
        self.end = end
        self.n_node_samples = end - start
        self.weighted_n_samples = weighted_n_samples
        self.weighted_n_node_samples = 0.
        cdef SIZE_t i
        cdef SIZE_t p
        cdef SIZE_t k
        cdef DOUBLE_t y_ik
        cdef DOUBLE_t w_y_ik
        cdef DOUBLE_t w = 1.0
        self.sq_sum_total = 0.0
        memset(self.sum_total, 0, self.n_outputs * sizeof(double))
        for p in range(start, end):
            i = samples[p]
            if sample_weight != NULL:
                w = sample_weight[i]
            for k in range(self.n_outputs):
                y_ik = y[i * y_stride + k]
                w_y_ik = w * y_ik
                self.sum_total[k] += w_y_ik
                self.sq_sum_total += w_y_ik * y_ik
            self.weighted_n_node_samples += w
                self.reset()
        return 0
    cdef int reset(self) nogil except -1:
                cdef SIZE_t n_bytes = self.n_outputs * sizeof(double)
        memset(self.sum_left, 0, n_bytes)
        memcpy(self.sum_right, self.sum_total, n_bytes)
        self.weighted_n_left = 0.0
        self.weighted_n_right = self.weighted_n_node_samples
        self.pos = self.start
        return 0
    cdef int reverse_reset(self) nogil except -1:
                cdef SIZE_t n_bytes = self.n_outputs * sizeof(double)
        memset(self.sum_right, 0, n_bytes)
        memcpy(self.sum_left, self.sum_total, n_bytes)
        self.weighted_n_right = 0.0
        self.weighted_n_left = self.weighted_n_node_samples
        self.pos = self.end
        return 0
    cdef int update(self, SIZE_t new_pos) nogil except -1:
        
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef double* sum_total = self.sum_total
        cdef double* sample_weight = self.sample_weight
        cdef SIZE_t* samples = self.samples
        cdef DOUBLE_t* y = self.y
        cdef SIZE_t pos = self.pos
        cdef SIZE_t end = self.end
        cdef SIZE_t i
        cdef SIZE_t p
        cdef SIZE_t k
        cdef DOUBLE_t w = 1.0
        cdef DOUBLE_t y_ik
                                                        
        if (new_pos - pos) <= (end - new_pos):
            for p in range(pos, new_pos):
                i = samples[p]
                if sample_weight != NULL:
                    w = sample_weight[i]
                for k in range(self.n_outputs):
                    y_ik = y[i * self.y_stride + k]
                    sum_left[k] += w * y_ik
                self.weighted_n_left += w
        else:
            self.reverse_reset()
            for p in range(end - 1, new_pos - 1, -1):
                i = samples[p]
                if sample_weight != NULL:
                    w = sample_weight[i]
                for k in range(self.n_outputs):
                    y_ik = y[i * self.y_stride + k]
                    sum_left[k] -= w * y_ik
                self.weighted_n_left -= w
        self.weighted_n_right = (self.weighted_n_node_samples -
                                 self.weighted_n_left)
        for k in range(self.n_outputs):
            sum_right[k] = sum_total[k] - sum_left[k]
        self.pos = new_pos
        return 0
    cdef double node_impurity(self) nogil:
        pass
    cdef void children_impurity(self, double* impurity_left,
                                double* impurity_right) nogil:
        pass
    cdef void node_value(self, double* dest) nogil:
        
        cdef SIZE_t k
        for k in range(self.n_outputs):
            dest[k] = self.sum_total[k] / self.weighted_n_node_samples

cdef class MSE(RegressionCriterion):
    
    cdef double node_impurity(self) nogil:
        
        cdef double* sum_total = self.sum_total
        cdef double impurity
        cdef SIZE_t k
        impurity = self.sq_sum_total / self.weighted_n_node_samples
        for k in range(self.n_outputs):
            impurity -= (sum_total[k] / self.weighted_n_node_samples)**2.0
        return impurity / self.n_outputs
    cdef double proxy_impurity_improvement(self) nogil:
        
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef SIZE_t k
        cdef double proxy_impurity_left = 0.0
        cdef double proxy_impurity_right = 0.0
        for k in range(self.n_outputs):
            proxy_impurity_left += sum_left[k] * sum_left[k]
            proxy_impurity_right += sum_right[k] * sum_right[k]
        return (proxy_impurity_left / self.weighted_n_left +
                proxy_impurity_right / self.weighted_n_right)
    cdef void children_impurity(self, double* impurity_left,
                                double* impurity_right) nogil:
        
        cdef DOUBLE_t* y = self.y
        cdef DOUBLE_t* sample_weight = self.sample_weight
        cdef SIZE_t* samples = self.samples
        cdef SIZE_t pos = self.pos
        cdef SIZE_t start = self.start
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef double sq_sum_left = 0.0
        cdef double sq_sum_right
        cdef SIZE_t i
        cdef SIZE_t p
        cdef SIZE_t k
        cdef DOUBLE_t w = 1.0
        cdef DOUBLE_t y_ik
        for p in range(start, pos):
            i = samples[p]
            if sample_weight != NULL:
                w = sample_weight[i]
            for k in range(self.n_outputs):
                y_ik = y[i * self.y_stride + k]
                sq_sum_left += w * y_ik * y_ik
        sq_sum_right = self.sq_sum_total - sq_sum_left
        impurity_left[0] = sq_sum_left / self.weighted_n_left
        impurity_right[0] = sq_sum_right / self.weighted_n_right
        for k in range(self.n_outputs):
            impurity_left[0] -= (sum_left[k] / self.weighted_n_left) ** 2.0
            impurity_right[0] -= (sum_right[k] / self.weighted_n_right) ** 2.0
        impurity_left[0] /= self.n_outputs
        impurity_right[0] /= self.n_outputs
cdef class MAE(RegressionCriterion):
        def __dealloc__(self):
                free(self.node_medians)
    cdef np.ndarray left_child
    cdef np.ndarray right_child
    cdef DOUBLE_t* node_medians
    def __cinit__(self, SIZE_t n_outputs, SIZE_t n_samples):
        
                self.y = NULL
        self.y_stride = 0
        self.sample_weight = NULL
        self.samples = NULL
        self.start = 0
        self.pos = 0
        self.end = 0
        self.n_outputs = n_outputs
        self.n_samples = n_samples
        self.n_node_samples = 0
        self.weighted_n_node_samples = 0.0
        self.weighted_n_left = 0.0
        self.weighted_n_right = 0.0
                        self.node_medians = NULL
                safe_realloc(&self.node_medians, n_outputs)
        self.left_child = np.empty(n_outputs, dtype='object')
        self.right_child = np.empty(n_outputs, dtype='object')
                for k in range(n_outputs):
            self.left_child[k] = WeightedMedianCalculator(n_samples)
            self.right_child[k] = WeightedMedianCalculator(n_samples)
    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,
                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,
                  SIZE_t end) nogil except -1:
        
        cdef SIZE_t i, p, k
        cdef DOUBLE_t y_ik
        cdef DOUBLE_t w = 1.0
                self.y = y
        self.y_stride = y_stride
        self.sample_weight = sample_weight
        self.samples = samples
        self.start = start
        self.end = end
        self.n_node_samples = end - start
        self.weighted_n_samples = weighted_n_samples
        self.weighted_n_node_samples = 0.
        cdef void** left_child
        cdef void** right_child
        left_child = <void**> self.left_child.data
        right_child = <void**> self.right_child.data
        for k in range(self.n_outputs):
            (<WeightedMedianCalculator> left_child[k]).reset()
            (<WeightedMedianCalculator> right_child[k]).reset()
        for p in range(start, end):
            i = samples[p]
            if sample_weight != NULL:
                w = sample_weight[i]
            for k in range(self.n_outputs):
                y_ik = y[i * y_stride + k]
                                                                (<WeightedMedianCalculator> right_child[k]).push(y_ik, w)
            self.weighted_n_node_samples += w
                for k in range(self.n_outputs):
            self.node_medians[k] = (<WeightedMedianCalculator> right_child[k]).get_median()
                self.reset()
        return 0
    cdef int reset(self) nogil except -1:
        
        cdef SIZE_t i, k
        cdef DOUBLE_t value
        cdef DOUBLE_t weight
        cdef void** left_child = <void**> self.left_child.data
        cdef void** right_child = <void**> self.right_child.data
        self.weighted_n_left = 0.0
        self.weighted_n_right = self.weighted_n_node_samples
        self.pos = self.start
                
        for k in range(self.n_outputs):
                        for i in range((<WeightedMedianCalculator> left_child[k]).size()):
                                (<WeightedMedianCalculator> left_child[k]).pop(&value,
                                                               &weight)
                                (<WeightedMedianCalculator> right_child[k]).push(value,
                                                                 weight)
        return 0
    cdef int reverse_reset(self) nogil except -1:
        
        self.weighted_n_right = 0.0
        self.weighted_n_left = self.weighted_n_node_samples
        self.pos = self.end
        cdef DOUBLE_t value
        cdef DOUBLE_t weight
        cdef void** left_child = <void**> self.left_child.data
        cdef void** right_child = <void**> self.right_child.data
                        for k in range(self.n_outputs):
                        for i in range((<WeightedMedianCalculator> right_child[k]).size()):
                                (<WeightedMedianCalculator> right_child[k]).pop(&value,
                                                                &weight)
                                (<WeightedMedianCalculator> left_child[k]).push(value,
                                                                weight)
        return 0
    cdef int update(self, SIZE_t new_pos) nogil except -1:
        
        cdef DOUBLE_t* sample_weight = self.sample_weight
        cdef SIZE_t* samples = self.samples
        cdef void** left_child = <void**> self.left_child.data
        cdef void** right_child = <void**> self.right_child.data
        cdef DOUBLE_t* y = self.y
        cdef SIZE_t pos = self.pos
        cdef SIZE_t end = self.end
        cdef SIZE_t i, p, k
        cdef DOUBLE_t w = 1.0
        cdef DOUBLE_t y_ik
                                        
        if (new_pos - pos) <= (end - new_pos):
            for p in range(pos, new_pos):
                i = samples[p]
                if sample_weight != NULL:
                    w = sample_weight[i]
                for k in range(self.n_outputs):
                    y_ik = y[i * self.y_stride + k]
                                        (<WeightedMedianCalculator> right_child[k]).remove(y_ik, w)
                                        (<WeightedMedianCalculator> left_child[k]).push(y_ik, w)
                self.weighted_n_left += w
        else:
            self.reverse_reset()
            for p in range(end - 1, new_pos - 1, -1):
                i = samples[p]
                if sample_weight != NULL:
                    w = sample_weight[i]
                for k in range(self.n_outputs):
                    y_ik = y[i * self.y_stride + k]
                                        (<WeightedMedianCalculator> left_child[k]).remove(y_ik, w)
                    (<WeightedMedianCalculator> right_child[k]).push(y_ik, w)
                self.weighted_n_left -= w
        self.weighted_n_right = (self.weighted_n_node_samples -
                                 self.weighted_n_left)
        self.pos = new_pos
        return 0
    cdef void node_value(self, double* dest) nogil:
        
        cdef SIZE_t k
        for k in range(self.n_outputs):
            dest[k] = <double> self.node_medians[k]
    cdef double node_impurity(self) nogil:
        
        cdef DOUBLE_t* y = self.y
        cdef DOUBLE_t* sample_weight = self.sample_weight
        cdef SIZE_t* samples = self.samples
        cdef SIZE_t i, p, k
        cdef DOUBLE_t y_ik
        cdef DOUBLE_t w_y_ik
        cdef double impurity = 0.0
        for k in range(self.n_outputs):
            for p in range(self.start, self.end):
                i = samples[p]
                y_ik = y[i * self.y_stride + k]
                impurity += <double> fabs((<double> y_ik) - <double> self.node_medians[k])
        return impurity / (self.weighted_n_node_samples * self.n_outputs)
    cdef void children_impurity(self, double* impurity_left,
                                double* impurity_right) nogil:
        
        cdef DOUBLE_t* y = self.y
        cdef DOUBLE_t* sample_weight = self.sample_weight
        cdef SIZE_t* samples = self.samples
        cdef SIZE_t start = self.start
        cdef SIZE_t pos = self.pos
        cdef SIZE_t end = self.end
        cdef SIZE_t i, p, k
        cdef DOUBLE_t y_ik
        cdef DOUBLE_t median
        cdef void** left_child = <void**> self.left_child.data
        cdef void** right_child = <void**> self.right_child.data
        impurity_left[0] = 0.0
        impurity_right[0] = 0.0
        for k in range(self.n_outputs):
            median = (<WeightedMedianCalculator> left_child[k]).get_median()
            for p in range(start, pos):
                i = samples[p]
                y_ik = y[i * self.y_stride + k]
                impurity_left[0] += <double>fabs((<double> y_ik) -
                                                 <double> median)
        impurity_left[0] /= <double>((self.weighted_n_left) * self.n_outputs)
        for k in range(self.n_outputs):
            median = (<WeightedMedianCalculator> right_child[k]).get_median()
            for p in range(pos, end):
                i = samples[p]
                y_ik = y[i * self.y_stride + k]
                impurity_right[0] += <double>fabs((<double> y_ik) -
                                                  <double> median)
        impurity_right[0] /= <double>((self.weighted_n_right) *
                                      self.n_outputs)

cdef class FriedmanMSE(MSE):
    
    cdef double proxy_impurity_improvement(self) nogil:
        
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef double total_sum_left = 0.0
        cdef double total_sum_right = 0.0
        cdef SIZE_t k
        cdef double diff = 0.0
        for k in range(self.n_outputs):
            total_sum_left += sum_left[k]
            total_sum_right += sum_right[k]
        diff = (self.weighted_n_right * total_sum_left -
                self.weighted_n_left * total_sum_right)
        return diff * diff / (self.weighted_n_left * self.weighted_n_right)
    cdef double impurity_improvement(self, double impurity) nogil:
        cdef double* sum_left = self.sum_left
        cdef double* sum_right = self.sum_right
        cdef double total_sum_left = 0.0
        cdef double total_sum_right = 0.0
        cdef SIZE_t k
        cdef double diff = 0.0
        for k in range(self.n_outputs):
            total_sum_left += sum_left[k]
            total_sum_right += sum_right[k]
        diff = (self.weighted_n_right * total_sum_left -
                self.weighted_n_left * total_sum_right) / self.n_outputs
        return (diff * diff / (self.weighted_n_left * self.weighted_n_right *
                               self.weighted_n_node_samples))

from ._criterion cimport Criterion
from libc.stdlib cimport free
from libc.stdlib cimport qsort
from libc.string cimport memcpy
from libc.string cimport memset
import numpy as np
cimport numpy as np
np.import_array()
from scipy.sparse import csc_matrix
from ._utils cimport log
from ._utils cimport rand_int
from ._utils cimport rand_uniform
from ._utils cimport RAND_R_MAX
from ._utils cimport safe_realloc
cdef double INFINITY = np.inf
cdef DTYPE_t FEATURE_THRESHOLD = 1e-7
cdef DTYPE_t EXTRACT_NNZ_SWITCH = 0.1
cdef inline void _init_split(SplitRecord* self, SIZE_t start_pos) nogil:
    self.impurity_left = INFINITY
    self.impurity_right = INFINITY
    self.pos = start_pos
    self.feature = 0
    self.threshold = 0.
    self.improvement = -INFINITY
cdef class Splitter:
    
    def __cinit__(self, Criterion criterion, SIZE_t max_features,
                  SIZE_t min_samples_leaf, double min_weight_leaf,
                  object random_state, bint presort):
        
        self.criterion = criterion
        self.samples = NULL
        self.n_samples = 0
        self.features = NULL
        self.n_features = 0
        self.feature_values = NULL
        self.y = NULL
        self.y_stride = 0
        self.sample_weight = NULL
        self.max_features = max_features
        self.min_samples_leaf = min_samples_leaf
        self.min_weight_leaf = min_weight_leaf
        self.random_state = random_state
        self.presort = presort
    def __dealloc__(self):
        
        free(self.samples)
        free(self.features)
        free(self.constant_features)
        free(self.feature_values)
    def __getstate__(self):
        return {}
    def __setstate__(self, d):
        pass
    cdef int init(self,
                   object X,
                   np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
                   DOUBLE_t* sample_weight,
                   np.ndarray X_idx_sorted=None) except -1:
        
        self.rand_r_state = self.random_state.randint(0, RAND_R_MAX)
        cdef SIZE_t n_samples = X.shape[0]
                        cdef SIZE_t* samples = safe_realloc(&self.samples, n_samples)
        cdef SIZE_t i, j
        cdef double weighted_n_samples = 0.0
        j = 0
        for i in range(n_samples):
                        if sample_weight == NULL or sample_weight[i] != 0.0:
                samples[j] = i
                j += 1
            if sample_weight != NULL:
                weighted_n_samples += sample_weight[i]
            else:
                weighted_n_samples += 1.0
                self.n_samples = j
        self.weighted_n_samples = weighted_n_samples
        cdef SIZE_t n_features = X.shape[1]
        cdef SIZE_t* features = safe_realloc(&self.features, n_features)
        for i in range(n_features):
            features[i] = i
        self.n_features = n_features
        safe_realloc(&self.feature_values, n_samples)
        safe_realloc(&self.constant_features, n_features)
        self.y = <DOUBLE_t*> y.data
        self.y_stride = <SIZE_t> y.strides[0] / <SIZE_t> y.itemsize
        self.sample_weight = sample_weight
        return 0
    cdef int node_reset(self, SIZE_t start, SIZE_t end,
                        double* weighted_n_node_samples) nogil except -1:
        
        self.start = start
        self.end = end
        self.criterion.init(self.y,
                            self.y_stride,
                            self.sample_weight,
                            self.weighted_n_samples,
                            self.samples,
                            start,
                            end)
        weighted_n_node_samples[0] = self.criterion.weighted_n_node_samples
        return 0
    cdef int node_split(self, double impurity, SplitRecord* split,
                        SIZE_t* n_constant_features) nogil except -1:
        
        pass
    cdef void node_value(self, double* dest) nogil:
        
        self.criterion.node_value(dest)
    cdef double node_impurity(self) nogil:
        
        return self.criterion.node_impurity()

cdef class BaseDenseSplitter(Splitter):
    cdef DTYPE_t* X
    cdef SIZE_t X_sample_stride
    cdef SIZE_t X_feature_stride
    cdef np.ndarray X_idx_sorted
    cdef INT32_t* X_idx_sorted_ptr
    cdef SIZE_t X_idx_sorted_stride
    cdef SIZE_t n_total_samples
    cdef SIZE_t* sample_mask
    def __cinit__(self, Criterion criterion, SIZE_t max_features,
                  SIZE_t min_samples_leaf, double min_weight_leaf,
                  object random_state, bint presort):
        self.X = NULL
        self.X_sample_stride = 0
        self.X_feature_stride = 0
        self.X_idx_sorted_ptr = NULL
        self.X_idx_sorted_stride = 0
        self.sample_mask = NULL
        self.presort = presort
    def __dealloc__(self):
                if self.presort == 1:
            free(self.sample_mask)
    cdef int init(self,
                  object X,
                  np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
                  DOUBLE_t* sample_weight,
                  np.ndarray X_idx_sorted=None) except -1:
        
                Splitter.init(self, X, y, sample_weight)
                cdef np.ndarray X_ndarray = X
        self.X = <DTYPE_t*> X_ndarray.data
        self.X_sample_stride = <SIZE_t> X.strides[0] / <SIZE_t> X.itemsize
        self.X_feature_stride = <SIZE_t> X.strides[1] / <SIZE_t> X.itemsize
        if self.presort == 1:
            self.X_idx_sorted = X_idx_sorted
            self.X_idx_sorted_ptr = <INT32_t*> self.X_idx_sorted.data
            self.X_idx_sorted_stride = (<SIZE_t> self.X_idx_sorted.strides[1] /
                                        <SIZE_t> self.X_idx_sorted.itemsize)
            self.n_total_samples = X.shape[0]
            safe_realloc(&self.sample_mask, self.n_total_samples)
            memset(self.sample_mask, 0, self.n_total_samples*sizeof(SIZE_t))
        return 0

cdef class BestSplitter(BaseDenseSplitter):
        def __reduce__(self):
        return (BestSplitter, (self.criterion,
                               self.max_features,
                               self.min_samples_leaf,
                               self.min_weight_leaf,
                               self.random_state,
                               self.presort), self.__getstate__())
    cdef int node_split(self, double impurity, SplitRecord* split,
                        SIZE_t* n_constant_features) nogil except -1:
                        cdef SIZE_t* samples = self.samples
        cdef SIZE_t start = self.start
        cdef SIZE_t end = self.end
        cdef SIZE_t* features = self.features
        cdef SIZE_t* constant_features = self.constant_features
        cdef SIZE_t n_features = self.n_features
        cdef DTYPE_t* X = self.X
        cdef DTYPE_t* Xf = self.feature_values
        cdef SIZE_t X_sample_stride = self.X_sample_stride
        cdef SIZE_t X_feature_stride = self.X_feature_stride
        cdef SIZE_t max_features = self.max_features
        cdef SIZE_t min_samples_leaf = self.min_samples_leaf
        cdef double min_weight_leaf = self.min_weight_leaf
        cdef UINT32_t* random_state = &self.rand_r_state
        cdef INT32_t* X_idx_sorted = self.X_idx_sorted_ptr
        cdef SIZE_t* sample_mask = self.sample_mask
        cdef SplitRecord best, current
        cdef double current_proxy_improvement = -INFINITY
        cdef double best_proxy_improvement = -INFINITY
        cdef SIZE_t f_i = n_features
        cdef SIZE_t f_j
        cdef SIZE_t tmp
        cdef SIZE_t p
        cdef SIZE_t feature_idx_offset
        cdef SIZE_t feature_offset
        cdef SIZE_t i
        cdef SIZE_t j
        cdef SIZE_t n_visited_features = 0
                cdef SIZE_t n_found_constants = 0
                cdef SIZE_t n_drawn_constants = 0
        cdef SIZE_t n_known_constants = n_constant_features[0]
                cdef SIZE_t n_total_constants = n_known_constants
        cdef DTYPE_t current_feature_value
        cdef SIZE_t partition_end
        _init_split(&best, end)
        if self.presort == 1:
            for p in range(start, end):
                sample_mask[samples[p]] = 1
                                                                                while (f_i > n_total_constants and                                                              (n_visited_features < max_features or
                                  n_visited_features <= n_found_constants + n_drawn_constants)):
            n_visited_features += 1
                                                                                                                        
                        f_j = rand_int(n_drawn_constants, f_i - n_found_constants,
                           random_state)
            if f_j < n_known_constants:
                                tmp = features[f_j]
                features[f_j] = features[n_drawn_constants]
                features[n_drawn_constants] = tmp
                n_drawn_constants += 1
            else:
                                f_j += n_found_constants
                                current.feature = features[f_j]
                feature_offset = self.X_feature_stride * current.feature
                                                                                if self.presort == 1:
                    p = start
                    feature_idx_offset = self.X_idx_sorted_stride * current.feature
                    for i in range(self.n_total_samples): 
                        j = X_idx_sorted[i + feature_idx_offset]
                        if sample_mask[j] == 1:
                            samples[p] = j
                            Xf[p] = X[self.X_sample_stride * j + feature_offset]
                            p += 1
                else:
                    for i in range(start, end):
                        Xf[i] = X[self.X_sample_stride * samples[i] + feature_offset]
                    sort(Xf + start, samples + start, end - start)
                if Xf[end - 1] <= Xf[start] + FEATURE_THRESHOLD:
                    features[f_j] = features[n_total_constants]
                    features[n_total_constants] = current.feature
                    n_found_constants += 1
                    n_total_constants += 1
                else:
                    f_i -= 1
                    features[f_i], features[f_j] = features[f_j], features[f_i]
                                        self.criterion.reset()
                    p = start
                    while p < end:
                        while (p + 1 < end and
                               Xf[p + 1] <= Xf[p] + FEATURE_THRESHOLD):
                            p += 1
                                                                        p += 1
                                                
                        if p < end:
                            current.pos = p
                                                        if (((current.pos - start) < min_samples_leaf) or
                                    ((end - current.pos) < min_samples_leaf)):
                                continue
                            self.criterion.update(current.pos)
                                                        if ((self.criterion.weighted_n_left < min_weight_leaf) or
                                    (self.criterion.weighted_n_right < min_weight_leaf)):
                                continue
                            current_proxy_improvement = self.criterion.proxy_impurity_improvement()
                            if current_proxy_improvement > best_proxy_improvement:
                                best_proxy_improvement = current_proxy_improvement
                                current.threshold = (Xf[p - 1] + Xf[p]) / 2.0
                                if current.threshold == Xf[p]:
                                    current.threshold = Xf[p - 1]
                                best = current  
                if best.pos < end:
            feature_offset = X_feature_stride * best.feature
            partition_end = end
            p = start
            while p < partition_end:
                if X[X_sample_stride * samples[p] + feature_offset] <= best.threshold:
                    p += 1
                else:
                    partition_end -= 1
                    tmp = samples[partition_end]
                    samples[partition_end] = samples[p]
                    samples[p] = tmp
            self.criterion.reset()
            self.criterion.update(best.pos)
            best.improvement = self.criterion.impurity_improvement(impurity)
            self.criterion.children_impurity(&best.impurity_left,
                                             &best.impurity_right)
                if self.presort == 1:
            for p in range(start, end):
                sample_mask[samples[p]] = 0
                                memcpy(features, constant_features, sizeof(SIZE_t) * n_known_constants)
                memcpy(constant_features + n_known_constants,
               features + n_known_constants,
               sizeof(SIZE_t) * n_found_constants)
                split[0] = best
        n_constant_features[0] = n_total_constants
        return 0

cdef inline void sort(DTYPE_t* Xf, SIZE_t* samples, SIZE_t n) nogil:
    cdef int maxd = 2 * <int>log(n)
    introsort(Xf, samples, n, maxd)

cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples,
        SIZE_t i, SIZE_t j) nogil:
        Xf[i], Xf[j] = Xf[j], Xf[i]
    samples[i], samples[j] = samples[j], samples[i]

cdef inline DTYPE_t median3(DTYPE_t* Xf, SIZE_t n) nogil:
            cdef DTYPE_t a = Xf[0], b = Xf[n / 2], c = Xf[n - 1]
    if a < b:
        if b < c:
            return b
        elif a < c:
            return c
        else:
            return a
    elif b < c:
        if a < c:
            return a
        else:
            return c
    else:
        return b

cdef void introsort(DTYPE_t* Xf, SIZE_t *samples,
                    SIZE_t n, int maxd) nogil:
    cdef DTYPE_t pivot
    cdef SIZE_t i, l, r
    while n > 1:
        if maxd <= 0:               heapsort(Xf, samples, n)
            return
        maxd -= 1
        pivot = median3(Xf, n)
                i = l = 0
        r = n
        while i < r:
            if Xf[i] < pivot:
                swap(Xf, samples, i, l)
                i += 1
                l += 1
            elif Xf[i] > pivot:
                r -= 1
                swap(Xf, samples, i, r)
            else:
                i += 1
        introsort(Xf, samples, l, maxd)
        Xf += r
        samples += r
        n -= r

cdef inline void sift_down(DTYPE_t* Xf, SIZE_t* samples,
                           SIZE_t start, SIZE_t end) nogil:
        cdef SIZE_t child, maxind, root
    root = start
    while True:
        child = root * 2 + 1
                maxind = root
        if child < end and Xf[maxind] < Xf[child]:
            maxind = child
        if child + 1 < end and Xf[maxind] < Xf[child + 1]:
            maxind = child + 1
        if maxind == root:
            break
        else:
            swap(Xf, samples, root, maxind)
            root = maxind

cdef void heapsort(DTYPE_t* Xf, SIZE_t* samples, SIZE_t n) nogil:
    cdef SIZE_t start, end
        start = (n - 2) / 2
    end = n
    while True:
        sift_down(Xf, samples, start, end)
        if start == 0:
            break
        start -= 1
        end = n - 1
    while end > 0:
        swap(Xf, samples, 0, end)
        sift_down(Xf, samples, 0, end)
        end = end - 1

cdef class RandomSplitter(BaseDenseSplitter):
        def __reduce__(self):
        return (RandomSplitter, (self.criterion,
                                 self.max_features,
                                 self.min_samples_leaf,
                                 self.min_weight_leaf,
                                 self.random_state,
                                 self.presort), self.__getstate__())
    cdef int node_split(self, double impurity, SplitRecord* split,
                        SIZE_t* n_constant_features) nogil except -1:
                        cdef SIZE_t* samples = self.samples
        cdef SIZE_t start = self.start
        cdef SIZE_t end = self.end
        cdef SIZE_t* features = self.features
        cdef SIZE_t* constant_features = self.constant_features
        cdef SIZE_t n_features = self.n_features
        cdef DTYPE_t* X = self.X
        cdef DTYPE_t* Xf = self.feature_values
        cdef SIZE_t X_sample_stride = self.X_sample_stride
        cdef SIZE_t X_feature_stride = self.X_feature_stride
        cdef SIZE_t max_features = self.max_features
        cdef SIZE_t min_samples_leaf = self.min_samples_leaf
        cdef double min_weight_leaf = self.min_weight_leaf
        cdef UINT32_t* random_state = &self.rand_r_state
        cdef SplitRecord best, current
        cdef double current_proxy_improvement = - INFINITY
        cdef double best_proxy_improvement = - INFINITY
        cdef SIZE_t f_i = n_features
        cdef SIZE_t f_j
        cdef SIZE_t p
        cdef SIZE_t tmp
        cdef SIZE_t feature_stride
                cdef SIZE_t n_found_constants = 0
                cdef SIZE_t n_drawn_constants = 0
        cdef SIZE_t n_known_constants = n_constant_features[0]
                cdef SIZE_t n_total_constants = n_known_constants
        cdef SIZE_t n_visited_features = 0
        cdef DTYPE_t min_feature_value
        cdef DTYPE_t max_feature_value
        cdef DTYPE_t current_feature_value
        cdef SIZE_t partition_end
        _init_split(&best, end)
                                                                                while (f_i > n_total_constants and                                                              (n_visited_features < max_features or
                                  n_visited_features <= n_found_constants + n_drawn_constants)):
            n_visited_features += 1
                                                                                                                        
                        f_j = rand_int(n_drawn_constants, f_i - n_found_constants,
                           random_state)
            if f_j < n_known_constants:
                                tmp = features[f_j]
                features[f_j] = features[n_drawn_constants]
                features[n_drawn_constants] = tmp
                n_drawn_constants += 1
            else:
                                f_j += n_found_constants
                
                current.feature = features[f_j]
                feature_stride = X_feature_stride * current.feature
                                min_feature_value = X[X_sample_stride * samples[start] + feature_stride]
                max_feature_value = min_feature_value
                Xf[start] = min_feature_value
                for p in range(start + 1, end):
                    current_feature_value = X[X_sample_stride * samples[p] + feature_stride]
                    Xf[p] = current_feature_value
                    if current_feature_value < min_feature_value:
                        min_feature_value = current_feature_value
                    elif current_feature_value > max_feature_value:
                        max_feature_value = current_feature_value
                if max_feature_value <= min_feature_value + FEATURE_THRESHOLD:
                    features[f_j] = features[n_total_constants]
                    features[n_total_constants] = current.feature
                    n_found_constants += 1
                    n_total_constants += 1
                else:
                    f_i -= 1
                    features[f_i], features[f_j] = features[f_j], features[f_i]
                                        current.threshold = rand_uniform(min_feature_value,
                                                     max_feature_value,
                                                     random_state)
                    if current.threshold == max_feature_value:
                        current.threshold = min_feature_value
                                        partition_end = end
                    p = start
                    while p < partition_end:
                        current_feature_value = Xf[p]
                        if current_feature_value <= current.threshold:
                            p += 1
                        else:
                            partition_end -= 1
                            Xf[p] = Xf[partition_end]
                            Xf[partition_end] = current_feature_value
                            tmp = samples[partition_end]
                            samples[partition_end] = samples[p]
                            samples[p] = tmp
                    current.pos = partition_end
                                        if (((current.pos - start) < min_samples_leaf) or
                            ((end - current.pos) < min_samples_leaf)):
                        continue
                                        self.criterion.reset()
                    self.criterion.update(current.pos)
                                        if ((self.criterion.weighted_n_left < min_weight_leaf) or
                            (self.criterion.weighted_n_right < min_weight_leaf)):
                        continue
                    current_proxy_improvement = self.criterion.proxy_impurity_improvement()
                    if current_proxy_improvement > best_proxy_improvement:
                        best_proxy_improvement = current_proxy_improvement
                        best = current  
                feature_stride = X_feature_stride * best.feature
        if best.pos < end:
            if current.feature != best.feature:
                partition_end = end
                p = start
                while p < partition_end:
                    if X[X_sample_stride * samples[p] + feature_stride] <= best.threshold:
                        p += 1
                    else:
                        partition_end -= 1
                        tmp = samples[partition_end]
                        samples[partition_end] = samples[p]
                        samples[p] = tmp

            self.criterion.reset()
            self.criterion.update(best.pos)
            best.improvement = self.criterion.impurity_improvement(impurity)
            self.criterion.children_impurity(&best.impurity_left,
                                             &best.impurity_right)
                                memcpy(features, constant_features, sizeof(SIZE_t) * n_known_constants)
                memcpy(constant_features + n_known_constants,
               features + n_known_constants,
               sizeof(SIZE_t) * n_found_constants)
                split[0] = best
        n_constant_features[0] = n_total_constants
        return 0

cdef class BaseSparseSplitter(Splitter):
        cdef DTYPE_t* X_data
    cdef INT32_t* X_indices
    cdef INT32_t* X_indptr
    cdef SIZE_t n_total_samples
    cdef SIZE_t* index_to_samples
    cdef SIZE_t* sorted_samples
    def __cinit__(self, Criterion criterion, SIZE_t max_features,
                  SIZE_t min_samples_leaf, double min_weight_leaf,
                  object random_state, bint presort):
        
        self.X_data = NULL
        self.X_indices = NULL
        self.X_indptr = NULL
        self.n_total_samples = 0
        self.index_to_samples = NULL
        self.sorted_samples = NULL
    def __dealloc__(self):
                free(self.index_to_samples)
        free(self.sorted_samples)
    cdef int init(self,
                  object X,
                  np.ndarray[DOUBLE_t, ndim=2, mode="c"] y,
                  DOUBLE_t* sample_weight,
                  np.ndarray X_idx_sorted=None) except -1:
                        Splitter.init(self, X, y, sample_weight)
        if not isinstance(X, csc_matrix):
            raise ValueError("X should be in csc format")
        cdef SIZE_t* samples = self.samples
        cdef SIZE_t n_samples = self.n_samples
                cdef np.ndarray[dtype=DTYPE_t, ndim=1] data = X.data
        cdef np.ndarray[dtype=INT32_t, ndim=1] indices = X.indices
        cdef np.ndarray[dtype=INT32_t, ndim=1] indptr = X.indptr
        cdef SIZE_t n_total_samples = X.shape[0]
        self.X_data = <DTYPE_t*> data.data
        self.X_indices = <INT32_t*> indices.data
        self.X_indptr = <INT32_t*> indptr.data
        self.n_total_samples = n_total_samples
                safe_realloc(&self.index_to_samples, n_total_samples)
        safe_realloc(&self.sorted_samples, n_samples)
        cdef SIZE_t* index_to_samples = self.index_to_samples
        cdef SIZE_t p
        for p in range(n_total_samples):
            index_to_samples[p] = -1
        for p in range(n_samples):
            index_to_samples[samples[p]] = p
        return 0
    cdef inline SIZE_t _partition(self, double threshold,
                                  SIZE_t end_negative, SIZE_t start_positive,
                                  SIZE_t zero_pos) nogil:
        
        cdef double value
        cdef SIZE_t partition_end
        cdef SIZE_t p
        cdef DTYPE_t* Xf = self.feature_values
        cdef SIZE_t* samples = self.samples
        cdef SIZE_t* index_to_samples = self.index_to_samples
        if threshold < 0.:
            p = self.start
            partition_end = end_negative
        elif threshold > 0.:
            p = start_positive
            partition_end = self.end
        else:
                        return zero_pos
        while p < partition_end:
            value = Xf[p]
            if value <= threshold:
                p += 1
            else:
                partition_end -= 1
                Xf[p] = Xf[partition_end]
                Xf[partition_end] = value
                sparse_swap(index_to_samples, samples, p, partition_end)
        return partition_end
    cdef inline void extract_nnz(self, SIZE_t feature,
                                 SIZE_t* end_negative, SIZE_t* start_positive,
                                 bint* is_samples_sorted) nogil:
                cdef SIZE_t indptr_start = self.X_indptr[feature],
        cdef SIZE_t indptr_end = self.X_indptr[feature + 1]
        cdef SIZE_t n_indices = <SIZE_t>(indptr_end - indptr_start)
        cdef SIZE_t n_samples = self.end - self.start
                                                if ((1 - is_samples_sorted[0]) * n_samples * log(n_samples) +
                n_samples * log(n_indices) < EXTRACT_NNZ_SWITCH * n_indices):
            extract_nnz_binary_search(self.X_indices, self.X_data,
                                      indptr_start, indptr_end,
                                      self.samples, self.start, self.end,
                                      self.index_to_samples,
                                      self.feature_values,
                                      end_negative, start_positive,
                                      self.sorted_samples, is_samples_sorted)
                        else:
            extract_nnz_index_to_samples(self.X_indices, self.X_data,
                                         indptr_start, indptr_end,
                                         self.samples, self.start, self.end,
                                         self.index_to_samples,
                                         self.feature_values,
                                         end_negative, start_positive)

cdef int compare_SIZE_t(const void* a, const void* b) nogil:
        return <int>((<SIZE_t*>a)[0] - (<SIZE_t*>b)[0])

cdef inline void binary_search(INT32_t* sorted_array,
                               INT32_t start, INT32_t end,
                               SIZE_t value, SIZE_t* index,
                               INT32_t* new_start) nogil:
        cdef INT32_t pivot
    index[0] = -1
    while start < end:
        pivot = start + (end - start) / 2
        if sorted_array[pivot] == value:
            index[0] = pivot
            start = pivot + 1
            break
        if sorted_array[pivot] < value:
            start = pivot + 1
        else:
            end = pivot
    new_start[0] = start

cdef inline void extract_nnz_index_to_samples(INT32_t* X_indices,
                                              DTYPE_t* X_data,
                                              INT32_t indptr_start,
                                              INT32_t indptr_end,
                                              SIZE_t* samples,
                                              SIZE_t start,
                                              SIZE_t end,
                                              SIZE_t* index_to_samples,
                                              DTYPE_t* Xf,
                                              SIZE_t* end_negative,
                                              SIZE_t* start_positive) nogil:
        cdef INT32_t k
    cdef SIZE_t index
    cdef SIZE_t end_negative_ = start
    cdef SIZE_t start_positive_ = end
    for k in range(indptr_start, indptr_end):
        if start <= index_to_samples[X_indices[k]] < end:
            if X_data[k] > 0:
                start_positive_ -= 1
                Xf[start_positive_] = X_data[k]
                index = index_to_samples[X_indices[k]]
                sparse_swap(index_to_samples, samples, index, start_positive_)

            elif X_data[k] < 0:
                Xf[end_negative_] = X_data[k]
                index = index_to_samples[X_indices[k]]
                sparse_swap(index_to_samples, samples, index, end_negative_)
                end_negative_ += 1
        end_negative[0] = end_negative_
    start_positive[0] = start_positive_

cdef inline void extract_nnz_binary_search(INT32_t* X_indices,
                                           DTYPE_t* X_data,
                                           INT32_t indptr_start,
                                           INT32_t indptr_end,
                                           SIZE_t* samples,
                                           SIZE_t start,
                                           SIZE_t end,
                                           SIZE_t* index_to_samples,
                                           DTYPE_t* Xf,
                                           SIZE_t* end_negative,
                                           SIZE_t* start_positive,
                                           SIZE_t* sorted_samples,
                                           bint* is_samples_sorted) nogil:
        cdef SIZE_t n_samples
    if not is_samples_sorted[0]:
        n_samples = end - start
        memcpy(sorted_samples + start, samples + start,
               n_samples * sizeof(SIZE_t))
        qsort(sorted_samples + start, n_samples, sizeof(SIZE_t),
              compare_SIZE_t)
        is_samples_sorted[0] = 1
    while (indptr_start < indptr_end and
           sorted_samples[start] > X_indices[indptr_start]):
        indptr_start += 1
    while (indptr_start < indptr_end and
           sorted_samples[end - 1] < X_indices[indptr_end - 1]):
        indptr_end -= 1
    cdef SIZE_t p = start
    cdef SIZE_t index
    cdef SIZE_t k
    cdef SIZE_t end_negative_ = start
    cdef SIZE_t start_positive_ = end
    while (p < end and indptr_start < indptr_end):
                binary_search(X_indices, indptr_start, indptr_end,
                      sorted_samples[p], &k, &indptr_start)
        if k != -1:
             
            if X_data[k] > 0:
                start_positive_ -= 1
                Xf[start_positive_] = X_data[k]
                index = index_to_samples[X_indices[k]]
                sparse_swap(index_to_samples, samples, index, start_positive_)

            elif X_data[k] < 0:
                Xf[end_negative_] = X_data[k]
                index = index_to_samples[X_indices[k]]
                sparse_swap(index_to_samples, samples, index, end_negative_)
                end_negative_ += 1
        p += 1
        end_negative[0] = end_negative_
    start_positive[0] = start_positive_

cdef inline void sparse_swap(SIZE_t* index_to_samples, SIZE_t* samples,
                             SIZE_t pos_1, SIZE_t pos_2) nogil:
        samples[pos_1], samples[pos_2] =  samples[pos_2], samples[pos_1]
    index_to_samples[samples[pos_1]] = pos_1
    index_to_samples[samples[pos_2]] = pos_2

cdef class BestSparseSplitter(BaseSparseSplitter):
    
    def __reduce__(self):
        return (BestSparseSplitter, (self.criterion,
                                     self.max_features,
                                     self.min_samples_leaf,
                                     self.min_weight_leaf,
                                     self.random_state,
                                     self.presort), self.__getstate__())
    cdef int node_split(self, double impurity, SplitRecord* split,
                        SIZE_t* n_constant_features) nogil except -1:
                        cdef SIZE_t* samples = self.samples
        cdef SIZE_t start = self.start
        cdef SIZE_t end = self.end
        cdef INT32_t* X_indices = self.X_indices
        cdef INT32_t* X_indptr = self.X_indptr
        cdef DTYPE_t* X_data = self.X_data
        cdef SIZE_t* features = self.features
        cdef SIZE_t* constant_features = self.constant_features
        cdef SIZE_t n_features = self.n_features
        cdef DTYPE_t* Xf = self.feature_values
        cdef SIZE_t* sorted_samples = self.sorted_samples
        cdef SIZE_t* index_to_samples = self.index_to_samples
        cdef SIZE_t max_features = self.max_features
        cdef SIZE_t min_samples_leaf = self.min_samples_leaf
        cdef double min_weight_leaf = self.min_weight_leaf
        cdef UINT32_t* random_state = &self.rand_r_state
        cdef SplitRecord best, current
        _init_split(&best, end)
        cdef double current_proxy_improvement = - INFINITY
        cdef double best_proxy_improvement = - INFINITY
        cdef SIZE_t f_i = n_features
        cdef SIZE_t f_j, p, tmp
        cdef SIZE_t n_visited_features = 0
                cdef SIZE_t n_found_constants = 0
                cdef SIZE_t n_drawn_constants = 0
        cdef SIZE_t n_known_constants = n_constant_features[0]
                cdef SIZE_t n_total_constants = n_known_constants
        cdef DTYPE_t current_feature_value
        cdef SIZE_t p_next
        cdef SIZE_t p_prev
        cdef bint is_samples_sorted = 0                                           
                        cdef SIZE_t start_positive
        cdef SIZE_t end_negative
                                                                                while (f_i > n_total_constants and                                                              (n_visited_features < max_features or
                                  n_visited_features <= n_found_constants + n_drawn_constants)):
            n_visited_features += 1
                                                                                                                        
                        f_j = rand_int(n_drawn_constants, f_i - n_found_constants,
                           random_state)
            if f_j < n_known_constants:
                                tmp = features[f_j]
                features[f_j] = features[n_drawn_constants]
                features[n_drawn_constants] = tmp
                n_drawn_constants += 1
            else:
                                f_j += n_found_constants
                
                current.feature = features[f_j]
                self.extract_nnz(current.feature,
                                 &end_negative, &start_positive,
                                 &is_samples_sorted)
                                sort(Xf + start, samples + start, end_negative - start)
                sort(Xf + start_positive, samples + start_positive,
                     end - start_positive)
                                for p in range(start, end_negative):
                    index_to_samples[samples[p]] = p
                for p in range(start_positive, end):
                    index_to_samples[samples[p]] = p
                                if end_negative < start_positive:
                    start_positive -= 1
                    Xf[start_positive] = 0.
                    if end_negative != start_positive:
                        Xf[end_negative] = 0.
                        end_negative += 1
                if Xf[end - 1] <= Xf[start] + FEATURE_THRESHOLD:
                    features[f_j] = features[n_total_constants]
                    features[n_total_constants] = current.feature
                    n_found_constants += 1
                    n_total_constants += 1
                else:
                    f_i -= 1
                    features[f_i], features[f_j] = features[f_j], features[f_i]
                                        self.criterion.reset()
                    p = start
                    while p < end:
                        if p + 1 != end_negative:
                            p_next = p + 1
                        else:
                            p_next = start_positive
                        while (p_next < end and
                               Xf[p_next] <= Xf[p] + FEATURE_THRESHOLD):
                            p = p_next
                            if p + 1 != end_negative:
                                p_next = p + 1
                            else:
                                p_next = start_positive

                                                                        p_prev = p
                        p = p_next
                                                
                        if p < end:
                            current.pos = p
                                                        if (((current.pos - start) < min_samples_leaf) or
                                    ((end - current.pos) < min_samples_leaf)):
                                continue
                            self.criterion.update(current.pos)
                                                        if ((self.criterion.weighted_n_left < min_weight_leaf) or
                                    (self.criterion.weighted_n_right < min_weight_leaf)):
                                continue
                            current_proxy_improvement = self.criterion.proxy_impurity_improvement()
                            if current_proxy_improvement > best_proxy_improvement:
                                best_proxy_improvement = current_proxy_improvement
                                current.threshold = (Xf[p_prev] + Xf[p]) / 2.0
                                if current.threshold == Xf[p]:
                                    current.threshold = Xf[p_prev]
                                best = current
                if best.pos < end:
            self.extract_nnz(best.feature, &end_negative, &start_positive,
                             &is_samples_sorted)
            self._partition(best.threshold, end_negative, start_positive,
                            best.pos)
            self.criterion.reset()
            self.criterion.update(best.pos)
            best.improvement = self.criterion.impurity_improvement(impurity)
            self.criterion.children_impurity(&best.impurity_left,
                                             &best.impurity_right)
                                memcpy(features, constant_features, sizeof(SIZE_t) * n_known_constants)
                memcpy(constant_features + n_known_constants,
               features + n_known_constants,
               sizeof(SIZE_t) * n_found_constants)
                split[0] = best
        n_constant_features[0] = n_total_constants
        return 0

cdef class RandomSparseSplitter(BaseSparseSplitter):
    
    def __reduce__(self):
        return (RandomSparseSplitter, (self.criterion,
                                       self.max_features,
                                       self.min_samples_leaf,
                                       self.min_weight_leaf,
                                       self.random_state,
                                       self.presort), self.__getstate__())
    cdef int node_split(self, double impurity, SplitRecord* split,
                        SIZE_t* n_constant_features) nogil except -1:
                        cdef SIZE_t* samples = self.samples
        cdef SIZE_t start = self.start
        cdef SIZE_t end = self.end
        cdef INT32_t* X_indices = self.X_indices
        cdef INT32_t* X_indptr = self.X_indptr
        cdef DTYPE_t* X_data = self.X_data
        cdef SIZE_t* features = self.features
        cdef SIZE_t* constant_features = self.constant_features
        cdef SIZE_t n_features = self.n_features
        cdef DTYPE_t* Xf = self.feature_values
        cdef SIZE_t* sorted_samples = self.sorted_samples
        cdef SIZE_t* index_to_samples = self.index_to_samples
        cdef SIZE_t max_features = self.max_features
        cdef SIZE_t min_samples_leaf = self.min_samples_leaf
        cdef double min_weight_leaf = self.min_weight_leaf
        cdef UINT32_t* random_state = &self.rand_r_state
        cdef SplitRecord best, current
        _init_split(&best, end)
        cdef double current_proxy_improvement = - INFINITY
        cdef double best_proxy_improvement = - INFINITY
        cdef DTYPE_t current_feature_value
        cdef SIZE_t f_i = n_features
        cdef SIZE_t f_j, p, tmp
        cdef SIZE_t n_visited_features = 0
                cdef SIZE_t n_found_constants = 0
                cdef SIZE_t n_drawn_constants = 0
        cdef SIZE_t n_known_constants = n_constant_features[0]
                cdef SIZE_t n_total_constants = n_known_constants
        cdef SIZE_t partition_end
        cdef DTYPE_t min_feature_value
        cdef DTYPE_t max_feature_value
        cdef bint is_samples_sorted = 0                                           
                        cdef SIZE_t start_positive
        cdef SIZE_t end_negative
                                                                                while (f_i > n_total_constants and                                                              (n_visited_features < max_features or
                                  n_visited_features <= n_found_constants + n_drawn_constants)):
            n_visited_features += 1
                                                                                                                        
                        f_j = rand_int(n_drawn_constants, f_i - n_found_constants,
                           random_state)
            if f_j < n_known_constants:
                                tmp = features[f_j]
                features[f_j] = features[n_drawn_constants]
                features[n_drawn_constants] = tmp
                n_drawn_constants += 1
            else:
                                f_j += n_found_constants
                
                current.feature = features[f_j]
                self.extract_nnz(current.feature,
                                 &end_negative, &start_positive,
                                 &is_samples_sorted)
                                if end_negative < start_positive:
                    start_positive -= 1
                    Xf[start_positive] = 0.
                    if end_negative != start_positive:
                        Xf[end_negative] = 0.
                        end_negative += 1
                                min_feature_value = Xf[start]
                max_feature_value = min_feature_value
                for p in range(start, end_negative):
                    current_feature_value = Xf[p]
                    if current_feature_value < min_feature_value:
                        min_feature_value = current_feature_value
                    elif current_feature_value > max_feature_value:
                        max_feature_value = current_feature_value
                                for p in range(start_positive, end):
                    current_feature_value = Xf[p]
                    if current_feature_value < min_feature_value:
                        min_feature_value = current_feature_value
                    elif current_feature_value > max_feature_value:
                        max_feature_value = current_feature_value
                if max_feature_value <= min_feature_value + FEATURE_THRESHOLD:
                    features[f_j] = features[n_total_constants]
                    features[n_total_constants] = current.feature
                    n_found_constants += 1
                    n_total_constants += 1
                else:
                    f_i -= 1
                    features[f_i], features[f_j] = features[f_j], features[f_i]
                                        current.threshold = rand_uniform(min_feature_value,
                                                     max_feature_value,
                                                     random_state)
                    if current.threshold == max_feature_value:
                        current.threshold = min_feature_value
                                        current.pos = self._partition(current.threshold,
                                                  end_negative,
                                                  start_positive,
                                                  start_positive +
                                                  (Xf[start_positive] == 0.))
                                        if (((current.pos - start) < min_samples_leaf) or
                            ((end - current.pos) < min_samples_leaf)):
                        continue
                                        self.criterion.reset()
                    self.criterion.update(current.pos)
                                        if ((self.criterion.weighted_n_left < min_weight_leaf) or
                            (self.criterion.weighted_n_right < min_weight_leaf)):
                        continue
                    current_proxy_improvement = self.criterion.proxy_impurity_improvement()
                    if current_proxy_improvement > best_proxy_improvement:
                        best_proxy_improvement = current_proxy_improvement
                        current.improvement = self.criterion.impurity_improvement(impurity)
                        self.criterion.children_impurity(&current.impurity_left,
                                                         &current.impurity_right)
                        best = current
                if best.pos < end:
            if current.feature != best.feature:
                self.extract_nnz(best.feature, &end_negative, &start_positive,
                                 &is_samples_sorted)
                self._partition(best.threshold, end_negative, start_positive,
                                best.pos)
            self.criterion.reset()
            self.criterion.update(best.pos)
            best.improvement = self.criterion.impurity_improvement(impurity)
            self.criterion.children_impurity(&best.impurity_left,
                                             &best.impurity_right)
                                memcpy(features, constant_features, sizeof(SIZE_t) * n_known_constants)
                memcpy(constant_features + n_known_constants,
               features + n_known_constants,
               sizeof(SIZE_t) * n_found_constants)
                split[0] = best
        n_constant_features[0] = n_total_constants
        return 0

from cpython cimport Py_INCREF, PyObject
from libc.stdlib cimport free
from libc.string cimport memcpy
from libc.string cimport memset
import numpy as np
cimport numpy as np
np.import_array()
from scipy.sparse import issparse
from scipy.sparse import csc_matrix
from scipy.sparse import csr_matrix
from ._utils cimport Stack
from ._utils cimport StackRecord
from ._utils cimport PriorityHeap
from ._utils cimport PriorityHeapRecord
from ._utils cimport safe_realloc
from ._utils cimport sizet_ptr_to_ndarray
cdef extern from "numpy/arrayobject.h":
    object PyArray_NewFromDescr(object subtype, np.dtype descr,
                                int nd, np.npy_intp* dims,
                                np.npy_intp* strides,
                                void* data, int flags, object obj)

from numpy import float32 as DTYPE
from numpy import float64 as DOUBLE
cdef double INFINITY = np.inf
cdef int IS_FIRST = 1
cdef int IS_NOT_FIRST = 0
cdef int IS_LEFT = 1
cdef int IS_NOT_LEFT = 0
TREE_LEAF = -1
TREE_UNDEFINED = -2
cdef SIZE_t _TREE_LEAF = TREE_LEAF
cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED
cdef SIZE_t INITIAL_STACK_SIZE = 10
NODE_DTYPE = np.dtype({
    'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity',
              'n_node_samples', 'weighted_n_node_samples'],
    'formats': [np.intp, np.intp, np.intp, np.float64, np.float64, np.intp,
                np.float64],
    'offsets': [
        <Py_ssize_t> &(<Node*> NULL).left_child,
        <Py_ssize_t> &(<Node*> NULL).right_child,
        <Py_ssize_t> &(<Node*> NULL).feature,
        <Py_ssize_t> &(<Node*> NULL).threshold,
        <Py_ssize_t> &(<Node*> NULL).impurity,
        <Py_ssize_t> &(<Node*> NULL).n_node_samples,
        <Py_ssize_t> &(<Node*> NULL).weighted_n_node_samples
    ]
})

cdef class TreeBuilder:
    
    cpdef build(self, Tree tree, object X, np.ndarray y,
                np.ndarray sample_weight=None,
                np.ndarray X_idx_sorted=None):
                pass
    cdef inline _check_input(self, object X, np.ndarray y,
                             np.ndarray sample_weight):
                if issparse(X):
            X = X.tocsc()
            X.sort_indices()
            if X.data.dtype != DTYPE:
                X.data = np.ascontiguousarray(X.data, dtype=DTYPE)
            if X.indices.dtype != np.int32 or X.indptr.dtype != np.int32:
                raise ValueError("No support for np.int64 index based "
                                 "sparse matrices")
        elif X.dtype != DTYPE:
                        X = np.asfortranarray(X, dtype=DTYPE)
        if y.dtype != DOUBLE or not y.flags.contiguous:
            y = np.ascontiguousarray(y, dtype=DOUBLE)
        if (sample_weight is not None and
            (sample_weight.dtype != DOUBLE or
            not sample_weight.flags.contiguous)):
                sample_weight = np.asarray(sample_weight, dtype=DOUBLE,
                                           order="C")
        return X, y, sample_weight

cdef class DepthFirstTreeBuilder(TreeBuilder):
    
    def __cinit__(self, Splitter splitter, SIZE_t min_samples_split,
                  SIZE_t min_samples_leaf, double min_weight_leaf,
                  SIZE_t max_depth, double min_impurity_split):
        self.splitter = splitter
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_weight_leaf = min_weight_leaf
        self.max_depth = max_depth
        self.min_impurity_split = min_impurity_split
    cpdef build(self, Tree tree, object X, np.ndarray y,
                np.ndarray sample_weight=None,
                np.ndarray X_idx_sorted=None):
        
                X, y, sample_weight = self._check_input(X, y, sample_weight)
        cdef DOUBLE_t* sample_weight_ptr = NULL
        if sample_weight is not None:
            sample_weight_ptr = <DOUBLE_t*> sample_weight.data
                cdef int init_capacity
        if tree.max_depth <= 10:
            init_capacity = (2 ** (tree.max_depth + 1)) - 1
        else:
            init_capacity = 2047
        tree._resize(init_capacity)
                cdef Splitter splitter = self.splitter
        cdef SIZE_t max_depth = self.max_depth
        cdef SIZE_t min_samples_leaf = self.min_samples_leaf
        cdef double min_weight_leaf = self.min_weight_leaf
        cdef SIZE_t min_samples_split = self.min_samples_split
        cdef double min_impurity_split = self.min_impurity_split
                splitter.init(X, y, sample_weight_ptr, X_idx_sorted)
        cdef SIZE_t start
        cdef SIZE_t end
        cdef SIZE_t depth
        cdef SIZE_t parent
        cdef bint is_left
        cdef SIZE_t n_node_samples = splitter.n_samples
        cdef double weighted_n_samples = splitter.weighted_n_samples
        cdef double weighted_n_node_samples
        cdef SplitRecord split
        cdef SIZE_t node_id
        cdef double threshold
        cdef double impurity = INFINITY
        cdef SIZE_t n_constant_features
        cdef bint is_leaf
        cdef bint first = 1
        cdef SIZE_t max_depth_seen = -1
        cdef int rc = 0
        cdef Stack stack = Stack(INITIAL_STACK_SIZE)
        cdef StackRecord stack_record
        with nogil:
                        rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)
            if rc == -1:
                                with gil:
                    raise MemoryError()
            while not stack.is_empty():
                stack.pop(&stack_record)
                start = stack_record.start
                end = stack_record.end
                depth = stack_record.depth
                parent = stack_record.parent
                is_left = stack_record.is_left
                impurity = stack_record.impurity
                n_constant_features = stack_record.n_constant_features
                n_node_samples = end - start
                splitter.node_reset(start, end, &weighted_n_node_samples)
                is_leaf = (depth >= max_depth or
                           n_node_samples < min_samples_split or
                           n_node_samples < 2 * min_samples_leaf or
                           weighted_n_node_samples < 2 * min_weight_leaf)
                if first:
                    impurity = splitter.node_impurity()
                    first = 0
                is_leaf = (is_leaf or
                           (impurity <= min_impurity_split))
                if not is_leaf:
                    splitter.node_split(impurity, &split, &n_constant_features)
                    is_leaf = is_leaf or (split.pos >= end)
                node_id = tree._add_node(parent, is_left, is_leaf, split.feature,
                                         split.threshold, impurity, n_node_samples,
                                         weighted_n_node_samples)
                if node_id == <SIZE_t>(-1):
                    rc = -1
                    break
                                                splitter.node_value(tree.value + node_id * tree.value_stride)
                if not is_leaf:
                                        rc = stack.push(split.pos, end, depth + 1, node_id, 0,
                                    split.impurity_right, n_constant_features)
                    if rc == -1:
                        break
                                        rc = stack.push(start, split.pos, depth + 1, node_id, 1,
                                    split.impurity_left, n_constant_features)
                    if rc == -1:
                        break
                if depth > max_depth_seen:
                    max_depth_seen = depth
            if rc >= 0:
                rc = tree._resize_c(tree.node_count)
            if rc >= 0:
                tree.max_depth = max_depth_seen
        if rc == -1:
            raise MemoryError()

cdef inline int _add_to_frontier(PriorityHeapRecord* rec,
                                 PriorityHeap frontier) nogil except -1:
        return frontier.push(rec.node_id, rec.start, rec.end, rec.pos, rec.depth,
                         rec.is_leaf, rec.improvement, rec.impurity,
                         rec.impurity_left, rec.impurity_right)

cdef class BestFirstTreeBuilder(TreeBuilder):
        cdef SIZE_t max_leaf_nodes
    def __cinit__(self, Splitter splitter, SIZE_t min_samples_split,
                  SIZE_t min_samples_leaf,  min_weight_leaf,
                  SIZE_t max_depth, SIZE_t max_leaf_nodes,
                  double min_impurity_split):
        self.splitter = splitter
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_weight_leaf = min_weight_leaf
        self.max_depth = max_depth
        self.max_leaf_nodes = max_leaf_nodes
        self.min_impurity_split = min_impurity_split
    cpdef build(self, Tree tree, object X, np.ndarray y,
                np.ndarray sample_weight=None,
                np.ndarray X_idx_sorted=None):
        
                X, y, sample_weight = self._check_input(X, y, sample_weight)
        cdef DOUBLE_t* sample_weight_ptr = NULL
        if sample_weight is not None:
            sample_weight_ptr = <DOUBLE_t*> sample_weight.data
                cdef Splitter splitter = self.splitter
        cdef SIZE_t max_leaf_nodes = self.max_leaf_nodes
        cdef SIZE_t min_samples_leaf = self.min_samples_leaf
        cdef double min_weight_leaf = self.min_weight_leaf
        cdef SIZE_t min_samples_split = self.min_samples_split
                splitter.init(X, y, sample_weight_ptr, X_idx_sorted)
        cdef PriorityHeap frontier = PriorityHeap(INITIAL_STACK_SIZE)
        cdef PriorityHeapRecord record
        cdef PriorityHeapRecord split_node_left
        cdef PriorityHeapRecord split_node_right
        cdef SIZE_t n_node_samples = splitter.n_samples
        cdef SIZE_t max_split_nodes = max_leaf_nodes - 1
        cdef bint is_leaf
        cdef SIZE_t max_depth_seen = -1
        cdef int rc = 0
        cdef Node* node
                cdef SIZE_t init_capacity = max_split_nodes + max_leaf_nodes
        tree._resize(init_capacity)
        with nogil:
                        rc = self._add_split_node(splitter, tree, 0, n_node_samples,
                                      INFINITY, IS_FIRST, IS_LEFT, NULL, 0,
                                      &split_node_left)
            if rc >= 0:
                rc = _add_to_frontier(&split_node_left, frontier)
            if rc == -1:
                with gil:
                    raise MemoryError()
            while not frontier.is_empty():
                frontier.pop(&record)
                node = &tree.nodes[record.node_id]
                is_leaf = (record.is_leaf or max_split_nodes <= 0)
                if is_leaf:
                                        node.left_child = _TREE_LEAF
                    node.right_child = _TREE_LEAF
                    node.feature = _TREE_UNDEFINED
                    node.threshold = _TREE_UNDEFINED
                else:
                    
                                        max_split_nodes -= 1
                                        rc = self._add_split_node(splitter, tree,
                                              record.start, record.pos,
                                              record.impurity_left,
                                              IS_NOT_FIRST, IS_LEFT, node,
                                              record.depth + 1,
                                              &split_node_left)
                    if rc == -1:
                        break
                                        node = &tree.nodes[record.node_id]
                                        rc = self._add_split_node(splitter, tree, record.pos,
                                              record.end,
                                              record.impurity_right,
                                              IS_NOT_FIRST, IS_NOT_LEFT, node,
                                              record.depth + 1,
                                              &split_node_right)
                    if rc == -1:
                        break
                                        rc = _add_to_frontier(&split_node_left, frontier)
                    if rc == -1:
                        break
                    rc = _add_to_frontier(&split_node_right, frontier)
                    if rc == -1:
                        break
                if record.depth > max_depth_seen:
                    max_depth_seen = record.depth
            if rc >= 0:
                rc = tree._resize_c(tree.node_count)
            if rc >= 0:
                tree.max_depth = max_depth_seen
        if rc == -1:
            raise MemoryError()
    cdef inline int _add_split_node(self, Splitter splitter, Tree tree,
                                    SIZE_t start, SIZE_t end, double impurity,
                                    bint is_first, bint is_left, Node* parent,
                                    SIZE_t depth,
                                    PriorityHeapRecord* res) nogil except -1:
                cdef SplitRecord split
        cdef SIZE_t node_id
        cdef SIZE_t n_node_samples
        cdef SIZE_t n_constant_features = 0
        cdef double weighted_n_samples = splitter.weighted_n_samples
        cdef double min_impurity_split = self.min_impurity_split
        cdef double weighted_n_node_samples
        cdef bint is_leaf
        cdef SIZE_t n_left, n_right
        cdef double imp_diff
        splitter.node_reset(start, end, &weighted_n_node_samples)
        if is_first:
            impurity = splitter.node_impurity()
        n_node_samples = end - start
        is_leaf = (depth > self.max_depth or
                   n_node_samples < self.min_samples_split or
                   n_node_samples < 2 * self.min_samples_leaf or
                   weighted_n_node_samples < 2 * self.min_weight_leaf or
                   impurity <= min_impurity_split)
        if not is_leaf:
            splitter.node_split(impurity, &split, &n_constant_features)
            is_leaf = is_leaf or (split.pos >= end)
        node_id = tree._add_node(parent - tree.nodes
                                 if parent != NULL
                                 else _TREE_UNDEFINED,
                                 is_left, is_leaf,
                                 split.feature, split.threshold, impurity, n_node_samples,
                                 weighted_n_node_samples)
        if node_id == <SIZE_t>(-1):
            return -1
                splitter.node_value(tree.value + node_id * tree.value_stride)
        res.node_id = node_id
        res.start = start
        res.end = end
        res.depth = depth
        res.impurity = impurity
        if not is_leaf:
                        res.pos = split.pos
            res.is_leaf = 0
            res.improvement = split.improvement
            res.impurity_left = split.impurity_left
            res.impurity_right = split.impurity_right
        else:
                        res.pos = end
            res.is_leaf = 1
            res.improvement = 0.0
            res.impurity_left = impurity
            res.impurity_right = impurity
        return 0

cdef class Tree:
                        property n_classes:
        def __get__(self):
            return sizet_ptr_to_ndarray(self.n_classes, self.n_outputs)
    property children_left:
        def __get__(self):
            return self._get_node_ndarray()['left_child'][:self.node_count]
    property children_right:
        def __get__(self):
            return self._get_node_ndarray()['right_child'][:self.node_count]
    property feature:
        def __get__(self):
            return self._get_node_ndarray()['feature'][:self.node_count]
    property threshold:
        def __get__(self):
            return self._get_node_ndarray()['threshold'][:self.node_count]
    property impurity:
        def __get__(self):
            return self._get_node_ndarray()['impurity'][:self.node_count]
    property n_node_samples:
        def __get__(self):
            return self._get_node_ndarray()['n_node_samples'][:self.node_count]
    property weighted_n_node_samples:
        def __get__(self):
            return self._get_node_ndarray()['weighted_n_node_samples'][:self.node_count]
    property value:
        def __get__(self):
            return self._get_value_ndarray()[:self.node_count]
    def __cinit__(self, int n_features, np.ndarray[SIZE_t, ndim=1] n_classes,
                  int n_outputs):
                        self.n_features = n_features
        self.n_outputs = n_outputs
        self.n_classes = NULL
        safe_realloc(&self.n_classes, n_outputs)
        self.max_n_classes = np.max(n_classes)
        self.value_stride = n_outputs * self.max_n_classes
        cdef SIZE_t k
        for k in range(n_outputs):
            self.n_classes[k] = n_classes[k]
                self.max_depth = 0
        self.node_count = 0
        self.capacity = 0
        self.value = NULL
        self.nodes = NULL
    def __dealloc__(self):
                        free(self.n_classes)
        free(self.value)
        free(self.nodes)
    def __reduce__(self):
                return (Tree, (self.n_features,
                       sizet_ptr_to_ndarray(self.n_classes, self.n_outputs),
                       self.n_outputs), self.__getstate__())
    def __getstate__(self):
                d = {}
                d["max_depth"] = self.max_depth
        d["node_count"] = self.node_count
        d["nodes"] = self._get_node_ndarray()
        d["values"] = self._get_value_ndarray()
        return d
    def __setstate__(self, d):
                self.max_depth = d["max_depth"]
        self.node_count = d["node_count"]
        if 'nodes' not in d:
            raise ValueError('You have loaded Tree version which '
                             'cannot be imported')
        node_ndarray = d['nodes']
        value_ndarray = d['values']
        value_shape = (node_ndarray.shape[0], self.n_outputs,
                       self.max_n_classes)
        if (node_ndarray.ndim != 1 or
                node_ndarray.dtype != NODE_DTYPE or
                not node_ndarray.flags.c_contiguous or
                value_ndarray.shape != value_shape or
                not value_ndarray.flags.c_contiguous or
                value_ndarray.dtype != np.float64):
            raise ValueError('Did not recognise loaded array layout')
        self.capacity = node_ndarray.shape[0]
        if self._resize_c(self.capacity) != 0:
            raise MemoryError("resizing tree to %d" % self.capacity)
        nodes = memcpy(self.nodes, (<np.ndarray> node_ndarray).data,
                       self.capacity * sizeof(Node))
        value = memcpy(self.value, (<np.ndarray> value_ndarray).data,
                       self.capacity * self.value_stride * sizeof(double))
    cdef int _resize(self, SIZE_t capacity) nogil except -1:
                if self._resize_c(capacity) != 0:
                        with gil:
                raise MemoryError()
            cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil except -1:
                if capacity == self.capacity and self.nodes != NULL:
            return 0
        if capacity == <SIZE_t>(-1):
            if self.capacity == 0:
                capacity = 3              else:
                capacity = 2 * self.capacity
        safe_realloc(&self.nodes, capacity)
        safe_realloc(&self.value, capacity * self.value_stride)
                if capacity > self.capacity:
            memset(<void*>(self.value + self.capacity * self.value_stride), 0,
                   (capacity - self.capacity) * self.value_stride *
                   sizeof(double))
                if capacity < self.node_count:
            self.node_count = capacity
        self.capacity = capacity
        return 0
    cdef SIZE_t _add_node(self, SIZE_t parent, bint is_left, bint is_leaf,
                          SIZE_t feature, double threshold, double impurity,
                          SIZE_t n_node_samples,
                          double weighted_n_node_samples) nogil except -1:
                cdef SIZE_t node_id = self.node_count
        if node_id >= self.capacity:
            if self._resize_c() != 0:
                return <SIZE_t>(-1)
        cdef Node* node = &self.nodes[node_id]
        node.impurity = impurity
        node.n_node_samples = n_node_samples
        node.weighted_n_node_samples = weighted_n_node_samples
        if parent != _TREE_UNDEFINED:
            if is_left:
                self.nodes[parent].left_child = node_id
            else:
                self.nodes[parent].right_child = node_id
        if is_leaf:
            node.left_child = _TREE_LEAF
            node.right_child = _TREE_LEAF
            node.feature = _TREE_UNDEFINED
            node.threshold = _TREE_UNDEFINED
        else:
                        node.feature = feature
            node.threshold = threshold
        self.node_count += 1
        return node_id
    cpdef np.ndarray predict(self, object X):
                out = self._get_value_ndarray().take(self.apply(X), axis=0,
                                             mode='clip')
        if self.n_outputs == 1:
            out = out.reshape(X.shape[0], self.max_n_classes)
        return out
    cpdef np.ndarray apply(self, object X):
                if issparse(X):
            return self._apply_sparse_csr(X)
        else:
            return self._apply_dense(X)
    cdef inline np.ndarray _apply_dense(self, object X):
        
                if not isinstance(X, np.ndarray):
            raise ValueError("X should be in np.ndarray format, got %s"
                             % type(X))
        if X.dtype != DTYPE:
            raise ValueError("X.dtype should be np.float32, got %s" % X.dtype)
                cdef np.ndarray X_ndarray = X
        cdef DTYPE_t* X_ptr = <DTYPE_t*> X_ndarray.data
        cdef SIZE_t X_sample_stride = <SIZE_t> X.strides[0] / <SIZE_t> X.itemsize
        cdef SIZE_t X_fx_stride = <SIZE_t> X.strides[1] / <SIZE_t> X.itemsize
        cdef SIZE_t n_samples = X.shape[0]
                cdef np.ndarray[SIZE_t] out = np.zeros((n_samples,), dtype=np.intp)
        cdef SIZE_t* out_ptr = <SIZE_t*> out.data
                cdef Node* node = NULL
        cdef SIZE_t i = 0
        with nogil:
            for i in range(n_samples):
                node = self.nodes
                                while node.left_child != _TREE_LEAF:
                                        if X_ptr[X_sample_stride * i +
                             X_fx_stride * node.feature] <= node.threshold:
                        node = &self.nodes[node.left_child]
                    else:
                        node = &self.nodes[node.right_child]
                out_ptr[i] = <SIZE_t>(node - self.nodes)  
        return out
    cdef inline np.ndarray _apply_sparse_csr(self, object X):
                        if not isinstance(X, csr_matrix):
            raise ValueError("X should be in csr_matrix format, got %s"
                             % type(X))
        if X.dtype != DTYPE:
            raise ValueError("X.dtype should be np.float32, got %s" % X.dtype)
                cdef np.ndarray[ndim=1, dtype=DTYPE_t] X_data_ndarray = X.data
        cdef np.ndarray[ndim=1, dtype=INT32_t] X_indices_ndarray  = X.indices
        cdef np.ndarray[ndim=1, dtype=INT32_t] X_indptr_ndarray  = X.indptr
        cdef DTYPE_t* X_data = <DTYPE_t*>X_data_ndarray.data
        cdef INT32_t* X_indices = <INT32_t*>X_indices_ndarray.data
        cdef INT32_t* X_indptr = <INT32_t*>X_indptr_ndarray.data
        cdef SIZE_t n_samples = X.shape[0]
        cdef SIZE_t n_features = X.shape[1]
                cdef np.ndarray[SIZE_t, ndim=1] out = np.zeros((n_samples,),
                                                       dtype=np.intp)
        cdef SIZE_t* out_ptr = <SIZE_t*> out.data
                cdef DTYPE_t feature_value = 0.
        cdef Node* node = NULL
        cdef DTYPE_t* X_sample = NULL
        cdef SIZE_t i = 0
        cdef INT32_t k = 0
                                cdef SIZE_t* feature_to_sample = NULL
        safe_realloc(&X_sample, n_features)
        safe_realloc(&feature_to_sample, n_features)
        with nogil:
            memset(feature_to_sample, -1, n_features * sizeof(SIZE_t))
            for i in range(n_samples):
                node = self.nodes
                for k in range(X_indptr[i], X_indptr[i + 1]):
                    feature_to_sample[X_indices[k]] = i
                    X_sample[X_indices[k]] = X_data[k]
                                while node.left_child != _TREE_LEAF:
                                        if feature_to_sample[node.feature] == i:
                        feature_value = X_sample[node.feature]
                    else:
                        feature_value = 0.
                    if feature_value <= node.threshold:
                        node = &self.nodes[node.left_child]
                    else:
                        node = &self.nodes[node.right_child]
                out_ptr[i] = <SIZE_t>(node - self.nodes)  
                        free(X_sample)
            free(feature_to_sample)
        return out
    cpdef object decision_path(self, object X):
                if issparse(X):
            return self._decision_path_sparse_csr(X)
        else:
            return self._decision_path_dense(X)
    cdef inline object _decision_path_dense(self, object X):
        
                if not isinstance(X, np.ndarray):
            raise ValueError("X should be in np.ndarray format, got %s"
                             % type(X))
        if X.dtype != DTYPE:
            raise ValueError("X.dtype should be np.float32, got %s" % X.dtype)
                cdef np.ndarray X_ndarray = X
        cdef DTYPE_t* X_ptr = <DTYPE_t*> X_ndarray.data
        cdef SIZE_t X_sample_stride = <SIZE_t> X.strides[0] / <SIZE_t> X.itemsize
        cdef SIZE_t X_fx_stride = <SIZE_t> X.strides[1] / <SIZE_t> X.itemsize
        cdef SIZE_t n_samples = X.shape[0]
                cdef np.ndarray[SIZE_t] indptr = np.zeros(n_samples + 1, dtype=np.intp)
        cdef SIZE_t* indptr_ptr = <SIZE_t*> indptr.data
        cdef np.ndarray[SIZE_t] indices = np.zeros(n_samples *
                                                   (1 + self.max_depth),
                                                   dtype=np.intp)
        cdef SIZE_t* indices_ptr = <SIZE_t*> indices.data
                cdef Node* node = NULL
        cdef SIZE_t i = 0
        with nogil:
            for i in range(n_samples):
                node = self.nodes
                indptr_ptr[i + 1] = indptr_ptr[i]
                                while node.left_child != _TREE_LEAF:
                                        indices_ptr[indptr_ptr[i + 1]] = <SIZE_t>(node - self.nodes)
                    indptr_ptr[i + 1] += 1
                    if X_ptr[X_sample_stride * i +
                             X_fx_stride * node.feature] <= node.threshold:
                        node = &self.nodes[node.left_child]
                    else:
                        node = &self.nodes[node.right_child]
                                indices_ptr[indptr_ptr[i + 1]] = <SIZE_t>(node - self.nodes)
                indptr_ptr[i + 1] += 1
        indices = indices[:indptr[n_samples]]
        cdef np.ndarray[SIZE_t] data = np.ones(shape=len(indices),
                                               dtype=np.intp)
        out = csr_matrix((data, indices, indptr),
                         shape=(n_samples, self.node_count))
        return out
    cdef inline object _decision_path_sparse_csr(self, object X):
        
                if not isinstance(X, csr_matrix):
            raise ValueError("X should be in csr_matrix format, got %s"
                             % type(X))
        if X.dtype != DTYPE:
            raise ValueError("X.dtype should be np.float32, got %s" % X.dtype)
                cdef np.ndarray[ndim=1, dtype=DTYPE_t] X_data_ndarray = X.data
        cdef np.ndarray[ndim=1, dtype=INT32_t] X_indices_ndarray  = X.indices
        cdef np.ndarray[ndim=1, dtype=INT32_t] X_indptr_ndarray  = X.indptr
        cdef DTYPE_t* X_data = <DTYPE_t*>X_data_ndarray.data
        cdef INT32_t* X_indices = <INT32_t*>X_indices_ndarray.data
        cdef INT32_t* X_indptr = <INT32_t*>X_indptr_ndarray.data
        cdef SIZE_t n_samples = X.shape[0]
        cdef SIZE_t n_features = X.shape[1]
                cdef np.ndarray[SIZE_t] indptr = np.zeros(n_samples + 1, dtype=np.intp)
        cdef SIZE_t* indptr_ptr = <SIZE_t*> indptr.data
        cdef np.ndarray[SIZE_t] indices = np.zeros(n_samples *
                                                   (1 + self.max_depth),
                                                   dtype=np.intp)
        cdef SIZE_t* indices_ptr = <SIZE_t*> indices.data
                cdef DTYPE_t feature_value = 0.
        cdef Node* node = NULL
        cdef DTYPE_t* X_sample = NULL
        cdef SIZE_t i = 0
        cdef INT32_t k = 0
                                cdef SIZE_t* feature_to_sample = NULL
        safe_realloc(&X_sample, n_features)
        safe_realloc(&feature_to_sample, n_features)
        with nogil:
            memset(feature_to_sample, -1, n_features * sizeof(SIZE_t))
            for i in range(n_samples):
                node = self.nodes
                indptr_ptr[i + 1] = indptr_ptr[i]
                for k in range(X_indptr[i], X_indptr[i + 1]):
                    feature_to_sample[X_indices[k]] = i
                    X_sample[X_indices[k]] = X_data[k]
                                while node.left_child != _TREE_LEAF:
                    
                    indices_ptr[indptr_ptr[i + 1]] = <SIZE_t>(node - self.nodes)
                    indptr_ptr[i + 1] += 1
                    if feature_to_sample[node.feature] == i:
                        feature_value = X_sample[node.feature]
                    else:
                        feature_value = 0.
                    if feature_value <= node.threshold:
                        node = &self.nodes[node.left_child]
                    else:
                        node = &self.nodes[node.right_child]
                                indices_ptr[indptr_ptr[i + 1]] = <SIZE_t>(node - self.nodes)
                indptr_ptr[i + 1] += 1
                        free(X_sample)
            free(feature_to_sample)
        indices = indices[:indptr[n_samples]]
        cdef np.ndarray[SIZE_t] data = np.ones(shape=len(indices),
                                               dtype=np.intp)
        out = csr_matrix((data, indices, indptr),
                         shape=(n_samples, self.node_count))
        return out

    cpdef compute_feature_importances(self, normalize=True):
                cdef Node* left
        cdef Node* right
        cdef Node* nodes = self.nodes
        cdef Node* node = nodes
        cdef Node* end_node = node + self.node_count
        cdef double normalizer = 0.
        cdef np.ndarray[np.float64_t, ndim=1] importances
        importances = np.zeros((self.n_features,))
        cdef DOUBLE_t* importance_data = <DOUBLE_t*>importances.data
        with nogil:
            while node != end_node:
                if node.left_child != _TREE_LEAF:
                                        left = &nodes[node.left_child]
                    right = &nodes[node.right_child]
                    importance_data[node.feature] += (
                        node.weighted_n_node_samples * node.impurity -
                        left.weighted_n_node_samples * left.impurity -
                        right.weighted_n_node_samples * right.impurity)
                node += 1
        importances /= nodes[0].weighted_n_node_samples
        if normalize:
            normalizer = np.sum(importances)
            if normalizer > 0.0:
                                importances /= normalizer
        return importances
    cdef np.ndarray _get_value_ndarray(self):
                cdef np.npy_intp shape[3]
        shape[0] = <np.npy_intp> self.node_count
        shape[1] = <np.npy_intp> self.n_outputs
        shape[2] = <np.npy_intp> self.max_n_classes
        cdef np.ndarray arr
        arr = np.PyArray_SimpleNewFromData(3, shape, np.NPY_DOUBLE, self.value)
        Py_INCREF(self)
        arr.base = <PyObject*> self
        return arr
    cdef np.ndarray _get_node_ndarray(self):
                cdef np.npy_intp shape[1]
        shape[0] = <np.npy_intp> self.node_count
        cdef np.npy_intp strides[1]
        strides[0] = sizeof(Node)
        cdef np.ndarray arr
        Py_INCREF(NODE_DTYPE)
        arr = PyArray_NewFromDescr(np.ndarray, <np.dtype> NODE_DTYPE, 1, shape,
                                   strides, <void*> self.nodes,
                                   np.NPY_DEFAULT, None)
        Py_INCREF(self)
        arr.base = <PyObject*> self
        return arr

from libc.stdlib cimport free
from libc.stdlib cimport malloc
from libc.stdlib cimport realloc
from libc.math cimport log as ln
import numpy as np
cimport numpy as np
np.import_array()

cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *:
            cdef size_t nbytes = nelems * sizeof(p[0][0])
    if nbytes / sizeof(p[0][0]) != nelems:
                with gil:
            raise MemoryError("could not allocate (%d * %d) bytes"
                              % (nelems, sizeof(p[0][0])))
    cdef realloc_ptr tmp = <realloc_ptr>realloc(p[0], nbytes)
    if tmp == NULL:
        with gil:
            raise MemoryError("could not allocate %d bytes" % nbytes)
    p[0] = tmp
    return tmp  
def _realloc_test():
            cdef SIZE_t* p = NULL
    safe_realloc(&p, <size_t>(-1) / 2)
    if p != NULL:
        free(p)
        assert False

cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:
    seed[0] ^= <UINT32_t>(seed[0] << 13)
    seed[0] ^= <UINT32_t>(seed[0] >> 17)
    seed[0] ^= <UINT32_t>(seed[0] << 5)
    return seed[0] % (<UINT32_t>RAND_R_MAX + 1)

cdef inline np.ndarray sizet_ptr_to_ndarray(SIZE_t* data, SIZE_t size):
        cdef np.npy_intp shape[1]
    shape[0] = <np.npy_intp> size
    return np.PyArray_SimpleNewFromData(1, shape, np.NPY_INTP, data).copy()

cdef inline SIZE_t rand_int(SIZE_t low, SIZE_t high,
                            UINT32_t* random_state) nogil:
        return low + our_rand_r(random_state) % (high - low)

cdef inline double rand_uniform(double low, double high,
                                UINT32_t* random_state) nogil:
        return ((high - low) * <double> our_rand_r(random_state) /
            <double> RAND_R_MAX) + low

cdef inline double log(double x) nogil:
    return ln(x) / ln(2.0)

cdef class Stack:
    
    def __cinit__(self, SIZE_t capacity):
        self.capacity = capacity
        self.top = 0
        self.stack_ = <StackRecord*> malloc(capacity * sizeof(StackRecord))
    def __dealloc__(self):
        free(self.stack_)
    cdef bint is_empty(self) nogil:
        return self.top <= 0
    cdef int push(self, SIZE_t start, SIZE_t end, SIZE_t depth, SIZE_t parent,
                  bint is_left, double impurity,
                  SIZE_t n_constant_features) nogil except -1:
                cdef SIZE_t top = self.top
        cdef StackRecord* stack = NULL
                if top >= self.capacity:
            self.capacity *= 2
                        safe_realloc(&self.stack_, self.capacity)
        stack = self.stack_
        stack[top].start = start
        stack[top].end = end
        stack[top].depth = depth
        stack[top].parent = parent
        stack[top].is_left = is_left
        stack[top].impurity = impurity
        stack[top].n_constant_features = n_constant_features
                self.top = top + 1
        return 0
    cdef int pop(self, StackRecord* res) nogil:
                cdef SIZE_t top = self.top
        cdef StackRecord* stack = self.stack_
        if top <= 0:
            return -1
        res[0] = stack[top - 1]
        self.top = top - 1
        return 0

cdef class PriorityHeap:
    
    def __cinit__(self, SIZE_t capacity):
        self.capacity = capacity
        self.heap_ptr = 0
        safe_realloc(&self.heap_, capacity)
    def __dealloc__(self):
        free(self.heap_)
    cdef bint is_empty(self) nogil:
        return self.heap_ptr <= 0
    cdef void heapify_up(self, PriorityHeapRecord* heap, SIZE_t pos) nogil:
                if pos == 0:
            return
        cdef SIZE_t parent_pos = (pos - 1) / 2
        if heap[parent_pos].improvement < heap[pos].improvement:
            heap[parent_pos], heap[pos] = heap[pos], heap[parent_pos]
            self.heapify_up(heap, parent_pos)
    cdef void heapify_down(self, PriorityHeapRecord* heap, SIZE_t pos,
                           SIZE_t heap_length) nogil:
                cdef SIZE_t left_pos = 2 * (pos + 1) - 1
        cdef SIZE_t right_pos = 2 * (pos + 1)
        cdef SIZE_t largest = pos
        if (left_pos < heap_length and
                heap[left_pos].improvement > heap[largest].improvement):
            largest = left_pos
        if (right_pos < heap_length and
                heap[right_pos].improvement > heap[largest].improvement):
            largest = right_pos
        if largest != pos:
            heap[pos], heap[largest] = heap[largest], heap[pos]
            self.heapify_down(heap, largest, heap_length)
    cdef int push(self, SIZE_t node_id, SIZE_t start, SIZE_t end, SIZE_t pos,
                  SIZE_t depth, bint is_leaf, double improvement,
                  double impurity, double impurity_left,
                  double impurity_right) nogil except -1:
                cdef SIZE_t heap_ptr = self.heap_ptr
        cdef PriorityHeapRecord* heap = NULL
                if heap_ptr >= self.capacity:
            self.capacity *= 2
                        safe_realloc(&self.heap_, self.capacity)
                heap = self.heap_
        heap[heap_ptr].node_id = node_id
        heap[heap_ptr].start = start
        heap[heap_ptr].end = end
        heap[heap_ptr].pos = pos
        heap[heap_ptr].depth = depth
        heap[heap_ptr].is_leaf = is_leaf
        heap[heap_ptr].impurity = impurity
        heap[heap_ptr].impurity_left = impurity_left
        heap[heap_ptr].impurity_right = impurity_right
        heap[heap_ptr].improvement = improvement
                self.heapify_up(heap, heap_ptr)
                self.heap_ptr = heap_ptr + 1
        return 0
    cdef int pop(self, PriorityHeapRecord* res) nogil:
                cdef SIZE_t heap_ptr = self.heap_ptr
        cdef PriorityHeapRecord* heap = self.heap_
        if heap_ptr <= 0:
            return -1
                res[0] = heap[0]
                heap[0], heap[heap_ptr - 1] = heap[heap_ptr - 1], heap[0]
                if heap_ptr > 1:
            self.heapify_down(heap, 0, heap_ptr - 1)
        self.heap_ptr = heap_ptr - 1
        return 0

cdef class WeightedPQueue:
    
    def __cinit__(self, SIZE_t capacity):
        self.capacity = capacity
        self.array_ptr = 0
        safe_realloc(&self.array_, capacity)
    def __dealloc__(self):
        free(self.array_)
    cdef int reset(self) nogil except -1:
                self.array_ptr = 0
                safe_realloc(&self.array_, self.capacity)
        return 0
    cdef bint is_empty(self) nogil:
        return self.array_ptr <= 0
    cdef SIZE_t size(self) nogil:
        return self.array_ptr
    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:
                cdef SIZE_t array_ptr = self.array_ptr
        cdef WeightedPQueueRecord* array = NULL
        cdef SIZE_t i
                if array_ptr >= self.capacity:
            self.capacity *= 2
                        safe_realloc(&self.array_, self.capacity)
                array = self.array_
        array[array_ptr].data = data
        array[array_ptr].weight = weight
                        i = array_ptr
        while(i != 0 and array[i].data < array[i-1].data):
            array[i], array[i-1] = array[i-1], array[i]
            i -= 1
                self.array_ptr = array_ptr + 1
        return 0
    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:
                cdef SIZE_t array_ptr = self.array_ptr
        cdef WeightedPQueueRecord* array = self.array_
        cdef SIZE_t idx_to_remove = -1
        cdef SIZE_t i
        if array_ptr <= 0:
            return -1
                for i in range(array_ptr):
            if array[i].data == data and array[i].weight == weight:
                idx_to_remove = i
                break
        if idx_to_remove == -1:
            return -1
                        for i in range(idx_to_remove, array_ptr-1):
            array[i] = array[i+1]
        self.array_ptr = array_ptr - 1
        return 0
    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:
                cdef SIZE_t array_ptr = self.array_ptr
        cdef WeightedPQueueRecord* array = self.array_
        cdef SIZE_t i
        if array_ptr <= 0:
            return -1
        data[0] = array[0].data
        weight[0] = array[0].weight
                        for i in range(0, array_ptr-1):
            array[i] = array[i+1]
        self.array_ptr = array_ptr - 1
        return 0
    cdef int peek(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:
                cdef WeightedPQueueRecord* array = self.array_
        if self.array_ptr <= 0:
            return -1
                data[0] = array[0].data
        weight[0] = array[0].weight
        return 0
    cdef DOUBLE_t get_weight_from_index(self, SIZE_t index) nogil:
                cdef WeightedPQueueRecord* array = self.array_
                return array[index].weight
    cdef DOUBLE_t get_value_from_index(self, SIZE_t index) nogil:
                cdef WeightedPQueueRecord* array = self.array_
                return array[index].data

cdef class WeightedMedianCalculator:
    
    def __cinit__(self, SIZE_t initial_capacity):
        self.initial_capacity = initial_capacity
        self.samples = WeightedPQueue(initial_capacity)
        self.total_weight = 0
        self.k = 0
        self.sum_w_0_k = 0
    cdef SIZE_t size(self) nogil:
                cdef int return_value
        cdef DOUBLE_t original_median
        if self.size() != 0:
            original_median = self.get_median()
                return_value = self.samples.push(data, weight)
        self.update_median_parameters_post_push(data, weight,
                                                original_median)
        return return_value
    cdef int update_median_parameters_post_push(
            self, DOUBLE_t data, DOUBLE_t weight,
            DOUBLE_t original_median) nogil:
        
                if self.size() == 1:
            self.k = 1
            self.total_weight = weight
            self.sum_w_0_k = self.total_weight
            return 0
                self.total_weight += weight
        if data < original_median:
                                                self.k += 1
                        self.sum_w_0_k += weight
                                    while(self.k > 1 and ((self.sum_w_0_k -
                                   self.samples.get_weight_from_index(self.k-1))
                                  >= self.total_weight / 2.0)):
                self.k -= 1
                self.sum_w_0_k -= self.samples.get_weight_from_index(self.k)
            return 0
        if data >= original_median:
                                    while(self.k < self.samples.size() and
                  (self.sum_w_0_k < self.total_weight / 2.0)):
                self.k += 1
                self.sum_w_0_k += self.samples.get_weight_from_index(self.k-1)
            return 0
    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:
                cdef int return_value
        cdef DOUBLE_t original_median
        if self.size() != 0:
            original_median = self.get_median()
        return_value = self.samples.remove(data, weight)
        self.update_median_parameters_post_remove(data, weight,
                                                  original_median)
        return return_value
    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:
                cdef int return_value
        cdef double original_median
        if self.size() != 0:
            original_median = self.get_median()
                if self.samples.size() == 0:
            return -1
        return_value = self.samples.pop(data, weight)
        self.update_median_parameters_post_remove(data[0],
                                                  weight[0],
                                                  original_median)
        return return_value
    cdef int update_median_parameters_post_remove(
            self, DOUBLE_t data, DOUBLE_t weight,
            double original_median) nogil:
                        if self.samples.size() == 0:
            self.k = 0
            self.total_weight = 0
            self.sum_w_0_k = 0
            return 0
                if self.samples.size() == 1:
            self.k = 1
            self.total_weight -= weight
            self.sum_w_0_k = self.total_weight
            return 0
                self.total_weight -= weight
        if data < original_median:
                                    
            self.k -= 1
                        self.sum_w_0_k -= weight
                                                while(self.k < self.samples.size() and
                  (self.sum_w_0_k < self.total_weight / 2.0)):
                self.k += 1
                self.sum_w_0_k += self.samples.get_weight_from_index(self.k-1)
            return 0
        if data >= original_median:
                                    while(self.k > 1 and ((self.sum_w_0_k -
                                   self.samples.get_weight_from_index(self.k-1))
                                  >= self.total_weight / 2.0)):
                self.k -= 1
                self.sum_w_0_k -= self.samples.get_weight_from_index(self.k)
            return 0
    cdef DOUBLE_t get_median(self) nogil:
                if self.sum_w_0_k == (self.total_weight / 2.0):
                        return (self.samples.get_value_from_index(self.k) +
                    self.samples.get_value_from_index(self.k-1)) / 2.0
        if self.sum_w_0_k > (self.total_weight / 2.0):
                        return self.samples.get_value_from_index(self.k-1)
from .tree import DecisionTreeClassifier
from .tree import DecisionTreeRegressor
from .tree import ExtraTreeClassifier
from .tree import ExtraTreeRegressor
from .export import export_graphviz
__all__ = ["DecisionTreeClassifier", "DecisionTreeRegressor",
           "ExtraTreeClassifier", "ExtraTreeRegressor", "export_graphviz"]


from scipy.sparse.linalg.eigen.arpack import _arpack
import numpy as np
from scipy.sparse.linalg.interface import aslinearoperator, LinearOperator
from scipy.sparse import identity, isspmatrix, isspmatrix_csr
from scipy.linalg import lu_factor, lu_solve
from scipy.sparse.sputils import isdense
from scipy.sparse.linalg import gmres, splu
import scipy
import functools
import operator
from distutils.version import LooseVersion
__docformat__ = "restructuredtext en"
__all__ = ['eigs', 'eigsh', 'svds', 'ArpackError', 'ArpackNoConvergence']
_type_conv = {'f': 's', 'd': 'd', 'F': 'c', 'D': 'z'}
_ndigits = {'f': 5, 'd': 12, 'F': 5, 'D': 12}
DNAUPD_ERRORS = {
    0: "Normal exit.",
    1: "Maximum number of iterations taken. "
       "All possible eigenvalues of OP has been found. IPARAM(5) "
       "returns the number of wanted converged Ritz values.",
    2: "No longer an informational error. Deprecated starting "
       "with release 2 of ARPACK.",
    3: "No shifts could be applied during a cycle of the "
       "Implicitly restarted Arnoldi iteration. One possibility "
       "is to increase the size of NCV relative to NEV. ",
    -1: "N must be positive.",
    -2: "NEV must be positive.",
    -3: "NCV-NEV >= 2 and less than or equal to N.",
    -4: "The maximum number of Arnoldi update iterations allowed "
        "must be greater than zero.",
    -5: " WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'",
    -6: "BMAT must be one of 'I' or 'G'.",
    -7: "Length of private work array WORKL is not sufficient.",
    -8: "Error return from LAPACK eigenvalue calculation;",
    -9: "Starting vector is zero.",
    -10: "IPARAM(7) must be 1,2,3,4.",
    -11: "IPARAM(7) = 1 and BMAT = 'G' are incompatible.",
    -12: "IPARAM(1) must be equal to 0 or 1.",
    -13: "NEV and WHICH = 'BE' are incompatible.",
    -9999: "Could not build an Arnoldi factorization. "
           "IPARAM(5) returns the size of the current Arnoldi "
           "factorization. The user is advised to check that "
           "enough workspace and array storage has been allocated."
}
SNAUPD_ERRORS = DNAUPD_ERRORS
ZNAUPD_ERRORS = DNAUPD_ERRORS.copy()
ZNAUPD_ERRORS[-10] = "IPARAM(7) must be 1,2,3."
CNAUPD_ERRORS = ZNAUPD_ERRORS
DSAUPD_ERRORS = {
    0: "Normal exit.",
    1: "Maximum number of iterations taken. "
       "All possible eigenvalues of OP has been found.",
    2: "No longer an informational error. Deprecated starting with "
       "release 2 of ARPACK.",
    3: "No shifts could be applied during a cycle of the Implicitly "
       "restarted Arnoldi iteration. One possibility is to increase "
       "the size of NCV relative to NEV. ",
    -1: "N must be positive.",
    -2: "NEV must be positive.",
    -3: "NCV must be greater than NEV and less than or equal to N.",
    -4: "The maximum number of Arnoldi update iterations allowed "
        "must be greater than zero.",
    -5: "WHICH must be one of 'LM', 'SM', 'LA', 'SA' or 'BE'.",
    -6: "BMAT must be one of 'I' or 'G'.",
    -7: "Length of private work array WORKL is not sufficient.",
    -8: "Error return from trid. eigenvalue calculation; "
        "Informational error from LAPACK routine dsteqr .",
    -9: "Starting vector is zero.",
    -10: "IPARAM(7) must be 1,2,3,4,5.",
    -11: "IPARAM(7) = 1 and BMAT = 'G' are incompatible.",
    -12: "IPARAM(1) must be equal to 0 or 1.",
    -13: "NEV and WHICH = 'BE' are incompatible. ",
    -9999: "Could not build an Arnoldi factorization. "
           "IPARAM(5) returns the size of the current Arnoldi "
           "factorization. The user is advised to check that "
           "enough workspace and array storage has been allocated.",
}
SSAUPD_ERRORS = DSAUPD_ERRORS
DNEUPD_ERRORS = {
    0: "Normal exit.",
    1: "The Schur form computed by LAPACK routine dlahqr "
       "could not be reordered by LAPACK routine dtrsen. "
       "Re-enter subroutine dneupd  with IPARAM(5)NCV and "
       "increase the size of the arrays DR and DI to have "
       "dimension at least dimension NCV and allocate at least NCV "
       "columns for Z. NOTE: Not necessary if Z and V share "
       "the same space. Please notify the authors if this error"
       "occurs.",
    -1: "N must be positive.",
    -2: "NEV must be positive.",
    -3: "NCV-NEV >= 2 and less than or equal to N.",
    -5: "WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'",
    -6: "BMAT must be one of 'I' or 'G'.",
    -7: "Length of private work WORKL array is not sufficient.",
    -8: "Error return from calculation of a real Schur form. "
        "Informational error from LAPACK routine dlahqr .",
    -9: "Error return from calculation of eigenvectors. "
        "Informational error from LAPACK routine dtrevc.",
    -10: "IPARAM(7) must be 1,2,3,4.",
    -11: "IPARAM(7) = 1 and BMAT = 'G' are incompatible.",
    -12: "HOWMNY = 'S' not yet implemented",
    -13: "HOWMNY must be one of 'A' or 'P' if RVEC = .true.",
    -14: "DNAUPD  did not find any eigenvalues to sufficient "
         "accuracy.",
    -15: "DNEUPD got a different count of the number of converged "
         "Ritz values than DNAUPD got.  This indicates the user "
         "probably made an error in passing data from DNAUPD to "
         "DNEUPD or that the data was modified before entering "
         "DNEUPD",
}
SNEUPD_ERRORS = DNEUPD_ERRORS.copy()
SNEUPD_ERRORS[1] = ("The Schur form computed by LAPACK routine slahqr "
                    "could not be reordered by LAPACK routine strsen . "
                    "Re-enter subroutine dneupd  with IPARAM(5)=NCV and "
                    "increase the size of the arrays DR and DI to have "
                    "dimension at least dimension NCV and allocate at least "
                    "NCV columns for Z. NOTE: Not necessary if Z and V share "
                    "the same space. Please notify the authors if this error "
                    "occurs.")
SNEUPD_ERRORS[-14] = ("SNAUPD did not find any eigenvalues to sufficient "
                      "accuracy.")
SNEUPD_ERRORS[-15] = ("SNEUPD got a different count of the number of "
                      "converged Ritz values than SNAUPD got.  This indicates "
                      "the user probably made an error in passing data from "
                      "SNAUPD to SNEUPD or that the data was modified before "
                      "entering SNEUPD")
ZNEUPD_ERRORS = {0: "Normal exit.",
                 1: "The Schur form computed by LAPACK routine csheqr "
                    "could not be reordered by LAPACK routine ztrsen. "
                    "Re-enter subroutine zneupd with IPARAM(5)=NCV and "
                    "increase the size of the array D to have "
                    "dimension at least dimension NCV and allocate at least "
                    "NCV columns for Z. NOTE: Not necessary if Z and V share "
                    "the same space. Please notify the authors if this error "
                    "occurs.",
                 -1: "N must be positive.",
                 -2: "NEV must be positive.",
                 -3: "NCV-NEV >= 1 and less than or equal to N.",
                 -5: "WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'",
                 -6: "BMAT must be one of 'I' or 'G'.",
                 -7: "Length of private work WORKL array is not sufficient.",
                 -8: "Error return from LAPACK eigenvalue calculation. "
                     "This should never happened.",
                 -9: "Error return from calculation of eigenvectors. "
                     "Informational error from LAPACK routine ztrevc.",
                 -10: "IPARAM(7) must be 1,2,3",
                 -11: "IPARAM(7) = 1 and BMAT = 'G' are incompatible.",
                 -12: "HOWMNY = 'S' not yet implemented",
                 -13: "HOWMNY must be one of 'A' or 'P' if RVEC = .true.",
                 -14: "ZNAUPD did not find any eigenvalues to sufficient "
                      "accuracy.",
                 -15: "ZNEUPD got a different count of the number of "
                      "converged Ritz values than ZNAUPD got.  This "
                      "indicates the user probably made an error in passing "
                      "data from ZNAUPD to ZNEUPD or that the data was "
                      "modified before entering ZNEUPD"
                 }
CNEUPD_ERRORS = ZNEUPD_ERRORS.copy()
CNEUPD_ERRORS[-14] = ("CNAUPD did not find any eigenvalues to sufficient "
                      "accuracy.")
CNEUPD_ERRORS[-15] = ("CNEUPD got a different count of the number of "
                      "converged Ritz values than CNAUPD got.  This indicates "
                      "the user probably made an error in passing data from "
                      "CNAUPD to CNEUPD or that the data was modified before "
                      "entering CNEUPD")
DSEUPD_ERRORS = {
    0: "Normal exit.",
    -1: "N must be positive.",
    -2: "NEV must be positive.",
    -3: "NCV must be greater than NEV and less than or equal to N.",
    -5: "WHICH must be one of 'LM', 'SM', 'LA', 'SA' or 'BE'.",
    -6: "BMAT must be one of 'I' or 'G'.",
    -7: "Length of private work WORKL array is not sufficient.",
    -8: ("Error return from trid. eigenvalue calculation; "
         "Information error from LAPACK routine dsteqr."),
    -9: "Starting vector is zero.",
    -10: "IPARAM(7) must be 1,2,3,4,5.",
    -11: "IPARAM(7) = 1 and BMAT = 'G' are incompatible.",
    -12: "NEV and WHICH = 'BE' are incompatible.",
    -14: "DSAUPD  did not find any eigenvalues to sufficient accuracy.",
    -15: "HOWMNY must be one of 'A' or 'S' if RVEC = .true.",
    -16: "HOWMNY = 'S' not yet implemented",
    -17: ("DSEUPD  got a different count of the number of converged "
          "Ritz values than DSAUPD  got.  This indicates the user "
          "probably made an error in passing data from DSAUPD  to "
          "DSEUPD  or that the data was modified before entering  "
          "DSEUPD.")
}
SSEUPD_ERRORS = DSEUPD_ERRORS.copy()
SSEUPD_ERRORS[-14] = ("SSAUPD  did not find any eigenvalues "
                      "to sufficient accuracy.")
SSEUPD_ERRORS[-17] = ("SSEUPD  got a different count of the number of "
                      "converged "
                      "Ritz values than SSAUPD  got.  This indicates the user "
                      "probably made an error in passing data from SSAUPD  to "
                      "SSEUPD  or that the data was modified before entering  "
                      "SSEUPD.")
_SAUPD_ERRORS = {'d': DSAUPD_ERRORS,
                 's': SSAUPD_ERRORS}
_NAUPD_ERRORS = {'d': DNAUPD_ERRORS,
                 's': SNAUPD_ERRORS,
                 'z': ZNAUPD_ERRORS,
                 'c': CNAUPD_ERRORS}
_SEUPD_ERRORS = {'d': DSEUPD_ERRORS,
                 's': SSEUPD_ERRORS}
_NEUPD_ERRORS = {'d': DNEUPD_ERRORS,
                 's': SNEUPD_ERRORS,
                 'z': ZNEUPD_ERRORS,
                 'c': CNEUPD_ERRORS}
_SEUPD_WHICH = ['LM', 'SM', 'LA', 'SA', 'BE']
_NEUPD_WHICH = ['LM', 'SM', 'LR', 'SR', 'LI', 'SI']

if scipy.version.version >= LooseVersion('0.12'):
    BACKPORT_TO = None
elif scipy.version.version >= LooseVersion('0.11'):
    BACKPORT_TO = '0.10'
else:
    BACKPORT_TO = '0.09'

def _aligned_zeros(shape, dtype=float, order="C", align=None):
        dtype = np.dtype(dtype)
    if align is None:
        align = dtype.alignment
    if not hasattr(shape, '__len__'):
        shape = (shape,)
    size = functools.reduce(operator.mul, shape) * dtype.itemsize
    buf = np.empty(size + align + 1, np.uint8)
    offset = buf.__array_interface__['data'][0] % align
    if offset != 0:
        offset = align - offset
            buf = buf[offset:offset+size+1][:-1]
    data = np.ndarray(shape, dtype, buf, order=order)
    data.fill(0)
    return data

class ArpackError(RuntimeError):
        def __init__(self, info, infodict=_NAUPD_ERRORS):
        msg = infodict.get(info, "Unknown error")
        RuntimeError.__init__(self, "ARPACK error %d: %s" % (info, msg))

class ArpackNoConvergence(ArpackError):
        def __init__(self, msg, eigenvalues, eigenvectors):
        ArpackError.__init__(self, -1, {-1: msg})
        self.eigenvalues = eigenvalues
        self.eigenvectors = eigenvectors

class _ArpackParams(object):
    def __init__(self, n, k, tp, mode=1, sigma=None,
                 ncv=None, v0=None, maxiter=None, which="LM", tol=0):
        if k <= 0:
            raise ValueError("k must be positive, k=%d" % k)
        if maxiter is None:
            maxiter = n * 10
        if maxiter <= 0:
            raise ValueError("maxiter must be positive, maxiter=%d" % maxiter)
        if tp not in 'fdFD':
            raise ValueError("matrix type must be 'f', 'd', 'F', or 'D'")
        if v0 is not None:
                        self.resid = np.array(v0, copy=True)
            info = 1
        else:
                        self.resid = np.zeros(n, tp)
            info = 0
        if sigma is None:
                        self.sigma = 0
        else:
            self.sigma = sigma
        if ncv is None:
            ncv = 2 * k + 1
        ncv = min(ncv, n)
        self.v = np.zeros((n, ncv), tp)          self.iparam = np.zeros(11, "int")
                ishfts = 1
        self.mode = mode
        self.iparam[0] = ishfts
        self.iparam[2] = maxiter
        self.iparam[3] = 1
        self.iparam[6] = mode
        self.n = n
        self.tol = tol
        self.k = k
        self.maxiter = maxiter
        self.ncv = ncv
        self.which = which
        self.tp = tp
        self.info = info
        self.converged = False
        self.ido = 0
    def _raise_no_convergence(self):
        msg = "No convergence (%d iterations, %d/%d eigenvectors converged)"
        k_ok = self.iparam[4]
        num_iter = self.iparam[2]
        try:
            ev, vec = self.extract(True)
        except ArpackError as err:
            msg = "%s [%s]" % (msg, err)
            ev = np.zeros((0,))
            vec = np.zeros((self.n, 0))
            k_ok = 0
        raise ArpackNoConvergence(msg % (num_iter, k_ok, self.k), ev, vec)

class _SymmetricArpackParams(_ArpackParams):
    def __init__(self, n, k, tp, matvec, mode=1, M_matvec=None,
                 Minv_matvec=None, sigma=None,
                 ncv=None, v0=None, maxiter=None, which="LM", tol=0):
                                                                                                                                                                                                                                                                                                                                                                                                                                if mode == 1:
            if matvec is None:
                raise ValueError("matvec must be specified for mode=1")
            if M_matvec is not None:
                raise ValueError("M_matvec cannot be specified for mode=1")
            if Minv_matvec is not None:
                raise ValueError("Minv_matvec cannot be specified for mode=1")
            self.OP = matvec
            self.B = lambda x: x
            self.bmat = 'I'
        elif mode == 2:
            if matvec is None:
                raise ValueError("matvec must be specified for mode=2")
            if M_matvec is None:
                raise ValueError("M_matvec must be specified for mode=2")
            if Minv_matvec is None:
                raise ValueError("Minv_matvec must be specified for mode=2")
            self.OP = lambda x: Minv_matvec(matvec(x))
            self.OPa = Minv_matvec
            self.OPb = matvec
            self.B = M_matvec
            self.bmat = 'G'
        elif mode == 3:
            if matvec is not None:
                raise ValueError("matvec must not be specified for mode=3")
            if Minv_matvec is None:
                raise ValueError("Minv_matvec must be specified for mode=3")
            if M_matvec is None:
                self.OP = Minv_matvec
                self.OPa = Minv_matvec
                self.B = lambda x: x
                self.bmat = 'I'
            else:
                self.OP = lambda x: Minv_matvec(M_matvec(x))
                self.OPa = Minv_matvec
                self.B = M_matvec
                self.bmat = 'G'
        elif mode == 4:
            if matvec is None:
                raise ValueError("matvec must be specified for mode=4")
            if M_matvec is not None:
                raise ValueError("M_matvec must not be specified for mode=4")
            if Minv_matvec is None:
                raise ValueError("Minv_matvec must be specified for mode=4")
            self.OPa = Minv_matvec
            self.OP = lambda x: self.OPa(matvec(x))
            self.B = matvec
            self.bmat = 'G'
        elif mode == 5:
            if matvec is None:
                raise ValueError("matvec must be specified for mode=5")
            if Minv_matvec is None:
                raise ValueError("Minv_matvec must be specified for mode=5")
            self.OPa = Minv_matvec
            self.A_matvec = matvec
            if M_matvec is None:
                self.OP = lambda x: Minv_matvec(matvec(x) + sigma * x)
                self.B = lambda x: x
                self.bmat = 'I'
            else:
                self.OP = lambda x: Minv_matvec(matvec(x) +
                                                sigma * M_matvec(x))
                self.B = M_matvec
                self.bmat = 'G'
        else:
            raise ValueError("mode=%i not implemented" % mode)
        if which not in _SEUPD_WHICH:
            raise ValueError("which must be one of %s"
                             % ' '.join(_SEUPD_WHICH))
        if k >= n:
            raise ValueError("k must be less than ndim(A), k=%d" % k)
        _ArpackParams.__init__(self, n, k, tp, mode, sigma,
                               ncv, v0, maxiter, which, tol)
        if self.ncv > n or self.ncv <= k:
            raise ValueError("ncv must be k<ncv<=n, ncv=%s" % self.ncv)
                self.workd = _aligned_zeros(3 * n, self.tp)
        self.workl = _aligned_zeros(self.ncv * (self.ncv + 8), self.tp)
        ltr = _type_conv[self.tp]
        if ltr not in ["s", "d"]:
            raise ValueError("Input matrix is not real-valued.")
        self._arpack_solver = _arpack.__dict__[ltr + 'saupd']
        self._arpack_extract = _arpack.__dict__[ltr + 'seupd']
        self.iterate_infodict = _SAUPD_ERRORS[ltr]
        self.extract_infodict = _SEUPD_ERRORS[ltr]
        self.ipntr = np.zeros(11, "int")
    def iterate(self):
        if BACKPORT_TO is None:
            return None
        if BACKPORT_TO == '0.10':
            self.ido, self.tol, self.resid, self.v, self.iparam, self.ipntr, self.info = \
                self._arpack_solver(self.ido, self.bmat, self.which, self.k,
                                    self.tol, self.resid, self.v, self.iparam,
                                    self.ipntr, self.workd, self.workl, self.info)
        elif BACKPORT_TO == '0.09':
            self.ido, self.resid, self.v, self.iparam, self.ipntr, self.info = \
                self._arpack_solver(self.ido, self.bmat, self.which, self.k,
                                    self.tol, self.resid, self.v, self.iparam,
                                    self.ipntr, self.workd, self.workl, self.info)
        xslice = slice(self.ipntr[0] - 1, self.ipntr[0] - 1 + self.n)
        yslice = slice(self.ipntr[1] - 1, self.ipntr[1] - 1 + self.n)
        if self.ido == -1:
                        self.workd[yslice] = self.OP(self.workd[xslice])
        elif self.ido == 1:
                        if self.mode == 1:
                self.workd[yslice] = self.OP(self.workd[xslice])
            elif self.mode == 2:
                self.workd[xslice] = self.OPb(self.workd[xslice])
                self.workd[yslice] = self.OPa(self.workd[xslice])
            elif self.mode == 5:
                Bxslice = slice(self.ipntr[2] - 1, self.ipntr[2] - 1 + self.n)
                Ax = self.A_matvec(self.workd[xslice])
                self.workd[yslice] = self.OPa(Ax + (self.sigma *
                                                    self.workd[Bxslice]))
            else:
                Bxslice = slice(self.ipntr[2] - 1, self.ipntr[2] - 1 + self.n)
                self.workd[yslice] = self.OPa(self.workd[Bxslice])
        elif self.ido == 2:
            self.workd[yslice] = self.B(self.workd[xslice])
        elif self.ido == 3:
            raise ValueError("ARPACK requested user shifts.  Assure ISHIFT==0")
        else:
            self.converged = True
            if self.info == 0:
                pass
            elif self.info == 1:
                self._raise_no_convergence()
            else:
                raise ArpackError(self.info, infodict=self.iterate_infodict)
    def extract(self, return_eigenvectors):
        rvec = return_eigenvectors
        ierr = 0
        howmny = 'A'          sselect = np.zeros(self.ncv, 'int')          d, z, ierr = self._arpack_extract(rvec, howmny, sselect, self.sigma,
                                          self.bmat, self.which, self.k,
                                          self.tol, self.resid, self.v,
                                          self.iparam[0:7], self.ipntr,
                                          self.workd[0:2 * self.n],
                                          self.workl, ierr)
        if ierr != 0:
            raise ArpackError(ierr, infodict=self.extract_infodict)
        k_ok = self.iparam[4]
        d = d[:k_ok]
        z = z[:, :k_ok]
        if return_eigenvectors:
            return d, z
        else:
            return d

class _UnsymmetricArpackParams(_ArpackParams):
    def __init__(self, n, k, tp, matvec, mode=1, M_matvec=None,
                 Minv_matvec=None, sigma=None,
                 ncv=None, v0=None, maxiter=None, which="LM", tol=0):
                                                                                                                                                                                                                                                                                        if mode == 1:
            if matvec is None:
                raise ValueError("matvec must be specified for mode=1")
            if M_matvec is not None:
                raise ValueError("M_matvec cannot be specified for mode=1")
            if Minv_matvec is not None:
                raise ValueError("Minv_matvec cannot be specified for mode=1")
            self.OP = matvec
            self.B = lambda x: x
            self.bmat = 'I'
        elif mode == 2:
            if matvec is None:
                raise ValueError("matvec must be specified for mode=2")
            if M_matvec is None:
                raise ValueError("M_matvec must be specified for mode=2")
            if Minv_matvec is None:
                raise ValueError("Minv_matvec must be specified for mode=2")
            self.OP = lambda x: Minv_matvec(matvec(x))
            self.OPa = Minv_matvec
            self.OPb = matvec
            self.B = M_matvec
            self.bmat = 'G'
        elif mode in (3, 4):
            if matvec is None:
                raise ValueError("matvec must be specified "
                                 "for mode in (3,4)")
            if Minv_matvec is None:
                raise ValueError("Minv_matvec must be specified "
                                 "for mode in (3,4)")
            self.matvec = matvec
            if tp in 'DF':                  if mode == 3:
                    self.OPa = Minv_matvec
                else:
                    raise ValueError("mode=4 invalid for complex A")
            else:                  if mode == 3:
                    self.OPa = lambda x: np.real(Minv_matvec(x))
                else:
                    self.OPa = lambda x: np.imag(Minv_matvec(x))
            if M_matvec is None:
                self.B = lambda x: x
                self.bmat = 'I'
                self.OP = self.OPa
            else:
                self.B = M_matvec
                self.bmat = 'G'
                self.OP = lambda x: self.OPa(M_matvec(x))
        else:
            raise ValueError("mode=%i not implemented" % mode)
        if which not in _NEUPD_WHICH:
            raise ValueError("Parameter which must be one of %s"
                             % ' '.join(_NEUPD_WHICH))
        if k >= n - 1:
            raise ValueError("k must be less than ndim(A)-1, k=%d" % k)
        _ArpackParams.__init__(self, n, k, tp, mode, sigma,
                               ncv, v0, maxiter, which, tol)
        if self.ncv > n or self.ncv <= k + 1:
            raise ValueError("ncv must be k+1<ncv<=n, ncv=%s" % self.ncv)
                self.workd = _aligned_zeros(3 * n, self.tp)
        self.workl = _aligned_zeros(3 * self.ncv * (self.ncv + 2), self.tp)
        ltr = _type_conv[self.tp]
        self._arpack_solver = _arpack.__dict__[ltr + 'naupd']
        self._arpack_extract = _arpack.__dict__[ltr + 'neupd']
        self.iterate_infodict = _NAUPD_ERRORS[ltr]
        self.extract_infodict = _NEUPD_ERRORS[ltr]
        self.ipntr = np.zeros(14, "int")
        if self.tp in 'FD':
                        self.rwork = _aligned_zeros(self.ncv, self.tp.lower())
        else:
            self.rwork = None
    def iterate(self):
        if BACKPORT_TO is None:
            return None
        if BACKPORT_TO == '0.10':
            if self.tp in 'fd':
                self.ido, self.tol, self.resid, self.v, self.iparam, self.ipntr, self.info =\
                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,
                                        self.tol, self.resid, self.v, self.iparam,
                                        self.ipntr, self.workd, self.workl,
                                        self.info)
            else:
                self.ido, self.tol, self.resid, self.v, self.iparam, self.ipntr, self.info =\
                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,
                                        self.tol, self.resid, self.v, self.iparam,
                                        self.ipntr, self.workd, self.workl,
                                        self.rwork, self.info)
        elif BACKPORT_TO == '0.09':
            if self.tp in 'fd':
                self.ido, self.resid, self.v, self.iparam, self.ipntr, self.info =\
                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,
                                        self.tol, self.resid, self.v, self.iparam,
                                        self.ipntr, self.workd, self.workl,
                                        self.info)
            else:
                self.ido, self.resid, self.v, self.iparam, self.ipntr, self.info =\
                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,
                                        self.tol, self.resid, self.v, self.iparam,
                                        self.ipntr, self.workd, self.workl,
                                        self.rwork, self.info)
        xslice = slice(self.ipntr[0] - 1, self.ipntr[0] - 1 + self.n)
        yslice = slice(self.ipntr[1] - 1, self.ipntr[1] - 1 + self.n)
        if self.ido == -1:
                        self.workd[yslice] = self.OP(self.workd[xslice])
        elif self.ido == 1:
                        if self.mode in (1, 2):
                self.workd[yslice] = self.OP(self.workd[xslice])
            else:
                Bxslice = slice(self.ipntr[2] - 1, self.ipntr[2] - 1 + self.n)
                self.workd[yslice] = self.OPa(self.workd[Bxslice])
        elif self.ido == 2:
            self.workd[yslice] = self.B(self.workd[xslice])
        elif self.ido == 3:
            raise ValueError("ARPACK requested user shifts.  Assure ISHIFT==0")
        else:
            self.converged = True
            if self.info == 0:
                pass
            elif self.info == 1:
                self._raise_no_convergence()
            else:
                raise ArpackError(self.info, infodict=self.iterate_infodict)
    def extract(self, return_eigenvectors):
        k, n = self.k, self.n
        ierr = 0
        howmny = 'A'          sselect = np.zeros(self.ncv, 'int')          sigmar = np.real(self.sigma)
        sigmai = np.imag(self.sigma)
        workev = np.zeros(3 * self.ncv, self.tp)
        if self.tp in 'fd':
            dr = np.zeros(k + 1, self.tp)
            di = np.zeros(k + 1, self.tp)
            zr = np.zeros((n, k + 1), self.tp)
            dr, di, zr, ierr = \
                self._arpack_extract(return_eigenvectors,
                       howmny, sselect, sigmar, sigmai, workev,
                       self.bmat, self.which, k, self.tol, self.resid,
                       self.v, self.iparam, self.ipntr,
                       self.workd, self.workl, self.info)
            if ierr != 0:
                raise ArpackError(ierr, infodict=self.extract_infodict)
            nreturned = self.iparam[4]  
                        d = dr + 1.0j * di
                                    z = zr.astype(self.tp.upper())
                                    
                                    if sigmai == 0:
                i = 0
                while i <= k:
                                        if abs(d[i].imag) != 0:
                                                                        if i < k:
                            z[:, i] = zr[:, i] + 1.0j * zr[:, i + 1]
                            z[:, i + 1] = z[:, i].conjugate()
                            i += 1
                        else:
                                                                                                                                            nreturned -= 1
                    i += 1
            else:
                                                                i = 0
                while i <= k:
                    if abs(d[i].imag) == 0:
                        d[i] = np.dot(zr[:, i], self.matvec(zr[:, i]))
                    else:
                        if i < k:
                            z[:, i] = zr[:, i] + 1.0j * zr[:, i + 1]
                            z[:, i + 1] = z[:, i].conjugate()
                            d[i] = ((np.dot(zr[:, i],
                                            self.matvec(zr[:, i])) +
                                     np.dot(zr[:, i + 1],
                                            self.matvec(zr[:, i + 1]))) +
                                    1j * (np.dot(zr[:, i],
                                          self.matvec(zr[:, i + 1])) -
                                          np.dot(zr[:, i + 1],
                                          self.matvec(zr[:, i]))))
                            d[i + 1] = d[i].conj()
                            i += 1
                        else:
                                                                                                                                            nreturned -= 1
                    i += 1
                        
            if nreturned <= k:
                                d = d[:nreturned]
                z = z[:, :nreturned]
            else:
                                                rd = np.round(d, decimals=_ndigits[self.tp])
                if self.which in ['LR', 'SR']:
                    ind = np.argsort(rd.real)
                elif self.which in ['LI', 'SI']:
                                                            ind = np.argsort(abs(rd.imag))
                else:
                    ind = np.argsort(abs(rd))
                if self.which in ['LR', 'LM', 'LI']:
                    d = d[ind[-k:]]
                    z = z[:, ind[-k:]]
                if self.which in ['SR', 'SM', 'SI']:
                    d = d[ind[:k]]
                    z = z[:, ind[:k]]
        else:
                        d, z, ierr =\
                    self._arpack_extract(return_eigenvectors,
                           howmny, sselect, self.sigma, workev,
                           self.bmat, self.which, k, self.tol, self.resid,
                           self.v, self.iparam, self.ipntr,
                           self.workd, self.workl, self.rwork, ierr)
            if ierr != 0:
                raise ArpackError(ierr, infodict=self.extract_infodict)
            k_ok = self.iparam[4]
            d = d[:k_ok]
            z = z[:, :k_ok]
        if return_eigenvectors:
            return d, z
        else:
            return d

def _aslinearoperator_with_dtype(m):
    m = aslinearoperator(m)
    if not hasattr(m, 'dtype'):
        x = np.zeros(m.shape[1])
        m.dtype = (m * x).dtype
    return m

class SpLuInv(LinearOperator):
        def __init__(self, M):
        self.M_lu = splu(M)
        self.shape = M.shape
        self.dtype = M.dtype
        self.isreal = not np.issubdtype(self.dtype, np.complexfloating)
    def _matvec(self, x):
                        x = np.asarray(x)
        if self.isreal and np.issubdtype(x.dtype, np.complexfloating):
            return (self.M_lu.solve(np.real(x).astype(self.dtype)) +
                    1j * self.M_lu.solve(np.imag(x).astype(self.dtype)))
        else:
            return self.M_lu.solve(x.astype(self.dtype))

class LuInv(LinearOperator):
        def __init__(self, M):
        self.M_lu = lu_factor(M)
        self.shape = M.shape
        self.dtype = M.dtype
    def _matvec(self, x):
        return lu_solve(self.M_lu, x)

class IterInv(LinearOperator):
        def __init__(self, M, ifunc=gmres, tol=0):
        if tol <= 0:
                                    tol = 2 * np.finfo(M.dtype).eps
        self.M = M
        self.ifunc = ifunc
        self.tol = tol
        if hasattr(M, 'dtype'):
            self.dtype = M.dtype
        else:
            x = np.zeros(M.shape[1])
            self.dtype = (M * x).dtype
        self.shape = M.shape
    def _matvec(self, x):
        b, info = self.ifunc(self.M, x, tol=self.tol)
        if info != 0:
            raise ValueError("Error in inverting M: function "
                             "%s did not converge (info = %i)."
                             % (self.ifunc.__name__, info))
        return b

class IterOpInv(LinearOperator):
        def __init__(self, A, M, sigma, ifunc=gmres, tol=0):
        if tol <= 0:
                                    tol = 2 * np.finfo(A.dtype).eps
        self.A = A
        self.M = M
        self.sigma = sigma
        self.ifunc = ifunc
        self.tol = tol
        def mult_func(x):
            return A.matvec(x) - sigma * M.matvec(x)
        def mult_func_M_None(x):
            return A.matvec(x) - sigma * x
        x = np.zeros(A.shape[1])
        if M is None:
            dtype = mult_func_M_None(x).dtype
            self.OP = LinearOperator(self.A.shape,
                                     mult_func_M_None,
                                     dtype=dtype)
        else:
            dtype = mult_func(x).dtype
            self.OP = LinearOperator(self.A.shape,
                                     mult_func,
                                     dtype=dtype)
        self.shape = A.shape
    def _matvec(self, x):
        b, info = self.ifunc(self.OP, x, tol=self.tol)
        if info != 0:
            raise ValueError("Error in inverting [A-sigma*M]: function "
                             "%s did not converge (info = %i)."
                             % (self.ifunc.__name__, info))
        return b
    @property
    def dtype(self):
        return self.OP.dtype

def get_inv_matvec(M, symmetric=False, tol=0):
    if isdense(M):
        return LuInv(M).matvec
    elif isspmatrix(M):
        if isspmatrix_csr(M) and symmetric:
            M = M.T
        return SpLuInv(M).matvec
    else:
        return IterInv(M, tol=tol).matvec

def get_OPinv_matvec(A, M, sigma, symmetric=False, tol=0):
    if sigma == 0:
        return get_inv_matvec(A, symmetric=symmetric, tol=tol)
    if M is None:
                if isdense(A):
            if (np.issubdtype(A.dtype, np.complexfloating) or
               np.imag(sigma) == 0):
                A = np.copy(A)
            else:
                A = A + 0j
            A.flat[::A.shape[1] + 1] -= sigma
            return LuInv(A).matvec
        elif isspmatrix(A):
            A = A - sigma * identity(A.shape[0])
            if symmetric and isspmatrix_csr(A):
                A = A.T
            return SpLuInv(A.tocsc()).matvec
        else:
            return IterOpInv(_aslinearoperator_with_dtype(A),
                             M, sigma, tol=tol).matvec
    else:
        if ((not isdense(A) and not isspmatrix(A)) or
                (not isdense(M) and not isspmatrix(M))):
            return IterOpInv(_aslinearoperator_with_dtype(A),
                             _aslinearoperator_with_dtype(M),
                             sigma, tol=tol).matvec
        elif isdense(A) or isdense(M):
            return LuInv(A - sigma * M).matvec
        else:
            OP = A - sigma * M
            if symmetric and isspmatrix_csr(OP):
                OP = OP.T
            return SpLuInv(OP.tocsc()).matvec

def _eigs(A, k=6, M=None, sigma=None, which='LM', v0=None,
          ncv=None, maxiter=None, tol=0, return_eigenvectors=True,
          Minv=None, OPinv=None, OPpart=None):
        if A.shape[0] != A.shape[1]:
        raise ValueError('expected square matrix (shape=%s)' % (A.shape,))
    if M is not None:
        if M.shape != A.shape:
            raise ValueError('wrong M dimensions %s, should be %s'
                             % (M.shape, A.shape))
        if np.dtype(M.dtype).char.lower() != np.dtype(A.dtype).char.lower():
            import warnings
            warnings.warn('M does not have the same type precision as A. '
                          'This may adversely affect ARPACK convergence')
    n = A.shape[0]
    if k <= 0 or k >= n:
        raise ValueError("k=%d must be between 1 and ndim(A)-1=%d"
                         % (k, n - 1))
    if sigma is None:
        matvec = _aslinearoperator_with_dtype(A).matvec
        if OPinv is not None:
            raise ValueError("OPinv should not be specified "
                             "with sigma = None.")
        if OPpart is not None:
            raise ValueError("OPpart should not be specified with "
                             "sigma = None or complex A")
        if M is None:
                        mode = 1
            M_matvec = None
            Minv_matvec = None
            if Minv is not None:
                raise ValueError("Minv should not be "
                                 "specified with M = None.")
        else:
                        mode = 2
            if Minv is None:
                Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol)
            else:
                Minv = _aslinearoperator_with_dtype(Minv)
                Minv_matvec = Minv.matvec
            M_matvec = _aslinearoperator_with_dtype(M).matvec
    else:
                if np.issubdtype(A.dtype, np.complexfloating):
            if OPpart is not None:
                raise ValueError("OPpart should not be specified "
                                 "with sigma=None or complex A")
            mode = 3
        elif OPpart is None or OPpart.lower() == 'r':
            mode = 3
        elif OPpart.lower() == 'i':
            if np.imag(sigma) == 0:
                raise ValueError("OPpart cannot be 'i' if sigma is real")
            mode = 4
        else:
            raise ValueError("OPpart must be one of ('r','i')")
        matvec = _aslinearoperator_with_dtype(A).matvec
        if Minv is not None:
            raise ValueError("Minv should not be specified when sigma is")
        if OPinv is None:
            Minv_matvec = get_OPinv_matvec(A, M, sigma,
                                           symmetric=False, tol=tol)
        else:
            OPinv = _aslinearoperator_with_dtype(OPinv)
            Minv_matvec = OPinv.matvec
        if M is None:
            M_matvec = None
        else:
            M_matvec = _aslinearoperator_with_dtype(M).matvec
    params = _UnsymmetricArpackParams(n, k, A.dtype.char, matvec, mode,
                                      M_matvec, Minv_matvec, sigma,
                                      ncv, v0, maxiter, which, tol)
    while not params.converged:
        params.iterate()
    return params.extract(return_eigenvectors)

def _eigsh(A, k=6, M=None, sigma=None, which='LM', v0=None,
           ncv=None, maxiter=None, tol=0, return_eigenvectors=True,
           Minv=None, OPinv=None, mode='normal'):
            if np.issubdtype(A.dtype, np.complexfloating):
        if mode != 'normal':
            raise ValueError("mode=%s cannot be used with "
                             "complex matrix A" % mode)
        if which == 'BE':
            raise ValueError("which='BE' cannot be used with complex matrix A")
        elif which == 'LA':
            which = 'LR'
        elif which == 'SA':
            which = 'SR'
        ret = eigs(A, k, M=M, sigma=sigma, which=which, v0=v0,
                   ncv=ncv, maxiter=maxiter, tol=tol,
                   return_eigenvectors=return_eigenvectors, Minv=Minv,
                   OPinv=OPinv)
        if return_eigenvectors:
            return ret[0].real, ret[1]
        else:
            return ret.real
    if A.shape[0] != A.shape[1]:
        raise ValueError('expected square matrix (shape=%s)' % (A.shape,))
    if M is not None:
        if M.shape != A.shape:
            raise ValueError('wrong M dimensions %s, should be %s'
                             % (M.shape, A.shape))
        if np.dtype(M.dtype).char.lower() != np.dtype(A.dtype).char.lower():
            import warnings
            warnings.warn('M does not have the same type precision as A. '
                          'This may adversely affect ARPACK convergence')
    n = A.shape[0]
    if k <= 0 or k >= n:
        raise ValueError("k must be between 1 and the order of the "
                         "square input matrix.")
    if sigma is None:
        A = _aslinearoperator_with_dtype(A)
        matvec = A.matvec
        if OPinv is not None:
            raise ValueError("OPinv should not be specified "
                             "with sigma = None.")
        if M is None:
                        mode = 1
            M_matvec = None
            Minv_matvec = None
            if Minv is not None:
                raise ValueError("Minv should not be "
                                 "specified with M = None.")
        else:
                        mode = 2
            if Minv is None:
                Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol)
            else:
                Minv = _aslinearoperator_with_dtype(Minv)
                Minv_matvec = Minv.matvec
            M_matvec = _aslinearoperator_with_dtype(M).matvec
    else:
                if Minv is not None:
            raise ValueError("Minv should not be specified when sigma is")
                if mode == 'normal':
            mode = 3
            matvec = None
            if OPinv is None:
                Minv_matvec = get_OPinv_matvec(A, M, sigma,
                                               symmetric=True, tol=tol)
            else:
                OPinv = _aslinearoperator_with_dtype(OPinv)
                Minv_matvec = OPinv.matvec
            if M is None:
                M_matvec = None
            else:
                M = _aslinearoperator_with_dtype(M)
                M_matvec = M.matvec
                elif mode == 'buckling':
            mode = 4
            if OPinv is None:
                Minv_matvec = get_OPinv_matvec(A, M, sigma,
                                               symmetric=True, tol=tol)
            else:
                Minv_matvec = _aslinearoperator_with_dtype(OPinv).matvec
            matvec = _aslinearoperator_with_dtype(A).matvec
            M_matvec = None
                elif mode == 'cayley':
            mode = 5
            matvec = _aslinearoperator_with_dtype(A).matvec
            if OPinv is None:
                Minv_matvec = get_OPinv_matvec(A, M, sigma,
                                               symmetric=True, tol=tol)
            else:
                Minv_matvec = _aslinearoperator_with_dtype(OPinv).matvec
            if M is None:
                M_matvec = None
            else:
                M_matvec = _aslinearoperator_with_dtype(M).matvec
                else:
            raise ValueError("unrecognized mode '%s'" % mode)
    params = _SymmetricArpackParams(n, k, A.dtype.char, matvec, mode,
                                    M_matvec, Minv_matvec, sigma,
                                    ncv, v0, maxiter, which, tol)
    while not params.converged:
        params.iterate()
    return params.extract(return_eigenvectors)

def _augmented_orthonormal_cols(x, k):
        n, m = x.shape
        y = np.empty((n, m+k), dtype=x.dtype)
    y[:, :m] = x
        for i in range(k):
                v = np.random.randn(n)
        if np.iscomplexobj(x):
            v = v + 1j*np.random.randn(n)
                for j in range(m+i):
            u = y[:, j]
            v -= (np.dot(v, u.conj()) / np.dot(u, u.conj())) * u
                v /= np.sqrt(np.dot(v, v.conj()))
                y[:, m+i] = v
        return y

def _augmented_orthonormal_rows(x, k):
    return _augmented_orthonormal_cols(x.T, k).T

def _herm(x):
    return x.T.conj()

def _svds(A, k=6, ncv=None, tol=0, which='LM', v0=None,
          maxiter=None, return_singular_vectors=True):
        if not (isinstance(A, LinearOperator) or isspmatrix(A)):
        A = np.asarray(A)
    n, m = A.shape
    if isinstance(A, LinearOperator):
        if n > m:
            X_dot = A.matvec
            X_matmat = A.matmat
            XH_dot = A.rmatvec
        else:
            X_dot = A.rmatvec
            XH_dot = A.matvec
            dtype = getattr(A, 'dtype', None)
            if dtype is None:
                dtype = A.dot(np.zeros([m, 1])).dtype
                                    def X_matmat(V):
                out = np.empty((V.shape[1], m), dtype=dtype)
                for i, col in enumerate(V.T):
                    out[i, :] = A.rmatvec(col.reshape(-1, 1)).T
                return out.T
    else:
        if n > m:
            X_dot = X_matmat = A.dot
            XH_dot = _herm(A).dot
        else:
            XH_dot = A.dot
            X_dot = X_matmat = _herm(A).dot
    def matvec_XH_X(x):
        return XH_dot(X_dot(x))
    XH_X = LinearOperator(matvec=matvec_XH_X, dtype=A.dtype,
                          shape=(min(A.shape), min(A.shape)))
            eigvals, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter,
                            ncv=ncv, which=which, v0=v0)
            if which == 'LM':
                eigvals = np.maximum(eigvals.real, 0)
                t = eigvec.dtype.char.lower()
        factor = {'f': 1E3, 'd': 1E6}
        cond = factor[t] * np.finfo(t).eps
        cutoff = cond * np.max(eigvals)
                        above_cutoff = (eigvals > cutoff)
        nlarge = above_cutoff.sum()
        nsmall = k - nlarge
        slarge = np.sqrt(eigvals[above_cutoff])
        s = np.zeros_like(eigvals)
        s[:nlarge] = slarge
        if not return_singular_vectors:
            return s
        if n > m:
            vlarge = eigvec[:, above_cutoff]
            ularge = X_matmat(vlarge) / slarge if return_singular_vectors != 'vh' else None
            vhlarge = _herm(vlarge)
        else:
            ularge = eigvec[:, above_cutoff]
            vhlarge = _herm(X_matmat(ularge) / slarge) if return_singular_vectors != 'u' else None
        u = _augmented_orthonormal_cols(ularge, nsmall) if ularge is not None else None
        vh = _augmented_orthonormal_rows(vhlarge, nsmall) if vhlarge is not None else None
    elif which == 'SM':
        s = np.sqrt(eigvals)
        if not return_singular_vectors:
            return s
        if n > m:
            v = eigvec
            u = X_matmat(v) / s if return_singular_vectors != 'vh' else None
            vh = _herm(v)
        else:
            u = eigvec
            vh = _herm(X_matmat(u) / s) if return_singular_vectors != 'u' else None
    else:
        raise ValueError("which must be either 'LM' or 'SM'.")
    return u, s, vh

if scipy.version.version >= LooseVersion('0.12'):
    from scipy.sparse.linalg import eigs, eigsh, svds
else:
    eigs, eigsh, svds = _eigs, _eigsh, _svds
cimport numpy as np
import  numpy as np
cimport cython
from libc.float cimport DBL_MAX, FLT_MAX
cdef extern from "src/cholesky_delete.h":
    int cholesky_delete_dbl(int m, int n, double *L, int go_out)
    int cholesky_delete_flt(int m, int n, float  *L, int go_out)
ctypedef np.float64_t DOUBLE

np.import_array()

def min_pos(np.ndarray X):
      if X.dtype.name == 'float32':
      return _float_min_pos(<float *> X.data, X.size)
   elif X.dtype.name == 'float64':
      return _double_min_pos(<double *> X.data, X.size)
   else:
      raise ValueError('Unsupported dtype for array X')

cdef float _float_min_pos(float *X, Py_ssize_t size):
   cdef Py_ssize_t i
   cdef float min_val = DBL_MAX
   for i in range(size):
      if 0. < X[i] < min_val:
         min_val = X[i]
   return min_val

cdef double _double_min_pos(double *X, Py_ssize_t size):
   cdef Py_ssize_t i
   cdef np.float64_t min_val = FLT_MAX
   for i in range(size):
      if 0. < X[i] < min_val:
         min_val = X[i]
   return min_val

def cholesky_delete(np.ndarray L, int go_out):
    cdef int n = <int> L.shape[0]
    cdef int m = <int> L.strides[0]
    if L.dtype.name == 'float64':
        cholesky_delete_dbl(m / sizeof(double), n, <double *> L.data, go_out)
    elif L.dtype.name == 'float32':
        cholesky_delete_flt(m / sizeof(float),  n, <float *> L.data,  go_out)
    else:
        raise TypeError("unsupported dtype %r." % L.dtype)

def total_seconds(delta):
    
    mu_sec = 1e-6  
    return delta.seconds + delta.microseconds * mu_sec
import numpy as np
from ..externals import six
from ..utils.fixes import in1d
from .fixes import bincount

def compute_class_weight(class_weight, classes, y):
            from ..preprocessing import LabelEncoder
    if set(y) - set(classes):
        raise ValueError("classes should include all valid labels that can "
                         "be in y")
    if class_weight is None or len(class_weight) == 0:
                weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
    elif class_weight == 'balanced':
                le = LabelEncoder()
        y_ind = le.fit_transform(y)
        if not all(np.in1d(classes, le.classes_)):
            raise ValueError("classes should have valid labels that are in y")
        recip_freq = len(y) / (len(le.classes_) *
                               bincount(y_ind).astype(np.float64))
        weight = recip_freq[le.transform(classes)]
    else:
                weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
        if not isinstance(class_weight, dict):
            raise ValueError("class_weight must be dict, 'balanced', or None,"
                             " got: %r" % class_weight)
        for c in class_weight:
            i = np.searchsorted(classes, c)
            if i >= len(classes) or classes[i] != c:
                raise ValueError("Class label {} not present.".format(c))
            else:
                weight[i] = class_weight[c]
    return weight

def compute_sample_weight(class_weight, y, indices=None):
    
    y = np.atleast_1d(y)
    if y.ndim == 1:
        y = np.reshape(y, (-1, 1))
    n_outputs = y.shape[1]
    if isinstance(class_weight, six.string_types):
        if class_weight not in ['balanced']:
            raise ValueError('The only valid preset for class_weight is '
                             '"balanced". Given "%s".' % class_weight)
    elif (indices is not None and
          not isinstance(class_weight, six.string_types)):
        raise ValueError('The only valid class_weight for subsampling is '
                         '"balanced". Given "%s".' % class_weight)
    elif n_outputs > 1:
        if (not hasattr(class_weight, "__iter__") or
                isinstance(class_weight, dict)):
            raise ValueError("For multi-output, class_weight should be a "
                             "list of dicts, or a valid string.")
        if len(class_weight) != n_outputs:
            raise ValueError("For multi-output, number of elements in "
                             "class_weight should match number of outputs.")
    expanded_class_weight = []
    for k in range(n_outputs):
        y_full = y[:, k]
        classes_full = np.unique(y_full)
        classes_missing = None
        if class_weight == 'balanced' or n_outputs == 1:
            class_weight_k = class_weight
        else:
            class_weight_k = class_weight[k]
        if indices is not None:
                                                y_subsample = y[indices, k]
            classes_subsample = np.unique(y_subsample)
            weight_k = np.choose(np.searchsorted(classes_subsample,
                                                 classes_full),
                                 compute_class_weight(class_weight_k,
                                                      classes_subsample,
                                                      y_subsample),
                                 mode='clip')
            classes_missing = set(classes_full) - set(classes_subsample)
        else:
            weight_k = compute_class_weight(class_weight_k,
                                            classes_full,
                                            y_full)
        weight_k = weight_k[np.searchsorted(classes_full, y_full)]
        if classes_missing:
                        weight_k[in1d(y_full, list(classes_missing))] = 0.
        expanded_class_weight.append(weight_k)
    expanded_class_weight = np.prod(expanded_class_weight,
                                    axis=0,
                                    dtype=np.float64)
    return expanded_class_weight
import warnings
__all__ = ["deprecated", ]

class deprecated(object):
    
        
    def __init__(self, extra=''):
                self.extra = extra
    def __call__(self, obj):
        if isinstance(obj, type):
            return self._decorate_class(obj)
        else:
            return self._decorate_fun(obj)
    def _decorate_class(self, cls):
        msg = "Class %s is deprecated" % cls.__name__
        if self.extra:
            msg += "; %s" % self.extra
                init = cls.__init__
        def wrapped(*args, **kwargs):
            warnings.warn(msg, category=DeprecationWarning)
            return init(*args, **kwargs)
        cls.__init__ = wrapped
        wrapped.__name__ = '__init__'
        wrapped.__doc__ = self._update_doc(init.__doc__)
        wrapped.deprecated_original = init
        return cls
    def _decorate_fun(self, fun):
        
        msg = "Function %s is deprecated" % fun.__name__
        if self.extra:
            msg += "; %s" % self.extra
        def wrapped(*args, **kwargs):
            warnings.warn(msg, category=DeprecationWarning)
            return fun(*args, **kwargs)
        wrapped.__name__ = fun.__name__
        wrapped.__dict__ = fun.__dict__
        wrapped.__doc__ = self._update_doc(fun.__doc__)
        return wrapped
    def _update_doc(self, olddoc):
        newdoc = "DEPRECATED"
        if self.extra:
            newdoc = "%s: %s" % (newdoc, self.extra)
        if olddoc:
            newdoc = "%s\n\n%s" % (newdoc, olddoc)
        return newdoc
from __future__ import print_function
import types
import warnings
import sys
import traceback
import pickle
from copy import deepcopy
import numpy as np
from scipy import sparse
from scipy.stats import rankdata
import struct
from sklearn.externals.six.moves import zip
from sklearn.externals.joblib import hash, Memory
from sklearn.utils.testing import assert_raises
from sklearn.utils.testing import assert_raises_regex
from sklearn.utils.testing import assert_raise_message
from sklearn.utils.testing import assert_equal
from sklearn.utils.testing import assert_not_equal
from sklearn.utils.testing import assert_true
from sklearn.utils.testing import assert_false
from sklearn.utils.testing import assert_in
from sklearn.utils.testing import assert_array_equal
from sklearn.utils.testing import assert_array_almost_equal
from sklearn.utils.testing import assert_warns_message
from sklearn.utils.testing import META_ESTIMATORS
from sklearn.utils.testing import set_random_state
from sklearn.utils.testing import assert_greater
from sklearn.utils.testing import assert_greater_equal
from sklearn.utils.testing import SkipTest
from sklearn.utils.testing import ignore_warnings
from sklearn.utils.testing import assert_dict_equal

from sklearn.base import (clone, ClassifierMixin, RegressorMixin,
                          TransformerMixin, ClusterMixin, BaseEstimator)
from sklearn.metrics import accuracy_score, adjusted_rand_score, f1_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.random_projection import BaseRandomProjection
from sklearn.feature_selection import SelectKBest
from sklearn.svm.base import BaseLibSVM
from sklearn.pipeline import make_pipeline
from sklearn.exceptions import ConvergenceWarning
from sklearn.exceptions import DataConversionWarning
from sklearn.exceptions import SkipTestWarning
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.utils.fixes import signature
from sklearn.utils.validation import has_fit_parameter
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris, load_boston, make_blobs

BOSTON = None
CROSS_DECOMPOSITION = ['PLSCanonical', 'PLSRegression', 'CCA', 'PLSSVD']
MULTI_OUTPUT = ['CCA', 'DecisionTreeRegressor', 'ElasticNet',
                'ExtraTreeRegressor', 'ExtraTreesRegressor', 'GaussianProcess',
                'GaussianProcessRegressor',
                'KNeighborsRegressor', 'KernelRidge', 'Lars', 'Lasso',
                'LassoLars', 'LinearRegression', 'MultiTaskElasticNet',
                'MultiTaskElasticNetCV', 'MultiTaskLasso', 'MultiTaskLassoCV',
                'OrthogonalMatchingPursuit', 'PLSCanonical', 'PLSRegression',
                'RANSACRegressor', 'RadiusNeighborsRegressor',
                'RandomForestRegressor', 'Ridge', 'RidgeCV']

def _yield_non_meta_checks(name, Estimator):
    yield check_estimators_dtypes
    yield check_fit_score_takes_y
    yield check_dtype_object
    yield check_sample_weights_pandas_series
    yield check_sample_weights_list
    yield check_estimators_fit_returns_self
            yield check_estimators_empty_data_messages
    if name not in CROSS_DECOMPOSITION + ['SpectralEmbedding']:
                                yield check_pipeline_consistency
    if name not in ['Imputer']:
                yield check_estimators_nan_inf
    if name not in ['GaussianProcess']:
                        yield check_estimators_overwrite_params
    if hasattr(Estimator, 'sparsify'):
        yield check_sparsify_coefficients
    yield check_estimator_sparse_data
            yield check_estimators_pickle

def _yield_classifier_checks(name, Classifier):
        yield check_classifier_data_not_an_array
        yield check_classifiers_one_label
    yield check_classifiers_classes
    yield check_estimators_partial_fit_n_features
        yield check_classifiers_train
    yield check_classifiers_regression_target
    if (name not in
        ["MultinomialNB", "LabelPropagation", "LabelSpreading"] and
               name not in ["DecisionTreeClassifier", "ExtraTreeClassifier"]):
                        
        yield check_supervised_y_2d
        yield check_estimators_unfitted
    if 'class_weight' in Classifier().get_params().keys():
        yield check_class_weight_classifiers
    yield check_non_transformer_estimators_n_iter
        yield check_decision_proba_consistency

@ignore_warnings(category=DeprecationWarning)
def check_supervised_y_no_nan(name, Estimator):
    
    rng = np.random.RandomState(888)
    X = rng.randn(10, 5)
    y = np.ones(10) * np.inf
    y = multioutput_estimator_convert_y_2d(name, y)
    errmsg = "Input contains NaN, infinity or a value too large for " \
             "dtype('float64')."
    try:
        Estimator().fit(X, y)
    except ValueError as e:
        if str(e) != errmsg:
            raise ValueError("Estimator {0} raised warning as expected, but "
                             "does not match expected error message"
                             .format(name))
    else:
        raise ValueError("Estimator {0} should have raised error on fitting "
                         "array y with NaN value.".format(name))

def _yield_regressor_checks(name, Regressor):
                yield check_regressors_train
    yield check_regressor_data_not_an_array
    yield check_estimators_partial_fit_n_features
    yield check_regressors_no_decision_function
    yield check_supervised_y_2d
    yield check_supervised_y_no_nan
    if name != 'CCA':
                yield check_regressors_int
    if name != "GaussianProcessRegressor":
                yield check_estimators_unfitted
    yield check_non_transformer_estimators_n_iter

def _yield_transformer_checks(name, Transformer):
            if name not in ['AdditiveChi2Sampler', 'Binarizer', 'Normalizer',
                    'PLSCanonical', 'PLSRegression', 'CCA', 'PLSSVD']:
        yield check_transformer_data_not_an_array
        if name not in ['AdditiveChi2Sampler', 'Binarizer',
                    'FunctionTransformer', 'Normalizer']:
                yield check_transformer_general
        yield check_transformers_unfitted
            external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding',
                       'RandomizedLasso', 'LogisticRegressionCV']
    if name not in external_solver:
        yield check_transformer_n_iter

def _yield_clustering_checks(name, Clusterer):
    yield check_clusterer_compute_labels_predict
    if name not in ('WardAgglomeration', "FeatureAgglomeration"):
                        yield check_clustering
        yield check_estimators_partial_fit_n_features
    yield check_non_transformer_estimators_n_iter

def _yield_all_checks(name, Estimator):
    for check in _yield_non_meta_checks(name, Estimator):
        yield check
    if issubclass(Estimator, ClassifierMixin):
        for check in _yield_classifier_checks(name, Estimator):
            yield check
    if issubclass(Estimator, RegressorMixin):
        for check in _yield_regressor_checks(name, Estimator):
            yield check
    if issubclass(Estimator, TransformerMixin):
        for check in _yield_transformer_checks(name, Estimator):
            yield check
    if issubclass(Estimator, ClusterMixin):
        for check in _yield_clustering_checks(name, Estimator):
            yield check
    yield check_fit2d_predict1d
    yield check_fit2d_1sample
    yield check_fit2d_1feature
    yield check_fit1d_1feature
    yield check_fit1d_1sample
    yield check_get_params_invariance
    yield check_dict_unchanged
    yield check_no_fit_attributes_set_in_init
    yield check_dont_overwrite_parameters

def check_estimator(Estimator):
        name = Estimator.__name__
    check_parameters_default_constructible(name, Estimator)
    for check in _yield_all_checks(name, Estimator):
        try:
            check(name, Estimator)
        except SkipTest as message:
                                    warnings.warn(message, SkipTestWarning)

def _boston_subset(n_samples=200):
    global BOSTON
    if BOSTON is None:
        boston = load_boston()
        X, y = boston.data, boston.target
        X, y = shuffle(X, y, random_state=0)
        X, y = X[:n_samples], y[:n_samples]
        X = StandardScaler().fit_transform(X)
        BOSTON = X, y
    return BOSTON

def set_testing_parameters(estimator):
            params = estimator.get_params()
    if ("n_iter" in params and estimator.__class__.__name__ != "TSNE"):
        estimator.set_params(n_iter=5)
    if "max_iter" in params:
        warnings.simplefilter("ignore", ConvergenceWarning)
        if estimator.max_iter is not None:
            estimator.set_params(max_iter=min(5, estimator.max_iter))
                if estimator.__class__.__name__ == 'LinearSVR':
            estimator.set_params(max_iter=20)
                if estimator.__class__.__name__ == 'NMF':
            estimator.set_params(max_iter=100)
                if estimator.__class__.__name__ in ['MLPClassifier', 'MLPRegressor']:
            estimator.set_params(max_iter=100)
    if "n_resampling" in params:
                estimator.set_params(n_resampling=5)
    if "n_estimators" in params:
                estimator.set_params(n_estimators=min(5, estimator.n_estimators))
    if "max_trials" in params:
                estimator.set_params(max_trials=10)
    if "n_init" in params:
                estimator.set_params(n_init=2)
    if "decision_function_shape" in params:
                estimator.set_params(decision_function_shape='ovo')
    if estimator.__class__.__name__ == "SelectFdr":
                estimator.set_params(alpha=.5)
    if estimator.__class__.__name__ == "TheilSenRegressor":
        estimator.max_subpopulation = 100
    if isinstance(estimator, BaseRandomProjection):
                                        estimator.set_params(n_components=1)
    if isinstance(estimator, SelectKBest):
                        estimator.set_params(k=1)

class NotAnArray(object):
    " An object that is convertable to an array"
    def __init__(self, data):
        self.data = data
    def __array__(self, dtype=None):
        return self.data

def _is_32bit():
        return struct.calcsize('P') * 8 == 32

def check_estimator_sparse_data(name, Estimator):
    rng = np.random.RandomState(0)
    X = rng.rand(40, 10)
    X[X < .8] = 0
    X_csr = sparse.csr_matrix(X)
    y = (4 * rng.rand(40)).astype(np.int)
    for sparse_format in ['csr', 'csc', 'dok', 'lil', 'coo', 'dia', 'bsr']:
        X = X_csr.asformat(sparse_format)
                with ignore_warnings(category=DeprecationWarning):
            if name in ['Scaler', 'StandardScaler']:
                estimator = Estimator(with_mean=False)
            else:
                estimator = Estimator()
        set_testing_parameters(estimator)
                try:
            with ignore_warnings(category=DeprecationWarning):
                estimator.fit(X, y)
            if hasattr(estimator, "predict"):
                pred = estimator.predict(X)
                assert_equal(pred.shape, (X.shape[0],))
            if hasattr(estimator, 'predict_proba'):
                probs = estimator.predict_proba(X)
                assert_equal(probs.shape, (X.shape[0], 4))
        except TypeError as e:
            if 'sparse' not in repr(e):
                print("Estimator %s doesn't seem to fail gracefully on "
                      "sparse data: error message state explicitly that "
                      "sparse input is not supported if this is not the case."
                      % name)
                raise
        except Exception:
            print("Estimator %s doesn't seem to fail gracefully on "
                  "sparse data: it should raise a TypeError if sparse input "
                  "is explicitly not supported." % name)
            raise

@ignore_warnings(category=DeprecationWarning)
def check_sample_weights_pandas_series(name, Estimator):
            estimator = Estimator()
    if has_fit_parameter(estimator, "sample_weight"):
        try:
            import pandas as pd
            X = pd.DataFrame([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3]])
            y = pd.Series([1, 1, 1, 2, 2, 2])
            weights = pd.Series([1] * 6)
            try:
                estimator.fit(X, y, sample_weight=weights)
            except ValueError:
                raise ValueError("Estimator {0} raises error if "
                                 "'sample_weight' parameter is of "
                                 "type pandas.Series".format(name))
        except ImportError:
            raise SkipTest("pandas is not installed: not testing for "
                           "input of type pandas.Series to class weight.")

@ignore_warnings(category=DeprecationWarning)
def check_sample_weights_list(name, Estimator):
            estimator = Estimator()
    if has_fit_parameter(estimator, "sample_weight"):
        rnd = np.random.RandomState(0)
        X = rnd.uniform(size=(10, 3))
        y = np.arange(10) % 3
        y = multioutput_estimator_convert_y_2d(name, y)
        sample_weight = [3] * 10
                estimator.fit(X, y, sample_weight=sample_weight)

@ignore_warnings(category=(DeprecationWarning, UserWarning))
def check_dtype_object(name, Estimator):
        rng = np.random.RandomState(0)
    X = rng.rand(40, 10).astype(object)
    y = (X[:, 0] * 4).astype(np.int)
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    estimator.fit(X, y)
    if hasattr(estimator, "predict"):
        estimator.predict(X)
    if hasattr(estimator, "transform"):
        estimator.transform(X)
    try:
        estimator.fit(X, y.astype(object))
    except Exception as e:
        if "Unknown label type" not in str(e):
            raise
    X[0, 0] = {'foo': 'bar'}
    msg = "argument must be a string or a number"
    assert_raises_regex(TypeError, msg, estimator.fit, X, y)

@ignore_warnings
def check_dict_unchanged(name, Estimator):
                    if name in ['SpectralCoclustering']:
        return
    rnd = np.random.RandomState(0)
    if name in ['RANSACRegressor']:
        X = 3 * rnd.uniform(size=(20, 3))
    else:
        X = 2 * rnd.uniform(size=(20, 3))
    y = X[:, 0].astype(np.int)
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    if hasattr(estimator, "n_components"):
        estimator.n_components = 1
    if hasattr(estimator, "n_clusters"):
        estimator.n_clusters = 1
    if hasattr(estimator, "n_best"):
        estimator.n_best = 1
    set_random_state(estimator, 1)
            if name in ['SpectralBiclustering']:
        estimator.fit(X)
    else:
        estimator.fit(X, y)
    for method in ["predict", "transform", "decision_function",
                   "predict_proba"]:
        if hasattr(estimator, method):
            dict_before = estimator.__dict__.copy()
            getattr(estimator, method)(X)
            assert_dict_equal(estimator.__dict__, dict_before,
                              'Estimator changes __dict__ during %s' % method)

def is_public_parameter(attr):
    return not (attr.startswith('_') or attr.endswith('_'))

def check_dont_overwrite_parameters(name, Estimator):
        if hasattr(Estimator.__init__, "deprecated_original"):
                return
    rnd = np.random.RandomState(0)
    X = 3 * rnd.uniform(size=(20, 3))
    y = X[:, 0].astype(np.int)
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    if hasattr(estimator, "n_components"):
        estimator.n_components = 1
    if hasattr(estimator, "n_clusters"):
        estimator.n_clusters = 1
    set_random_state(estimator, 1)
    dict_before_fit = estimator.__dict__.copy()
    estimator.fit(X, y)
    dict_after_fit = estimator.__dict__
    public_keys_after_fit = [key for key in dict_after_fit.keys()
                             if is_public_parameter(key)]
    attrs_added_by_fit = [key for key in public_keys_after_fit
                          if key not in dict_before_fit.keys()]
        assert_true(not attrs_added_by_fit,
                ('Estimator adds public attribute(s) during'
                 ' the fit method.'
                 ' Estimators are only allowed to add private attributes'
                 ' either started with _ or ended'
                 ' with _ but %s added' % ', '.join(attrs_added_by_fit)))
        attrs_changed_by_fit = [key for key in public_keys_after_fit
                            if (dict_before_fit[key]
                                is not dict_after_fit[key])]
    assert_true(not attrs_changed_by_fit,
                ('Estimator changes public attribute(s) during'
                 ' the fit method. Estimators are only allowed'
                 ' to change attributes started'
                 ' or ended with _, but'
                 ' %s changed' % ', '.join(attrs_changed_by_fit)))

def check_fit2d_predict1d(name, Estimator):
        rnd = np.random.RandomState(0)
    X = 3 * rnd.uniform(size=(20, 3))
    y = X[:, 0].astype(np.int)
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    if hasattr(estimator, "n_components"):
        estimator.n_components = 1
    if hasattr(estimator, "n_clusters"):
        estimator.n_clusters = 1
    set_random_state(estimator, 1)
    estimator.fit(X, y)
    for method in ["predict", "transform", "decision_function",
                   "predict_proba"]:
        if hasattr(estimator, method):
            assert_raise_message(ValueError, "Reshape your data",
                                 getattr(estimator, method), X[0])

@ignore_warnings
def check_fit2d_1sample(name, Estimator):
        rnd = np.random.RandomState(0)
    X = 3 * rnd.uniform(size=(1, 10))
    y = X[:, 0].astype(np.int)
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    if hasattr(estimator, "n_components"):
        estimator.n_components = 1
    if hasattr(estimator, "n_clusters"):
        estimator.n_clusters = 1
    set_random_state(estimator, 1)
    try:
        estimator.fit(X, y)
    except ValueError:
        pass

@ignore_warnings
def check_fit2d_1feature(name, Estimator):
        rnd = np.random.RandomState(0)
    X = 3 * rnd.uniform(size=(10, 1))
    y = X[:, 0].astype(np.int)
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    if hasattr(estimator, "n_components"):
        estimator.n_components = 1
    if hasattr(estimator, "n_clusters"):
        estimator.n_clusters = 1
    set_random_state(estimator, 1)
    try:
        estimator.fit(X, y)
    except ValueError:
        pass

@ignore_warnings
def check_fit1d_1feature(name, Estimator):
        rnd = np.random.RandomState(0)
    X = 3 * rnd.uniform(size=(20))
    y = X.astype(np.int)
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    if hasattr(estimator, "n_components"):
        estimator.n_components = 1
    if hasattr(estimator, "n_clusters"):
        estimator.n_clusters = 1
    set_random_state(estimator, 1)
    try:
        estimator.fit(X, y)
    except ValueError:
        pass

@ignore_warnings
def check_fit1d_1sample(name, Estimator):
        rnd = np.random.RandomState(0)
    X = 3 * rnd.uniform(size=(20))
    y = np.array([1])
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    if hasattr(estimator, "n_components"):
        estimator.n_components = 1
    if hasattr(estimator, "n_clusters"):
        estimator.n_clusters = 1
    set_random_state(estimator, 1)
    try:
        estimator.fit(X, y)
    except ValueError:
        pass

@ignore_warnings(category=DeprecationWarning)
def check_transformer_general(name, Transformer):
    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                      random_state=0, n_features=2, cluster_std=0.1)
    X = StandardScaler().fit_transform(X)
    X -= X.min()
    _check_transformer(name, Transformer, X, y)
    _check_transformer(name, Transformer, X.tolist(), y.tolist())

@ignore_warnings(category=DeprecationWarning)
def check_transformer_data_not_an_array(name, Transformer):
    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                      random_state=0, n_features=2, cluster_std=0.1)
    X = StandardScaler().fit_transform(X)
            X -= X.min() - .1
    this_X = NotAnArray(X)
    this_y = NotAnArray(np.asarray(y))
    _check_transformer(name, Transformer, this_X, this_y)

def check_transformers_unfitted(name, Transformer):
    X, y = _boston_subset()
    with ignore_warnings(category=DeprecationWarning):
        transformer = Transformer()
    assert_raises((AttributeError, ValueError), transformer.transform, X)

def _check_transformer(name, Transformer, X, y):
    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():
                                                msg = name + ' is non deterministic on 32bit Python'
        raise SkipTest(msg)
    n_samples, n_features = np.asarray(X).shape
        transformer = Transformer()
    set_random_state(transformer)
    set_testing_parameters(transformer)
    
    if name in CROSS_DECOMPOSITION:
        y_ = np.c_[y, y]
        y_[::2, 1] *= 2
    else:
        y_ = y
    transformer.fit(X, y_)
        transformer_clone = clone(transformer)
    X_pred = transformer_clone.fit_transform(X, y=y_)
    if isinstance(X_pred, tuple):
        for x_pred in X_pred:
            assert_equal(x_pred.shape[0], n_samples)
    else:
                assert_equal(X_pred.shape[0], n_samples)
    if hasattr(transformer, 'transform'):
        if name in CROSS_DECOMPOSITION:
            X_pred2 = transformer.transform(X, y_)
            X_pred3 = transformer.fit_transform(X, y=y_)
        else:
            X_pred2 = transformer.transform(X)
            X_pred3 = transformer.fit_transform(X, y=y_)
        if isinstance(X_pred, tuple) and isinstance(X_pred2, tuple):
            for x_pred, x_pred2, x_pred3 in zip(X_pred, X_pred2, X_pred3):
                assert_array_almost_equal(
                    x_pred, x_pred2, 2,
                    "fit_transform and transform outcomes not consistent in %s"
                    % Transformer)
                assert_array_almost_equal(
                    x_pred, x_pred3, 2,
                    "consecutive fit_transform outcomes not consistent in %s"
                    % Transformer)
        else:
            assert_array_almost_equal(
                X_pred, X_pred2, 2,
                "fit_transform and transform outcomes not consistent in %s"
                % Transformer)
            assert_array_almost_equal(
                X_pred, X_pred3, 2,
                "consecutive fit_transform outcomes not consistent in %s"
                % Transformer)
            assert_equal(len(X_pred2), n_samples)
            assert_equal(len(X_pred3), n_samples)
                if hasattr(X, 'T'):
                        assert_raises(ValueError, transformer.transform, X.T)

@ignore_warnings
def check_pipeline_consistency(name, Estimator):
    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():
                                                msg = name + ' is non deterministic on 32bit Python'
        raise SkipTest(msg)
        X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                      random_state=0, n_features=2, cluster_std=0.1)
    X -= X.min()
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    set_random_state(estimator)
    pipeline = make_pipeline(estimator)
    estimator.fit(X, y)
    pipeline.fit(X, y)
    funcs = ["score", "fit_transform"]
    for func_name in funcs:
        func = getattr(estimator, func_name, None)
        if func is not None:
            func_pipeline = getattr(pipeline, func_name)
            result = func(X, y)
            result_pipe = func_pipeline(X, y)
            assert_array_almost_equal(result, result_pipe)

@ignore_warnings
def check_fit_score_takes_y(name, Estimator):
            rnd = np.random.RandomState(0)
    X = rnd.uniform(size=(10, 3))
    y = np.arange(10) % 3
    y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_testing_parameters(estimator)
    set_random_state(estimator)
    funcs = ["fit", "score", "partial_fit", "fit_predict", "fit_transform"]
    for func_name in funcs:
        func = getattr(estimator, func_name, None)
        if func is not None:
            func(X, y)
            args = [p.name for p in signature(func).parameters.values()]
            assert_true(args[1] in ["y", "Y"],
                        "Expected y or Y as second argument for method "
                        "%s of %s. Got arguments: %r."
                        % (func_name, Estimator.__name__, args))

@ignore_warnings
def check_estimators_dtypes(name, Estimator):
    rnd = np.random.RandomState(0)
    X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)
    X_train_64 = X_train_32.astype(np.float64)
    X_train_int_64 = X_train_32.astype(np.int64)
    X_train_int_32 = X_train_32.astype(np.int32)
    y = X_train_int_64[:, 0]
    y = multioutput_estimator_convert_y_2d(name, y)
    methods = ["predict", "transform", "decision_function", "predict_proba"]
    for X_train in [X_train_32, X_train_64, X_train_int_64, X_train_int_32]:
        estimator = Estimator()
        set_testing_parameters(estimator)
        set_random_state(estimator, 1)
        estimator.fit(X_train, y)
        for method in methods:
            if hasattr(estimator, method):
                getattr(estimator, method)(X_train)

@ignore_warnings(category=DeprecationWarning)
def check_estimators_empty_data_messages(name, Estimator):
    e = Estimator()
    set_testing_parameters(e)
    set_random_state(e, 1)
    X_zero_samples = np.empty(0).reshape(0, 3)
            assert_raises(ValueError, e.fit, X_zero_samples, [])
    X_zero_features = np.empty(0).reshape(3, 0)
            y = multioutput_estimator_convert_y_2d(name, np.array([1, 0, 1]))
    msg = ("0 feature\(s\) \(shape=\(3, 0\)\) while a minimum of \d* "
           "is required.")
    assert_raises_regex(ValueError, msg, e.fit, X_zero_features, y)

def check_estimators_nan_inf(name, Estimator):
        rnd = np.random.RandomState(0)
    X_train_finite = rnd.uniform(size=(10, 3))
    X_train_nan = rnd.uniform(size=(10, 3))
    X_train_nan[0, 0] = np.nan
    X_train_inf = rnd.uniform(size=(10, 3))
    X_train_inf[0, 0] = np.inf
    y = np.ones(10)
    y[:5] = 0
    y = multioutput_estimator_convert_y_2d(name, y)
    error_string_fit = "Estimator doesn't check for NaN and inf in fit."
    error_string_predict = ("Estimator doesn't check for NaN and inf in"
                            " predict.")
    error_string_transform = ("Estimator doesn't check for NaN and inf in"
                              " transform.")
    for X_train in [X_train_nan, X_train_inf]:
                with ignore_warnings(category=DeprecationWarning):
            estimator = Estimator()
            set_testing_parameters(estimator)
            set_random_state(estimator, 1)
                        try:
                estimator.fit(X_train, y)
            except ValueError as e:
                if 'inf' not in repr(e) and 'NaN' not in repr(e):
                    print(error_string_fit, Estimator, e)
                    traceback.print_exc(file=sys.stdout)
                    raise e
            except Exception as exc:
                print(error_string_fit, Estimator, exc)
                traceback.print_exc(file=sys.stdout)
                raise exc
            else:
                raise AssertionError(error_string_fit, Estimator)
                        estimator.fit(X_train_finite, y)
                        if hasattr(estimator, "predict"):
                try:
                    estimator.predict(X_train)
                except ValueError as e:
                    if 'inf' not in repr(e) and 'NaN' not in repr(e):
                        print(error_string_predict, Estimator, e)
                        traceback.print_exc(file=sys.stdout)
                        raise e
                except Exception as exc:
                    print(error_string_predict, Estimator, exc)
                    traceback.print_exc(file=sys.stdout)
                else:
                    raise AssertionError(error_string_predict, Estimator)
                        if hasattr(estimator, "transform"):
                try:
                    estimator.transform(X_train)
                except ValueError as e:
                    if 'inf' not in repr(e) and 'NaN' not in repr(e):
                        print(error_string_transform, Estimator, e)
                        traceback.print_exc(file=sys.stdout)
                        raise e
                except Exception as exc:
                    print(error_string_transform, Estimator, exc)
                    traceback.print_exc(file=sys.stdout)
                else:
                    raise AssertionError(error_string_transform, Estimator)

@ignore_warnings
def check_estimators_pickle(name, Estimator):
        check_methods = ["predict", "transform", "decision_function",
                     "predict_proba"]
    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
                      random_state=0, n_features=2, cluster_std=0.1)
        X -= X.min()
        y = multioutput_estimator_convert_y_2d(name, y)
    estimator = Estimator()
    set_random_state(estimator)
    set_testing_parameters(estimator)
    estimator.fit(X, y)
    result = dict()
    for method in check_methods:
        if hasattr(estimator, method):
            result[method] = getattr(estimator, method)(X)
        pickled_estimator = pickle.dumps(estimator)
    if Estimator.__module__.startswith('sklearn.'):
        assert_true(b"version" in pickled_estimator)
    unpickled_estimator = pickle.loads(pickled_estimator)
    for method in result:
        unpickled_result = getattr(unpickled_estimator, method)(X)
        assert_array_almost_equal(result[method], unpickled_result)

def check_estimators_partial_fit_n_features(name, Alg):
        if not hasattr(Alg, 'partial_fit'):
        return
    X, y = make_blobs(n_samples=50, random_state=1)
    X -= X.min()
    with ignore_warnings(category=DeprecationWarning):
        alg = Alg()
    if not hasattr(alg, 'partial_fit'):
                return
    set_testing_parameters(alg)
    try:
        if isinstance(alg, ClassifierMixin):
            classes = np.unique(y)
            alg.partial_fit(X, y, classes=classes)
        else:
            alg.partial_fit(X, y)
    except NotImplementedError:
        return
    assert_raises(ValueError, alg.partial_fit, X[:, :-1], y)

def check_clustering(name, Alg):
    X, y = make_blobs(n_samples=50, random_state=1)
    X, y = shuffle(X, y, random_state=7)
    X = StandardScaler().fit_transform(X)
    n_samples, n_features = X.shape
        with ignore_warnings(category=DeprecationWarning):
        alg = Alg()
    set_testing_parameters(alg)
    if hasattr(alg, "n_clusters"):
        alg.set_params(n_clusters=3)
    set_random_state(alg)
    if name == 'AffinityPropagation':
        alg.set_params(preference=-100)
        alg.set_params(max_iter=100)
        alg.fit(X)
        alg.fit(X.tolist())
    assert_equal(alg.labels_.shape, (n_samples,))
    pred = alg.labels_
    assert_greater(adjusted_rand_score(pred, y), 0.4)
        if name == 'SpectralClustering':
                return
    set_random_state(alg)
    with warnings.catch_warnings(record=True):
        pred2 = alg.fit_predict(X)
    assert_array_equal(pred, pred2)

def check_clusterer_compute_labels_predict(name, Clusterer):
        X, y = make_blobs(n_samples=20, random_state=0)
    clusterer = Clusterer()
    if hasattr(clusterer, "compute_labels"):
                if hasattr(clusterer, "random_state"):
            clusterer.set_params(random_state=0)
        X_pred1 = clusterer.fit(X).predict(X)
        clusterer.set_params(compute_labels=False)
        X_pred2 = clusterer.fit(X).predict(X)
        assert_array_equal(X_pred1, X_pred2)

def check_classifiers_one_label(name, Classifier):
    error_string_fit = "Classifier can't train when only one class is present."
    error_string_predict = ("Classifier can't predict when only one class is "
                            "present.")
    rnd = np.random.RandomState(0)
    X_train = rnd.uniform(size=(10, 3))
    X_test = rnd.uniform(size=(10, 3))
    y = np.ones(10)
        with ignore_warnings(category=DeprecationWarning):
        classifier = Classifier()
        set_testing_parameters(classifier)
                try:
            classifier.fit(X_train, y)
        except ValueError as e:
            if 'class' not in repr(e):
                print(error_string_fit, Classifier, e)
                traceback.print_exc(file=sys.stdout)
                raise e
            else:
                return
        except Exception as exc:
            print(error_string_fit, Classifier, exc)
            traceback.print_exc(file=sys.stdout)
            raise exc
                try:
            assert_array_equal(classifier.predict(X_test), y)
        except Exception as exc:
            print(error_string_predict, Classifier, exc)
            raise exc

@ignore_warnings  def check_classifiers_train(name, Classifier):
    X_m, y_m = make_blobs(n_samples=300, random_state=0)
    X_m, y_m = shuffle(X_m, y_m, random_state=7)
    X_m = StandardScaler().fit_transform(X_m)
        y_b = y_m[y_m != 2]
    X_b = X_m[y_m != 2]
    for (X, y) in [(X_m, y_m), (X_b, y_b)]:
        classes = np.unique(y)
        n_classes = len(classes)
        n_samples, n_features = X.shape
        classifier = Classifier()
        if name in ['BernoulliNB', 'MultinomialNB']:
            X -= X.min()
        set_testing_parameters(classifier)
        set_random_state(classifier)
                assert_raises(ValueError, classifier.fit, X, y[:-1])
                classifier.fit(X, y)
                classifier.fit(X.tolist(), y.tolist())
        assert_true(hasattr(classifier, "classes_"))
        y_pred = classifier.predict(X)
        assert_equal(y_pred.shape, (n_samples,))
                if name not in ['BernoulliNB', 'MultinomialNB']:
            assert_greater(accuracy_score(y, y_pred), 0.83)
                assert_raises(ValueError, classifier.predict, X.T)
        if hasattr(classifier, "decision_function"):
            try:
                                decision = classifier.decision_function(X)
                if n_classes is 2:
                    assert_equal(decision.shape, (n_samples,))
                    dec_pred = (decision.ravel() > 0).astype(np.int)
                    assert_array_equal(dec_pred, y_pred)
                if (n_classes is 3 and not isinstance(classifier, BaseLibSVM)):
                                        assert_equal(decision.shape, (n_samples, n_classes))
                    assert_array_equal(np.argmax(decision, axis=1), y_pred)
                                assert_raises(ValueError,
                              classifier.decision_function, X.T)
                                assert_raises(ValueError,
                              classifier.decision_function, X.T)
            except NotImplementedError:
                pass
        if hasattr(classifier, "predict_proba"):
                        y_prob = classifier.predict_proba(X)
            assert_equal(y_prob.shape, (n_samples, n_classes))
            assert_array_equal(np.argmax(y_prob, axis=1), y_pred)
                        assert_array_almost_equal(np.sum(y_prob, axis=1),
                                      np.ones(n_samples))
                        assert_raises(ValueError, classifier.predict_proba, X.T)
                        assert_raises(ValueError, classifier.predict_proba, X.T)
            if hasattr(classifier, "predict_log_proba"):
                                y_log_prob = classifier.predict_log_proba(X)
                assert_array_almost_equal(y_log_prob, np.log(y_prob), 8)
                assert_array_equal(np.argsort(y_log_prob), np.argsort(y_prob))

@ignore_warnings(category=DeprecationWarning)
def check_estimators_fit_returns_self(name, Estimator):
        X, y = make_blobs(random_state=0, n_samples=9, n_features=4)
    y = multioutput_estimator_convert_y_2d(name, y)
        X -= X.min()
    estimator = Estimator()
    set_testing_parameters(estimator)
    set_random_state(estimator)
    assert_true(estimator.fit(X, y) is estimator)

@ignore_warnings
def check_estimators_unfitted(name, Estimator):
    
        X, y = _boston_subset()
    est = Estimator()
    msg = "fit"
    if hasattr(est, 'predict'):
        assert_raise_message((AttributeError, ValueError), msg,
                             est.predict, X)
    if hasattr(est, 'decision_function'):
        assert_raise_message((AttributeError, ValueError), msg,
                             est.decision_function, X)
    if hasattr(est, 'predict_proba'):
        assert_raise_message((AttributeError, ValueError), msg,
                             est.predict_proba, X)
    if hasattr(est, 'predict_log_proba'):
        assert_raise_message((AttributeError, ValueError), msg,
                             est.predict_log_proba, X)

@ignore_warnings(category=DeprecationWarning)
def check_supervised_y_2d(name, Estimator):
    if "MultiTask" in name:
                return
    rnd = np.random.RandomState(0)
    X = rnd.uniform(size=(10, 3))
    y = np.arange(10) % 3
    estimator = Estimator()
    set_testing_parameters(estimator)
    set_random_state(estimator)
        estimator.fit(X, y)
    y_pred = estimator.predict(X)
    set_random_state(estimator)
            with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always", DataConversionWarning)
        warnings.simplefilter("ignore", RuntimeWarning)
        estimator.fit(X, y[:, np.newaxis])
    y_pred_2d = estimator.predict(X)
    msg = "expected 1 DataConversionWarning, got: %s" % (
        ", ".join([str(w_x) for w_x in w]))
    if name not in MULTI_OUTPUT:
                assert_greater(len(w), 0, msg)
        assert_true("DataConversionWarning('A column-vector y"
                    " was passed when a 1d array was expected" in msg)
    assert_array_almost_equal(y_pred.ravel(), y_pred_2d.ravel())

def check_classifiers_classes(name, Classifier):
    X, y = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)
    X, y = shuffle(X, y, random_state=7)
    X = StandardScaler().fit_transform(X)
            X -= X.min() - .1
    y_names = np.array(["one", "two", "three"])[y]
    for y_names in [y_names, y_names.astype('O')]:
        if name in ["LabelPropagation", "LabelSpreading"]:
                        y_ = y
        else:
            y_ = y_names
        classes = np.unique(y_)
        with ignore_warnings(category=DeprecationWarning):
            classifier = Classifier()
        if name == 'BernoulliNB':
            classifier.set_params(binarize=X.mean())
        set_testing_parameters(classifier)
        set_random_state(classifier)
                classifier.fit(X, y_)
        y_pred = classifier.predict(X)
                assert_array_equal(np.unique(y_), np.unique(y_pred))
        if np.any(classifier.classes_ != classes):
            print("Unexpected classes_ attribute for %r: "
                  "expected %s, got %s" %
                  (classifier, classes, classifier.classes_))

@ignore_warnings(category=DeprecationWarning)
def check_regressors_int(name, Regressor):
    X, _ = _boston_subset()
    X = X[:50]
    rnd = np.random.RandomState(0)
    y = rnd.randint(3, size=X.shape[0])
    y = multioutput_estimator_convert_y_2d(name, y)
    rnd = np.random.RandomState(0)
        regressor_1 = Regressor()
    regressor_2 = Regressor()
    set_testing_parameters(regressor_1)
    set_testing_parameters(regressor_2)
    set_random_state(regressor_1)
    set_random_state(regressor_2)
    if name in CROSS_DECOMPOSITION:
        y_ = np.vstack([y, 2 * y + rnd.randint(2, size=len(y))])
        y_ = y_.T
    else:
        y_ = y
        regressor_1.fit(X, y_)
    pred1 = regressor_1.predict(X)
    regressor_2.fit(X, y_.astype(np.float))
    pred2 = regressor_2.predict(X)
    assert_array_almost_equal(pred1, pred2, 2, name)

@ignore_warnings(category=DeprecationWarning)
def check_regressors_train(name, Regressor):
    X, y = _boston_subset()
    y = StandardScaler().fit_transform(y.reshape(-1, 1))      y = y.ravel()
    y = multioutput_estimator_convert_y_2d(name, y)
    rnd = np.random.RandomState(0)
        regressor = Regressor()
    set_testing_parameters(regressor)
    if not hasattr(regressor, 'alphas') and hasattr(regressor, 'alpha'):
                regressor.alpha = 0.01
    if name == 'PassiveAggressiveRegressor':
        regressor.C = 0.01
        assert_raises(ValueError, regressor.fit, X, y[:-1])
        if name in CROSS_DECOMPOSITION:
        y_ = np.vstack([y, 2 * y + rnd.randint(2, size=len(y))])
        y_ = y_.T
    else:
        y_ = y
    set_random_state(regressor)
    regressor.fit(X, y_)
    regressor.fit(X.tolist(), y_.tolist())
    y_pred = regressor.predict(X)
    assert_equal(y_pred.shape, y_.shape)
                if name not in ('PLSCanonical', 'CCA', 'RANSACRegressor'):
        assert_greater(regressor.score(X, y_), 0.5)

@ignore_warnings
def check_regressors_no_decision_function(name, Regressor):
        rng = np.random.RandomState(0)
    X = rng.normal(size=(10, 4))
    y = multioutput_estimator_convert_y_2d(name, X[:, 0])
    regressor = Regressor()
    set_testing_parameters(regressor)
    if hasattr(regressor, "n_components"):
                regressor.n_components = 1
    regressor.fit(X, y)
    funcs = ["decision_function", "predict_proba", "predict_log_proba"]
    for func_name in funcs:
        func = getattr(regressor, func_name, None)
        if func is None:
                        continue
                msg = func_name
        assert_warns_message(DeprecationWarning, msg, func, X)

def check_class_weight_classifiers(name, Classifier):
    if name == "NuSVC":
                raise SkipTest
    if name.endswith("NB"):
                        raise SkipTest
    for n_centers in [2, 3]:
                X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
                                                            random_state=0)
        n_centers = len(np.unique(y_train))
        if n_centers == 2:
            class_weight = {0: 1000, 1: 0.0001}
        else:
            class_weight = {0: 1000, 1: 0.0001, 2: 0.0001}
        with ignore_warnings(category=DeprecationWarning):
            classifier = Classifier(class_weight=class_weight)
        if hasattr(classifier, "n_iter"):
            classifier.set_params(n_iter=100)
        if hasattr(classifier, "min_weight_fraction_leaf"):
            classifier.set_params(min_weight_fraction_leaf=0.01)
        set_random_state(classifier)
        classifier.fit(X_train, y_train)
        y_pred = classifier.predict(X_test)
        assert_greater(np.mean(y_pred == 0), 0.89)

def check_class_weight_balanced_classifiers(name, Classifier, X_train, y_train,
                                            X_test, y_test, weights):
    with ignore_warnings(category=DeprecationWarning):
        classifier = Classifier()
    if hasattr(classifier, "n_iter"):
        classifier.set_params(n_iter=100)
    set_random_state(classifier)
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    classifier.set_params(class_weight='balanced')
    classifier.fit(X_train, y_train)
    y_pred_balanced = classifier.predict(X_test)
    assert_greater(f1_score(y_test, y_pred_balanced, average='weighted'),
                   f1_score(y_test, y_pred, average='weighted'))

def check_class_weight_balanced_linear_classifier(name, Classifier):
        X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
                  [1.0, 1.0], [1.0, 0.0]])
    y = np.array([1, 1, 1, -1, -1])
    with ignore_warnings(category=DeprecationWarning):
        classifier = Classifier()
    if hasattr(classifier, "n_iter"):
                        classifier.set_params(n_iter=1000)
    set_random_state(classifier)
        classifier.set_params(class_weight='balanced')
    coef_balanced = classifier.fit(X, y).coef_.copy()
        n_samples = len(y)
    n_classes = float(len(np.unique(y)))
    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),
                    -1: n_samples / (np.sum(y == -1) * n_classes)}
    classifier.set_params(class_weight=class_weight)
    coef_manual = classifier.fit(X, y).coef_.copy()
    assert_array_almost_equal(coef_balanced, coef_manual)

@ignore_warnings(category=DeprecationWarning)
def check_estimators_overwrite_params(name, Estimator):
    X, y = make_blobs(random_state=0, n_samples=9)
    y = multioutput_estimator_convert_y_2d(name, y)
        X -= X.min()
    estimator = Estimator()
    set_testing_parameters(estimator)
    set_random_state(estimator)
        params = estimator.get_params()
    original_params = deepcopy(params)
        estimator.fit(X, y)
        new_params = estimator.get_params()
    for param_name, original_value in original_params.items():
        new_value = new_params[param_name]
                                                        assert_equal(hash(new_value), hash(original_value),
                     "Estimator %s should not change or mutate "
                     " the parameter %s from %s to %s during fit."
                     % (name, param_name, original_value, new_value))

def check_no_fit_attributes_set_in_init(name, Estimator):
        estimator = Estimator()
    for attr in dir(estimator):
        if attr.endswith("_") and not attr.startswith("__"):
                                                assert_false(
                hasattr(estimator, attr),
                "By convention, attributes ending with '_' are "
                'estimated from data in scikit-learn. Consequently they '
                'should not be initialized in the constructor of an '
                'estimator but in the fit method. Attribute {!r} '
                'was found in estimator {}'.format(attr, name))

def check_sparsify_coefficients(name, Estimator):
    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1],
                  [-1, -2], [2, 2], [-2, -2]])
    y = [1, 1, 1, 2, 2, 2, 3, 3, 3]
    est = Estimator()
    est.fit(X, y)
    pred_orig = est.predict(X)
        est.sparsify()
    assert_true(sparse.issparse(est.coef_))
    pred = est.predict(X)
    assert_array_equal(pred, pred_orig)
        est = pickle.loads(pickle.dumps(est))
    assert_true(sparse.issparse(est.coef_))
    pred = est.predict(X)
    assert_array_equal(pred, pred_orig)

def check_classifier_data_not_an_array(name, Estimator):
    X = np.array([[3, 0], [0, 1], [0, 2], [1, 1], [1, 2], [2, 1]])
    y = [1, 1, 1, 2, 2, 2]
    y = multioutput_estimator_convert_y_2d(name, y)
    check_estimators_data_not_an_array(name, Estimator, X, y)

def check_regressor_data_not_an_array(name, Estimator):
    X, y = _boston_subset(n_samples=50)
    y = multioutput_estimator_convert_y_2d(name, y)
    check_estimators_data_not_an_array(name, Estimator, X, y)

@ignore_warnings(category=DeprecationWarning)
def check_estimators_data_not_an_array(name, Estimator, X, y):
    if name in CROSS_DECOMPOSITION:
        raise SkipTest
        estimator_1 = Estimator()
    estimator_2 = Estimator()
    set_testing_parameters(estimator_1)
    set_testing_parameters(estimator_2)
    set_random_state(estimator_1)
    set_random_state(estimator_2)
    y_ = NotAnArray(np.asarray(y))
    X_ = NotAnArray(np.asarray(X))
        estimator_1.fit(X_, y_)
    pred1 = estimator_1.predict(X_)
    estimator_2.fit(X, y)
    pred2 = estimator_2.predict(X)
    assert_array_almost_equal(pred1, pred2, 2, name)

def check_parameters_default_constructible(name, Estimator):
    classifier = LinearDiscriminantAnalysis()
            with ignore_warnings(category=DeprecationWarning):
        if name in META_ESTIMATORS:
            estimator = Estimator(classifier)
        else:
            estimator = Estimator()
                clone(estimator)
                repr(estimator)
                assert_true(estimator.set_params() is estimator)
                                
                init = getattr(estimator.__init__, 'deprecated_original',
                       estimator.__init__)
        try:
            def param_filter(p):
                Extended math utilities.
    x = np.asarray(x)
    nrm2, = linalg.get_blas_funcs(['nrm2'], [x])
    return nrm2(x)

if np_version < (1, 7, 1):
    _ravel = np.ravel
else:
    _ravel = partial(np.ravel, order='K')

def squared_norm(x):
        x = _ravel(x)
    return np.dot(x, x)

def row_norms(X, squared=False):
        if issparse(X):
        if not isinstance(X, csr_matrix):
            X = csr_matrix(X)
        norms = csr_row_norms(X)
    else:
        norms = np.einsum('ij,ij->i', X, X)
    if not squared:
        np.sqrt(norms, norms)
    return norms

def fast_logdet(A):
        sign, ld = np.linalg.slogdet(A)
    if not sign > 0:
        return -np.inf
    return ld

def _impose_f_order(X):
                if X.flags.c_contiguous:
        return check_array(X.T, copy=False, order='F'), True
    else:
        return check_array(X, copy=False, order='F'), False

def _fast_dot(A, B):
    if B.shape[0] != A.shape[A.ndim - 1]:          raise ValueError
    if A.dtype != B.dtype or any(x.dtype not in (np.float32, np.float64)
                                 for x in [A, B]):
        warnings.warn('Falling back to np.dot. '
                      'Data must be of same type of either '
                      '32 or 64 bit float for the BLAS function, gemm, to be '
                      'used for an efficient dot operation. ',
                      NonBLASDotWarning)
        raise ValueError
    if min(A.shape) == 1 or min(B.shape) == 1 or A.ndim != 2 or B.ndim != 2:
        raise ValueError
        dot = linalg.get_blas_funcs(['gemm'], (A, B))[0]
    A, trans_a = _impose_f_order(A)
    B, trans_b = _impose_f_order(B)
    return dot(alpha=1.0, a=A, b=B, trans_a=trans_a, trans_b=trans_b)

def _have_blas_gemm():
    try:
        linalg.get_blas_funcs(['gemm'])
        return True
    except (AttributeError, ValueError):
        warnings.warn('Could not import BLAS, falling back to np.dot')
        return False

if np_version < (1, 7, 2) and _have_blas_gemm():
    def fast_dot(A, B):
                try:
            return _fast_dot(A, B)
        except ValueError:
                        return np.dot(A, B)
else:
    fast_dot = np.dot

def density(w, **kwargs):
        if hasattr(w, "toarray"):
        d = float(w.nnz) / (w.shape[0] * w.shape[1])
    else:
        d = 0 if w is None else float((w != 0).sum()) / w.size
    return d

def safe_sparse_dot(a, b, dense_output=False):
        if issparse(a) or issparse(b):
        ret = a * b
        if dense_output and hasattr(ret, "toarray"):
            ret = ret.toarray()
        return ret
    else:
        return fast_dot(a, b)

def randomized_range_finder(A, size, n_iter,
                            power_iteration_normalizer='auto',
                            random_state=None):
        random_state = check_random_state(random_state)
        Q = random_state.normal(size=(A.shape[1], size))
        if power_iteration_normalizer == 'auto':
        if n_iter <= 2:
            power_iteration_normalizer = 'none'
        else:
            power_iteration_normalizer = 'LU'
            for i in range(n_iter):
        if power_iteration_normalizer == 'none':
            Q = safe_sparse_dot(A, Q)
            Q = safe_sparse_dot(A.T, Q)
        elif power_iteration_normalizer == 'LU':
            Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)
            Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)
        elif power_iteration_normalizer == 'QR':
            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')
            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')
            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')
    return Q

def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',
                   power_iteration_normalizer='auto', transpose='auto',
                   flip_sign=True, random_state=0):
        random_state = check_random_state(random_state)
    n_random = n_components + n_oversamples
    n_samples, n_features = M.shape
    if n_iter == 'auto':
                        n_iter = 7 if n_components < .1 * min(M.shape) else 4
    if transpose == 'auto':
        transpose = n_samples < n_features
    if transpose:
                M = M.T
    Q = randomized_range_finder(M, n_random, n_iter,
                                power_iteration_normalizer, random_state)
        B = safe_sparse_dot(Q.T, M)
        Uhat, s, V = linalg.svd(B, full_matrices=False)
    del B
    U = np.dot(Q, Uhat)
    if flip_sign:
        if not transpose:
            U, V = svd_flip(U, V)
        else:
                                    U, V = svd_flip(U, V, u_based_decision=False)
    if transpose:
                return V[:n_components, :].T, s[:n_components], U[:, :n_components].T
    else:
        return U[:, :n_components], s[:n_components], V[:n_components, :]

def logsumexp(arr, axis=0):
        arr = np.rollaxis(arr, axis)
            vmax = arr.max(axis=0)
    out = np.log(np.sum(np.exp(arr - vmax), axis=0))
    out += vmax
    return out

def weighted_mode(a, w, axis=0):
        if axis is None:
        a = np.ravel(a)
        w = np.ravel(w)
        axis = 0
    else:
        a = np.asarray(a)
        w = np.asarray(w)
        axis = axis
    if a.shape != w.shape:
        w = np.zeros(a.shape, dtype=w.dtype) + w
    scores = np.unique(np.ravel(a))           testshape = list(a.shape)
    testshape[axis] = 1
    oldmostfreq = np.zeros(testshape)
    oldcounts = np.zeros(testshape)
    for score in scores:
        template = np.zeros(a.shape)
        ind = (a == score)
        template[ind] = w[ind]
        counts = np.expand_dims(np.sum(template, axis), axis)
        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)
        oldcounts = np.maximum(counts, oldcounts)
        oldmostfreq = mostfrequent
    return mostfrequent, oldcounts

def pinvh(a, cond=None, rcond=None, lower=True):
        a = np.asarray_chkfinite(a)
    s, u = linalg.eigh(a, lower=lower)
    if rcond is not None:
        cond = rcond
    if cond in [None, -1]:
        t = u.dtype.char.lower()
        factor = {'f': 1E3, 'd': 1E6}
        cond = factor[t] * np.finfo(t).eps
        above_cutoff = (abs(s) > cond * np.max(abs(s)))
    psigma_diag = np.zeros_like(s)
    psigma_diag[above_cutoff] = 1.0 / s[above_cutoff]
    return np.dot(u * psigma_diag, np.conjugate(u).T)

def cartesian(arrays, out=None):
        arrays = [np.asarray(x) for x in arrays]
    shape = (len(x) for x in arrays)
    dtype = arrays[0].dtype
    ix = np.indices(shape)
    ix = ix.reshape(len(arrays), -1).T
    if out is None:
        out = np.empty_like(ix, dtype=dtype)
    for n, arr in enumerate(arrays):
        out[:, n] = arrays[n][ix[:, n]]
    return out

def svd_flip(u, v, u_based_decision=True):
        if u_based_decision:
                max_abs_cols = np.argmax(np.abs(u), axis=0)
        signs = np.sign(u[max_abs_cols, xrange(u.shape[1])])
        u *= signs
        v *= signs[:, np.newaxis]
    else:
                max_abs_rows = np.argmax(np.abs(v), axis=1)
        signs = np.sign(v[xrange(v.shape[0]), max_abs_rows])
        u *= signs
        v *= signs[:, np.newaxis]
    return u, v

def log_logistic(X, out=None):
        is_1d = X.ndim == 1
    X = np.atleast_2d(X)
    X = check_array(X, dtype=np.float64)
    n_samples, n_features = X.shape
    if out is None:
        out = np.empty_like(X)
    _log_logistic_sigmoid(n_samples, n_features, X, out)
    if is_1d:
        return np.squeeze(out)
    return out

def softmax(X, copy=True):
        if copy:
        X = np.copy(X)
    max_prob = np.max(X, axis=1).reshape((-1, 1))
    X -= max_prob
    np.exp(X, X)
    sum_prob = np.sum(X, axis=1).reshape((-1, 1))
    X /= sum_prob
    return X

def safe_min(X):
        if issparse(X):
        if len(X.data) == 0:
            return 0
        m = X.data.min()
        return m if X.getnnz() == X.size else min(m, 0)
    else:
        return X.min()

def make_nonnegative(X, min_value=0):
        min_ = safe_min(X)
    if min_ < min_value:
        if issparse(X):
            raise ValueError("Cannot make the data matrix"
                             " nonnegative because it is sparse."
                             " Adding a value to every entry would"
                             " make it no longer sparse.")
        X = X + (min_value - min_)
    return X

def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,
                              last_sample_count=0):
                    last_sum = last_mean * last_sample_count
    new_sum = X.sum(axis=0)
    new_sample_count = X.shape[0]
    updated_sample_count = last_sample_count + new_sample_count
    updated_mean = (last_sum + new_sum) / updated_sample_count
    if last_variance is None:
        updated_variance = None
    else:
        new_unnormalized_variance = X.var(axis=0) * new_sample_count
        if last_sample_count == 0:              updated_unnormalized_variance = new_unnormalized_variance
        else:
            last_over_new_count = last_sample_count / new_sample_count
            last_unnormalized_variance = last_variance * last_sample_count
            updated_unnormalized_variance = (
                last_unnormalized_variance +
                new_unnormalized_variance +
                last_over_new_count / updated_sample_count *
                (last_sum / last_over_new_count - new_sum) ** 2)
        updated_variance = updated_unnormalized_variance / updated_sample_count
    return updated_mean, updated_variance, updated_sample_count

def _deterministic_vector_sign_flip(u):
        max_abs_rows = np.argmax(np.abs(u), axis=1)
    signs = np.sign(u[range(u.shape[0]), max_abs_rows])
    u *= signs[:, np.newaxis]
    return u

def stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):
            if np_version < (1, 9):
        return np.cumsum(arr, axis=axis, dtype=np.float64)
    out = np.cumsum(arr, axis=axis, dtype=np.float64)
    expected = np.sum(arr, axis=axis, dtype=np.float64)
    if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol,
                             atol=atol, equal_nan=True)):
        warnings.warn('cumsum was found to be unstable: '
                      'its last element does not correspond to sum',
                      RuntimeWarning)
    return out
cimport cython
from cython.operator cimport dereference as deref, preincrement as inc, \
    predecrement as dec
from libcpp.utility cimport pair
from libcpp.map cimport map as cpp_map
import numpy as np
cimport numpy as np
np.import_array()


cdef class IntFloatDict:
    @cython.boundscheck(False)
    @cython.wraparound(False)
    def __init__(self, np.ndarray[ITYPE_t, ndim=1] keys,
                       np.ndarray[DTYPE_t, ndim=1] values):
        cdef int i
        cdef int size = values.size
                        for i in range(size):
            self.my_map[keys[i]] = values[i]
    def __len__(self):
        return self.my_map.size()
    def __getitem__(self, int key):
        cdef cpp_map[ITYPE_t, DTYPE_t].iterator it = self.my_map.find(key)
        if it == self.my_map.end():
                        raise KeyError('%i' % key)
        return deref(it).second
    def __setitem__(self, int key, float value):
        self.my_map[key] = value
                                    
    def __iter__(self):
        cdef int size = self.my_map.size()
        cdef ITYPE_t [:] keys = np.empty(size, dtype=np.intp)
        cdef DTYPE_t [:] values = np.empty(size, dtype=np.float64)
        self._to_arrays(keys, values)
        cdef int idx
        cdef ITYPE_t key
        cdef DTYPE_t value
        for idx in range(size):
            key = keys[idx]
            value = values[idx]
            yield key, value
    def to_arrays(self):
                cdef int size = self.my_map.size()
        cdef np.ndarray[ITYPE_t, ndim=1] keys = np.empty(size,
                                                         dtype=np.intp)
        cdef np.ndarray[DTYPE_t, ndim=1] values = np.empty(size,
                                                           dtype=np.float64)
        self._to_arrays(keys, values)
        return keys, values
    cdef _to_arrays(self, ITYPE_t [:] keys, DTYPE_t [:] values):
                cdef cpp_map[ITYPE_t, DTYPE_t].iterator it = self.my_map.begin()
        cdef cpp_map[ITYPE_t, DTYPE_t].iterator end = self.my_map.end()
        cdef int index = 0
        while it != end:
            keys[index] = deref(it).first
            values[index] = deref(it).second
            inc(it)
            index += 1
    def update(self, IntFloatDict other):
        cdef cpp_map[ITYPE_t, DTYPE_t].iterator it = other.my_map.begin()
        cdef cpp_map[ITYPE_t, DTYPE_t].iterator end = other.my_map.end()
        while it != end:
            self.my_map[deref(it).first] = deref(it).second
            inc(it)
    def copy(self):
        cdef IntFloatDict out_obj = IntFloatDict.__new__(IntFloatDict)
                out_obj.my_map = self.my_map
        return out_obj
    def append(self, ITYPE_t key, DTYPE_t value):
        cdef cpp_map[ITYPE_t, DTYPE_t].iterator end = self.my_map.end()
                dec(end)
                cdef pair[ITYPE_t, DTYPE_t] args
        args.first = key
        args.second = value
        self.my_map.insert(end, args)

def argmin(IntFloatDict d):
    cdef cpp_map[ITYPE_t, DTYPE_t].iterator it = d.my_map.begin()
    cdef cpp_map[ITYPE_t, DTYPE_t].iterator end = d.my_map.end()
    cdef ITYPE_t min_key
    cdef DTYPE_t min_value = np.inf
    while it != end:
        if deref(it).second < min_value:
            min_value = deref(it).second
            min_key = deref(it).first
        inc(it)
    return min_key, min_value

import warnings
import sys
import functools
import os
import errno
import numpy as np
import scipy.sparse as sp
import scipy
try:
    from inspect import signature
except ImportError:
    from ..externals.funcsigs import signature

def _parse_version(version_string):
    version = []
    for x in version_string.split('.'):
        try:
            version.append(int(x))
        except ValueError:
                        version.append(x)
    return tuple(version)
euler_gamma = getattr(np, 'euler_gamma',
                      0.577215664901532860606512090082402431)
np_version = _parse_version(np.__version__)
sp_version = _parse_version(scipy.__version__)

try:
    from scipy.special import expit         with np.errstate(invalid='ignore', over='ignore'):
        if np.isnan(expit(1000)):                   raise ImportError("no stable expit in scipy.special")
except ImportError:
    def expit(x, out=None):
                if out is None:
            out = np.empty(np.atleast_1d(x).shape, dtype=np.float64)
        out[:] = x
                        out *= .5
        np.tanh(out, out)
        out += 1
        out *= .5
        return out.reshape(np.shape(x))

if 'order' in signature(np.copy).parameters:
    def safe_copy(X):
                return np.copy(X, order='K')
else:
            safe_copy = np.copy
try:
    if (not np.allclose(np.divide(.4, 1, casting="unsafe"),
                        np.divide(.4, 1, casting="unsafe", dtype=np.float64))
            or not np.allclose(np.divide(.4, 1), .4)):
        raise TypeError('Divide not working with dtype: '
                        'https://github.com/numpy/numpy/issues/3484')
    divide = np.divide
except TypeError:
            def divide(x1, x2, out=None, dtype=None):
        out_orig = out
        if out is None:
            out = np.asarray(x1, dtype=dtype)
            if out is x1:
                out = x1.copy()
        else:
            if out is not x1:
                out[:] = x1
        if dtype is not None and out.dtype != dtype:
            out = out.astype(dtype)
        out /= x2
        if out_orig is None and np.isscalar(x1):
            out = np.asscalar(out)
        return out

try:
    np.array(5).astype(float, copy=False)
except TypeError:
        def astype(array, dtype, copy=True):
        if not copy and array.dtype == dtype:
            return array
        return array.astype(dtype)
else:
    astype = np.ndarray.astype

try:
    with warnings.catch_warnings(record=True):
                        warnings.simplefilter('always')
        sp.csr_matrix([1.0, 2.0, 3.0]).max(axis=0)
except (TypeError, AttributeError):
        
    def _minor_reduce(X, ufunc):
        major_index = np.flatnonzero(np.diff(X.indptr))
        if X.data.size == 0 and major_index.size == 0:
                        value = np.zeros_like(X.data)
        else:
            value = ufunc.reduceat(X.data, X.indptr[major_index])
        return major_index, value
    def _min_or_max_axis(X, axis, min_or_max):
        N = X.shape[axis]
        if N == 0:
            raise ValueError("zero-size array to reduction operation")
        M = X.shape[1 - axis]
        mat = X.tocsc() if axis == 0 else X.tocsr()
        mat.sum_duplicates()
        major_index, value = _minor_reduce(mat, min_or_max)
        not_full = np.diff(mat.indptr)[major_index] < N
        value[not_full] = min_or_max(value[not_full], 0)
        mask = value != 0
        major_index = np.compress(mask, major_index)
        value = np.compress(mask, value)
        from scipy.sparse import coo_matrix
        if axis == 0:
            res = coo_matrix((value, (np.zeros(len(value)), major_index)),
                             dtype=X.dtype, shape=(1, M))
        else:
            res = coo_matrix((value, (major_index, np.zeros(len(value)))),
                             dtype=X.dtype, shape=(M, 1))
        return res.A.ravel()
    def _sparse_min_or_max(X, axis, min_or_max):
        if axis is None:
            if 0 in X.shape:
                raise ValueError("zero-size array to reduction operation")
            zero = X.dtype.type(0)
            if X.nnz == 0:
                return zero
            m = min_or_max.reduce(X.data.ravel())
            if X.nnz != np.product(X.shape):
                m = min_or_max(zero, m)
            return m
        if axis < 0:
            axis += 2
        if (axis == 0) or (axis == 1):
            return _min_or_max_axis(X, axis, min_or_max)
        else:
            raise ValueError("invalid axis, use 0 for rows, or 1 for columns")
    def sparse_min_max(X, axis):
        return (_sparse_min_or_max(X, axis, np.minimum),
                _sparse_min_or_max(X, axis, np.maximum))
else:
    def sparse_min_max(X, axis):
        return (X.min(axis=axis).toarray().ravel(),
                X.max(axis=axis).toarray().ravel())

try:
    from numpy import argpartition
except ImportError:
        def argpartition(a, kth, axis=-1, kind='introselect', order=None):
        return np.argsort(a, axis=axis, order=order)
try:
    from numpy import partition
except ImportError:
    warnings.warn('Using `sort` instead of partition.'
                  'Upgrade numpy to 1.8 for better performace on large number'
                  'of clusters')
    def partition(a, kth, axis=-1, kind='introselect', order=None):
        return np.sort(a, axis=axis, order=order)

if np_version < (1, 7):
        def frombuffer_empty(buf, dtype):
        if len(buf) == 0:
            return np.empty(0, dtype=dtype)
        else:
            return np.frombuffer(buf, dtype=dtype)
else:
    frombuffer_empty = np.frombuffer

if np_version < (1, 8):
    def in1d(ar1, ar2, assume_unique=False, invert=False):
                        ar1 = np.asarray(ar1).ravel()
        ar2 = np.asarray(ar2).ravel()
                if len(ar2) < 10 * len(ar1) ** 0.145:
            if invert:
                mask = np.ones(len(ar1), dtype=np.bool)
                for a in ar2:
                    mask &= (ar1 != a)
            else:
                mask = np.zeros(len(ar1), dtype=np.bool)
                for a in ar2:
                    mask |= (ar1 == a)
            return mask
                if not assume_unique:
            ar1, rev_idx = np.unique(ar1, return_inverse=True)
            ar2 = np.unique(ar2)
        ar = np.concatenate((ar1, ar2))
                                order = ar.argsort(kind='mergesort')
        sar = ar[order]
        if invert:
            bool_ar = (sar[1:] != sar[:-1])
        else:
            bool_ar = (sar[1:] == sar[:-1])
        flag = np.concatenate((bool_ar, [invert]))
        indx = order.argsort(kind='mergesort')[:len(ar1)]
        if assume_unique:
            return flag[indx]
        else:
            return flag[indx][rev_idx]
else:
    from numpy import in1d

if sp_version < (0, 15):
        from ._scipy_sparse_lsqr_backport import lsqr as sparse_lsqr
else:
    from scipy.sparse.linalg import lsqr as sparse_lsqr

def parallel_helper(obj, methodname, *args, **kwargs):
        return getattr(obj, methodname)(*args, **kwargs)

if np_version < (1, 6, 2):
            def bincount(x, weights=None, minlength=None):
        if len(x) > 0:
            return np.bincount(x, weights, minlength)
        else:
            if minlength is None:
                minlength = 0
            minlength = np.asscalar(np.asarray(minlength, dtype=np.intp))
            return np.zeros(minlength, dtype=np.intp)
else:
    from numpy import bincount

if 'exist_ok' in signature(os.makedirs).parameters:
    makedirs = os.makedirs
else:
    def makedirs(name, mode=0o777, exist_ok=False):
        
        try:
            os.makedirs(name, mode=mode)
        except OSError as e:
            if (not exist_ok or e.errno != errno.EEXIST
                    or not os.path.isdir(name)):
                raise

if np_version < (1, 8, 1):
    def array_equal(a1, a2):
                try:
            a1, a2 = np.asarray(a1), np.asarray(a2)
        except:
            return False
        if a1.shape != a2.shape:
            return False
        return bool(np.asarray(a1 == a2).all())
else:
    from numpy import array_equal
if sp_version < (0, 13, 0):
    def rankdata(a, method='average'):
        if method not in ('average', 'min', 'max', 'dense', 'ordinal'):
            raise ValueError('unknown method "{0}"'.format(method))
        arr = np.ravel(np.asarray(a))
        algo = 'mergesort' if method == 'ordinal' else 'quicksort'
        sorter = np.argsort(arr, kind=algo)
        inv = np.empty(sorter.size, dtype=np.intp)
        inv[sorter] = np.arange(sorter.size, dtype=np.intp)
        if method == 'ordinal':
            return inv + 1
        arr = arr[sorter]
        obs = np.r_[True, arr[1:] != arr[:-1]]
        dense = obs.cumsum()[inv]
        if method == 'dense':
            return dense
                count = np.r_[np.nonzero(obs)[0], len(obs)]
        if method == 'max':
            return count[dense]
        if method == 'min':
            return count[dense - 1] + 1
                return .5 * (count[dense] + count[dense - 1] + 1)
else:
    from scipy.stats import rankdata

if np_version < (1, 12):
    class MaskedArray(np.ma.MaskedArray):
                                def __getstate__(self):
                        cf = 'CF'[self.flags.fnc]
            data_state = super(np.ma.MaskedArray, self).__reduce__()[2]
            return data_state + (np.ma.getmaskarray(self).tostring(cf),
                                 self._fill_value)
else:
    from numpy.ma import MaskedArray    
if 'axis' not in signature(np.linalg.norm).parameters:
    def norm(X, ord=None, axis=None):
        
        if axis is None or X.ndim == 1:
            result = np.linalg.norm(X, ord=ord)
            return result
        if axis not in (0, 1):
            raise NotImplementedError(Graph utilities and algorithms
Graphs are represented with their adjacency matrices, preferably using
sparse matrices.
    if sparse.isspmatrix(graph):
        graph = graph.tolil()
    else:
        graph = sparse.lil_matrix(graph)
    seen = {}                       level = 0                       next_level = [source]           while next_level:
        this_level = next_level             next_level = set()                  for v in this_level:
            if v not in seen:
                seen[v] = level                     next_level.update(graph.rows[v])
        if cutoff is not None and cutoff <= level:
            break
        level += 1
    return seen  
if hasattr(sparse, 'connected_components'):
    connected_components = sparse.connected_components
else:
    from .sparsetools import connected_components

def graph_laplacian(csgraph, normed=False, return_diag=False):
        if csgraph.ndim != 2 or csgraph.shape[0] != csgraph.shape[1]:
        raise ValueError('csgraph must be a square matrix or array')
    if normed and (np.issubdtype(csgraph.dtype, np.int)
                   or np.issubdtype(csgraph.dtype, np.uint)):
        csgraph = check_array(csgraph, dtype=np.float64, accept_sparse=True)
    if sparse.isspmatrix(csgraph):
        return _laplacian_sparse(csgraph, normed=normed,
                                 return_diag=return_diag)
    else:
        return _laplacian_dense(csgraph, normed=normed,
                                return_diag=return_diag)

def _laplacian_sparse(graph, normed=False, return_diag=False):
    n_nodes = graph.shape[0]
    if not graph.format == 'coo':
        lap = (-graph).tocoo()
    else:
        lap = -graph.copy()
    diag_mask = (lap.row == lap.col)
    if not diag_mask.sum() == n_nodes:
                        diag_idx = lap.row[diag_mask]
        diagonal_holes = list(set(range(n_nodes)).difference(diag_idx))
        new_data = np.concatenate([lap.data, np.ones(len(diagonal_holes))])
        new_row = np.concatenate([lap.row, diagonal_holes])
        new_col = np.concatenate([lap.col, diagonal_holes])
        lap = sparse.coo_matrix((new_data, (new_row, new_col)),
                                shape=lap.shape)
        diag_mask = (lap.row == lap.col)
    lap.data[diag_mask] = 0
    w = -np.asarray(lap.sum(axis=1)).squeeze()
    if normed:
        w = np.sqrt(w)
        w_zeros = (w == 0)
        w[w_zeros] = 1
        lap.data /= w[lap.row]
        lap.data /= w[lap.col]
        lap.data[diag_mask] = (1 - w_zeros[lap.row[diag_mask]]).astype(
            lap.data.dtype)
    else:
        lap.data[diag_mask] = w[lap.row[diag_mask]]
    if return_diag:
        return lap, w
    return lap

def _laplacian_dense(graph, normed=False, return_diag=False):
    n_nodes = graph.shape[0]
    lap = -np.asarray(graph)  
        lap.flat[::n_nodes + 1] = 0
    w = -lap.sum(axis=0)
    if normed:
        w = np.sqrt(w)
        w_zeros = (w == 0)
        w[w_zeros] = 1
        lap /= w
        lap /= w[:, np.newaxis]
        lap.flat[::n_nodes + 1] = (1 - w_zeros).astype(lap.dtype)
    else:
        lap.flat[::n_nodes + 1] = w.astype(lap.dtype)
    if return_diag:
        return lap, w
    return lap
cdef extern from "src/gamma.h":
    cdef double sklearn_lgamma(double x)

cdef double lgamma(double x):
    if x <= 0:
        raise ValueError("x must be strictly positive, got %f" % x)
    return sklearn_lgamma(x)

import numpy as np
from .fixes import astype

def linear_assignment(X):
        indices = _hungarian(X).tolist()
    indices.sort()
        indices = np.array(indices, dtype=int)
            indices.shape = (-1, 2)
    return indices

class _HungarianState(object):
    
    def __init__(self, cost_matrix):
        cost_matrix = np.atleast_2d(cost_matrix)
                                        transposed = (cost_matrix.shape[1] < cost_matrix.shape[0])
        if transposed:
            self.C = (cost_matrix.T).copy()
        else:
            self.C = cost_matrix.copy()
        self.transposed = transposed
                n, m = self.C.shape
        self.row_uncovered = np.ones(n, dtype=np.bool)
        self.col_uncovered = np.ones(m, dtype=np.bool)
        self.Z0_r = 0
        self.Z0_c = 0
        self.path = np.zeros((n + m, 2), dtype=int)
        self.marked = np.zeros((n, m), dtype=int)
    def _find_prime_in_row(self, row):
                col = np.argmax(self.marked[row] == 2)
        if self.marked[row, col] != 2:
            col = -1
        return col
    def _clear_covers(self):
                self.row_uncovered[:] = True
        self.col_uncovered[:] = True

def _hungarian(cost_matrix):
        state = _HungarianState(cost_matrix)
            step = None if 0 in cost_matrix.shape else _step1
    while step is not None:
        step = step(state)
        results = np.array(np.where(state.marked == 1)).T
            if state.transposed:
        results = results[:, ::-1]
    return results

def _step1(state):
    
            state.C -= state.C.min(axis=1)[:, np.newaxis]
                for i, j in zip(*np.where(state.C == 0)):
        if state.col_uncovered[j] and state.row_uncovered[i]:
            state.marked[i, j] = 1
            state.col_uncovered[j] = False
            state.row_uncovered[i] = False
    state._clear_covers()
    return _step3

def _step3(state):
        marked = (state.marked == 1)
    state.col_uncovered[np.any(marked, axis=0)] = False
    if marked.sum() < state.C.shape[0]:
        return _step4

def _step4(state):
            C = (state.C == 0).astype(np.int)
    covered_C = C * state.row_uncovered[:, np.newaxis]
    covered_C *= astype(state.col_uncovered, dtype=np.int, copy=False)
    n = state.C.shape[0]
    m = state.C.shape[1]
    while True:
                row, col = np.unravel_index(np.argmax(covered_C), (n, m))
        if covered_C[row, col] == 0:
            return _step6
        else:
            state.marked[row, col] = 2
                        star_col = np.argmax(state.marked[row] == 1)
            if not state.marked[row, star_col] == 1:
                                state.Z0_r = row
                state.Z0_c = col
                return _step5
            else:
                col = star_col
                state.row_uncovered[row] = False
                state.col_uncovered[col] = True
                covered_C[:, col] = C[:, col] * (
                    astype(state.row_uncovered, dtype=np.int, copy=False))
                covered_C[row] = 0

def _step5(state):
        count = 0
    path = state.path
    path[count, 0] = state.Z0_r
    path[count, 1] = state.Z0_c
    while True:
                        row = np.argmax(state.marked[:, path[count, 1]] == 1)
        if not state.marked[row, path[count, 1]] == 1:
                        break
        else:
            count += 1
            path[count, 0] = row
            path[count, 1] = path[count - 1, 1]
                        col = np.argmax(state.marked[path[count, 0]] == 2)
        if state.marked[row, col] != 2:
            col = -1
        count += 1
        path[count, 0] = path[count - 1, 0]
        path[count, 1] = col
        for i in range(count + 1):
        if state.marked[path[i, 0], path[i, 1]] == 1:
            state.marked[path[i, 0], path[i, 1]] = 0
        else:
            state.marked[path[i, 0], path[i, 1]] = 1
    state._clear_covers()
        state.marked[state.marked == 2] = 0
    return _step3

def _step6(state):
            if np.any(state.row_uncovered) and np.any(state.col_uncovered):
        minval = np.min(state.C[state.row_uncovered], axis=0)
        minval = np.min(minval[state.col_uncovered])
        state.C[np.logical_not(state.row_uncovered)] += minval
        state.C[:, state.col_uncovered] -= minval
    return _step4
from operator import attrgetter
from functools import update_wrapper
import numpy as np
from ..utils import safe_indexing
__all__ = ['if_delegate_has_method']

class _IffHasAttrDescriptor(object):
        def __init__(self, fn, delegate_names, attribute_name):
        self.fn = fn
        self.delegate_names = delegate_names
        self.attribute_name = attribute_name
                update_wrapper(self, fn)
    def __get__(self, obj, type=None):
                if obj is not None:
                                    for delegate_name in self.delegate_names:
                try:
                    delegate = attrgetter(delegate_name)(obj)
                except AttributeError:
                    continue
                else:
                    getattr(delegate, self.attribute_name)
                    break
            else:
                attrgetter(self.delegate_names[-1])(obj)
                out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
                update_wrapper(out, self.fn)
        return out

def if_delegate_has_method(delegate):
        if isinstance(delegate, list):
        delegate = tuple(delegate)
    if not isinstance(delegate, tuple):
        delegate = (delegate,)
    return lambda fn: _IffHasAttrDescriptor(fn, delegate,
                                            attribute_name=fn.__name__)

def _safe_split(estimator, X, y, indices, train_indices=None):
        if getattr(estimator, "_pairwise", False):
        if not hasattr(X, "shape"):
            raise ValueError("Precomputed kernels or affinity matrices have "
                             "to be passed as arrays or sparse matrices.")
                if X.shape[0] != X.shape[1]:
            raise ValueError("X should be a square kernel matrix")
        if train_indices is None:
            X_subset = X[np.ix_(indices, indices)]
        else:
            X_subset = X[np.ix_(indices, train_indices)]
    else:
        X_subset = safe_indexing(X, indices)
    if y is not None:
        y_subset = safe_indexing(y, indices)
    else:
        y_subset = None
    return X_subset, y_subset
import numpy as np
from ..base import BaseEstimator, ClassifierMixin
from .testing import assert_true
from .validation import _num_samples, check_array

class ArraySlicingWrapper(object):
    def __init__(self, array):
        self.array = array
    def __getitem__(self, aslice):
        return MockDataFrame(self.array[aslice])

class MockDataFrame(object):
        def __init__(self, array):
        self.array = array
        self.values = array
        self.shape = array.shape
        self.ndim = array.ndim
                self.iloc = ArraySlicingWrapper(array)
    def __len__(self):
        return len(self.array)
    def __array__(self, dtype=None):
                                return self.array
    def __eq__(self, other):
        return MockDataFrame(self.array == other.array)

class CheckingClassifier(BaseEstimator, ClassifierMixin):
        def __init__(self, check_y=None, check_X=None, foo_param=0,
                 expected_fit_params=None):
        self.check_y = check_y
        self.check_X = check_X
        self.foo_param = foo_param
        self.expected_fit_params = expected_fit_params
    def fit(self, X, y, **fit_params):
        assert_true(len(X) == len(y))
        if self.check_X is not None:
            assert_true(self.check_X(X))
        if self.check_y is not None:
            assert_true(self.check_y(y))
        self.classes_ = np.unique(check_array(y, ensure_2d=False,
                                              allow_nd=True))
        if self.expected_fit_params:
            missing = set(self.expected_fit_params) - set(fit_params)
            assert_true(len(missing) == 0, 'Expected fit parameter(s) %s not '
                                           'seen.' % list(missing))
            for key, value in fit_params.items():
                assert_true(len(value) == len(X),
                            'Fit parameter %s has length %d; '
                            'expected %d.' % (key, len(value), len(X)))
        return self
    def predict(self, T):
        if self.check_X is not None:
            assert_true(self.check_X(T))
        return self.classes_[np.zeros(_num_samples(T), dtype=np.int)]
    def score(self, X=None, Y=None):
        if self.foo_param > 1:
            score = 1.
        else:
            score = 0.
        return score
from __future__ import division
from collections import Sequence
from itertools import chain
from scipy.sparse import issparse
from scipy.sparse.base import spmatrix
from scipy.sparse import dok_matrix
from scipy.sparse import lil_matrix
import numpy as np
from ..externals.six import string_types
from .validation import check_array
from ..utils.fixes import bincount
from ..utils.fixes import array_equal
def _unique_multiclass(y):
    if hasattr(y, '__array__'):
        return np.unique(np.asarray(y))
    else:
        return set(y)

def _unique_indicator(y):
    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])

_FN_UNIQUE_LABELS = {
    'binary': _unique_multiclass,
    'multiclass': _unique_multiclass,
    'multilabel-indicator': _unique_indicator,
}

def unique_labels(*ys):
        if not ys:
        raise ValueError('No argument has been passed.')
    
    ys_types = set(type_of_target(x) for x in ys)
    if ys_types == set(["binary", "multiclass"]):
        ys_types = set(["multiclass"])
    if len(ys_types) > 1:
        raise ValueError("Mix type of y not allowed, got types %s" % ys_types)
    label_type = ys_types.pop()
        if (label_type == "multilabel-indicator" and
            len(set(check_array(y, ['csr', 'csc', 'coo']).shape[1]
                    for y in ys)) > 1):
        raise ValueError("Multi-label binary indicator input with "
                         "different numbers of labels")
        _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)
    if not _unique_labels:
        raise ValueError("Unknown label type: %s" % repr(ys))
    ys_labels = set(chain.from_iterable(_unique_labels(y) for y in ys))
        if (len(set(isinstance(label, string_types) for label in ys_labels)) > 1):
        raise ValueError("Mix of label input types (string and number)")
    return np.array(sorted(ys_labels))

def _is_integral_float(y):
    return y.dtype.kind == 'f' and np.all(y.astype(int) == y)

def is_multilabel(y):
        if hasattr(y, '__array__'):
        y = np.asarray(y)
    if not (hasattr(y, "shape") and y.ndim == 2 and y.shape[1] > 1):
        return False
    if issparse(y):
        if isinstance(y, (dok_matrix, lil_matrix)):
            y = y.tocsr()
        return (len(y.data) == 0 or np.unique(y.data).size == 1 and
                (y.dtype.kind in 'biu' or                   _is_integral_float(np.unique(y.data))))
    else:
        labels = np.unique(y)
        return len(labels) < 3 and (y.dtype.kind in 'biu' or                                      _is_integral_float(labels))
def check_classification_targets(y):
        y_type = type_of_target(y)
    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',
            'multilabel-indicator', 'multilabel-sequences']:
        raise ValueError("Unknown label type: %r" % y_type)

def type_of_target(y):
        valid = ((isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__'))
             and not isinstance(y, string_types))
    if not valid:
        raise ValueError('Expected array-like (array or non-string sequence), '
                         'got %r' % y)
    if is_multilabel(y):
        return 'multilabel-indicator'
    try:
        y = np.asarray(y)
    except ValueError:
                return 'unknown'
        try:
        if (not hasattr(y[0], '__array__') and isinstance(y[0], Sequence)
                and not isinstance(y[0], string_types)):
            raise ValueError('You appear to be using a legacy multi-label data'
                             ' representation. Sequence of sequences are no'
                             ' longer supported; use a binary array or sparse'
                             ' matrix instead.')
    except IndexError:
        pass
        if y.ndim > 2 or (y.dtype == object and len(y) and
                      not isinstance(y.flat[0], string_types)):
        return 'unknown'  
    if y.ndim == 2 and y.shape[1] == 0:
        return 'unknown'  
    if y.ndim == 2 and y.shape[1] > 1:
        suffix = "-multioutput"      else:
        suffix = ""  
        if y.dtype.kind == 'f' and np.any(y != y.astype(int)):
                return 'continuous' + suffix
    if (len(np.unique(y)) > 2) or (y.ndim >= 2 and len(y[0]) > 1):
        return 'multiclass' + suffix      else:
        return 'binary'  
def _check_partial_fit_first_call(clf, classes=None):
        if getattr(clf, 'classes_', None) is None and classes is None:
        raise ValueError("classes must be passed on the first call "
                         "to partial_fit.")
    elif classes is not None:
        if getattr(clf, 'classes_', None) is not None:
            if not array_equal(clf.classes_, unique_labels(classes)):
                raise ValueError(
                    "`classes=%r` is not the same as on last call "
                    "to partial_fit, was: %r" % (classes, clf.classes_))
        else:
                        clf.classes_ = unique_labels(classes)
            return True
            return False

def class_distribution(y, sample_weight=None):
        classes = []
    n_classes = []
    class_prior = []
    n_samples, n_outputs = y.shape
    if issparse(y):
        y = y.tocsc()
        y_nnz = np.diff(y.indptr)
        for k in range(n_outputs):
            col_nonzero = y.indices[y.indptr[k]:y.indptr[k + 1]]
                        if sample_weight is not None:
                nz_samp_weight = np.asarray(sample_weight)[col_nonzero]
                zeros_samp_weight_sum = (np.sum(sample_weight) -
                                         np.sum(nz_samp_weight))
            else:
                nz_samp_weight = None
                zeros_samp_weight_sum = y.shape[0] - y_nnz[k]
            classes_k, y_k = np.unique(y.data[y.indptr[k]:y.indptr[k + 1]],
                                       return_inverse=True)
            class_prior_k = bincount(y_k, weights=nz_samp_weight)
                                    if 0 in classes_k:
                class_prior_k[classes_k == 0] += zeros_samp_weight_sum
                                    if 0 not in classes_k and y_nnz[k] < y.shape[0]:
                classes_k = np.insert(classes_k, 0, 0)
                class_prior_k = np.insert(class_prior_k, 0,
                                          zeros_samp_weight_sum)
            classes.append(classes_k)
            n_classes.append(classes_k.shape[0])
            class_prior.append(class_prior_k / class_prior_k.sum())
    else:
        for k in range(n_outputs):
            classes_k, y_k = np.unique(y[:, k], return_inverse=True)
            classes.append(classes_k)
            n_classes.append(classes_k.shape[0])
            class_prior_k = bincount(y_k, weights=sample_weight)
            class_prior.append(class_prior_k / class_prior_k.sum())
    return (classes, n_classes, class_prior)

def _ovr_decision_function(predictions, confidences, n_classes):
        n_samples = predictions.shape[0]
    votes = np.zeros((n_samples, n_classes))
    sum_of_confidences = np.zeros((n_samples, n_classes))
    k = 0
    for i in range(n_classes):
        for j in range(i + 1, n_classes):
            sum_of_confidences[:, i] -= confidences[:, k]
            sum_of_confidences[:, j] += confidences[:, k]
            votes[predictions[:, k] == 0, i] += 1
            votes[predictions[:, k] == 1, j] += 1
            k += 1
    max_confidences = sum_of_confidences.max()
    min_confidences = sum_of_confidences.min()
    if max_confidences == min_confidences:
        return votes
                    eps = np.finfo(sum_of_confidences.dtype).eps
    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))
    scale = (0.5 - eps) / max_abs_confidence
    return votes + sum_of_confidences * scale
cimport cython
cimport numpy as np
import numpy as np
cdef extern from "src/MurmurHash3.h":
    void MurmurHash3_x86_32(void *key, int len, np.uint32_t seed, void *out)
    void MurmurHash3_x86_128(void *key, int len, np.uint32_t seed, void *out)
    void MurmurHash3_x64_128 (void *key, int len, np.uint32_t seed, void *out)
np.import_array()

cpdef np.uint32_t murmurhash3_int_u32(int key, unsigned int seed):
        cdef np.uint32_t out
    MurmurHash3_x86_32(&key, sizeof(int), seed, &out)
    return out

cpdef np.int32_t murmurhash3_int_s32(int key, unsigned int seed):
        cdef np.int32_t out
    MurmurHash3_x86_32(&key, sizeof(int), seed, &out)
    return out

cpdef np.uint32_t murmurhash3_bytes_u32(bytes key, unsigned int seed):
        cdef np.uint32_t out
    MurmurHash3_x86_32(<char*> key, len(key), seed, &out)
    return out

cpdef np.int32_t murmurhash3_bytes_s32(bytes key, unsigned int seed):
        cdef np.int32_t out
    MurmurHash3_x86_32(<char*> key, len(key), seed, &out)
    return out

@cython.boundscheck(False)
cpdef np.ndarray[np.uint32_t, ndim=1] murmurhash3_bytes_array_u32(
    np.ndarray[np.int32_t] key, unsigned int seed):
            cdef np.ndarray[np.uint32_t, ndim=1] out = np.zeros(key.size, np.uint32)
    cdef Py_ssize_t i
    for i in range(key.shape[0]):
        out[i] = murmurhash3_int_u32(key[i], seed)
    return out

@cython.boundscheck(False)
cpdef np.ndarray[np.int32_t, ndim=1] murmurhash3_bytes_array_s32(
    np.ndarray[np.int32_t] key, unsigned int seed):
            cdef np.ndarray[np.int32_t, ndim=1] out = np.zeros(key.size, np.int32)
    cdef Py_ssize_t i
    for i in range(key.shape[0]):
        out[i] = murmurhash3_int_s32(key[i], seed)
    return out

def murmurhash3_32(key, seed=0, positive=False):
        if isinstance(key, bytes):
        if positive:
            return murmurhash3_bytes_u32(key, seed)
        else:
            return murmurhash3_bytes_s32(key, seed)
    elif isinstance(key, unicode):
        if positive:
            return murmurhash3_bytes_u32(key.encode('utf-8'), seed)
        else:
            return murmurhash3_bytes_s32(key.encode('utf-8'), seed)
    elif isinstance(key, int) or isinstance(key, np.int32):
        if positive:
            return murmurhash3_int_u32(<np.int32_t>key, seed)
        else:
            return murmurhash3_int_s32(<np.int32_t>key, seed)
    elif isinstance(key, np.ndarray):
        if key.dtype != np.int32:
            raise TypeError(
                "key.dtype should be int32, got %s" % key.dtype)
        if positive:
            return murmurhash3_bytes_array_u32(key.ravel(),
                                               seed).reshape(key.shape)
        else:
            return murmurhash3_bytes_array_s32(key.ravel(),
                                               seed).reshape(key.shape)
    else:
        raise TypeError(
            "key %r with type %s is not supported. "
            "Explicit conversion to bytes is required" % (key, type(key)))
import numpy as np
import warnings
from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1
from ..exceptions import ConvergenceWarning

class _LineSearchError(RuntimeError):
    pass

def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval,
                         **kwargs):
        ret = line_search_wolfe1(f, fprime, xk, pk, gfk,
                             old_fval, old_old_fval,
                             **kwargs)
    if ret[0] is None:
                ret = line_search_wolfe2(f, fprime, xk, pk, gfk,
                                 old_fval, old_old_fval, **kwargs)
    if ret[0] is None:
        raise _LineSearchError()
    return ret

def _cg(fhess_p, fgrad, maxiter, tol):
        xsupi = np.zeros(len(fgrad), dtype=fgrad.dtype)
    ri = fgrad
    psupi = -ri
    i = 0
    dri0 = np.dot(ri, ri)
    while i <= maxiter:
        if np.sum(np.abs(ri)) <= tol:
            break
        Ap = fhess_p(psupi)
                curv = np.dot(psupi, Ap)
        if 0 <= curv <= 3 * np.finfo(np.float64).eps:
            break
        elif curv < 0:
            if i > 0:
                break
            else:
                                xsupi += dri0 / curv * psupi
                break
        alphai = dri0 / curv
        xsupi += alphai * psupi
        ri = ri + alphai * Ap
        dri1 = np.dot(ri, ri)
        betai = dri1 / dri0
        psupi = -ri + betai * psupi
        i = i + 1
        dri0 = dri1          
    return xsupi

def newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
              maxiter=100, maxinner=200, line_search=True, warn=True):
        x0 = np.asarray(x0).flatten()
    xk = x0
    k = 0
    if line_search:
        old_fval = func(x0, *args)
        old_old_fval = None
        while k < maxiter:
                        fgrad, fhess_p = grad_hess(xk, *args)
        absgrad = np.abs(fgrad)
        if np.max(absgrad) < tol:
            break
        maggrad = np.sum(absgrad)
        eta = min([0.5, np.sqrt(maggrad)])
        termcond = eta * maggrad
                        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
        alphak = 1.0
        if line_search:
            try:
                alphak, fc, gc, old_fval, old_old_fval, gfkp1 = \
                    _line_search_wolfe12(func, grad, xk, xsupi, fgrad,
                                         old_fval, old_old_fval, args=args)
            except _LineSearchError:
                warnings.warn('Line Search failed')
                break
        xk = xk + alphak * xsupi                k += 1
    if warn and k >= maxiter:
        warnings.warn("newton-cg failed to converge. Increase the "
                      "number of iterations.", ConvergenceWarning)
    return xk, k
from __future__ import division
import numpy as np
import scipy.sparse as sp
import operator
import array
from sklearn.utils import check_random_state
from sklearn.utils.fixes import astype
from ._random import sample_without_replacement
__all__ = ['sample_without_replacement', 'choice']

def choice(a, size=None, replace=True, p=None, random_state=None):
        random_state = check_random_state(random_state)
        a = np.array(a, copy=False)
    if a.ndim == 0:
        try:
                        pop_size = operator.index(a.item())
        except TypeError:
            raise ValueError("a must be 1-dimensional or an integer")
        if pop_size <= 0:
            raise ValueError("a must be greater than 0")
    elif a.ndim != 1:
        raise ValueError("a must be 1-dimensional")
    else:
        pop_size = a.shape[0]
        if pop_size is 0:
            raise ValueError("a must be non-empty")
    if p is not None:
        p = np.array(p, dtype=np.double, ndmin=1, copy=False)
        if p.ndim != 1:
            raise ValueError("p must be 1-dimensional")
        if p.size != pop_size:
            raise ValueError("a and p must have same size")
        if np.any(p < 0):
            raise ValueError("probabilities are not non-negative")
        if not np.allclose(p.sum(), 1):
            raise ValueError("probabilities do not sum to 1")
    shape = size
    if shape is not None:
        size = np.prod(shape, dtype=np.intp)
    else:
        size = 1
        if replace:
        if p is not None:
            cdf = p.cumsum()
            cdf /= cdf[-1]
            uniform_samples = random_state.random_sample(shape)
            idx = cdf.searchsorted(uniform_samples, side='right')
                        idx = np.array(idx, copy=False)
        else:
            idx = random_state.randint(0, pop_size, size=shape)
    else:
        if size > pop_size:
            raise ValueError("Cannot take a larger sample than "
                             "population when 'replace=False'")
        if p is not None:
            if np.sum(p > 0) < size:
                raise ValueError("Fewer non-zero entries in p than size")
            n_uniq = 0
            p = p.copy()
            found = np.zeros(shape, dtype=np.int)
            flat_found = found.ravel()
            while n_uniq < size:
                x = random_state.rand(size - n_uniq)
                if n_uniq > 0:
                    p[flat_found[0:n_uniq]] = 0
                cdf = np.cumsum(p)
                cdf /= cdf[-1]
                new = cdf.searchsorted(x, side='right')
                _, unique_indices = np.unique(new, return_index=True)
                unique_indices.sort()
                new = new.take(unique_indices)
                flat_found[n_uniq:n_uniq + new.size] = new
                n_uniq += new.size
            idx = found
        else:
            idx = random_state.permutation(pop_size)[:size]
            if shape is not None:
                idx.shape = shape
    if shape is None and isinstance(idx, np.ndarray):
                idx = idx.item(0)
        if a.ndim == 0:
        return idx
    if shape is not None and idx.ndim == 0:
                                                res = np.empty((), dtype=a.dtype)
        res[()] = a[idx]
        return res
    return a[idx]

def random_choice_csc(n_samples, classes, class_probability=None,
                      random_state=None):
        data = array.array('i')
    indices = array.array('i')
    indptr = array.array('i', [0])
    for j in range(len(classes)):
        classes[j] = np.asarray(classes[j])
        if classes[j].dtype.kind != 'i':
            raise ValueError("class dtype %s is not supported" %
                             classes[j].dtype)
        classes[j] = astype(classes[j], np.int64, copy=False)
                if class_probability is None:
            class_prob_j = np.empty(shape=classes[j].shape[0])
            class_prob_j.fill(1 / classes[j].shape[0])
        else:
            class_prob_j = np.asarray(class_probability[j])
        if np.sum(class_prob_j) != 1.0:
            raise ValueError("Probability array at index {0} does not sum to "
                             "one".format(j))
        if class_prob_j.shape[0] != classes[j].shape[0]:
            raise ValueError("classes[{0}] (length {1}) and "
                             "class_probability[{0}] (length {2}) have "
                             "different length.".format(j,
                                                        classes[j].shape[0],
                                                        class_prob_j.shape[0]))
                if 0 not in classes[j]:
            classes[j] = np.insert(classes[j], 0, 0)
            class_prob_j = np.insert(class_prob_j, 0, 0.0)
                rng = check_random_state(random_state)
        if classes[j].shape[0] > 1:
            p_nonzero = 1 - class_prob_j[classes[j] == 0]
            nnz = int(n_samples * p_nonzero)
            ind_sample = sample_without_replacement(n_population=n_samples,
                                                    n_samples=nnz,
                                                    random_state=random_state)
            indices.extend(ind_sample)
                        classes_j_nonzero = classes[j] != 0
            class_probability_nz = class_prob_j[classes_j_nonzero]
            class_probability_nz_norm = (class_probability_nz /
                                         np.sum(class_probability_nz))
            classes_ind = np.searchsorted(class_probability_nz_norm.cumsum(),
                                          rng.rand(nnz))
            data.extend(classes[j][classes_j_nonzero][classes_ind])
        indptr.append(len(indices))
    return sp.csc_matrix((data, indices, indptr),
                         (n_samples, len(classes)),
                         dtype=int)
cimport cython
from libc.limits cimport INT_MAX
cimport numpy as np
import numpy as np
np.import_array()

cdef class SequentialDataset:
    
    cdef void next(self, double **x_data_ptr, int **x_ind_ptr,
                   int *nnz, double *y, double *sample_weight) nogil:
                cdef int current_index = self._get_next_index()
        self._sample(x_data_ptr, x_ind_ptr, nnz, y, sample_weight,
                     current_index)
    cdef int random(self, double **x_data_ptr, int **x_ind_ptr,
                    int *nnz, double *y, double *sample_weight) nogil:
                cdef int current_index = self._get_random_index()
        self._sample(x_data_ptr, x_ind_ptr, nnz, y, sample_weight,
                     current_index)
        return current_index
    cdef void shuffle(self, np.uint32_t seed) nogil:
                        cdef int *ind = self.index_data_ptr
        cdef int n = self.n_samples
        cdef unsigned i, j
        for i in range(n - 1):
            j = i + our_rand_r(&seed) % (n - i)
            ind[i], ind[j] = ind[j], ind[i]
    cdef int _get_next_index(self) nogil:
        cdef int current_index = self.current_index
        if current_index >= (self.n_samples - 1):
            current_index = -1
        current_index += 1
        self.current_index = current_index
        return self.current_index
    cdef int _get_random_index(self) nogil:
        cdef int n = self.n_samples
        cdef int current_index = our_rand_r(&self.seed) % n
        self.current_index = current_index
        return current_index
    cdef void _sample(self, double **x_data_ptr, int **x_ind_ptr,
                      int *nnz, double *y, double *sample_weight,
                      int current_index) nogil:
        pass
    def _shuffle_py(self, np.uint32_t seed):
                self.shuffle(seed)
    def _next_py(self):
                cdef int current_index = self._get_next_index()
        return self._sample_py(current_index)
    def _random_py(self):
                cdef int current_index = self._get_random_index()
        return self._sample_py(current_index)
    def _sample_py(self, int current_index):
                cdef double* x_data_ptr
        cdef int* x_indices_ptr
        cdef int nnz, j
        cdef double y, sample_weight
                self._sample(&x_data_ptr, &x_indices_ptr, &nnz, &y, &sample_weight,
                     current_index)
                cdef np.ndarray[double, ndim=1] x_data = np.empty(nnz)
        cdef np.ndarray[int, ndim=1] x_indices = np.empty(nnz, dtype=np.int32)
        cdef np.ndarray[int, ndim=1] x_indptr = np.asarray([0, nnz],
                                                           dtype=np.int32)
        for j in range(nnz):
            x_data[j] = x_data_ptr[j]
            x_indices[j] = x_indices_ptr[j]
        cdef int sample_idx = self.index_data_ptr[current_index]
        return (x_data, x_indices, x_indptr), y, sample_weight, sample_idx
cdef class ArrayDataset(SequentialDataset):
    
    def __cinit__(self, np.ndarray[double, ndim=2, mode='c'] X,
                  np.ndarray[double, ndim=1, mode='c'] Y,
                  np.ndarray[double, ndim=1, mode='c'] sample_weights,
                  np.uint32_t seed=1):
                if X.shape[0] > INT_MAX or X.shape[1] > INT_MAX:
            raise ValueError("More than %d samples or features not supported;"
                             " got (%d, %d)."
                             % (INT_MAX, X.shape[0], X.shape[1]))
                self.X = X
        self.Y = Y
        self.sample_weights = sample_weights
        self.n_samples = X.shape[0]
        self.n_features = X.shape[1]
        cdef np.ndarray[int, ndim=1, mode='c'] feature_indices = \
            np.arange(0, self.n_features, dtype=np.intc)
        self.feature_indices = feature_indices
        self.feature_indices_ptr = <int *> feature_indices.data
        self.current_index = -1
        self.X_stride = X.strides[0] / X.itemsize
        self.X_data_ptr = <double *>X.data
        self.Y_data_ptr = <double *>Y.data
        self.sample_weight_data = <double *>sample_weights.data
                cdef np.ndarray[int, ndim=1, mode='c'] index = \
            np.arange(0, self.n_samples, dtype=np.intc)
        self.index = index
        self.index_data_ptr = <int *>index.data
                self.seed = max(seed, 1)
    cdef void _sample(self, double **x_data_ptr, int **x_ind_ptr,
                      int *nnz, double *y, double *sample_weight,
                      int current_index) nogil:
        cdef long long sample_idx = self.index_data_ptr[current_index]
        cdef long long offset = sample_idx * self.X_stride
        y[0] = self.Y_data_ptr[sample_idx]
        x_data_ptr[0] = self.X_data_ptr + offset
        x_ind_ptr[0] = self.feature_indices_ptr
        nnz[0] = self.n_features
        sample_weight[0] = self.sample_weight_data[sample_idx]

cdef class CSRDataset(SequentialDataset):
    
    def __cinit__(self, np.ndarray[double, ndim=1, mode='c'] X_data,
                  np.ndarray[int, ndim=1, mode='c'] X_indptr,
                  np.ndarray[int, ndim=1, mode='c'] X_indices,
                  np.ndarray[double, ndim=1, mode='c'] Y,
                  np.ndarray[double, ndim=1, mode='c'] sample_weights,
                  np.uint32_t seed=1):
                        self.X_data = X_data
        self.X_indptr = X_indptr
        self.X_indices = X_indices
        self.Y = Y
        self.sample_weights = sample_weights
        self.n_samples = Y.shape[0]
        self.current_index = -1
        self.X_data_ptr = <double *>X_data.data
        self.X_indptr_ptr = <int *>X_indptr.data
        self.X_indices_ptr = <int *>X_indices.data
        self.Y_data_ptr = <double *>Y.data
        self.sample_weight_data = <double *>sample_weights.data
                cdef np.ndarray[int, ndim=1, mode='c'] idx = np.arange(self.n_samples,
                                                               dtype=np.intc)
        self.index = idx
        self.index_data_ptr = <int *>idx.data
                self.seed = max(seed, 1)
    cdef void _sample(self, double **x_data_ptr, int **x_ind_ptr,
                      int *nnz, double *y, double *sample_weight,
                      int current_index) nogil:
        cdef long long sample_idx = self.index_data_ptr[current_index]
        cdef long long offset = self.X_indptr_ptr[sample_idx]
        y[0] = self.Y_data_ptr[sample_idx]
        x_data_ptr[0] = self.X_data_ptr + offset
        x_ind_ptr[0] = self.X_indices_ptr + offset
        nnz[0] = self.X_indptr_ptr[sample_idx + 1] - offset
        sample_weight[0] = self.sample_weight_data[sample_idx]

cdef enum:
    RAND_R_MAX = 0x7FFFFFFF

cdef inline np.uint32_t our_rand_r(np.uint32_t* seed) nogil:
    seed[0] ^= <np.uint32_t>(seed[0] << 13)
    seed[0] ^= <np.uint32_t>(seed[0] >> 17)
    seed[0] ^= <np.uint32_t>(seed[0] << 5)
    return seed[0] % (<np.uint32_t>RAND_R_MAX + 1)
import os
from os.path import join
from sklearn._build_utils import get_blas_info

def configuration(parent_package='', top_path=None):
    import numpy
    from numpy.distutils.misc_util import Configuration
    config = Configuration('utils', parent_package, top_path)
    config.add_subpackage('sparsetools')
    cblas_libs, blas_info = get_blas_info()
    cblas_compile_args = blas_info.pop('extra_compile_args', [])
    cblas_includes = [join('..', 'src', 'cblas'),
                      numpy.get_include(),
                      blas_info.pop('include_dirs', [])]
    libraries = []
    if os.name == 'posix':
        libraries.append('m')
        cblas_libs.append('m')
    config.add_extension('sparsefuncs_fast', sources=['sparsefuncs_fast.pyx'],
                         libraries=libraries)
    config.add_extension('arrayfuncs',
                         sources=['arrayfuncs.pyx'],
                         depends=[join('src', 'cholesky_delete.h')],
                         libraries=cblas_libs,
                         include_dirs=cblas_includes,
                         extra_compile_args=cblas_compile_args,
                         **blas_info
                         )
    config.add_extension('murmurhash',
                         sources=['murmurhash.pyx', join(
                             'src', 'MurmurHash3.cpp')],
                         include_dirs=['src'])
    config.add_extension('lgamma',
                         sources=['lgamma.pyx', join('src', 'gamma.c')],
                         include_dirs=['src'],
                         libraries=libraries)
    config.add_extension('graph_shortest_path',
                         sources=['graph_shortest_path.pyx'],
                         include_dirs=[numpy.get_include()])
    config.add_extension('fast_dict',
                         sources=['fast_dict.pyx'],
                         language="c++",
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_extension('seq_dataset',
                         sources=['seq_dataset.pyx'],
                         include_dirs=[numpy.get_include()])
    config.add_extension('weight_vector',
                         sources=['weight_vector.pyx'],
                         include_dirs=cblas_includes,
                         libraries=cblas_libs,
                         **blas_info)
    config.add_extension("_random",
                         sources=["_random.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_extension("_logistic_sigmoid",
                         sources=["_logistic_sigmoid.pyx"],
                         include_dirs=[numpy.get_include()],
                         libraries=libraries)
    config.add_subpackage('tests')
    return config

if __name__ == '__main__':
    from numpy.distutils.core import setup
    setup(**configuration(top_path='').todict())
import scipy.sparse as sp
import numpy as np
from .fixes import sparse_min_max, bincount
from .sparsefuncs_fast import (
    csr_mean_variance_axis0 as _csr_mean_var_axis0,
    csc_mean_variance_axis0 as _csc_mean_var_axis0,
    incr_mean_variance_axis0 as _incr_mean_var_axis0)

def _raise_typeerror(X):
        input_type = X.format if sp.issparse(X) else type(X)
    err = "Expected a CSR or CSC sparse matrix, got %s." % input_type
    raise TypeError(err)

def _raise_error_wrong_axis(axis):
    if axis not in (0, 1):
        raise ValueError(
            "Unknown axis value: %d. Use 0 for rows, or 1 for columns" % axis)

def inplace_csr_column_scale(X, scale):
        assert scale.shape[0] == X.shape[1]
    X.data *= scale.take(X.indices, mode='clip')

def inplace_csr_row_scale(X, scale):
        assert scale.shape[0] == X.shape[0]
    X.data *= np.repeat(scale, np.diff(X.indptr))

def mean_variance_axis(X, axis):
        _raise_error_wrong_axis(axis)
    if isinstance(X, sp.csr_matrix):
        if axis == 0:
            return _csr_mean_var_axis0(X)
        else:
            return _csc_mean_var_axis0(X.T)
    elif isinstance(X, sp.csc_matrix):
        if axis == 0:
            return _csc_mean_var_axis0(X)
        else:
            return _csr_mean_var_axis0(X.T)
    else:
        _raise_typeerror(X)

def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):
        _raise_error_wrong_axis(axis)
    if isinstance(X, sp.csr_matrix):
        if axis == 0:
            return _incr_mean_var_axis0(X, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
        else:
            return _incr_mean_var_axis0(X.T, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
    elif isinstance(X, sp.csc_matrix):
        if axis == 0:
            return _incr_mean_var_axis0(X, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
        else:
            return _incr_mean_var_axis0(X.T, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
    else:
        _raise_typeerror(X)

def inplace_column_scale(X, scale):
        if isinstance(X, sp.csc_matrix):
        inplace_csr_row_scale(X.T, scale)
    elif isinstance(X, sp.csr_matrix):
        inplace_csr_column_scale(X, scale)
    else:
        _raise_typeerror(X)

def inplace_row_scale(X, scale):
        if isinstance(X, sp.csc_matrix):
        inplace_csr_column_scale(X.T, scale)
    elif isinstance(X, sp.csr_matrix):
        inplace_csr_row_scale(X, scale)
    else:
        _raise_typeerror(X)

def inplace_swap_row_csc(X, m, n):
        for t in [m, n]:
        if isinstance(t, np.ndarray):
            raise TypeError("m and n should be valid integers")
    if m < 0:
        m += X.shape[0]
    if n < 0:
        n += X.shape[0]
    m_mask = X.indices == m
    X.indices[X.indices == n] = m
    X.indices[m_mask] = n

def inplace_swap_row_csr(X, m, n):
        for t in [m, n]:
        if isinstance(t, np.ndarray):
            raise TypeError("m and n should be valid integers")
    if m < 0:
        m += X.shape[0]
    if n < 0:
        n += X.shape[0]
            if m > n:
        m, n = n, m
    indptr = X.indptr
    m_start = indptr[m]
    m_stop = indptr[m + 1]
    n_start = indptr[n]
    n_stop = indptr[n + 1]
    nz_m = m_stop - m_start
    nz_n = n_stop - n_start
    if nz_m != nz_n:
                X.indptr[m + 2:n] += nz_n - nz_m
        X.indptr[m + 1] = m_start + nz_n
        X.indptr[n] = n_stop - nz_m
    X.indices = np.concatenate([X.indices[:m_start],
                                X.indices[n_start:n_stop],
                                X.indices[m_stop:n_start],
                                X.indices[m_start:m_stop],
                                X.indices[n_stop:]])
    X.data = np.concatenate([X.data[:m_start],
                             X.data[n_start:n_stop],
                             X.data[m_stop:n_start],
                             X.data[m_start:m_stop],
                             X.data[n_stop:]])

def inplace_swap_row(X, m, n):
        if isinstance(X, sp.csc_matrix):
        return inplace_swap_row_csc(X, m, n)
    elif isinstance(X, sp.csr_matrix):
        return inplace_swap_row_csr(X, m, n)
    else:
        _raise_typeerror(X)

def inplace_swap_column(X, m, n):
        if m < 0:
        m += X.shape[1]
    if n < 0:
        n += X.shape[1]
    if isinstance(X, sp.csc_matrix):
        return inplace_swap_row_csr(X, m, n)
    elif isinstance(X, sp.csr_matrix):
        return inplace_swap_row_csc(X, m, n)
    else:
        _raise_typeerror(X)

def min_max_axis(X, axis):
        if isinstance(X, sp.csr_matrix) or isinstance(X, sp.csc_matrix):
        return sparse_min_max(X, axis=axis)
    else:
        _raise_typeerror(X)

def count_nonzero(X, axis=None, sample_weight=None):
        if axis == -1:
        axis = 1
    elif axis == -2:
        axis = 0
    elif X.format != 'csr':
        raise TypeError('Expected CSR sparse format, got {0}'.format(X.format))
                    if axis is None:
        if sample_weight is None:
            return X.nnz
        else:
            return np.dot(np.diff(X.indptr), sample_weight)
    elif axis == 1:
        out = np.diff(X.indptr)
        if sample_weight is None:
            return out
        return out * sample_weight
    elif axis == 0:
        if sample_weight is None:
            return bincount(X.indices, minlength=X.shape[1])
        else:
            weights = np.repeat(sample_weight, np.diff(X.indptr))
            return bincount(X.indices, minlength=X.shape[1],
                            weights=weights)
    else:
        raise ValueError('Unsupported axis: {0}'.format(axis))

def _get_median(data, n_zeros):
        n_elems = len(data) + n_zeros
    if not n_elems:
        return np.nan
    n_negative = np.count_nonzero(data < 0)
    middle, is_odd = divmod(n_elems, 2)
    data.sort()
    if is_odd:
        return _get_elem_at_rank(middle, data, n_negative, n_zeros)
    return (_get_elem_at_rank(middle - 1, data, n_negative, n_zeros) +
            _get_elem_at_rank(middle, data, n_negative, n_zeros)) / 2.

def _get_elem_at_rank(rank, data, n_negative, n_zeros):
        if rank < n_negative:
        return data[rank]
    if rank - n_negative < n_zeros:
        return 0
    return data[rank - n_zeros]

def csc_median_axis_0(X):
        if not isinstance(X, sp.csc_matrix):
        raise TypeError("Expected matrix of CSC format, got %s" % X.format)
    indptr = X.indptr
    n_samples, n_features = X.shape
    median = np.zeros(n_features)
    for f_ind, (start, end) in enumerate(zip(indptr[:-1], indptr[1:])):
                data = np.copy(X.data[start: end])
        nz = n_samples - data.size
        median[f_ind] = _get_median(data, nz)
    return median

from libc.math cimport fabs, sqrt, pow
cimport numpy as np
import numpy as np
import scipy.sparse as sp
cimport cython
from cython cimport floating
np.import_array()

ctypedef np.float64_t DOUBLE
def csr_row_norms(X):
        if X.dtype != np.float32:
        X = X.astype(np.float64)
    return _csr_row_norms(X.data, X.shape, X.indices, X.indptr)

def _csr_row_norms(np.ndarray[floating, ndim=1, mode="c"] X_data,
                   shape,
                   np.ndarray[int, ndim=1, mode="c"] X_indices,
                   np.ndarray[int, ndim=1, mode="c"] X_indptr):
    cdef:
        unsigned int n_samples = shape[0]
        unsigned int n_features = shape[1]
        np.ndarray[DOUBLE, ndim=1, mode="c"] norms
        np.npy_intp i, j
        double sum_
    norms = np.zeros(n_samples, dtype=np.float64)
    for i in range(n_samples):
        sum_ = 0.0
        for j in range(X_indptr[i], X_indptr[i + 1]):
            sum_ += X_data[j] * X_data[j]
        norms[i] = sum_
    return norms

def csr_mean_variance_axis0(X):
        if X.dtype != np.float32:
        X = X.astype(np.float64)
    return _csr_mean_variance_axis0(X.data, X.shape, X.indices)

def _csr_mean_variance_axis0(np.ndarray[floating, ndim=1, mode="c"] X_data,
                             shape,
                             np.ndarray[int, ndim=1] X_indices):
            cdef unsigned int n_samples = shape[0]
    cdef unsigned int n_features = shape[1]
    cdef unsigned int i
    cdef unsigned int non_zero = X_indices.shape[0]
    cdef unsigned int col_ind
    cdef floating diff
        cdef np.ndarray[floating, ndim=1] means
        cdef np.ndarray[floating, ndim=1] variances
    if floating is float:
        dtype = np.float32
    else:
        dtype = np.float64
    means = np.zeros(n_features, dtype=dtype)
    variances = np.zeros_like(means, dtype=dtype)
        cdef np.ndarray[int, ndim=1] counts = np.zeros(n_features,
                                                   dtype=np.int32)
    for i in xrange(non_zero):
        col_ind = X_indices[i]
        means[col_ind] += X_data[i]
    means /= n_samples
    for i in xrange(non_zero):
        col_ind = X_indices[i]
        diff = X_data[i] - means[col_ind]
        variances[col_ind] += diff * diff
        counts[col_ind] += 1
    for i in xrange(n_features):
        variances[i] += (n_samples - counts[i]) * means[i] ** 2
        variances[i] /= n_samples
    return means, variances

def csc_mean_variance_axis0(X):
        if X.dtype != np.float32:
        X = X.astype(np.float64)
    return _csc_mean_variance_axis0(X.data, X.shape, X.indices, X.indptr)

def _csc_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
                             shape,
                             np.ndarray[int, ndim=1] X_indices,
                             np.ndarray[int, ndim=1] X_indptr):
            cdef unsigned int n_samples = shape[0]
    cdef unsigned int n_features = shape[1]
    cdef unsigned int i
    cdef unsigned int j
    cdef unsigned int counts
    cdef unsigned int startptr
    cdef unsigned int endptr
    cdef floating diff
        cdef np.ndarray[floating, ndim=1] means
        cdef np.ndarray[floating, ndim=1] variances
    if floating is float:
        dtype = np.float32
    else:
        dtype = np.float64
    means = np.zeros(n_features, dtype=dtype)
    variances = np.zeros_like(means, dtype=dtype)
    for i in xrange(n_features):
        startptr = X_indptr[i]
        endptr = X_indptr[i + 1]
        counts = endptr - startptr
        for j in xrange(startptr, endptr):
            means[i] += X_data[j]
        means[i] /= n_samples
        for j in xrange(startptr, endptr):
            diff = X_data[j] - means[i]
            variances[i] += diff * diff
        variances[i] += (n_samples - counts) * means[i] * means[i]
        variances[i] /= n_samples
    return means, variances

def incr_mean_variance_axis0(X, last_mean, last_var, unsigned long last_n):
        if X.dtype != np.float32:
        X = X.astype(np.float64)
    return _incr_mean_variance_axis0(X.data, X.shape, X.indices, X.indptr,
                                     X.format, last_mean, last_var, last_n)

def _incr_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
                              shape,
                              np.ndarray[int, ndim=1] X_indices,
                              np.ndarray[int, ndim=1] X_indptr,
                              X_format,
                              last_mean,
                              last_var,
                              unsigned long last_n):
            cdef unsigned long n_samples = shape[0]
    cdef unsigned int n_features = shape[1]
    cdef unsigned int i
                    cdef np.ndarray[floating, ndim=1] new_mean
    cdef np.ndarray[floating, ndim=1] new_var
    cdef np.ndarray[floating, ndim=1] updated_mean
    cdef np.ndarray[floating, ndim=1] updated_var
    if floating is float:
        dtype = np.float32
    else:
        dtype = np.float64
    new_mean = np.zeros(n_features, dtype=dtype)
    new_var = np.zeros_like(new_mean, dtype=dtype)
    updated_mean = np.zeros_like(new_mean, dtype=dtype)
    updated_var = np.zeros_like(new_mean, dtype=dtype)
    cdef unsigned long new_n
    cdef unsigned long updated_n
    cdef floating last_over_new_n
        new_n = n_samples
    if X_format == 'csr':
                new_mean, new_var = _csr_mean_variance_axis0(X_data, shape, X_indices)
    else:
                new_mean, new_var = _csc_mean_variance_axis0(X_data, shape, X_indices,
                                                     X_indptr)
        if last_n == 0:
        return new_mean, new_var, new_n
        else:
        updated_n = last_n + new_n
        last_over_new_n = last_n / new_n
    for i in xrange(n_features):
                last_mean[i] *= last_n
        last_var[i] *= last_n
                new_mean[i] *= new_n
        new_var[i] *= new_n
                updated_var[i] = (last_var[i] + new_var[i] +
                          last_over_new_n / updated_n *
                          (last_mean[i] / last_over_new_n - new_mean[i]) ** 2)
        updated_mean[i] = (last_mean[i] + new_mean[i]) / updated_n
        updated_var[i] = updated_var[i] / updated_n
    return updated_mean, updated_var, updated_n

def inplace_csr_row_normalize_l1(X):
        _inplace_csr_row_normalize_l1(X.data, X.shape, X.indices, X.indptr)

def _inplace_csr_row_normalize_l1(np.ndarray[floating, ndim=1] X_data,
                                  shape,
                                  np.ndarray[int, ndim=1] X_indices,
                                  np.ndarray[int, ndim=1] X_indptr):
    cdef unsigned int n_samples = shape[0]
    cdef unsigned int n_features = shape[1]
                    cdef unsigned int i
    cdef unsigned int j
    cdef double sum_
    for i in xrange(n_samples):
        sum_ = 0.0
        for j in xrange(X_indptr[i], X_indptr[i + 1]):
            sum_ += fabs(X_data[j])
        if sum_ == 0.0:
                                    continue
        for j in xrange(X_indptr[i], X_indptr[i + 1]):
            X_data[j] /= sum_

def inplace_csr_row_normalize_l2(X):
        _inplace_csr_row_normalize_l2(X.data, X.shape, X.indices, X.indptr)

def _inplace_csr_row_normalize_l2(np.ndarray[floating, ndim=1] X_data,
                                  shape,
                                  np.ndarray[int, ndim=1] X_indices,
                                  np.ndarray[int, ndim=1] X_indptr):
    cdef unsigned int n_samples = shape[0]
    cdef unsigned int n_features = shape[1]
    cdef unsigned int i
    cdef unsigned int j
    cdef double sum_
    for i in xrange(n_samples):
        sum_ = 0.0
        for j in xrange(X_indptr[i], X_indptr[i + 1]):
            sum_ += (X_data[j] * X_data[j])
        if sum_ == 0.0:
                                    continue
        sum_ = sqrt(sum_)
        for j in xrange(X_indptr[i], X_indptr[i + 1]):
            X_data[j] /= sum_

def assign_rows_csr(X,
                    np.ndarray[np.npy_intp, ndim=1] X_rows,
                    np.ndarray[np.npy_intp, ndim=1] out_rows,
                    np.ndarray[floating, ndim=2, mode="c"] out):
        cdef:
                        int i, ind, j
        np.npy_intp rX
        np.ndarray[floating, ndim=1] data = X.data
        np.ndarray[int, ndim=1] indices = X.indices, indptr = X.indptr
    if X_rows.shape[0] != out_rows.shape[0]:
        raise ValueError("cannot assign %d rows to %d"
                         % (X_rows.shape[0], out_rows.shape[0]))
    out[out_rows] = 0.
    for i in range(X_rows.shape[0]):
        rX = X_rows[i]
        for ind in range(indptr[rX], indptr[rX + 1]):
            j = indices[ind]
            out[out_rows[i], j] = data[ind]
import numpy as np
from scipy.stats import rankdata as _sp_rankdata
from .fixes import bincount
from ..utils.extmath import stable_cumsum

def _rankdata(a, method="average"):
        if method != "max":
        raise NotImplementedError()
    unique_all, inverse = np.unique(a, return_inverse=True)
    count = bincount(inverse, minlength=unique_all.size)
    cum_count = count.cumsum()
    rank = cum_count[inverse]
    return rank
try:
    _sp_rankdata([1.], 'max')
    rankdata = _sp_rankdata
except TypeError as e:
    rankdata = _rankdata

def _weighted_percentile(array, sample_weight, percentile=50):
        sorted_idx = np.argsort(array)
        weight_cdf = stable_cumsum(sample_weight[sorted_idx])
    percentile_idx = np.searchsorted(
        weight_cdf, (percentile / 100.) * weight_cdf[-1])
    return array[sorted_idx[percentile_idx]]

import warnings
import numbers
import numpy as np
import scipy.sparse as sp
from ..externals import six
from ..utils.fixes import signature
from ..exceptions import NonBLASDotWarning
from ..exceptions import NotFittedError
from ..exceptions import DataConversionWarning

FLOAT_DTYPES = (np.float64, np.float32, np.float16)
warnings.simplefilter('ignore', NonBLASDotWarning)

def _assert_all_finite(X):
        X = np.asanyarray(X)
                if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())
            and not np.isfinite(X).all()):
        raise ValueError("Input contains NaN, infinity"
                         " or a value too large for %r." % X.dtype)

def assert_all_finite(X):
        _assert_all_finite(X.data if sp.issparse(X) else X)

def as_float_array(X, copy=True, force_all_finite=True):
        if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray)
                                    and not sp.issparse(X)):
        return check_array(X, ['csr', 'csc', 'coo'], dtype=np.float64,
                           copy=copy, force_all_finite=force_all_finite,
                           ensure_2d=False)
    elif sp.issparse(X) and X.dtype in [np.float32, np.float64]:
        return X.copy() if copy else X
    elif X.dtype in [np.float32, np.float64]:          return X.copy('F' if X.flags['F_CONTIGUOUS'] else 'C') if copy else X
    else:
        return X.astype(np.float32 if X.dtype == np.int32 else np.float64)

def _is_arraylike(x):
        return (hasattr(x, '__len__') or
            hasattr(x, 'shape') or
            hasattr(x, '__array__'))

def _num_samples(x):
        if hasattr(x, 'fit') and callable(x.fit):
                raise TypeError('Expected sequence or array-like, got '
                        'estimator %s' % x)
    if not hasattr(x, '__len__') and not hasattr(x, 'shape'):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError("Expected sequence or array-like, got %s" %
                            type(x))
    if hasattr(x, 'shape'):
        if len(x.shape) == 0:
            raise TypeError("Singleton array %r cannot be considered"
                            " a valid collection." % x)
        return x.shape[0]
    else:
        return len(x)

def _shape_repr(shape):
        if len(shape) == 0:
        return "()"
    joined = ", ".join("%d" % e for e in shape)
    if len(shape) == 1:
                joined += ','
    return "(%s)" % joined

def check_consistent_length(*arrays):
    
    lengths = [_num_samples(X) for X in arrays if X is not None]
    uniques = np.unique(lengths)
    if len(uniques) > 1:
        raise ValueError("Found input variables with inconsistent numbers of"
                         " samples: %r" % [int(l) for l in lengths])

def indexable(*iterables):
        result = []
    for X in iterables:
        if sp.issparse(X):
            result.append(X.tocsr())
        elif hasattr(X, "__getitem__") or hasattr(X, "iloc"):
            result.append(X)
        elif X is None:
            result.append(X)
        else:
            result.append(np.array(X))
    check_consistent_length(*result)
    return result

def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy,
                          force_all_finite):
        if dtype is None:
        dtype = spmatrix.dtype
    changed_format = False
    if isinstance(accept_sparse, six.string_types):
        accept_sparse = [accept_sparse]
    if accept_sparse is False:
        raise TypeError('A sparse matrix was passed, but dense '
                        'data is required. Use X.toarray() to '
                        'convert to a dense numpy array.')
    elif isinstance(accept_sparse, (list, tuple)):
        if len(accept_sparse) == 0:
            raise ValueError("When providing 'accept_sparse' "
                             "as a tuple or list, it must contain at "
                             "least one string value.")
                if spmatrix.format not in accept_sparse:
                        spmatrix = spmatrix.asformat(accept_sparse[0])
            changed_format = True
    elif accept_sparse is not True:
                raise ValueError("Parameter 'accept_sparse' should be a string, "
                         "boolean or list of strings. You provided "
                         "'accept_sparse={}'.".format(accept_sparse))
    if dtype != spmatrix.dtype:
                spmatrix = spmatrix.astype(dtype)
    elif copy and not changed_format:
                spmatrix = spmatrix.copy()
    if force_all_finite:
        if not hasattr(spmatrix, "data"):
            warnings.warn("Can't check %s sparse matrix for nan or inf."
                          % spmatrix.format)
        else:
            _assert_all_finite(spmatrix.data)
    return spmatrix

def check_array(array, accept_sparse=False, dtype="numeric", order=None,
                copy=False, force_all_finite=True, ensure_2d=True,
                allow_nd=False, ensure_min_samples=1, ensure_min_features=1,
                warn_on_dtype=False, estimator=None):
            if accept_sparse is None:
        warnings.warn(
            "Passing 'None' to parameter 'accept_sparse' in methods "
            "check_array and check_X_y is deprecated in version 0.19 "
            "and will be removed in 0.21. Use 'accept_sparse=False' "
            " instead.", DeprecationWarning)
        accept_sparse = False
        dtype_numeric = dtype == "numeric"
    dtype_orig = getattr(array, "dtype", None)
    if not hasattr(dtype_orig, 'kind'):
                dtype_orig = None
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == "O":
                        dtype = np.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
                        dtype = None
        else:
                                    dtype = dtype[0]
    if estimator is not None:
        if isinstance(estimator, six.string_types):
            estimator_name = estimator
        else:
            estimator_name = estimator.__class__.__name__
    else:
        estimator_name = "Estimator"
    context = " by %s" % estimator_name if estimator is not None else ""
    if sp.issparse(array):
        array = _ensure_sparse_format(array, accept_sparse, dtype, copy,
                                      force_all_finite)
    else:
        array = np.array(array, dtype=dtype, order=order, copy=copy)
        if ensure_2d:
            if array.ndim == 1:
                raise ValueError(
                    "Got X with X.ndim=1. Reshape your data either using "
                    "X.reshape(-1, 1) if your data has a single feature or "
                    "X.reshape(1, -1) if it contains a single sample.")
            array = np.atleast_2d(array)
                        array = np.array(array, dtype=dtype, order=order, copy=copy)
                if dtype_numeric and array.dtype.kind == "O":
            array = array.astype(np.float64)
        if not allow_nd and array.ndim >= 3:
            raise ValueError("Found array with dim %d. %s expected <= 2."
                             % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array)
    shape_repr = _shape_repr(array.shape)
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError("Found array with %d sample(s) (shape=%s) while a"
                             " minimum of %d is required%s."
                             % (n_samples, shape_repr, ensure_min_samples,
                                context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError("Found array with %d feature(s) (shape=%s) while"
                             " a minimum of %d is required%s."
                             % (n_features, shape_repr, ensure_min_features,
                                context))
    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:
        msg = ("Data with input dtype %s was converted to %s%s."
               % (dtype_orig, array.dtype, context))
        warnings.warn(msg, DataConversionWarning)
    return array

def check_X_y(X, y, accept_sparse=False, dtype="numeric", order=None,
              copy=False, force_all_finite=True, ensure_2d=True,
              allow_nd=False, multi_output=False, ensure_min_samples=1,
              ensure_min_features=1, y_numeric=False,
              warn_on_dtype=False, estimator=None):
        X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
                    ensure_2d, allow_nd, ensure_min_samples,
                    ensure_min_features, warn_on_dtype, estimator)
    if multi_output:
        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
                        dtype=None)
    else:
        y = column_or_1d(y, warn=True)
        _assert_all_finite(y)
    if y_numeric and y.dtype.kind == 'O':
        y = y.astype(np.float64)
    check_consistent_length(X, y)
    return X, y

def column_or_1d(y, warn=False):
        shape = np.shape(y)
    if len(shape) == 1:
        return np.ravel(y)
    if len(shape) == 2 and shape[1] == 1:
        if warn:
            warnings.warn("A column-vector y was passed when a 1d array was"
                          " expected. Please change the shape of y to "
                          "(n_samples, ), for example using ravel().",
                          DataConversionWarning, stacklevel=2)
        return np.ravel(y)
    raise ValueError("bad input shape {0}".format(shape))

def check_random_state(seed):
        if seed is None or seed is np.random:
        return np.random.mtrand._rand
    if isinstance(seed, (numbers.Integral, np.integer)):
        return np.random.RandomState(seed)
    if isinstance(seed, np.random.RandomState):
        return seed
    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)

def has_fit_parameter(estimator, parameter):
        return parameter in signature(estimator.fit).parameters

def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
        if (array.ndim != 2) or (array.shape[0] != array.shape[1]):
        raise ValueError("array must be 2-dimensional and square. "
                         "shape = {0}".format(array.shape))
    if sp.issparse(array):
        diff = array - array.T
                if diff.format not in ['csr', 'csc', 'coo']:
            diff = diff.tocsr()
        symmetric = np.all(abs(diff.data) < tol)
    else:
        symmetric = np.allclose(array, array.T, atol=tol)
    if not symmetric:
        if raise_exception:
            raise ValueError("Array must be symmetric")
        if raise_warning:
            warnings.warn("Array is not symmetric, and will be converted "
                          "to symmetric by average with its transpose.")
        if sp.issparse(array):
            conversion = 'to' + array.format
            array = getattr(0.5 * (array + array.T), conversion)()
        else:
            array = 0.5 * (array + array.T)
    return array

def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):
        if msg is None:
        msg = ("This %(name)s instance is not fitted yet. Call 'fit' with "
               "appropriate arguments before using this method.")
    if not hasattr(estimator, 'fit'):
        raise TypeError("%s is not an estimator instance." % (estimator))
    if not isinstance(attributes, (list, tuple)):
        attributes = [attributes]
    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):
        raise NotFittedError(msg % {'name': type(estimator).__name__})

def check_non_negative(X, whom):
        X = X.data if sp.issparse(X) else X
    if (X < 0).any():
        raise ValueError("Negative values in data passed to %s" % whom)
cimport cython
from libc.limits cimport INT_MAX
from libc.math cimport sqrt
import numpy as np
cimport numpy as np
cdef extern from "cblas.h":
    double ddot "cblas_ddot"(int, double *, int, double *, int) nogil
    void dscal "cblas_dscal"(int, double, double *, int) nogil
    void daxpy "cblas_daxpy" (int, double, const double*,
                              int, double*, int) nogil

np.import_array()

cdef class WeightVector(object):
    
    def __cinit__(self,
                  np.ndarray[double, ndim=1, mode='c'] w,
                  np.ndarray[double, ndim=1, mode='c'] aw):
        cdef double *wdata = <double *>w.data
        if w.shape[0] > INT_MAX:
            raise ValueError("More than %d features not supported; got %d."
                             % (INT_MAX, w.shape[0]))
        self.w = w
        self.w_data_ptr = wdata
        self.wscale = 1.0
        self.n_features = w.shape[0]
        self.sq_norm = ddot(<int>w.shape[0], wdata, 1, wdata, 1)
        self.aw = aw
        if self.aw is not None:
            self.aw_data_ptr = <double *>aw.data
            self.average_a = 0.0
            self.average_b = 1.0
    cdef void add(self, double *x_data_ptr, int *x_ind_ptr, int xnnz,
                  double c) nogil:
                cdef int j
        cdef int idx
        cdef double val
        cdef double innerprod = 0.0
        cdef double xsqnorm = 0.0
                cdef double wscale = self.wscale
        cdef double* w_data_ptr = self.w_data_ptr
        for j in range(xnnz):
            idx = x_ind_ptr[j]
            val = x_data_ptr[j]
            innerprod += (w_data_ptr[idx] * val)
            xsqnorm += (val * val)
            w_data_ptr[idx] += val * (c / wscale)
        self.sq_norm += (xsqnorm * c * c) + (2.0 * innerprod * wscale * c)
                cdef void add_average(self, double *x_data_ptr, int *x_ind_ptr, int xnnz,
                          double c, double num_iter) nogil:
                cdef int j
        cdef int idx
        cdef double val
        cdef double mu = 1.0 / num_iter
        cdef double average_a = self.average_a
        cdef double wscale = self.wscale
        cdef double* aw_data_ptr = self.aw_data_ptr
        for j in range(xnnz):
            idx = x_ind_ptr[j]
            val = x_data_ptr[j]
            aw_data_ptr[idx] += (self.average_a * val * (-c / wscale))
                        if num_iter > 1:
            self.average_b /= (1.0 - mu)
        self.average_a += mu * self.average_b * wscale
    cdef double dot(self, double *x_data_ptr, int *x_ind_ptr,
                    int xnnz) nogil:
                cdef int j
        cdef int idx
        cdef double innerprod = 0.0
        cdef double* w_data_ptr = self.w_data_ptr
        for j in range(xnnz):
            idx = x_ind_ptr[j]
            innerprod += w_data_ptr[idx] * x_data_ptr[j]
        innerprod *= self.wscale
        return innerprod
    cdef void scale(self, double c) nogil:
                self.wscale *= c
        self.sq_norm *= (c * c)
        if self.wscale < 1e-9:
            self.reset_wscale()
    cdef void reset_wscale(self) nogil:
                if self.aw is not None:
            daxpy(<int>self.aw.shape[0], self.average_a,
                  <double *>self.w.data, 1, <double *>self.aw.data, 1)
            dscal(<int>self.aw.shape[0], 1.0 / self.average_b,
                  <double *>self.aw.data, 1)
            self.average_a = 0.0
            self.average_b = 1.0
        dscal(<int>self.w.shape[0], self.wscale, <double *>self.w.data, 1)
        self.wscale = 1.0
    cdef double norm(self) nogil:
                return sqrt(self.sq_norm)
from libc.math cimport log, exp
import numpy as np
cimport numpy as np
ctypedef np.float64_t DTYPE_t

cdef DTYPE_t _inner_log_logistic_sigmoid(DTYPE_t x):
        if x > 0:
        return -log(1 + exp(-x))
    else:
        return x - log(1 + exp(x))

def _log_logistic_sigmoid(int n_samples, int n_features, 
                           np.ndarray[DTYPE_t, ndim=2] X,
                           np.ndarray[DTYPE_t, ndim=2] out):
    for i in range(n_samples):
        for j in range(n_features):
            out[i, j] = _inner_log_logistic_sigmoid(X[i, j])
    return out
from __future__ import division
cimport cython
import numpy as np
cimport numpy as np
np.import_array()
from sklearn.utils import check_random_state

cpdef _sample_without_replacement_check_input(np.int_t n_population,
                                              np.int_t n_samples):
        if n_population < 0:
        raise ValueError('n_population should be greater than 0, got %s.'
                         % n_population)
    if n_samples > n_population:
        raise ValueError('n_population should be greater or equal than '
                         'n_samples, got n_samples > n_population (%s > %s)'
                         % (n_samples, n_population))

cpdef _sample_without_replacement_with_tracking_selection(
        np.int_t n_population,
        np.int_t n_samples,
        random_state=None):
        _sample_without_replacement_check_input(n_population, n_samples)
    cdef np.int_t i
    cdef np.int_t j
    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ),
                                                     dtype=np.int)
    rng = check_random_state(random_state)
    rng_randint = rng.randint
            cdef set selected = set()
    for i in range(n_samples):
        j = rng_randint(n_population)
        while j in selected:
            j = rng_randint(n_population)
        selected.add(j)
        out[i] = j
    return out

cpdef _sample_without_replacement_with_pool(np.int_t n_population,
                                            np.int_t n_samples,
                                            random_state=None):
        _sample_without_replacement_check_input(n_population, n_samples)
    cdef np.int_t i
    cdef np.int_t j
    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ),
                                                     dtype=np.int)
    cdef np.ndarray[np.int_t, ndim=1] pool = np.empty((n_population, ),
                                                      dtype=np.int)
    rng = check_random_state(random_state)
    rng_randint = rng.randint
        for i in xrange(n_population):
        pool[i] = i
            for i in xrange(n_samples):
        j = rng_randint(n_population - i)          out[i] = pool[j]
        pool[j] = pool[n_population - i - 1]                                                
    return out

cpdef _sample_without_replacement_with_reservoir_sampling(
    np.int_t n_population,
    np.int_t n_samples,
    random_state=None):
        _sample_without_replacement_check_input(n_population, n_samples)
    cdef np.int_t i
    cdef np.int_t j
    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ),
                                                     dtype=np.int)
    rng = check_random_state(random_state)
    rng_randint = rng.randint
                    for i in range(n_samples):
        out[i] = i
    for i from n_samples <= i < n_population:
        j = rng_randint(0, i + 1)
        if j < n_samples:
            out[j] = i
    return out

cpdef sample_without_replacement(np.int_t n_population,
                                 np.int_t n_samples,
                                 method="auto",
                                 random_state=None):
        _sample_without_replacement_check_input(n_population, n_samples)
    all_methods = ("auto", "tracking_selection", "reservoir_sampling", "pool")
    ratio = n_samples / n_population if n_population != 0.0 else 1.0
        if method == "auto" and ratio > 0.01 and ratio < 0.99:
        rng = check_random_state(random_state)
        return rng.permutation(n_population)[:n_samples]
    if method == "auto" or method == "tracking_selection":
                        
                if ratio < 0.2:
            return _sample_without_replacement_with_tracking_selection(
                n_population, n_samples, random_state)
        else:
            return _sample_without_replacement_with_reservoir_sampling(
                n_population, n_samples, random_state)
    elif method == "reservoir_sampling":
        return _sample_without_replacement_with_reservoir_sampling(
            n_population, n_samples, random_state)
    elif method == "pool":
        return _sample_without_replacement_with_pool(n_population, n_samples,
                                                     random_state)
    else:
        raise ValueError('Expected a method name in %s, got %s. '
                         % (all_methods, method))
from __future__ import division, print_function, absolute_import
__all__ = ['lsqr']
import numpy as np
from math import sqrt
from scipy.sparse.linalg.interface import aslinearoperator
eps = np.finfo(np.float64).eps

def _sym_ortho(a, b):
        if b == 0:
        return np.sign(a), 0, abs(a)
    elif a == 0:
        return 0, np.sign(b), abs(b)
    elif abs(b) > abs(a):
        tau = a / b
        s = np.sign(b) / sqrt(1 + tau * tau)
        c = s * tau
        r = b / s
    else:
        tau = b / a
        c = np.sign(a) / sqrt(1+tau*tau)
        s = c * tau
        r = a / c
    return c, s, r

def lsqr(A, b, damp=0.0, atol=1e-8, btol=1e-8, conlim=1e8,
         iter_lim=None, show=False, calc_var=False):
        A = aslinearoperator(A)
    if len(b.shape) > 1:
        b = b.squeeze()
    m, n = A.shape
    if iter_lim is None:
        iter_lim = 2 * n
    var = np.zeros(n)
    msg = ('The exact solution is  x = 0                              ',
         'Ax - b is small enough, given atol, btol                  ',
         'The least-squares solution is good enough, given atol     ',
         'The estimate of cond(Abar) has exceeded conlim            ',
         'Ax - b is small enough for this machine                   ',
         'The least-squares solution is good enough for this machine',
         'Cond(Abar) seems to be too large for this machine         ',
         'The iteration limit has been reached                      ')
    if show:
        print(' ')
        print('LSQR            Least-squares solution of  Ax = b')
        str1 = 'The matrix A has %8g rows  and %8g cols' % (m, n)
        str2 = 'damp = %20.14e   calc_var = %8g' % (damp, calc_var)
        str3 = 'atol = %8.2e                 conlim = %8.2e' % (atol, conlim)
        str4 = 'btol = %8.2e               iter_lim = %8g' % (btol, iter_lim)
        print(str1)
        print(str2)
        print(str3)
        print(str4)
    itn = 0
    istop = 0
    nstop = 0
    ctol = 0
    if conlim > 0:
        ctol = 1/conlim
    anorm = 0
    acond = 0
    dampsq = damp**2
    ddnorm = 0
    res2 = 0
    xnorm = 0
    xxnorm = 0
    z = 0
    cs2 = -1
    sn2 = 0
        __xm = np.zeros(m)      __xn = np.zeros(n)      v = np.zeros(n)
    u = b
    x = np.zeros(n)
    alfa = 0
    beta = np.linalg.norm(u)
    w = np.zeros(n)
    if beta > 0:
        u = (1/beta) * u
        v = A.rmatvec(u)
        alfa = np.linalg.norm(v)
    if alfa > 0:
        v = (1/alfa) * v
        w = v.copy()
    rhobar = alfa
    phibar = beta
    bnorm = beta
    rnorm = beta
    r1norm = rnorm
    r2norm = rnorm
            arnorm = alfa * beta
    if arnorm == 0:
        print(msg[0])
        return x, istop, itn, r1norm, r2norm, anorm, acond, arnorm, xnorm, var
    head1 = '   Itn      x[0]       r1norm     r2norm '
    head2 = ' Compatible    LS      Norm A   Cond A'
    if show:
        print(' ')
        print(head1, head2)
        test1 = 1
        test2 = alfa / beta
        str1 = '%6g %12.5e' % (itn, x[0])
        str2 = ' %10.3e %10.3e' % (r1norm, r2norm)
        str3 = '  %8.1e %8.1e' % (test1, test2)
        print(str1, str2, str3)
        while itn < iter_lim:
        itn = itn + 1
                u = A.matvec(v) - alfa * u
        beta = np.linalg.norm(u)
        if beta > 0:
            u = (1/beta) * u
            anorm = sqrt(anorm**2 + alfa**2 + beta**2 + damp**2)
            v = A.rmatvec(u) - beta * v
            alfa = np.linalg.norm(v)
            if alfa > 0:
                v = (1 / alfa) * v
                        rhobar1 = sqrt(rhobar**2 + damp**2)
        cs1 = rhobar / rhobar1
        sn1 = damp / rhobar1
        psi = sn1 * phibar
        phibar = cs1 * phibar
                        cs, sn, rho = _sym_ortho(rhobar1, beta)
        theta = sn * alfa
        rhobar = -cs * alfa
        phi = cs * phibar
        phibar = sn * phibar
        tau = sn * phi
                t1 = phi / rho
        t2 = -theta / rho
        dk = (1 / rho) * w
        x = x + t1 * w
        w = v + t2 * w
        ddnorm = ddnorm + np.linalg.norm(dk)**2
        if calc_var:
            var = var + dk**2
                                delta = sn2 * rho
        gambar = -cs2 * rho
        rhs = phi - delta * z
        zbar = rhs / gambar
        xnorm = sqrt(xxnorm + zbar**2)
        gamma = sqrt(gambar**2 + theta**2)
        cs2 = gambar / gamma
        sn2 = theta / gamma
        z = rhs / gamma
        xxnorm = xxnorm + z**2
                                acond = anorm * sqrt(ddnorm)
        res1 = phibar**2
        res2 = res2 + psi**2
        rnorm = sqrt(res1 + res2)
        arnorm = alfa * abs(tau)
                                                                r1sq = rnorm**2 - dampsq * xxnorm
        r1norm = sqrt(abs(r1sq))
        if r1sq < 0:
            r1norm = -r1norm
        r2norm = rnorm
                        test1 = rnorm / bnorm
        test2 = arnorm / (anorm * rnorm + eps)
        test3 = 1 / (acond + eps)
        t1 = test1 / (1 + anorm * xnorm / bnorm)
        rtol = btol + atol * anorm * xnorm / bnorm
                                                if itn >= iter_lim:
            istop = 7
        if 1 + test3 <= 1:
            istop = 6
        if 1 + test2 <= 1:
            istop = 5
        if 1 + t1 <= 1:
            istop = 4
                if test3 <= ctol:
            istop = 3
        if test2 <= atol:
            istop = 2
        if test1 <= rtol:
            istop = 1
                prnt = False
        if n <= 40:
            prnt = True
        if itn <= 10:
            prnt = True
        if itn >= iter_lim-10:
            prnt = True
                if test3 <= 2*ctol:
            prnt = True
        if test2 <= 10*atol:
            prnt = True
        if test1 <= 10*rtol:
            prnt = True
        if istop != 0:
            prnt = True
        if prnt:
            if show:
                str1 = '%6g %12.5e' % (itn, x[0])
                str2 = ' %10.3e %10.3e' % (r1norm, r2norm)
                str3 = '  %8.1e %8.1e' % (test1, test2)
                str4 = ' %8.1e %8.1e' % (anorm, acond)
                print(str1, str2, str3, str4)
        if istop != 0:
            break
            if show:
        print(' ')
        print('LSQR finished')
        print(msg[istop])
        print(' ')
        str1 = 'istop =%8g   r1norm =%8.1e' % (istop, r1norm)
        str2 = 'anorm =%8.1e   arnorm =%8.1e' % (anorm, arnorm)
        str3 = 'itn   =%8g   r2norm =%8.1e' % (itn, r2norm)
        str4 = 'acond =%8.1e   xnorm  =%8.1e' % (acond, xnorm)
        print(str1 + '   ' + str2)
        print(str3 + '   ' + str4)
        print(' ')
    return x, istop, itn, r1norm, r2norm, anorm, acond, arnorm, xnorm, var
from collections import Sequence
import numpy as np
from scipy.sparse import issparse
import warnings
from .murmurhash import murmurhash3_32
from .validation import (as_float_array,
                         assert_all_finite,
                         check_random_state, column_or_1d, check_array,
                         check_consistent_length, check_X_y, indexable,
                         check_symmetric)
from .class_weight import compute_class_weight, compute_sample_weight
from ..externals.joblib import cpu_count
from ..exceptions import DataConversionWarning
from .deprecation import deprecated

__all__ = ["murmurhash3_32", "as_float_array",
           "assert_all_finite", "check_array",
           "check_random_state",
           "compute_class_weight", "compute_sample_weight",
           "column_or_1d", "safe_indexing",
           "check_consistent_length", "check_X_y", 'indexable',
           "check_symmetric", "indices_to_mask", "deprecated"]

def safe_mask(X, mask):
        mask = np.asarray(mask)
    if np.issubdtype(mask.dtype, np.int):
        return mask
    if hasattr(X, "toarray"):
        ind = np.arange(mask.shape[0])
        mask = ind[mask]
    return mask

def axis0_safe_slice(X, mask, len_mask):
        if len_mask != 0:
        return X[safe_mask(X, mask), :]
    return np.zeros(shape=(0, X.shape[1]))

def safe_indexing(X, indices):
        if hasattr(X, "iloc"):
                try:
            return X.iloc[indices]
        except ValueError:
                                    warnings.warn("Copying input dataframe for slicing.",
                          DataConversionWarning)
            return X.copy().iloc[indices]
    elif hasattr(X, "shape"):
        if hasattr(X, 'take') and (hasattr(indices, 'dtype') and
                                   indices.dtype.kind == 'i'):
                        return X.take(indices, axis=0)
        else:
            return X[indices]
    else:
        return [X[idx] for idx in indices]

def resample(*arrays, **options):
        random_state = check_random_state(options.pop('random_state', None))
    replace = options.pop('replace', True)
    max_n_samples = options.pop('n_samples', None)
    if options:
        raise ValueError("Unexpected kw arguments: %r" % options.keys())
    if len(arrays) == 0:
        return None
    first = arrays[0]
    n_samples = first.shape[0] if hasattr(first, 'shape') else len(first)
    if max_n_samples is None:
        max_n_samples = n_samples
    elif (max_n_samples > n_samples) and (not replace):
        raise ValueError("Cannot sample %d out of arrays with dim %d "
                         "when replace is False" % (max_n_samples,
                                                    n_samples))
    check_consistent_length(*arrays)
    if replace:
        indices = random_state.randint(0, n_samples, size=(max_n_samples,))
    else:
        indices = np.arange(n_samples)
        random_state.shuffle(indices)
        indices = indices[:max_n_samples]
        arrays = [a.tocsr() if issparse(a) else a for a in arrays]
    resampled_arrays = [safe_indexing(a, indices) for a in arrays]
    if len(resampled_arrays) == 1:
                return resampled_arrays[0]
    else:
        return resampled_arrays

def shuffle(*arrays, **options):
        options['replace'] = False
    return resample(*arrays, **options)

def safe_sqr(X, copy=True):
        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False)
    if issparse(X):
        if copy:
            X = X.copy()
        X.data **= 2
    else:
        if copy:
            X = X ** 2
        else:
            X **= 2
    return X

def gen_batches(n, batch_size):
        start = 0
    for _ in range(int(n // batch_size)):
        end = start + batch_size
        yield slice(start, end)
        start = end
    if start < n:
        yield slice(start, n)

def gen_even_slices(n, n_packs, n_samples=None):
        start = 0
    if n_packs < 1:
        raise ValueError("gen_even_slices got n_packs=%s, must be >=1"
                         % n_packs)
    for pack_num in range(n_packs):
        this_n = n // n_packs
        if pack_num < n % n_packs:
            this_n += 1
        if this_n > 0:
            end = start + this_n
            if n_samples is not None:
                end = min(n_samples, end)
            yield slice(start, end, None)
            start = end

def _get_n_jobs(n_jobs):
        if n_jobs < 0:
        return max(cpu_count() + 1 + n_jobs, 1)
    elif n_jobs == 0:
        raise ValueError('Parameter n_jobs == 0 has no meaning.')
    else:
        return n_jobs

def tosequence(x):
        if isinstance(x, np.ndarray):
        return np.asarray(x)
    elif isinstance(x, Sequence):
        return x
    else:
        return list(x)

def indices_to_mask(indices, mask_length):
        if mask_length <= np.max(indices):
        raise ValueError("mask_length must be greater than max(indices)")
    mask = np.zeros(mask_length, dtype=np.bool)
    mask[indices] = True
    return mask
import numpy

def configuration(parent_package='', top_path=None):
    from numpy.distutils.misc_util import Configuration
    config = Configuration('sparsetools', parent_package, top_path)
    config.add_extension('_traversal',
                         sources=['_traversal.pyx'],
                         include_dirs=[numpy.get_include()])
    config.add_extension('_graph_tools',
                         sources=['_graph_tools.pyx'],
                         include_dirs=[numpy.get_include()])
    config.add_subpackage('tests')
    return config
if __name__ == '__main__':
    from numpy.distutils.core import setup
    setup(**configuration(top_path='').todict())

import numpy as np
cimport numpy as np
from scipy.sparse import csr_matrix, isspmatrix,\
    isspmatrix_csr, isspmatrix_csc, isspmatrix_lil
DTYPE = np.float64
ctypedef np.float64_t DTYPE_t
ITYPE = np.int32
ctypedef np.int32_t ITYPE_t
cdef DTYPE_t DTYPE_EPS = 1E-15
cdef ITYPE_t NULL_IDX = -9999
def csgraph_from_masked(graph):
            graph = np.ma.asarray(graph)
    if graph.ndim != 2:
        raise ValueError("graph should have two dimensions")
    N = graph.shape[0]
    if graph.shape[1] != N:
        raise ValueError("graph should be a square array")
        if np.ma.is_masked(graph):
        data = graph.compressed()
        mask = ~graph.mask
    else:
        data = graph.data
        mask = np.ones(graph.shape, dtype='bool')
    data = np.asarray(data, dtype=DTYPE, order='c')
    idx_grid = np.empty((N, N), dtype=ITYPE)
    idx_grid[:] = np.arange(N, dtype=ITYPE)
    indices = np.asarray(idx_grid[mask], dtype=ITYPE, order='c')
    indptr = np.zeros(N + 1, dtype=ITYPE)
    indptr[1:] = mask.sum(1).cumsum()
    return csr_matrix((data, indices, indptr), (N, N))

def csgraph_masked_from_dense(graph,
                              null_value=0,
                              nan_null=True,
                              infinity_null=True,
                              copy=True):
        graph = np.array(graph, copy=copy)
        if graph.ndim != 2:
        raise ValueError("graph should have two dimensions")
    N = graph.shape[0]
    if graph.shape[1] != N:
        raise ValueError("graph should be a square array")
        if null_value is not None:
        null_value = DTYPE(null_value)        
        if np.isnan(null_value):
            nan_null = True
            null_value = None
        elif np.isinf(null_value):
            infinity_null = True
            null_value = None
    
        if null_value is None:
        mask = np.zeros(graph.shape, dtype='bool')
        graph = np.ma.masked_array(graph, mask, copy=False)
    else:
        graph = np.ma.masked_values(graph, null_value, copy=False)
    if infinity_null:
        graph.mask |= np.isinf(graph)
    if nan_null:
        graph.mask |= np.isnan(graph)
    return graph

def csgraph_from_dense(graph,
                       null_value=0,
                       nan_null=True,
                       infinity_null=True):
        return csgraph_from_masked(csgraph_masked_from_dense(graph,
                                                         null_value,
                                                         nan_null,
                                                         infinity_null))

def csgraph_to_dense(csgraph, null_value=0):
                if isspmatrix_csc(csgraph) or isspmatrix_lil(csgraph):
        csgraph = csgraph.tocsr()
    elif not isspmatrix_csr(csgraph):
        raise ValueError("csgraph must be lil, csr, or csc format")
    N = csgraph.shape[0]
    if csgraph.shape[1] != N:
        raise ValueError('csgraph should be a square matrix')
        data = np.asarray(csgraph.data, dtype=DTYPE, order='C')
    indices = np.asarray(csgraph.indices, dtype=ITYPE, order='C')
    indptr = np.asarray(csgraph.indptr, dtype=ITYPE, order='C')
        graph = np.empty(csgraph.shape, dtype=DTYPE)
    graph.fill(np.inf)
    _populate_graph(data, indices, indptr, graph, null_value)
    return graph

def csgraph_to_masked(csgraph):
        return np.ma.masked_invalid(csgraph_to_dense(csgraph, np.nan))

cdef void _populate_graph(np.ndarray[DTYPE_t, ndim=1, mode='c'] data,
                          np.ndarray[ITYPE_t, ndim=1, mode='c'] indices,
                          np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr,
                          np.ndarray[DTYPE_t, ndim=2, mode='c'] graph,
                          DTYPE_t null_value):
                cdef unsigned int N = graph.shape[0]
    cdef np.ndarray null_flag = np.ones((N, N), dtype=bool, order='C')
    cdef np.npy_bool* null_ptr = <np.npy_bool*> null_flag.data
    cdef unsigned int row, col, i
    for row from 0 <= row < N:
        for i from indptr[row] <= i < indptr[row + 1]:
            col = indices[i]
            null_ptr[col] = 0
                        if data[i] < graph[row, col]:
                graph[row, col] = data[i]
        null_ptr += N
    graph[null_flag] = null_value

def reconstruct_path(csgraph, predecessors, directed=True):
        from _validation import validate_graph
    csgraph = validate_graph(csgraph, directed, dense_output=False)
    N = csgraph.shape[0]
    nnull = (predecessors < 0).sum()
    indices = np.argsort(predecessors)[nnull:].astype(ITYPE)
    pind = predecessors[indices]
    indptr = pind.searchsorted(np.arange(N + 1)).astype(ITYPE)
    if directed == True:
        data = csgraph[pind, indices]
    else:
        data1 = csgraph[pind, indices]
        data2 = csgraph[indices, pind]
        data1[data1 == 0] = np.inf
        data2[data2 == 0] = np.inf
        data = np.minimum(data1, data2)
    data = np.asarray(data).ravel()
    return csr_matrix((data, indices, indptr), shape=(N, N))

def construct_dist_matrix(graph,
                          predecessors,
                          directed=True,
                          null_value=np.inf):
        from _validation import validate_graph
    graph = validate_graph(graph, directed, dtype=DTYPE,
                           csr_output=False,
                           copy_if_dense=not directed)
    predecessors = np.asarray(predecessors)
    if predecessors.shape != graph.shape:
        raise ValueError("graph and predecessors must have the same shape")
    dist_matrix = np.zeros(graph.shape, dtype=DTYPE)
    _construct_dist_matrix(graph, predecessors, dist_matrix,
                           directed, null_value)
    
    return dist_matrix

cdef void _construct_dist_matrix(np.ndarray[DTYPE_t, ndim=2] graph,
                                 np.ndarray[ITYPE_t, ndim=2] pred,
                                 np.ndarray[DTYPE_t, ndim=2] dist,
                                 int directed,
                                 DTYPE_t null_value):
                global NULL_IDX
    cdef int i, j, k1, k2, N, null_path
    N = graph.shape[0]
            if not directed:
        graph[graph == 0] = np.inf
        for i from 0 <= i < N:
            for j from i + 1 <= j < N:
                if graph[j, i] <= graph[i, j]:
                    graph[i, j] = graph[j, i]
                else:
                    graph[j, i] = graph[i, j]
    
    for i from 0 <= i < N:
        for j from 0 <= j < N:
            null_path = True
            k2 = j
            while k2 != i:
                k1 = pred[i, k2]
                if k1 == NULL_IDX:
                    break
                dist[i, j] += graph[k1, k2]
                null_path = False
                k2 = k1
            if null_path and i != j:
                dist[i, j] = null_value
from __future__ import division, print_function, absolute_import
import numpy as np
from scipy.sparse import csr_matrix, isspmatrix, isspmatrix_csc
from ._graph_tools import csgraph_to_dense, csgraph_from_dense,\
    csgraph_masked_from_dense, csgraph_from_masked
DTYPE = np.float64

def validate_graph(csgraph, directed, dtype=DTYPE,
                   csr_output=True, dense_output=True,
                   copy_if_dense=False, copy_if_sparse=False,
                   null_value_in=0, null_value_out=np.inf,
                   infinity_null=True, nan_null=True):
        if not (csr_output or dense_output):
        raise ValueError("Internal: dense or csr output must be true")
            if (not directed) and isspmatrix_csc(csgraph):
        csgraph = csgraph.T
    if isspmatrix(csgraph):
        if csr_output:
            csgraph = csr_matrix(csgraph, dtype=DTYPE, copy=copy_if_sparse)
        else:
            csgraph = csgraph_to_dense(csgraph, null_value=null_value_out)
    elif np.ma.is_masked(csgraph):
        if dense_output:
            mask = csgraph.mask
            csgraph = np.array(csgraph.data, dtype=DTYPE, copy=copy_if_dense)
            csgraph[mask] = null_value_out
        else:
            csgraph = csgraph_from_masked(csgraph)
    else:
        if dense_output:
            csgraph = csgraph_masked_from_dense(csgraph,
                                                copy=copy_if_dense,
                                                null_value=null_value_in,
                                                nan_null=nan_null,
                                                infinity_null=infinity_null)
            mask = csgraph.mask
            csgraph = np.asarray(csgraph.data, dtype=DTYPE)
            csgraph[mask] = null_value_out
        else:
            csgraph = csgraph_from_dense(csgraph, null_value=null_value_in,
                                         infinity_null=infinity_null,
                                         nan_null=nan_null)
    if csgraph.ndim != 2:
        raise ValueError("compressed-sparse graph must be two dimensional")
    if csgraph.shape[0] != csgraph.shape[1]:
        raise ValueError("compressed-sparse graph must be shape (N, N)")
    return csgraph

import numpy as np
cimport numpy as np
from scipy.sparse import csr_matrix, isspmatrix, isspmatrix_csr, isspmatrix_csc
from ._graph_validation import validate_graph
from ._graph_tools import reconstruct_path
cimport cython
from libc cimport stdlib
DTYPE = np.float64
ctypedef np.float64_t DTYPE_t
ITYPE = np.int32
ctypedef np.int32_t ITYPE_t
cdef DTYPE_t DTYPE_EPS = 1E-15
cdef ITYPE_t NULL_IDX = -9999
def connected_components(csgraph, directed=True, connection='weak',
                         return_labels=True):
        if connection.lower() not in ['weak', 'strong']:
        raise ValueError("connection must be 'weak' or 'strong'")
    
        if connection.lower() == 'weak':
        directed = False
    csgraph = validate_graph(csgraph, directed,
                             dense_output=False)
    labels = np.empty(csgraph.shape[0], dtype=ITYPE)
    labels.fill(NULL_IDX)
    if directed:
        n_components = _connected_components_directed(csgraph.indices,
                                                      csgraph.indptr,
                                                      labels)
    else:
        csgraph_T = csgraph.T.tocsr()
        n_components = _connected_components_undirected(csgraph.indices,
                                                        csgraph.indptr,
                                                        csgraph_T.indices,
                                                        csgraph_T.indptr,
                                                        labels)
    if return_labels:
        return n_components, labels
    else:
        return n_components
    
def breadth_first_tree(csgraph, i_start, directed=True):
        node_list, predecessors = breadth_first_order(csgraph, i_start,
                                                  directed, True)
    return reconstruct_path(csgraph, predecessors, directed)

def depth_first_tree(csgraph, i_start, directed=True):
        node_list, predecessors = depth_first_order(csgraph, i_start,
                                                directed, True)
    return reconstruct_path(csgraph, predecessors, directed)

def breadth_first_order(csgraph, i_start,
                        directed=True, return_predecessors=True):
        global NULL_IDX
    csgraph = validate_graph(csgraph, directed, dense_output=False)
    cdef int N = csgraph.shape[0]
    cdef np.ndarray node_list = np.empty(N, dtype=ITYPE)
    cdef np.ndarray predecessors = np.empty(N, dtype=ITYPE)
    node_list.fill(NULL_IDX)
    predecessors.fill(NULL_IDX)
    if directed:
        length = _breadth_first_directed(i_start,
                                csgraph.indices, csgraph.indptr,
                                node_list, predecessors)
    else:
        csgraph_T = csgraph.T.tocsr()
        length = _breadth_first_undirected(i_start,
                                           csgraph.indices, csgraph.indptr,
                                           csgraph_T.indices, csgraph_T.indptr,
                                           node_list, predecessors)
    if return_predecessors:
        return node_list[:length], predecessors
    else:
        return node_list[:length]
    
cdef unsigned int _breadth_first_directed(
                           unsigned int head_node,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] node_list,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] predecessors):
                                        global NULL_IDX
    cdef unsigned int i, pnode, cnode
    cdef unsigned int i_nl, i_nl_end
    cdef unsigned int N = node_list.shape[0]
    node_list[0] = head_node
    i_nl = 0
    i_nl_end = 1
    while i_nl < i_nl_end:
        pnode = node_list[i_nl]
        for i from indptr[pnode] <= i < indptr[pnode + 1]:
            cnode = indices[i]
            if (cnode == head_node):
                continue
            elif (predecessors[cnode] == NULL_IDX):
                node_list[i_nl_end] = cnode
                predecessors[cnode] = pnode
                i_nl_end += 1
        i_nl += 1
    return i_nl
    
cdef unsigned int _breadth_first_undirected(
                           unsigned int head_node,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices1,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr1,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices2,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr2,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] node_list,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] predecessors):
                                                global NULL_IDX
    cdef unsigned int i, pnode, cnode
    cdef unsigned int i_nl, i_nl_end
    cdef unsigned int N = node_list.shape[0]
    node_list[0] = head_node
    i_nl = 0
    i_nl_end = 1
    while i_nl < i_nl_end:
        pnode = node_list[i_nl]
        for i from indptr1[pnode] <= i < indptr1[pnode + 1]:
            cnode = indices1[i]
            if (cnode == head_node):
                continue
            elif (predecessors[cnode] == NULL_IDX):
                node_list[i_nl_end] = cnode
                predecessors[cnode] = pnode
                i_nl_end += 1
        for i from indptr2[pnode] <= i < indptr2[pnode + 1]:
            cnode = indices2[i]
            if (cnode == head_node):
                continue
            elif (predecessors[cnode] == NULL_IDX):
                node_list[i_nl_end] = cnode
                predecessors[cnode] = pnode
                i_nl_end += 1
        i_nl += 1
    return i_nl

def depth_first_order(csgraph, i_start,
                      directed=True, return_predecessors=True):
        global NULL_IDX
    csgraph = validate_graph(csgraph, directed, dense_output=False)
    cdef int N = csgraph.shape[0]
    node_list = np.empty(N, dtype=ITYPE)
    predecessors = np.empty(N, dtype=ITYPE)
    root_list = np.empty(N, dtype=ITYPE)
    flag = np.zeros(N, dtype=ITYPE)
    node_list.fill(NULL_IDX)
    predecessors.fill(NULL_IDX)
    root_list.fill(NULL_IDX)
    if directed:
        length = _depth_first_directed(i_start,
                              csgraph.indices, csgraph.indptr,
                              node_list, predecessors,
                              root_list, flag)
    else:
        csgraph_T = csgraph.T.tocsr()
        length = _depth_first_undirected(i_start,
                                         csgraph.indices, csgraph.indptr,
                                         csgraph_T.indices, csgraph_T.indptr,
                                         node_list, predecessors,
                                         root_list, flag)
    if return_predecessors:
        return node_list[:length], predecessors
    else:
        return node_list[:length]
    
cdef unsigned int _depth_first_directed(
                           unsigned int head_node,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] node_list,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] predecessors,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] root_list,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] flag):
    cdef unsigned int i, j, i_nl_end, cnode, pnode
    cdef unsigned int N = node_list.shape[0]
    cdef int no_children, i_root
    node_list[0] = head_node
    root_list[0] = head_node
    i_root = 0
    i_nl_end = 1
    flag[head_node] = 1
    while i_root >= 0:
        pnode = root_list[i_root]
        no_children = True
        for i from indptr[pnode] <= i < indptr[pnode + 1]:
            cnode = indices[i]
            if flag[cnode]:
                continue
            else:
                i_root += 1
                root_list[i_root] = cnode
                node_list[i_nl_end] = cnode
                predecessors[cnode] = pnode
                flag[cnode] = 1
                i_nl_end += 1
                no_children = False
                break
        if i_nl_end == N:
            break
        
        if no_children:
            i_root -= 1
    
    return i_nl_end
    
cdef unsigned int _depth_first_undirected(
                           unsigned int head_node,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices1,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr1,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices2,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr2,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] node_list,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] predecessors,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] root_list,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] flag):
    cdef unsigned int i, j, i_nl_end, cnode, pnode
    cdef unsigned int N = node_list.shape[0]
    cdef int no_children, i_root
    node_list[0] = head_node
    root_list[0] = head_node
    i_root = 0
    i_nl_end = 1
    flag[head_node] = 1
    while i_root >= 0:
        pnode = root_list[i_root]
        no_children = True
        for i from indptr1[pnode] <= i < indptr1[pnode + 1]:
            cnode = indices1[i]
            if flag[cnode]:
                continue
            else:
                i_root += 1
                root_list[i_root] = cnode
                node_list[i_nl_end] = cnode
                predecessors[cnode] = pnode
                flag[cnode] = 1
                i_nl_end += 1
                no_children = False
                break
        if no_children:
            for i from indptr2[pnode] <= i < indptr2[pnode + 1]:
                cnode = indices2[i]
                if flag[cnode]:
                    continue
                else:
                    i_root += 1
                    root_list[i_root] = cnode
                    node_list[i_nl_end] = cnode
                    predecessors[cnode] = pnode
                    flag[cnode] = 1
                    i_nl_end += 1
                    no_children = False
                    break
        if i_nl_end == N:
            break
        
        if no_children:
            i_root -= 1
    
    return i_nl_end

cdef int _connected_components_directed(
                                 np.ndarray[ITYPE_t, ndim=1, mode='c'] indices,
                                 np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr,
                                 np.ndarray[ITYPE_t, ndim=1, mode='c'] labels):
        cdef int v, w, index, low_v, low_w, label, j
    cdef int SS_head, root, stack_head, f, b
    cdef int VOID = -1
    cdef int END = -2
    cdef int N = labels.shape[0]
    cdef np.ndarray[ITYPE_t, ndim=1, mode="c"] SS, lowlinks, stack_f, stack_b
    lowlinks = labels
    SS = np.ndarray((N,), dtype=ITYPE)
    stack_b = np.ndarray((N,), dtype=ITYPE)
    stack_f = SS
        SS.fill(VOID)
    SS_head = END
            lowlinks.fill(VOID)
                    stack_head = END
    stack_f.fill(VOID)
    stack_b.fill(VOID)
    index = 0
        label = N - 1
    for v in range(N):
        if lowlinks[v] == VOID:
                        stack_head = v
            stack_f[v] = END
            stack_b[v] = END
            while stack_head != END:
                v = stack_head
                if lowlinks[v] == VOID:
                    lowlinks[v] = index
                    index += 1
                                        for j from indptr[v] <= j < indptr[v+1]:
                        w = indices[j]
                        if lowlinks[w] == VOID:
                                                        if stack_f[w] != VOID:
                                                                f = stack_f[w]
                                b = stack_b[w]
                                if b != END:
                                    stack_f[b] = f
                                if f != END:
                                    stack_b[f] = b
                            stack_f[w] = stack_head
                            stack_b[w] = END
                            stack_b[stack_head] = w
                            stack_head = w
                else:
                                        stack_head = stack_f[v]
                    if stack_head >= 0:
                        stack_b[stack_head] = END
                    stack_f[v] = VOID
                    stack_b[v] = VOID
                    root = 1                     low_v = lowlinks[v]
                    for j from indptr[v] <= j < indptr[v+1]:
                        low_w = lowlinks[indices[j]]
                        if low_w < low_v:
                            low_v = low_w
                            root = 0                     lowlinks[v] = low_v
                    if root:                         index -= 1
                                                while SS_head != END and lowlinks[v] <= lowlinks[SS_head]:
                            w = SS_head                                    SS_head = SS[w]
                            SS[w] = VOID
                            labels[w] = label                              index -= 1                                 labels[v] = label                          label -= 1                             else:
                        SS[v] = SS_head                          SS_head = v
            labels *= -1
    labels += (N - 1)
    return (N - 1) - label
cdef int _connected_components_undirected(
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices1,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr1,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indices2,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] indptr2,
                           np.ndarray[ITYPE_t, ndim=1, mode='c'] labels):
    cdef int v, w, j, label, SS_head
    cdef int N = labels.shape[0]
    cdef int VOID = -1
    cdef int END = -2
    labels.fill(VOID)
    label = 0
            cdef np.ndarray[ITYPE_t, ndim=1, mode="c"] SS = labels
    SS_head = END
    for v in range(N):
        if labels[v] == VOID:
                        SS_head = v
            SS[v] = END
            while SS_head != END:
                                v = SS_head
                SS_head = SS[v]
                labels[v] = label
                                                for j from indptr1[v] <= j < indptr1[v+1]:
                    w = indices1[j]
                    if SS[w] == VOID:
                        SS[w] = SS_head
                        SS_head = w
                for j from indptr2[v] <= j < indptr2[v+1]:
                    w = indices2[j]
                    if SS[w] == VOID:
                        SS[w] = SS_head
                        SS_head = w
            label += 1
    return label
from ._traversal import connected_components
__all__ = ["connected_components"]
from __future__ import division, print_function, absolute_import
import os
from distutils.version import LooseVersion
from numpy.distutils.system_info import get_info
DEFAULT_ROOT = 'sklearn'
CYTHON_MIN_VERSION = '0.23'

def get_blas_info():
    def atlas_not_found(blas_info_):
        def_macros = blas_info.get('define_macros', [])
        for x in def_macros:
            if x[0] == "NO_ATLAS_INFO":
                                                return True
            if x[0] == "ATLAS_INFO":
                if "None" in x[1]:
                                        return True
        return False
    blas_info = get_info('blas_opt', 0)
    if (not blas_info) or atlas_not_found(blas_info):
        cblas_libs = ['cblas']
        blas_info.pop('libraries', None)
    else:
        cblas_libs = blas_info.pop('libraries', [])
    return cblas_libs, blas_info

def build_from_c_and_cpp_files(extensions):
        for extension in extensions:
        sources = []
        for sfile in extension.sources:
            path, ext = os.path.splitext(sfile)
            if ext in ('.pyx', '.py'):
                if extension.language == 'c++':
                    ext = '.cpp'
                else:
                    ext = '.c'
                sfile = path + ext
            sources.append(sfile)
        extension.sources = sources

def maybe_cythonize_extensions(top_path, config):
        is_release = os.path.exists(os.path.join(top_path, 'PKG-INFO'))
    if is_release:
        build_from_c_and_cpp_files(config.ext_modules)
    else:
        message = ('Please install cython with a version >= {0} in order '
                   'to build a scikit-learn development version.').format(
                       CYTHON_MIN_VERSION)
        try:
            import Cython
            if LooseVersion(Cython.__version__) < CYTHON_MIN_VERSION:
                message += ' Your version of Cython was {0}.'.format(
                    Cython.__version__)
                raise ValueError(message)
            from Cython.Build import cythonize
        except ImportError as exc:
            exc.args += (message,)
            raise
        config.ext_modules = cythonize(config.ext_modules)
import numpy

def configuration(parent_package='', top_path=None):
    from numpy.distutils.misc_util import Configuration
    config = Configuration('__check_build', parent_package, top_path)
    config.add_extension('_check_build',
                         sources=['_check_build.pyx'],
                         include_dirs=[numpy.get_include()])
    return config
if __name__ == '__main__':
    from numpy.distutils.core import setup
    setup(**configuration(top_path='').todict())
def check_build():
    returnimport os
INPLACE_MSG = 
STANDARD_MSG = 
def raise_build_error(e):
            local_dir = os.path.split(__file__)[0]
    msg = STANDARD_MSG
    if local_dir == "sklearn/__check_build":
                        msg = INPLACE_MSG
    dir_content = list()
    for i, filename in enumerate(os.listdir(local_dir)):
        if ((i + 1) % 3):
            dir_content.append(filename.ljust(26))
        else:
            dir_content.append(filename + '\n')
    raise ImportError(from __future__ import (absolute_import, unicode_literals, print_function)
print(__doc__)
__author__ = 'Alex J. Champandard'
import sys
import time
import logging
import argparse
import itertools
import numpy
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
import logging
logging.basicConfig(format="%(message)s", level=logging.WARNING, stream=sys.stdout)
from sknn.platform import gpu32
from sknn import mlp

PARAMETERS = {
    'activation': ['Rectifier', 'Tanh', 'Sigmoid'],
    'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2],
    'dropout': [None, 0.25, 0.5, 0.75],
    'iterations': [100, 200, 500, 1000],
    'output': ['Softmax', 'Linear', 'Gaussian'],
    'regularize': [None, 'L1', 'L2', 'dropout'],
    'rules': ['sgd', 'momentum', 'nesterov', 'adadelta', 'rmsprop'],
    'units': [16, 64, 128, 256],
}
parser = argparse.ArgumentParser()
parser.add_argument('-p','--params', nargs='+', help='Parameter to visualize.',
                    choices=PARAMETERS.keys(), required=True)
args = parser.parse_args()
params = []
for p in sorted(PARAMETERS):
    values = PARAMETERS[p]
        if p in args.params:
        params.append(values)
        else:
        params.append(values[:1])
names = []
classifiers = []
for (activation, alpha, dropout, iterations, output, regularize, rule, units) in itertools.product(*params):
    params = {}
    classifiers.append(mlp.Classifier(
        layers=[mlp.Layer(activation, units=units, **params), mlp.Layer(output)], random_state=1,
        n_iter=iterations, n_stable=iterations, regularize=regularize,
        dropout_rate=dropout, learning_rule=rule, learning_rate=alpha),)
    t = []
    for k, v in zip(sorted(PARAMETERS), [activation, alpha, dropout, iterations, output, regularize, rule, units]):
        if k in args.params:
            t.append(str(v))
    names.append(','.join(t))
seed = int(time.time())
X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=0, n_clusters_per_class=1)
rng = numpy.random.RandomState(seed+1)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)
datasets = [make_moons(noise=0.3, random_state=seed+2),
            make_circles(noise=0.2, factor=0.5, random_state=seed+3),
            linearly_separable]
GRID_RESOLUTION = .02
figure = plt.figure(figsize=(18, 9))
i = 1
for X, y in datasets:
        X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, GRID_RESOLUTION),
                            numpy.arange(y_min, y_max, GRID_RESOLUTION))
        cm = plt.cm.get_cmap("PRGn")
    cm_bright = ListedColormap(['    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1
        for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)
                
        Z = clf.predict_proba(numpy.c_[xx.ravel(), yy.ravel()])[:, 1]
                Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
                ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
                ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   alpha=0.6)
        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right', fontweight='bold')
        i += 1
        sys.stdout.write('.'); sys.stdout.flush()
    sys.stdout.write('\n')
figure.subplots_adjust(left=.02, right=.98)
plt.show()
from __future__ import (absolute_import, unicode_literals, print_function)
__all__ = ['AutoEncoder', 'Layer']
import time
import logging
import itertools
log = logging.getLogger('sknn')

import sklearn
from . import nn, backend

class Layer(nn.Layer):
    
    def __init__(self,
                 activation,
                 warning=None,
                 type='autoencoder',
                 name=None,
                 units=None,
                 cost='msre',
                 tied_weights=True,
                 corruption_level=0.5):
        assert warning is None, \
            "Specify layer parameters as keyword arguments, not positional arguments."
        if type not in ['denoising', 'autoencoder']:
            raise NotImplementedError("AutoEncoder layer type `%s` is not implemented." % type)
        if cost not in ['msre', 'mbce']:
            raise NotImplementedError("Error type '%s' is not implemented." % cost)
        if activation not in ['Sigmoid', 'Tanh']:
            raise NotImplementedError("Activation type '%s' is not implemented." % activation)
        self.activation = activation
        self.type = type
        self.name = name
        self.units = units
        self.cost = cost
        self.tied_weights = tied_weights
        self.corruption_level = corruption_level

class AutoEncoder(nn.NeuralNetwork, sklearn.base.TransformerMixin):
    def _setup(self):
        assert not self.is_initialized,\
            "This auto-encoder has already been initialized."
        backend.setup()
        self._backend = backend.AutoEncoderBackend(self)
    def fit(self, X):
                num_samples, data_size = X.shape[0], X.size
        log.info("Training on dataset of {:,} samples with {:,} total size.".format(num_samples, data_size))
        if self.n_iter:
            log.debug("  - Terminating loop after {} total iterations.".format(self.n_iter))
        if self.n_stable:
            log.debug("  - Early termination after {} stable iterations.".format(self.n_stable))
        if self.verbose:
            log.debug("\nEpoch    Validation Error        Time"
                      "\n-------------------------------------")
        
        self._backend._fit_impl(X)
        return self
    def transform(self, X):
                return self._backend._transform_impl(X)
    def transfer(self, nn):
        assert not nn.is_initialized,\
            "Target multi-layer perceptron has already been initialized."
        for a, l in zip(self.layers, nn.layers):
            assert a.activation == l.type,\
                "Mismatch in activation types in target MLP; expected `%s` but found `%s`."\
                % (a.activation, l.type)
            assert a.units == l.units,\
                "Different number of units in target MLP; expected `%i` but found `%i`."\
                % (a.units, l.units)
       
        self._backend._transfer_impl(nn)
from __future__ import (absolute_import, unicode_literals, print_function)
__all__ = ['Regressor', 'Classifier', 'Layer', 'Convolution']
import os
import sys
import math
import time
import logging
import itertools
import contextlib
log = logging.getLogger('sknn')

import numpy
import theano
import sklearn.base
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.cross_validation
from .nn import NeuralNetwork, Layer, Convolution, Native, ansi
from . import backend

class MultiLayerPerceptron(NeuralNetwork, sklearn.base.BaseEstimator):
        __doc__ = NeuralNetwork.__doc__
    def _setup(self):
        pass
    def _initialize(self, X, y=None, w=None):
        assert not self.is_initialized,\
            "This neural network has already been initialized."
        self._create_specs(X, y)
        backend.setup()
        self._backend = backend.MultiLayerPerceptronBackend(self)
        return self._backend._initialize_impl(X, y, w)
    def _check_layer(self, layer, required, optional=[]):
        required.extend(['name', 'type'])
        for r in required:
            if getattr(layer, r) is None: raise\
                ValueError("Layer type `%s` requires parameter `%s`."\
                           % (layer.type, r))
        optional.extend(['weight_decay', 'dropout', 'normalize', 'frozen'])
        for a in layer.__dict__:
            if a in required+optional:
                continue
            if getattr(layer, a) is not None:
                log.warning("Parameter `%s` is unused for layer type `%s`."\
                            % (a, layer.type))
    def _create_specs(self, X, y=None):
                if y is not None and self.layers[-1].units is None:
            self.layers[-1].units = y.shape[1]
        else:
            assert y is None or self.layers[-1].units == y.shape[1],\
                "Mismatch between dataset size and units in output layer."
                self.unit_counts = [numpy.product(X.shape[1:]) if self.is_convolution() else X.shape[1]]
        res = X.shape[1:3] if self.is_convolution() else None
        for l in self.layers:
            if isinstance(l, Convolution):
                assert l.kernel_shape is not None,\
                    "Layer `%s` requires parameter `kernel_shape` to be set." % (l.name,)
                if l.border_mode == 'valid':
                    res = (int((res[0] - l.kernel_shape[0]) / l.pool_shape[0]) + 1,
                           int((res[1] - l.kernel_shape[1]) / l.pool_shape[1]) + 1)
                if l.border_mode == 'full':
                    res = (int((res[0] + l.kernel_shape[0]) / l.pool_shape[0]) - 1,
                           int((res[1] + l.kernel_shape[1]) / l.pool_shape[1]) - 1)
                           
                if l.scale_factor != (1, 1):
                    res = (int(l.scale_factor[0] * res[0]), int(l.scale_factor[1] * res[1]))
 
                unit_count = numpy.prod(res) * l.channels
            else:
                unit_count = l.units
            self.unit_counts.append(unit_count)
    def __getstate__(self):
        d = self.__dict__.copy()
                        if self._backend is not None:
            d['weights'] = self._backend._mlp_to_array()
        d['valid_set'] = None
        for k in [k for k in d.keys() if k.startswith('_')]:
            del d[k]
        return d
    def __setstate__(self, d):
        self.__dict__.update(d)
                        self._create_logger()
        self._backend = None
    def _reshape(self, X, y=None):
        if y is not None and y.ndim == 1:
            y = y.reshape((y.shape[0], 1))
        if self.is_convolution() and X.ndim == 3:
            X = X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))
        if self.is_convolution() and X.ndim == 2:
            size = math.sqrt(X.shape[1])
            assert size.is_integer(),\
                "Input array is not in image shape, and could not assume a square."
            X = X.reshape((X.shape[0], int(size), int(size), 1))
        if not self.is_convolution() and X.ndim > 2:
            X = X.reshape((X.shape[0], numpy.product(X.shape[1:])))
        return X, y
    def _do_callback(self, event, variables):
        if self.callback is None:
            return
        del variables['self']
        if isinstance(self.callback, dict):
            function = self.callback.get(event, None)
            return function(**variables) if function else True
        else:
            return self.callback(event, **variables)
    def _train(self, X, y, w=None):
        assert self.n_iter or self.n_stable,\
            "Neither n_iter nor n_stable were specified; training would loop forever."
        best_train_error, best_valid_error = float("inf"), float("inf")
        best_params = [] 
        n_stable = 0
        self._do_callback('on_train_start', locals())
        for i in itertools.count(1):
            start_time = time.time()
            self._do_callback('on_epoch_start', locals())
            is_best_train = False
            avg_train_error = self._backend._train_impl(X, y, w)
            if avg_train_error is not None:
                if math.isnan(avg_train_error):
                    raise RuntimeError("Training diverged and returned NaN.")
                
                best_train_error = min(best_train_error, avg_train_error)
                is_best_train = bool(avg_train_error < best_train_error * (1.0 + self.f_stable))
            is_best_valid = False
            avg_valid_error = None
            if self.valid_set is not None:
                avg_valid_error = self._backend._valid_impl(*self.valid_set)
                if avg_valid_error is not None:
                    best_valid_error = min(best_valid_error, avg_valid_error)
                    is_best_valid = bool(avg_valid_error < best_valid_error * (1.0 + self.f_stable))
            finish_time = time.time()
            log.debug("\r{:>5}         {}{}{}            {}{}{}        {:>5.1f}s".format(
                      i,
                      ansi.BLUE if is_best_train else "",
                      "{0:>10.3e}".format(float(avg_train_error)) if (avg_train_error is not None) else "     N/A  ",
                      ansi.ENDC if is_best_train else "",
                      ansi.GREEN if is_best_valid else "",
                      "{:>10.3e}".format(float(avg_valid_error)) if (avg_valid_error is not None) else "     N/A  ",
                      ansi.ENDC if is_best_valid else "",
                      finish_time - start_time
                      ))
            if is_best_valid or (self.valid_set is None and is_best_train):
                best_params = self._backend._mlp_to_array()
                n_stable = 0
            else:
                n_stable += 1
            if self._do_callback('on_epoch_finish', locals()) == False:
                log.debug("")
                log.info("User defined callback terminated at %i iterations.", i)
                break
            if self.n_stable is not None and n_stable >= self.n_stable:
                log.debug("")
                log.info("Early termination condition fired at %i iterations.", i)
                break
            if self.n_iter is not None and i >= self.n_iter:
                log.debug("")
                log.info("Terminating after specified %i total iterations.", i)
                break
        self._do_callback('on_train_finish', locals())
        self._backend._array_to_mlp(best_params, self._backend.mlp)
    def _fit(self, X, y, w=None):
        assert X.shape[0] == y.shape[0],\
            "Expecting same number of input and output samples."
        data_shape = X.shape
        known_size = hasattr(X, 'size') and hasattr(y, 'size')
        data_size = '{:,}'.format(X.size+y.size) if known_size else 'N/A'
        X, y = self._reshape(X, y)
        if not self.is_initialized:
            X, y = self._initialize(X, y, w)
        log.info("Training on dataset of {:,} samples with {} total size.".format(data_shape[0], data_size))
        if data_shape[1:] != X.shape[1:]:
            log.warning("  - Reshaping input array from {} to {}.".format(data_shape, X.shape))
        if self.valid_set is not None:
            X_v, _ = self.valid_set
            log.debug("  - Train: {: <9,}  Valid: {: <4,}".format(X.shape[0], X_v.shape[0]))
        regularize = self.regularize or self.auto_enabled.get('regularize', None)
        if regularize is not None:
            comment = ", auto-enabled from layers" if 'regularize' in self.auto_enabled else "" 
            log.debug("  - Using `%s` for regularization%s." % (regularize, comment))
        normalize = self.normalize or self.auto_enabled.get('normalize', None)
        if normalize is not None:
            comment = ", auto-enabled from layers" if 'normalize' in self.auto_enabled else ""
            log.debug("  - Using `%s` normalization%s." % (normalize, comment))
        if self.n_iter is not None:
            log.debug("  - Terminating loop after {} total iterations.".format(self.n_iter))
        if self.n_stable is not None and self.n_stable < (self.n_iter or sys.maxsize):
            log.debug("  - Early termination after {} stable iterations.".format(self.n_stable))
        if self.verbose:
            log.debug("\nEpoch       Training Error       Validation Error       Time"
                      "\n------------------------------------------------------------")
        try:
            self._train(X, y, w)
        except RuntimeError as e:
            log.error("\n{}{}{}\n\n{}\n".format(
                ansi.RED,
                "A runtime exception was caught during training. This likely occurred due to\n"
                "a divergence of the SGD algorithm, and NaN floats were found by the backend.",
                ansi.ENDC,
                "Try setting the `learning_rate` 10x lower to resolve this, for example:\n"
                "    learning_rate=%f" % (self.learning_rate * 0.1)))
            raise e
        return self
    def _predict(self, X):
        X, _ = self._reshape(X)
        if self._backend is None:
            assert self.layers[-1].units is not None,\
                "You must specify the number of units to predict without fitting."
            if self.weights is None:
                log.warning("WARNING: Computing estimates with an untrained network.")
            self._initialize(X)
        return self._backend._predict_impl(X)
    def get_params(self, deep=True):
        result = super(MultiLayerPerceptron, self).get_params(deep=True)
        for l in self.layers:
            result[l.name] = l
        return result

class Regressor(MultiLayerPerceptron, sklearn.base.RegressorMixin):
        
    def fit(self, X, y, w=None):
        
        if self.valid_set is not None:
            self.valid_set = self._reshape(*self.valid_set)
        return super(Regressor, self)._fit(X, y, w)
    def predict(self, X):
                return super(Regressor, self)._predict(X)
    @property
    def is_classifier(self):
        return False

class Classifier(MultiLayerPerceptron, sklearn.base.ClassifierMixin):
    
    def _setup(self):
        super(Classifier, self)._setup()
        self.label_binarizers = []
    @contextlib.contextmanager
    def _patch_sklearn(self):
                                                                import sklearn.preprocessing.label as spl
        backup = spl.type_of_target 
        spl.type_of_target = lambda _: "multiclass"
        yield
        spl.type_of_target = backup
    def fit(self, X, y, w=None):
        
        assert X.shape[0] == y.shape[0],\
            "Expecting same number of input and output samples."
        if y.ndim == 1:
            y = y.reshape((y.shape[0], 1))
        if y.shape[1] == 1 and self.layers[-1].type != 'Softmax':
            log.warning('{}WARNING: Expecting `Softmax` type for the last layer '
                        'in classifier.{}\n'.format(ansi.YELLOW, ansi.ENDC))
        if y.shape[1] > 1 and self.layers[-1].type != 'Sigmoid':
            log.warning('{}WARNING: Expecting `Sigmoid` as last layer in '
                        'multi-output classifier.{}\n'.format(ansi.YELLOW, ansi.ENDC))
                LB = sklearn.preprocessing.LabelBinarizer
        self.label_binarizers = [LB() for _ in range(y.shape[1])]
        with self._patch_sklearn():
            ys = [lb.fit_transform(y[:,i]) for i, lb in enumerate(self.label_binarizers)]
        yp = numpy.concatenate(ys, axis=1).astype(theano.config.floatX)
                if self.valid_set is not None:
            X_v, y_v = self.valid_set
            if y_v.ndim == 1:
                y_v = y_v.reshape((y_v.shape[0], 1))
            with self._patch_sklearn():
                ys = [lb.transform(y_v[:,i]) for i, lb in enumerate(self.label_binarizers)]
            y_vp = numpy.concatenate(ys, axis=1)
            self.valid_set = (X_v, y_vp)
                return super(Classifier, self)._fit(X, yp, w)
    def partial_fit(self, X, y, classes=None):
        if y.ndim == 1:
            y = y.reshape((y.shape[0], 1))
        if classes is not None:
            if isinstance(classes[0], int):
                classes = [classes]
            LB = sklearn.preprocessing.LabelBinarizer
            self.label_binarizers = [LB() for _ in range(y.shape[1])]
            for lb, cls in zip(self.label_binarizers, classes):
                lb.fit(cls)
        return self.fit(X, y)
    def predict_proba(self, X, collapse=True):
                proba = super(Classifier, self)._predict(X)
        index, yp = 0, []
        for lb in self.label_binarizers:
            sz = len(lb.classes_)
            p = proba[:,index:index+sz]
            yp.append(p / p.sum(1, keepdims=True))
            index += sz
        return yp[0] if (len(yp) == 1 and collapse) else yp
    def predict(self, X):
                assert self.label_binarizers != [],\
            "Can't predict without fitting: output classes are unknown."
        yp = self.predict_proba(X, collapse=False)
        ys = []
        index = 0
        for lb, p in zip(self.label_binarizers, yp):
             sz = len(lb.classes_)
             y = lb.inverse_transform(p, threshold=0.5)
             ys.append(y.reshape((-1, 1)))
             index += sz
        return numpy.concatenate(ys, axis=1)
    @property
    def is_classifier(self):
        return True
    
    @property
    def classes_(self):
            Specification for a layer to be passed to the neural network during construction.  This
    includes a variety of parameters to configure each layer based on its activation type.
    Parameters
    ----------
    type: str
        Select which activation function this layer should use, as a string.  Specifically,
        options are ``Rectifier``, ``Sigmoid``, ``Tanh``, and ``ExpLin`` for non-linear layers
        and ``Linear`` or ``Softmax`` for output layers.
    name: str, optional
        You optionally can specify a name for this layer, and its parameters
        will then be accessible to scikit-learn via a nested sub-object.  For example,
        if name is set to ``layer1``, then the parameter ``layer1__units`` from the network
        is bound to this layer's ``units`` variable.
        The name defaults to ``hiddenN`` where N is the integer index of that layer, and the
        final layer is always ``output`` without an index.
    units: int
        The number of units (also known as neurons) in this layer.  This applies to all
        layer types except for convolution.
    weight_decay: float, optional
        The coefficient for L1 or L2 regularization of the weights.  For example, a value of
        0.0001 is multiplied by the L1 or L2 weight decay equation.
    dropout: float, optional
        The ratio of inputs to drop out for this layer during training.  For example, 0.25
        means that 25% of the inputs will be excluded for each training sample, with the
        remaining inputs being renormalized accordingly.
    normalize: str, optional
        Enable normalization of this layer. Can be either `batch` for batch normalization
        or (soon) `weights` for weight normalization.  Default is no normalization.
    frozen: bool, optional
        Specify whether to freeze a layer's parameters so they are not adjusted during the
        training. This is useful when relying on pre-trained neural networks.
    warning: None
        You should use keyword arguments after `type` when initializing this object. If not,
        the code will raise an AssertionError.
            for k, v in params.items():
            if k not in self.__dict__:
                raise ValueError("Invalid parameter `%s` for layer `%s`." % (k, self.name))
            self.__dict__[k] = v
    def __eq__(self, other):
        return self.__dict__ == other.__dict__
    def __repr__(self):
        copy = self.__dict__.copy()
        del copy['type']
        params = ", ".join(["%s=%r" % (k, v) for k, v in copy.items() if v is not None])
        return "<sknn.nn.%s `%s`: %s>" % (self.__class__.__name__, self.type, params)

class Native(object):
    
    def __init__(self, constructor, *args, **keywords):
        for attr in ['name', 'units', 'frozen', 'weight_decay', 'normalize']:
            setattr(self, attr, keywords.pop(attr, None))
        self.type = constructor
        self.args = args
        self.keywords = keywords

class Convolution(Layer):
        def __init__(
            self,
            type,
            warning=None,
            name=None,
            channels=None,
            kernel_shape=None,
            kernel_stride=None,
            border_mode='valid',
            pool_shape=None,
            pool_type=None,
            scale_factor=None,
            weight_decay=None,
            dropout=None,
            normalize=None,
            frozen=False):
        assert warning is None,\
            "Specify layer parameters as keyword arguments, not positional arguments."
        if type not in ['Rectifier', 'Sigmoid', 'Tanh', 'Linear', 'ExpLin']:
            raise NotImplementedError("Convolution type `%s` is not implemented." % (type,))
        if border_mode not in ['valid', 'full', 'same']:
            raise NotImplementedError("Convolution border_mode `%s` is not implemented." % (border_mode,))
        super(Convolution, self).__init__(
                type,
                name=name,
                weight_decay=weight_decay,
                dropout=dropout,
                normalize=normalize,
                frozen=frozen)
        self.channels = channels
        self.kernel_shape = kernel_shape
        self.kernel_stride = kernel_stride or (1,1)
        self.border_mode = border_mode
        self.pool_shape = pool_shape or (1,1)
        self.pool_type = pool_type or ('max' if pool_shape else None)
        self.scale_factor = scale_factor or (1,1)

class NeuralNetwork(object):
    
    def __init__(
            self,
            layers,
            warning=None,
            parameters=None,
            random_state=None,
            learning_rule='sgd',
            learning_rate=0.01,
            learning_momentum=0.9,
            normalize=None,
            regularize=None,
            weight_decay=None,
            dropout_rate=None,
            batch_size=1,
            n_iter=None,
            n_stable=10,
            f_stable=0.001,
            valid_set=None,
            valid_size=0.0,
            loss_type=None,
            callback=None,
            debug=False,
            verbose=None,
            **params):
        assert warning is None,\
            "Specify network parameters as keyword arguments, not positional arguments."
        self.layers = []
        for i, layer in enumerate(layers):
            assert isinstance(layer, Layer) or isinstance(layer, Native),\
                "Specify each layer as an instance of a `sknn.mlp.Layer` object."
                        if layer.name is None:
                layer.name = ("hidden%i" % i) if i < len(layers)-1 else "output"
                        if layer.name in params:
                del params[layer.name]
            self.layers.append(layer)
                                assert len(params) == 0,\
            "The specified additional parameters are unknown: %s." % ','.join(params.keys())
                assert regularize in (None, 'L1', 'L2', 'dropout'),\
            "Unknown type of regularization specified: %s." % regularize
        assert loss_type in ('mse', 'mae', 'mcc', None),\
            "Unknown loss function type specified: %s." % loss_type
        self.weights = parameters
        self.random_state = random_state
        self.learning_rule = learning_rule
        self.learning_rate = learning_rate
        self.learning_momentum = learning_momentum
        self.normalize = normalize
        self.regularize = regularize or ('dropout' if dropout_rate else None)\
                                     or ('L2' if weight_decay else None)
        self.weight_decay = weight_decay
        self.dropout_rate = dropout_rate
        self.batch_size = batch_size
        self.n_iter = n_iter
        self.n_stable = n_stable
        self.f_stable = f_stable
        self.valid_set = valid_set
        self.valid_size = valid_size
        self.loss_type = loss_type
        self.debug = debug
        self.verbose = verbose
        self.callback = callback
        
        self.auto_enabled = {}
        self._backend = None
        self._create_logger()
        self._setup()
    def _setup(self):
        raise NotImplementedError("NeuralNetwork is an abstract class; "
                                  "use the mlp.Classifier or mlp.Regressor instead.")
    @property
    def is_initialized(self):
                return self._backend is not None and self._backend.is_initialized
    def is_convolution(self, input=None, output=False):
                check_output = output 
        check_input = False if check_output and input is None else True
        i = check_input and isinstance(self.layers[0], Convolution)
        o = check_output and isinstance(self.layers[-1], Convolution)
        return i or o
    @property
    def is_classifier(self):
                assert self._backend is not None,\
            "Backend was not initialized; could not retrieve network parameters."
        P = collections.namedtuple('Parameters', 'weights biases layer')
        return [P(w, b, s.name) for s, (w, b) in zip(self.layers, self._backend._mlp_to_array())]
    def set_parameters(self, storage):
        
                if self._backend is None:
            self.weights = storage
            return
        if isinstance(storage, dict):
            layers = [storage.get(l.name, None) for l in self.layers]
        else:
            layers = storage
        return self._backend._array_to_mlp(layers, self._backend.mlp)
from __future__ import (absolute_import, unicode_literals, print_function)
__author__ = 'alexjc, ssamot'
__version__ = '0.7'

import os
import re
import sys
import logging

class TheanoConfigurator(object):
    def __init__(self):
        self.configured = False
        self.log = logging.getLogger('sknn')
    def configure(self, flags):
        if self.configured is True:
            return
        self.configured = True
        
        if 'theano' in sys.modules:
            self.log.warning('Theano was already imported and cannot be reconfigured.')
            return
        os.environ.setdefault('THEANO_FLAGS', flags+',print_active_device=False')
        cuda = logging.getLogger('theano.sandbox.cuda')
        cuda.setLevel(logging.CRITICAL)
        import theano
        cuda.setLevel(logging.WARNING)
        try:
            import theano.sandbox.cuda as cd
            self.log.info('Using device gpu%i: %s', cd.active_device_number(), cd.active_device_name())
        except AttributeError:
            self.log.info('Using device cpu0, with %r.', theano.config.floatX)
    def __getattr__(self, name):
        flags = ''
        if name.endswith('32'):
            flags = ',floatX=float32'
        if name.endswith('64'):
            flags = ',floatX=float64'
        if name.startswith('cpu'):
            return self.configure('device=cpu'+flags)
        if name.startswith('gpu'):
            return self.configure('device=gpu'+flags)
        
        if name.startswith('thread'):
            try:
                count = int(re.sub('\D', '', name))
            except ValueError:
                import multiprocessing
                count = multiprocessing.cpu_count()
            os.environ.setdefault('THEANO_FLAGS', ','.join(['openmp=True', os.environ.get('THEANO_FLAGS', '')]))
            os.environ.setdefault('OMP_NUM_THREADS', str(count))
            return
        return getattr(sys.modules['sknn'], name)

sys.modules['sknn.platform'] = TheanoConfigurator()

try:
    import colorama; colorama.init(); del colorama
except ImportError:
    pass
from __future__ import (absolute_import, unicode_literals, print_function)

class BaseBackend(object):
    
    def __init__(self, spec):
        self.spec = spec
    
    def __getattr__(self, key):
        return getattr(self.spec, key)
    def __setattr__(self, key, value):
        if key != 'spec' and hasattr(self.spec, key):
            self.spec.__setattr__(key, value)
        else:
            super(BaseBackend, self).__setattr__(key, value)
from __future__ import (absolute_import, unicode_literals, print_function)

name = None
class MultiLayerPerceptronBackend(object):
    def __init__(self, _):
        raise NotImplementedError("No backend sub-module imported.")
class AutoEncoderBackend(object):
    def __init__(self, _):
        raise NotImplementedError("No backend sub-module imported.")

def setup():
    if name == None:
        from . import lasagne 
    assert name is not None, "No backend for module sknn was imported."
from __future__ import (absolute_import, division, unicode_literals, print_function)
__all__ = ['MultiLayerPerceptronBackend']
import os
import sys
import math
import time
import types
import logging
import itertools
log = logging.getLogger('sknn')

import numpy
import theano
import sklearn.base
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.cross_validation
import theano.tensor as T
import lasagne.layers
import lasagne.nonlinearities as nl
from ..base import BaseBackend
from ...nn import Layer, Convolution, Native, ansi

def explin(x):
    return x * (x>=0) + (x<0) * (T.exp(x) - 1)

class MultiLayerPerceptronBackend(BaseBackend):
    
    def __init__(self, spec):
        super(MultiLayerPerceptronBackend, self).__init__(spec)
        self.mlp = None
        self.f = None
        self.trainer = None
        self.validator = None
        self.regularizer = None
    def _create_mlp_trainer(self, params):
                layer_decay = {}
        if self.regularize in ('L1', 'L2') or any(l.weight_decay for l in self.layers):
            wd = self.weight_decay or 0.0001
            for l in self.layers:
                layer_decay[l.name] = l.weight_decay or wd
        assert len(layer_decay) == 0 or self.regularize in ('L1', 'L2', None)
        if len(layer_decay) > 0:
            if self.regularize is None:
                self.auto_enabled['regularize'] = 'L2'
            regularize = self.regularize or 'L2'
            penalty = getattr(lasagne.regularization, regularize.lower())
            apply_regularize = lasagne.regularization.apply_penalty
            self.regularizer = sum(layer_decay[s.name] * apply_regularize(l.get_params(regularizable=True), penalty)
                                   for s, l in zip(self.layers, self.mlp))
        if self.normalize is None and any([l.normalize != None for l in self.layers]):
            self.auto_enabled['normalize'] = 'batch'
        cost_functions = {'mse': 'squared_error', 'mcc': 'categorical_crossentropy'}
        loss_type = self.loss_type or ('mcc' if self.is_classifier else 'mse')
        assert loss_type in cost_functions,\
                    "Loss type `%s` not supported by Lasagne backend." % loss_type
        self.cost_function = getattr(lasagne.objectives, cost_functions[loss_type])
        cost_symbol = self.cost_function(self.trainer_output, self.data_output)
        cost_symbol = lasagne.objectives.aggregate(cost_symbol.T, self.data_mask, mode='mean')
        if self.regularizer is not None:
            cost_symbol = cost_symbol + self.regularizer
        return self._create_trainer_function(params, cost_symbol)
    def _create_trainer_function(self, params, cost):
        if self.learning_rule in ('sgd', 'adagrad', 'adadelta', 'rmsprop', 'adam'):
            lr = getattr(lasagne.updates, self.learning_rule)
            self._learning_rule = lr(cost, params, learning_rate=self.learning_rate)
        elif self.learning_rule in ('momentum', 'nesterov'):
            lasagne.updates.nesterov = lasagne.updates.nesterov_momentum
            lr = getattr(lasagne.updates, self.learning_rule)
            self._learning_rule = lr(cost, params, learning_rate=self.learning_rate, momentum=self.learning_momentum)
        else:
            raise NotImplementedError(
                "Learning rule type `%s` is not supported." % self.learning_rule)
        trainer = theano.function([self.data_input, self.data_output, self.data_mask], cost,
                                   updates=self._learning_rule,
                                   on_unused_input='ignore',
                                   allow_input_downcast=True)
        compare = self.cost_function(self.network_output, self.data_correct).mean()
        validator = theano.function([self.data_input, self.data_correct], compare,
                                    allow_input_downcast=True)
        return trainer, validator
    def _get_activation(self, l):
        nonlinearities = {'Rectifier': nl.rectify,
                          'Sigmoid': nl.sigmoid,
                          'Tanh': nl.tanh,
                          'Softmax': nl.softmax,
                          'Linear': nl.linear,
                          'ExpLin': explin}
        assert l.type in nonlinearities,\
            "Layer type `%s` is not supported for `%s`." % (l.type, l.name)
        return nonlinearities[l.type]
    def _create_convolution_layer(self, name, layer, network):
        self._check_layer(layer,
                          required=['channels', 'kernel_shape'],
                          optional=['units', 'kernel_stride', 'border_mode',
                                    'pool_shape', 'pool_type', 'scale_factor'])
        if layer.scale_factor != (1, 1):
            network = lasagne.layers.Upscale2DLayer(
                            network,
                            scale_factor=layer.scale_factor)
        network = lasagne.layers.Conv2DLayer(
                        network,
                        num_filters=layer.channels,
                        filter_size=layer.kernel_shape,
                        stride=layer.kernel_stride,
                        pad=layer.border_mode,
                        nonlinearity=self._get_activation(layer))
        normalize = layer.normalize or self.normalize
        if normalize == 'batch':
            network = lasagne.layers.batch_norm(network)
        if layer.pool_shape != (1, 1):
            network = lasagne.layers.Pool2DLayer(
                            network,
                            pool_size=layer.pool_shape,
                            stride=layer.pool_shape)
        return network
    def _create_native_layer(self, name, layer, network):
        if layer.units and 'num_units' not in layer.keywords:
            layer.keywords['num_units'] = layer.units
        return layer.type(network, *layer.args, **layer.keywords)
    def _create_layer(self, name, layer, network):
        if isinstance(layer, Native):
            return self._create_native_layer(name, layer, network)
        dropout = layer.dropout or self.dropout_rate
        if dropout is not None:
            network = lasagne.layers.dropout(network, dropout)
        if isinstance(layer, Convolution):
            return self._create_convolution_layer(name, layer, network)
        self._check_layer(layer, required=['units'])
        network = lasagne.layers.DenseLayer(network,
                                            num_units=layer.units,
                                            nonlinearity=self._get_activation(layer))
        normalize = layer.normalize or self.normalize
        if normalize == 'batch':
            network = lasagne.layers.batch_norm(network)
        return network
    def _create_mlp(self, X, w=None):
        self.data_input = T.tensor4('X') if self.is_convolution(input=True) else T.matrix('X')
        self.data_output = T.tensor4('y') if self.is_convolution(output=True) else T.matrix('y')
        self.data_mask = T.vector('m') if w is not None else T.scalar('m')
        self.data_correct = T.matrix('yp')
        lasagne.random.get_rng().seed(self.random_state)
        shape = list(X.shape)
        network = lasagne.layers.InputLayer([None]+shape[1:], self.data_input)
                self.mlp = []
        for i, layer in enumerate(self.layers):
            network = self._create_layer(layer.name, layer, network)
            network.name = layer.name
            self.mlp.append(network)
        log.info(
            "Initializing neural network with %i layers, %i inputs and %i outputs.",
            len(self.layers), self.unit_counts[0], self.layers[-1].units)
        for l, p, count in zip(self.layers, self.mlp, self.unit_counts[1:]):
            space = p.output_shape
            if isinstance(l, Convolution):
                log.debug("  - Convl: {}{: <10}{} Output: {}{: <10}{} Channels: {}{}{}".format(
                    ansi.BOLD, l.type, ansi.ENDC,
                    ansi.BOLD, repr(space[2:]), ansi.ENDC,
                    ansi.BOLD, space[1], ansi.ENDC))
                                                            elif isinstance(l, Native):
                log.debug("  - Nativ: {}{: <10}{} Output: {}{: <10}{} Channels: {}{}{}".format(
                    ansi.BOLD, l.type.__name__, ansi.ENDC,
                    ansi.BOLD, repr(space[2:]), ansi.ENDC,
                    ansi.BOLD, space[1], ansi.ENDC))
            else:
                log.debug("  - Dense: {}{: <10}{} Units:  {}{: <4}{}".format(
                    ansi.BOLD, l.type, ansi.ENDC, ansi.BOLD, l.units, ansi.ENDC))
                assert count == space[1],\
                    "Mismatch in the calculated number of dense layer outputs. {} != {}".format(count, space[1])
        if self.weights is not None:
            l  = min(len(self.weights), len(self.mlp))
            log.info("Reloading parameters for %i layer weights and biases." % (l,))
            self._array_to_mlp(self.weights, self.mlp)
            self.weights = None
        log.debug("")
        self.network_output = lasagne.layers.get_output(network, deterministic=True)
        self.trainer_output = lasagne.layers.get_output(network, deterministic=False)
        self.f = theano.function([self.data_input], self.network_output, allow_input_downcast=True)
    def _conv_transpose(self, arr):
        ok = arr.shape[-1] not in (1,3) and arr.shape[1] in (1,3)
        return arr if ok else numpy.transpose(arr, (0, 3, 1, 2))
    def _initialize_impl(self, X, y=None, w=None):
        if self.is_convolution(input=True):
            X = self._conv_transpose(X)
        if y is not None and self.is_convolution(output=True):
            y = self._conv_transpose(y)
        if self.mlp is None:
            self._create_mlp(X, w)
                if y is None:
            return
        if self.valid_size > 0.0:
            assert self.valid_set is None, "Can't specify valid_size and valid_set together."
            X, X_v, y, y_v = sklearn.cross_validation.train_test_split(
                                X, y,
                                test_size=self.valid_size,
                                random_state=self.random_state)
            self.valid_set = X_v, y_v
        if self.valid_set and self.is_convolution():
            X_v, y_v = self.valid_set
            if X_v.shape[-2:] != X.shape[-2:]:
                self.valid_set = numpy.transpose(X_v, (0, 3, 1, 2)), y_v
        params = []
        for spec, mlp_layer in zip(self.layers, self.mlp):
            if spec.frozen: continue
            params.extend(mlp_layer.get_params())
        self.trainer, self.validator = self._create_mlp_trainer(params)
        return X, y
    def _predict_impl(self, X):
        if self.is_convolution():
            X = numpy.transpose(X, (0, 3, 1, 2))
        y = None
        for Xb, _, _, idx  in self._iterate_data(self.batch_size, X, y, shuffle=False):
            yb = self.f(Xb)
            if y is None:
                if X.shape[0] <= self.batch_size:
                    y = yb
                    break
                else:
                    y = numpy.zeros(X.shape[:1] + yb.shape[1:], dtype=theano.config.floatX)
            y[idx] = yb
        return y
    def _iterate_data(self, batch_size, X, y=None, w=None, shuffle=False):
        def cast(array, indices):
            if array is None:
                return None
                        if type(array).__name__ == 'DataFrame':
                array = array.loc[indices]
            else:
                array = array[indices]
                                if hasattr(array, 'todense'):
                    array = array.todense()
            return array.astype(theano.config.floatX)
        total_size = X.shape[0]
        indices = numpy.arange(total_size)
        if shuffle:
            numpy.random.shuffle(indices)
        for index in range(0, total_size, batch_size):
            excerpt = indices[index:index + batch_size]
            Xb, yb, wb = cast(X, excerpt), cast(y, excerpt), cast(w, excerpt)
            yield Xb, yb, wb, excerpt
    def _print(self, text):
        if self.verbose:
            sys.stdout.write(text)
            sys.stdout.flush()
    def _batch_impl(self, X, y, w, processor, mode, output, shuffle):
        progress, batches = 0, X.shape[0] / self.batch_size
        loss, count = 0.0, 0
        for Xb, yb, wb, _ in self._iterate_data(self.batch_size, X, y, w, shuffle):
            self._do_callback('on_batch_start', locals())
            if mode == 'train':
                loss += processor(Xb, yb, wb if wb is not None else 1.0)
            else:
                loss += processor(Xb, yb)
            count += 1
            while count / batches > progress / 60:
                self._print(output)
                progress += 1
            self._do_callback('on_batch_finish', locals())
        self._print('\r')
        return loss / count
    def _train_impl(self, X, y, w=None):
        return self._batch_impl(X, y, w, self.trainer, mode='train', output='.', shuffle=True)
    def _valid_impl(self, X, y, w=None):
        return self._batch_impl(X, y, w, self.validator, mode='valid', output=' ', shuffle=False)
    @property
    def is_initialized(self):
                return not (self.f is None)
    def _mlp_get_layer_params(self, layer):
                assert layer.name is not None, "Expecting this layer to have a name."
        params = []
        while hasattr(layer, 'input_layer'):
            params.extend(layer.get_params())
            layer = layer.input_layer
            if layer.name is not None:
                break
        return params
    def _mlp_to_array(self):
        return [[p.get_value() for p in self._mlp_get_layer_params(l)] for l in self.mlp]
    def _array_to_mlp(self, array, nn):
        for layer, data in zip(nn, array):
            if data is None:
                continue
                                    string_types = getattr(types, 'StringTypes', tuple([str]))
            data = tuple([d for d in data if not isinstance(d, string_types)])
            params = self._mlp_get_layer_params(layer)
            assert len(data) == len(params),\
                            "Mismatch in data size for layer `%s`. %i != %i"\
                            % (layer.name, len(data), len(params))
            for p, d in zip(params, data):
                ps = tuple(p.shape.eval())
                assert ps == d.shape, "Layer parameter shape mismatch: %r != %r" % (ps, d.shape)
                p.set_value(d.astype(theano.config.floatX))
from __future__ import (absolute_import, unicode_literals, print_function)
from ... import backend
from .mlp import MultiLayerPerceptronBackend
backend.MultiLayerPerceptronBackend = MultiLayerPerceptronBackend
backend.name = 'lasagne'from sknn.backend import lasagne
try:
    from setuptools import setup
except ImportError:
    from distutils.core import setup
setup(name='scikit-optimize',
      version='0.3',
      description='Sequential model-based optimization toolbox.',
      long_description=('Scikit-Optimize, or `skopt`, is a simple and efficient'
                        ' library to minimize (very) expensive and noisy'
                        ' black-box functions. It implements several methods'
                        ' for sequential model-based optimization.'),
      url='https://scikit-optimize.github.io/',
      license='BSD',
      author='The scikit-optimize contributors',
      packages=['skopt', 'skopt.learning', 'skopt.optimizer', 'skopt.space',
                'skopt.learning.gaussian_process'],
      install_requires=["numpy", "scipy", "scikit-learn>=0.18",
                        "matplotlib"]
      )
import numpy as np
import argparse
from skopt.benchmarks import branin
from skopt import gp_minimize
from skopt import forest_minimize
from skopt import gbrt_minimize
def run(n_calls=200, n_runs=10, acq_optimizer="lbfgs"):
    bounds = [(-5.0, 10.0), (0.0, 15.0)]
    optimizers = [("gp_minimize", gp_minimize),
                  ("forest_minimize", forest_minimize),
                  ("gbrt_minimize", gbrt_minimize)]
    for name, optimizer in optimizers:
        print(name)
        results = []
        min_func_calls = []
        time_ = 0.0
        for random_state in range(n_runs):
            if name == "gp_minimize":
                res = optimizer(
                    branin, bounds, random_state=random_state, n_calls=n_calls,
                    noise=1e-10, verbose=True, acq_optimizer=acq_optimizer,
                    n_jobs=-1)
            else:
                res = optimizer(
                    branin, bounds, random_state=random_state, n_calls=n_calls,
                    acq_optimizer=acq_optimizer)
            results.append(res)
            func_vals = np.round(res.func_vals, 3)
            min_func_calls.append(np.argmin(func_vals) + 1)
        optimal_values = [result.fun for result in results]
        mean_optimum = np.mean(optimal_values)
        std = np.std(optimal_values)
        best = np.min(optimal_values)
        print("Mean optimum: " + str(mean_optimum))
        print("Std of optimal values" + str(std))
        print("Best optima:" + str(best))
        mean_fcalls = np.mean(min_func_calls)
        std_fcalls = np.std(min_func_calls)
        best_fcalls = np.min(min_func_calls)
        print("Mean func_calls to reach min: " + str(mean_fcalls))
        print("Std func_calls to reach min: " + str(std_fcalls))
        print("Fastest no of func_calls to reach min: " + str(best_fcalls))
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--n_calls', nargs="?", default=50, type=int, help="Number of function calls.")
    parser.add_argument(
        '--n_runs', nargs="?", default=5, type=int, help="Number of runs.")
    parser.add_argument(
        '--acq_optimizer', nargs="?", default="lbfgs", type=str,
        help="Acquistion optimizer.")
    args = parser.parse_args()
    run(args.n_calls, args.n_runs, args.acq_optimizer)
import argparse
import numpy as np
from skopt.benchmarks import hart6
from skopt import gp_minimize
from skopt import forest_minimize
from skopt import gbrt_minimize

def run(n_calls=200, n_runs=10, acq_optimizer="lbfgs"):
    bounds = np.tile((0., 1.), (6, 1))
    optimizers = [("gp_minimize", gp_minimize),
                  ("forest_minimize", forest_minimize),
                  ("gbrt_minimize", gbrt_minimize)]
    for name, optimizer in optimizers:
        print(name)
        results = []
        min_func_calls = []
        for random_state in range(n_runs):
            print(random_state)
            if name == "gp_minimize":
                res = optimizer(
                    hart6, bounds, random_state=random_state, n_calls=n_calls,
                    noise=1e-10, n_jobs=-1, acq_optimizer=acq_optimizer,
                    verbose=1)
            else:
                res = optimizer(
                    hart6, bounds, random_state=random_state, n_calls=n_calls)
            results.append(res)
            func_vals = np.round(res.func_vals, 3)
            min_func_calls.append(np.argmin(func_vals) + 1)
        optimal_values = [result.fun for result in results]
        mean_optimum = np.mean(optimal_values)
        std = np.std(optimal_values)
        best = np.min(optimal_values)
        print("Mean optimum: " + str(mean_optimum))
        print("Std of optimal values" + str(std))
        print("Best optima:" + str(best))
        mean_fcalls = np.mean(min_func_calls)
        std_fcalls = np.std(min_func_calls)
        best_fcalls = np.min(min_func_calls)
        print("Mean func_calls to reach min: " + str(mean_fcalls))
        print("Std func_calls to reach min: " + str(std_fcalls))
        print("Fastest no of func_calls to reach min: " + str(best_fcalls))
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--n_calls', nargs="?", default=200, type=int, help="Number of function calls.")
    parser.add_argument(
        '--n_runs', nargs="?", default=10, type=int, help="Number of runs.")
    parser.add_argument(
        '--acq_optimizer', nargs="?", default="lbfgs", type=str,
        help="Acquistion optimizer.")
    args = parser.parse_args()
    run(args.n_calls, args.n_runs, args.acq_optimizer)
from __future__ import absolute_import, division, print_function
import argparse
import codecs
import datetime
import imp
import os
import os.path as path
import subprocess
import sys
import tempfile
import glob
import pdoc
try:
    xrange = xrange
except NameError:
    xrange = range
version_suffix = '%d.%d' % (sys.version_info[0], sys.version_info[1])
default_http_dir = path.join(tempfile.gettempdir(), 'pdoc-%s' % version_suffix)
parser = argparse.ArgumentParser(
    description='Automatically generate API docs for Python modules.',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
aa = parser.add_argument
aa('module_name', type=str, nargs='?',
   help='The Python module name. This may be an import path resolvable in '
        'the current environment, or a file path to a Python module or '
        'package.')
aa('ident_name', type=str, nargs='?',
   help='When specified, only identifiers containing the name given '
        'will be shown in the output. Search is case sensitive. '
        'Has no effect when --http is set.')
aa('--version', action='store_true',
   help='Print the version of pdoc and exit.')
aa('--html', action='store_true',
   help='When set, the output will be HTML formatted.')
aa('--html-dir', type=str, default='.',
   help='The directory to output HTML files to. This option is ignored when '
        'outputting documentation as plain text.')
aa('--html-no-source', action='store_true',
   help='When set, source code will not be viewable in the generated HTML. '
        'This can speed up the time required to document large modules.')
aa('--overwrite', action='store_true',
   help='Overwrites any existing HTML files instead of producing an error.')
aa('--all-submodules', action='store_true',
   help='When set, every submodule will be included, regardless of whether '
        '__all__ is set and contains the submodule.')
aa('--external-links', action='store_true',
   help='When set, identifiers to external modules are turned into links. '
        'This is automatically set when using --http.')
aa('--template-dir', type=str, default=None,
   help='Specify a directory containing Mako templates. '
        'Alternatively, put your templates in $XDG_CONFIG_HOME/pdoc and '
        'pdoc will automatically find them.')
aa('--notebook-dir', type=str, default=None,
   help='Specify a directory containing Notebooks. ')
aa('--link-prefix', type=str, default='',
   help='A prefix to use for every link in the generated documentation. '
        'No link prefix results in all links being relative. '
        'Has no effect when combined with --http.')
aa('--only-pypath', action='store_true',
   help='When set, only modules in your PYTHONPATH will be documented.')
aa('--http', action='store_true',
   help='When set, pdoc will run as an HTTP server providing documentation '
        'of all installed modules. Only modules found in PYTHONPATH will be '
        'listed.')
aa('--http-dir', type=str, default=default_http_dir,
   help='The directory to cache HTML documentation when running as an HTTP '
        'server.')
aa('--http-host', type=str, default='localhost',
   help='The host on which to run the HTTP server.')
aa('--http-port', type=int, default=8080,
   help='The port on which to run the HTTP server.')
aa('--http-html', action='store_true',
   help='Internal use only. Do not set.')
args = parser.parse_args()

def quick_desc(imp, name, ispkg):
    if not hasattr(imp, 'path'):
                return ''
    if ispkg:
        fp = path.join(imp.path, name, '__init__.py')
    else:
        fp = path.join(imp.path, '%s.py' % name)
    if os.path.isfile(fp):
        with codecs.open(fp, 'r', 'utf-8') as f:
            quotes = None
            doco = []
            for i, line in enumerate(f):
                if i == 0:
                    if len(line) >= 3 and line[0:3] in ("'''", '    A wrapper around the acquisition function that is called by fmin_l_bfgs_b.
    This is because lbfgs allows only 1-D input.
        Wrapper so that the output of this function can be
    directly passed to a minimizer.
        Use the lower confidence bound to estimate the acquisition
    values.
    The trade-off between exploitation and exploration is left to
    be controlled by the user through the parameter ``kappa``.
    Parameters
    ----------
    * `X` [array-like, shape=(n_samples, n_features)]:
        Values where the acquisition function should be computed.
    * `model` [sklearn estimator that implements predict with ``return_std``]:
        The fit estimator that approximates the function through the
        method ``predict``.
        It should have a ``return_std`` parameter that returns the standard
        deviation.
    * `kappa`: [float, default 1.96]:
        Controls how much of the variance in the predicted values should be
        taken into account. If set to be very high, then we are favouring
        exploration over exploitation and vice versa.
        Useless if ``method`` is set to "LCB".
    * `return_grad`: [boolean, optional]:
        Whether or not to return the grad. Implemented only for the case where
        ``X`` is a single sample.
    Returns
    -------
    * `values`: [array-like, shape=(X.shape[0],)]:
        Acquisition function values computed at X.
    * `grad`: [array-like, shape=(n_samples, n_features)]:
        Gradient at X.
        Use the probability of improvement to calculate the acquisition values.
    The conditional probability `P(y=f(x) | x)`form a gaussian with a
    certain mean and standard deviation approximated by the model.
    The PI condition is derived by computing ``E[u(f(x))]``
    where ``u(f(x)) = 1``, if ``f(x) < y_opt`` and ``u(f(x)) = 0``,
    if``f(x) > y_opt``.
    This means that the PI condition does not care about how "better" the
    predictions are than the previous values, since it gives an equal reward
    to all of them.
    Note that the value returned by this function should be maximized to
    obtain the ``X`` with maximum improvement.
    Parameters
    ----------
    * `X` [array-like, shape=(n_samples, n_features)]:
        Values where the acquisition function should be computed.
    * `model` [sklearn estimator that implements predict with ``return_std``]:
        The fit estimator that approximates the function through the
        method ``predict``.
        It should have a ``return_std`` parameter that returns the standard
        deviation.
    * `y_opt` [float, default 0]:
        Previous minimum value which we would like to improve upon.
    * `xi`: [float, default=0.01]:
        Controls how much improvement one wants over the previous best
        values. Useful only when ``method`` is set to "EI"
    * `return_grad`: [boolean, optional]:
        Whether or not to return the grad. Implemented only for the case where
        ``X`` is a single sample.
    Returns
    -------
    * `values`: [array-like, shape=(X.shape[0],)]:
        Acquisition function values computed at X.
        Use the expected improvement to calculate the acquisition values.
    The conditional probability `P(y=f(x) | x)`form a gaussian with a certain
    mean and standard deviation approximated by the model.
    The EI condition is derived by computing ``E[u(f(x))]``
    where ``u(f(x)) = 0``, if ``f(x) > y_opt`` and ``u(f(x)) = y_opt - f(x)``,
    if``f(x) < y_opt``.
    This solves one of the issues of the PI condition by giving a reward
    proportional to the amount of improvement got.
    Note that the value returned by this function should be maximized to
    obtain the ``X`` with maximum improvement.
    Parameters
    ----------
    * `X` [array-like, shape=(n_samples, n_features)]:
        Values where the acquisition function should be computed.
    * `model` [sklearn estimator that implements predict with ``return_std``]:
        The fit estimator that approximates the function through the
        method ``predict``.
        It should have a ``return_std`` parameter that returns the standard
        deviation.
    * `y_opt` [float, default 0]:
        Previous minimum value which we would like to improve upon.
    * `xi`: [float, default=0.01]:
        Controls how much improvement one wants over the previous best
        values. Useful only when ``method`` is set to "EI"
    * `return_grad`: [boolean, optional]:
        Whether or not to return the grad. Implemented only for the case where
        ``X`` is a single sample.
    Returns
    -------
    * `values`: [array-like, shape=(X.shape[0],)]:
        Acquisition function values computed at X.
    
import numpy as np

def bench1(x):
        return x[0] ** 2

def bench2(x):
        if x[0] < 0:
        return x[0] ** 2
    else:
        return (x[0] - 5) ** 2 - 5

def bench3(x):
        return np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2))

def bench4(x):
        return float(x[0]) ** 2

def bench5(x):
        return float(x[0]) ** 2 + x[1] ** 2

def branin(x, a=1, b=5.1 / (4 * np.pi**2), c=5. / np.pi,
           r=6, s=10, t=1. / (8 * np.pi)):
        return (a * (x[1] - b * x[0] ** 2 + c * x[0] - r) ** 2 +
            s * (1 - t) * np.cos(x[0]) + s)

def hart6(x,
          alpha=np.asarray([1.0, 1.2, 3.0, 3.2]),
          P=10**-4 * np.asarray([[1312, 1696, 5569, 124, 8283, 5886],
                                 [2329, 4135, 8307, 3736, 1004, 9991],
                                 [2348, 1451, 3522, 2883, 3047, 6650],
                                 [4047, 8828, 8732, 5743, 1091, 381]]),
          A=np.asarray([[10, 3, 17, 3.50, 1.7, 8],
                        [0.05, 10, 17, 0.1, 8, 14],
                        [3, 3.5, 1.7, 10, 17, 8],
                        [17, 8, 0.05, 10, 0.1, 14]])):
        return -np.sum(alpha * np.exp(-np.sum(A * (np.array(x) - P)**2, axis=1)))
from collections import Callable
from time import time

def check_callback(callback):
        if callback is not None:
        if isinstance(callback, Callable):
            return [callback]
        elif (isinstance(callback, list) and
              all([isinstance(c, Callable) for c in callback])):
            return callback
        else:
            raise ValueError("callback should be either a callable or "
                             "a list of callables.")
    else:
        return []

class VerboseCallback(object):
    
    def __init__(self, n_total, n_init=0, n_random=0):
        self.n_init = n_init
        self.n_random = n_random
        self.n_total = n_total
        self.iter_no = 1
        self._start_time = time()
        self._print_info(start=True)
    def _print_info(self, start=True):
        iter_no = self.iter_no
        if start:
            status = "started"
            eval_status = "Evaluating function"
            search_status = "Searching for the next optimal point."
        else:
            status = "ended"
            eval_status = "Evaluation done"
            search_status = "Search finished for the next optimal point."
        if iter_no <= self.n_init:
            print("Iteration No: %d %s. %s at provided point."
                  % (iter_no, status, eval_status))
        elif self.n_init < iter_no <= (self.n_random + self.n_init):
            print("Iteration No: %d %s. %s at random point."
                  % (iter_no, status, eval_status))
        else:
            print("Iteration No: %d %s. %s"
                  % (iter_no, status, search_status))
    def __call__(self, res):
                time_taken = time() - self._start_time
        self._print_info(start=False)
        curr_y = res.func_vals[-1]
        curr_min = res.fun
        print("Time taken: %0.4f" % time_taken)
        print("Function value obtained: %0.4f" % curr_y)
        print("Current minimum: %0.4f" % curr_min)
        self.iter_no += 1
        if self.iter_no <= self.n_total:
            self._print_info(start=True)
            self._start_time = time()

class TimerCallback(object):
        def __init__(self):
        self._time = time()
        self.iter_time = []
    def __call__(self, res):
                elapsed_time = time() - self._time
        self.iter_time.append(elapsed_time)
        self._time = time()
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import cm
from matplotlib.ticker import LogLocator
from matplotlib.ticker import MaxNLocator
from scipy.interpolate import griddata
from scipy.optimize import OptimizeResult

def plot_convergence(*args, **kwargs):
            ax = kwargs.get("ax", None)
    true_minimum = kwargs.get("true_minimum", None)
    yscale = kwargs.get("yscale", None)
    if ax is None:
        ax = plt.gca()
    ax.set_title("Convergence plot")
    ax.set_xlabel("Number of calls $n$")
    ax.set_ylabel(r"$\min f(x)$ after $n$ calls")
    ax.grid()
    if yscale is not None:
        ax.set_yscale(yscale)
    colors = cm.viridis(np.linspace(0.25, 1.0, len(args)))
    for results, color in zip(args, colors):
        if isinstance(results, tuple):
            name, results = results
        else:
            name = None
        if isinstance(results, OptimizeResult):
            n_calls = len(results.x_iters)
            mins = [np.min(results.func_vals[:i])
                    for i in range(1, n_calls + 1)]
            ax.plot(range(1, n_calls + 1), mins, c=color,
                    marker=".", markersize=12, lw=2, label=name)
        elif isinstance(results, list):
            n_calls = len(results[0].x_iters)
            iterations = range(1, n_calls + 1)
            mins = [[np.min(r.func_vals[:i]) for i in iterations]
                    for r in results]
            for m in mins:
                ax.plot(iterations, m, c=color, alpha=0.2)
            ax.plot(iterations, np.mean(mins, axis=0), c=color,
                    marker=".", markersize=12, lw=2, label=name)
    if true_minimum:
        ax.axhline(true_minimum, linestyle="--",
                   color="r", lw=1,
                   label="True minimum")
    if true_minimum or name:
        ax.legend(loc="best")
    return ax

def _format_scatter_plot_axes(ax, space, ylabel):
            diagonal_ylim = (np.min([ax[i, i].get_ylim()[0]
                             for i in range(space.n_dims)]),
                     np.max([ax[i, i].get_ylim()[1]
                             for i in range(space.n_dims)]))
        for i in range(space.n_dims):          for j in range(space.n_dims):              ax_ = ax[i, j]
            if j > i:
                ax_.axis("off")
                        if i != j:
                                                ax_.set_ylim(*space.dimensions[i].bounds)
                ax_.set_xlim(*space.dimensions[j].bounds)
                if j > 0:
                    ax_.set_yticklabels([])
                else:
                    ax_.set_ylabel("$X_{%i}$" % i)
                                if i < space.n_dims - 1:
                    ax_.set_xticklabels([])
                                else:
                    [l.set_rotation(45) for l in ax_.get_xticklabels()]
                    ax_.set_xlabel("$X_{%i}$" % j)
            else:
                ax_.set_ylim(*diagonal_ylim)
                ax_.yaxis.tick_right()
                ax_.yaxis.set_label_position('right')
                ax_.yaxis.set_ticks_position('both')
                ax_.set_ylabel(ylabel)
                ax_.xaxis.tick_top()
                ax_.xaxis.set_label_position('top')
                ax_.set_xlabel("$X_{%i}$" % j)
            ax_.xaxis.set_major_locator(MaxNLocator(6, prune='both'))
            ax_.yaxis.set_major_locator(MaxNLocator(6, prune='both'))
    return ax

def partial_dependence(space, model, i, j=None, sample_points=None,
                       n_samples=250, n_points=40):
        if sample_points is None:
        sample_points = space.transform(space.rvs(n_samples=n_samples))
    if j is None:
        bounds = space.dimensions[i].bounds
                xi = np.linspace(bounds[0], bounds[1], n_points)
        xi_transformed = space.dimensions[i].transform(xi)
        yi = []
        for x_ in xi_transformed:
            rvs_ = np.array(sample_points)
            rvs_[:, i] = x_
            yi.append(np.mean(model.predict(rvs_)))
        return xi, yi
    else:
                bounds = space.dimensions[j].bounds
        xi = np.linspace(bounds[0], bounds[1], n_points)
        xi_transformed = space.dimensions[j].transform(xi)
        bounds = space.dimensions[i].bounds
        yi = np.linspace(bounds[0], bounds[1], n_points)
        yi_transformed = space.dimensions[i].transform(yi)
        zi = []
        for x_ in xi_transformed:
            row = []
            for y_ in yi_transformed:
                rvs_ = np.array(sample_points)
                rvs_[:, (j, i)] = (x_, y_)
                row.append(np.mean(model.predict(rvs_)))
            zi.append(row)
        return xi, yi, np.array(zi).T

def plot_objective(result, levels=10, n_points=40, n_samples=250,
                   zscale='linear'):
        space = result.space
    samples = np.asarray(result.x_iters)
    rvs_transformed = space.transform(space.rvs(n_samples=n_samples))
    if zscale == 'log':
        locator = LogLocator()
    elif zscale == 'linear':
        locator = None
    else:
        raise ValueError("Valid values for zscale are 'linear' and 'log',"
                         " not '%s'." % zscale)
    fig, ax = plt.subplots(space.n_dims, space.n_dims,
                           figsize=(2 * space.n_dims, 2 * space.n_dims))
    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95,
                        hspace=0.1, wspace=0.1)
    for i in range(space.n_dims):
        for j in range(space.n_dims):
            if i == j:
                xi, yi = partial_dependence(space, result.models[-1], i,
                                            j=None,
                                            sample_points=rvs_transformed,
                                            n_points=n_points)
                ax[i, i].plot(xi, yi)
                ax[i, i].axvline(result.x[i], linestyle="--", color="r", lw=1)
                        elif i > j:
                xi, yi, zi = partial_dependence(space, result.models[-1],
                                                i, j,
                                                rvs_transformed, n_points)
                ax[i, j].contourf(xi, yi, zi, levels,
                                  locator=locator, cmap='viridis_r')
                ax[i, j].scatter(samples[:, j], samples[:, i],
                                 c='k', s=10, lw=0.)
                ax[i, j].scatter(result.x[j], result.x[i],
                                 c=['r'], s=20, lw=0.)
    return _format_scatter_plot_axes(ax, space, "Partial dependence")

def plot_evaluations(result, bins=20):
        space = result.space
    samples = np.asarray(result.x_iters)
    order = range(samples.shape[0])
    fig, ax = plt.subplots(space.n_dims, space.n_dims,
                           figsize=(2 * space.n_dims, 2 * space.n_dims))
    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95,
                        hspace=0.1, wspace=0.1)
    for i in range(space.n_dims):
        for j in range(space.n_dims):
            if i == j:
                ax[i, i].hist(samples[:, j], bins=bins,
                              range=space.dimensions[j].bounds)
                        elif i > j:
                ax[i, j].scatter(samples[:, j], samples[:, i], c=order,
                                 s=40, lw=0., cmap='viridis')
                ax[i, j].scatter(result.x[j], result.x[i],
                                 c=['r'], s=20, lw=0.)
    return _format_scatter_plot_axes(ax, space, "Number of samples")
from copy import deepcopy
import numpy as np
from scipy.optimize import OptimizeResult
from sklearn.externals.joblib import dump as dump_
from sklearn.externals.joblib import load as load_
__all__ = (
    "load",
    "dump",
)

def create_result(Xi, yi, space=None, rng=None, specs=None, models=None):
        res = OptimizeResult()
    yi = np.asarray(yi)
    best = np.argmin(yi)
    res.x = Xi[best]
    res.fun = yi[best]
    res.func_vals = yi
    res.x_iters = Xi
    res.models = models
    res.space = space
    res.random_state = rng
    res.specs = specs
    return res

def dump(res, filename, store_objective=True, **kwargs):
        if store_objective:
        dump_(res, filename, **kwargs)
    elif 'func' in res.specs['args']:
                                res_without_func = deepcopy(res)
        del res_without_func.specs['args']['func']
        dump_(res_without_func, filename, **kwargs)
    else:
                        dump_(res, filename, **kwargs)

def load(filename, **kwargs):
        return load_(filename, **kwargs)
from . import acquisition
from . import benchmarks
from . import callbacks
from . import learning
from . import optimizer
from . import plots
from . import space
from .optimizer import dummy_minimize
from .optimizer import forest_minimize
from .optimizer import gbrt_minimize
from .optimizer import gp_minimize
from .optimizer import Optimizer
from .utils import load, dump

__version__ = "0.3"

__all__ = (
    "acquisition",
    "benchmarks",
    "callbacks",
    "learning",
    "optimizer",
    "plots",
    "space",
    "gp_minimize",
    "dummy_minimize",
    "forest_minimize",
    "gbrt_minimize",
    "Optimizer",
    "dump",
    "load",
)
import numpy as np
from sklearn.ensemble import RandomForestRegressor as sk_RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor as sk_ExtraTreesRegressor

def _return_std(X, trees, predictions, min_variance):
            std = np.zeros(len(X))
    for tree in trees:
        var_tree = tree.tree_.impurity[tree.apply(X)]
                                                var_tree[var_tree < min_variance] = min_variance
        mean_tree = tree.predict(X)
        std += var_tree + mean_tree ** 2
    std /= len(trees)
    std -= predictions ** 2.0
    std[std < 0.0] = 0.0
    std = std ** 0.5
    return std

class RandomForestRegressor(sk_RandomForestRegressor):
        def __init__(self, n_estimators=10, criterion='mse', max_depth=None,
                 min_samples_split=2, min_samples_leaf=1,
                 min_weight_fraction_leaf=0.0, max_features='auto',
                 max_leaf_nodes=None, bootstrap=True, oob_score=False,
                 n_jobs=1, random_state=None, verbose=0, warm_start=False,
                 min_variance=0.0):
        self.min_variance = min_variance
        super(RandomForestRegressor, self).__init__(
            n_estimators=n_estimators, criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features, max_leaf_nodes=max_leaf_nodes,
            bootstrap=bootstrap, oob_score=oob_score,
            n_jobs=n_jobs, random_state=random_state,
            verbose=verbose, warm_start=warm_start)
    def predict(self, X, return_std=False):
                mean = super(RandomForestRegressor, self).predict(X)
        if return_std:
            if self.criterion != "mse":
                raise ValueError(
                    "Expected impurity to be 'mse', got %s instead"
                    % self.criterion)
            std = _return_std(X, self.estimators_, mean, self.min_variance)
            return mean, std
        return mean

class ExtraTreesRegressor(sk_ExtraTreesRegressor):
        def __init__(self, n_estimators=10, criterion='mse', max_depth=None,
                 min_samples_split=2, min_samples_leaf=1,
                 min_weight_fraction_leaf=0.0, max_features='auto',
                 max_leaf_nodes=None, bootstrap=False, oob_score=False,
                 n_jobs=1, random_state=None, verbose=0, warm_start=False,
                 min_variance=0.0):
        self.min_variance = min_variance
        super(ExtraTreesRegressor, self).__init__(
            n_estimators=n_estimators, criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features, max_leaf_nodes=max_leaf_nodes,
            bootstrap=bootstrap, oob_score=oob_score,
            n_jobs=n_jobs, random_state=random_state,
            verbose=verbose, warm_start=warm_start)
    def predict(self, X, return_std=False):
                mean = super(ExtraTreesRegressor, self).predict(X)
        if return_std:
            if self.criterion != "mse":
                raise ValueError(
                    "Expected impurity to be 'mse', got %s instead"
                    % self.criterion)
            std = _return_std(X, self.estimators_, mean, self.min_variance)
            return mean, std
        return mean
import numpy as np
from sklearn.base import clone
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.utils import check_random_state
from sklearn.externals.joblib import Parallel, delayed

def _parallel_fit(regressor, X, y):
    return regressor.fit(X, y)

class GradientBoostingQuantileRegressor(BaseEstimator, RegressorMixin):
    
    def __init__(self, quantiles=[0.16, 0.5, 0.84], base_estimator=None,
                 n_jobs=1, random_state=None):
        self.quantiles = quantiles
        self.random_state = random_state
        self.base_estimator = base_estimator
        self.n_jobs = n_jobs
    def fit(self, X, y):
                rng = check_random_state(self.random_state)
        if self.base_estimator is None:
            base_estimator = GradientBoostingRegressor(loss='quantile')
        else:
            base_estimator = self.base_estimator
            if not isinstance(base_estimator, GradientBoostingRegressor):
                raise ValueError('base_estimator has to be of type'
                                 ' GradientBoostingRegressor.')
            if not base_estimator.loss == 'quantile':
                raise ValueError('base_estimator has to use quantile'
                                 ' loss not %s' % base_estimator.loss)
                        base_estimator.set_params(random_state=rng)
        regressors = []
        for q in self.quantiles:
            regressor = clone(base_estimator)
            regressor.set_params(alpha=q)
            regressors.append(regressor)
        self.regressors_ = Parallel(n_jobs=self.n_jobs, backend='threading')(
            delayed(_parallel_fit)(regressor, X, y)
            for regressor in regressors)
        return self
    def predict(self, X, return_std=False):
                predicted_quantiles = np.asarray(
            [rgr.predict(X) for rgr in self.regressors_])
        if not return_std:
            return predicted_quantiles.T
        else:
            std_quantiles = [0.16, 0.5, 0.84]
            is_present_mask = np.in1d(std_quantiles, self.quantiles)
            if not np.all(is_present_mask):
                raise ValueError(
                    "return_std works only if the quantiles during "
                    "instantiation include 0.16, 0.5 and 0.84")
            low = self.regressors_[self.quantiles.index(0.16)].predict(X)
            high = self.regressors_[self.quantiles.index(0.84)].predict(X)
            mean = self.regressors_[self.quantiles.index(0.5)].predict(X)
            return mean, ((high - low) / 2.0)
from .forest import RandomForestRegressor
from .forest import ExtraTreesRegressor
from .gbrt import GradientBoostingQuantileRegressor
from .gaussian_process import GaussianProcessRegressor

__all__ = ("RandomForestRegressor",
           "ExtraTreesRegressor",
           "GradientBoostingQuantileRegressor",
           "GaussianProcessRegressor")
import numpy as np
import warnings
from scipy.linalg import cho_solve
from scipy.linalg import solve_triangular
from sklearn.gaussian_process import GaussianProcessRegressor as sk_GaussianProcessRegressor
from sklearn.utils import check_array
from .kernels import ConstantKernel
from .kernels import Sum
from .kernels import RBF
from .kernels import WhiteKernel

def _param_for_white_kernel_in_Sum(kernel, kernel_str=""):
        if kernel_str != "":
        kernel_str = kernel_str + "__"
    if isinstance(kernel, Sum):
        for param, child in kernel.get_params(deep=False).items():
            if isinstance(child, WhiteKernel):
                return True, kernel_str + param
            else:
                present, child_str = _param_for_white_kernel_in_Sum(
                    child, kernel_str + param)
                if present:
                    return True, child_str
    return False, "_"

class GaussianProcessRegressor(sk_GaussianProcessRegressor):
        def __init__(self, kernel=None, alpha=0.0,
                 optimizer="fmin_l_bfgs_b", n_restarts_optimizer=0,
                 normalize_y=False, copy_X_train=True, random_state=None,
                 noise=None):
        self.noise = noise
        super(GaussianProcessRegressor, self).__init__(
            kernel=kernel, alpha=alpha, optimizer=optimizer,
            n_restarts_optimizer=n_restarts_optimizer,
            normalize_y=normalize_y, copy_X_train=copy_X_train,
            random_state=random_state)
    def fit(self, X, y):
                if isinstance(self.noise, str) and self.noise != "gaussian":
            raise ValueError("expected noise to be 'gaussian', got %s"
                             % self.noise)
        if self.kernel is None:
            self.kernel = ConstantKernel(1.0, constant_value_bounds="fixed") \
                          * RBF(1.0, length_scale_bounds="fixed")
        elif self.noise == "gaussian":
            self.kernel = self.kernel + WhiteKernel()
        elif self.noise:
            self.kernel = self.kernel + WhiteKernel(
                noise_level=self.noise, noise_level_bounds="fixed"
            )
        super(GaussianProcessRegressor, self).fit(X, y)
        self.noise_ = None
        if self.noise:
                                                                                                            if isinstance(self.kernel_, WhiteKernel):
                self.kernel_.set_params(noise_level=0.0)
            else:
                white_present, white_param = _param_for_white_kernel_in_Sum(
                    self.kernel_)
                                if white_present:
                    noise_kernel = self.kernel_.get_params()[white_param]
                    self.noise_ = noise_kernel.noise_level
                    self.kernel_.set_params(
                        **{white_param: WhiteKernel(noise_level=0.0)})
                L_inv = solve_triangular(self.L_.T, np.eye(self.L_.shape[0]))
        self.K_inv_ = L_inv.dot(L_inv.T)
        return self
    def predict(self, X, return_std=False, return_cov=False,
                return_mean_grad=False, return_std_grad=False):
                if return_std and return_cov:
            raise RuntimeError(
                "Not returning standard deviation of predictions when "
                "returning full covariance.")
        if return_std_grad and not return_std:
            raise ValueError(
                "Not returning std_gradient without returning "
                "the std.")
        X = check_array(X)
        if X.shape[0] != 1 and (return_mean_grad or return_std_grad):
            raise ValueError("Not implemented for n_samples > 1")
        if not hasattr(self, "X_train_"):              y_mean = np.zeros(X.shape[0])
            if return_cov:
                y_cov = self.kernel(X)
                return y_mean, y_cov
            elif return_std:
                y_var = self.kernel.diag(X)
                return y_mean, np.sqrt(y_var)
            else:
                return y_mean
        else:              K_trans = self.kernel_(X, self.X_train_)
            y_mean = K_trans.dot(self.alpha_)                y_mean = self.y_train_mean + y_mean  
            if return_cov:
                v = cho_solve((self.L_, True), K_trans.T)                  y_cov = self.kernel_(X) - K_trans.dot(v)                   return y_mean, y_cov
            elif return_std:
                K_inv = self.K_inv_
                                y_var = self.kernel_.diag(X)
                y_var -= np.einsum("ki,kj,ij->k", K_trans, K_trans, K_inv)
                                                y_var_negative = y_var < 0
                if np.any(y_var_negative):
                    warnings.warn("Predicted variances smaller than 0. "
                                  "Setting those variances to 0.")
                    y_var[y_var_negative] = 0.0
                y_std = np.sqrt(y_var)
            if return_mean_grad:
                grad = self.kernel_.gradient_x(X[0], self.X_train_)
                grad_mean = np.dot(grad.T, self.alpha_)
                if return_std_grad:
                    grad_std = np.zeros(X.shape[1])
                    if not np.allclose(y_std, grad_std):
                        grad_std = -np.dot(K_trans,
                                           np.dot(K_inv, grad))[0] / y_std
                    return y_mean, y_std, grad_mean, grad_std
                if return_std:
                    return y_mean, y_std, grad_mean
                else:
                    return y_mean, grad_mean
            else:
                if return_std:
                    return y_mean, y_std
                else:
                    return y_mean
from math import sqrt
import numpy as np
from sklearn.gaussian_process.kernels import Kernel as sk_Kernel
from sklearn.gaussian_process.kernels import ConstantKernel as sk_ConstantKernel
from sklearn.gaussian_process.kernels import DotProduct as sk_DotProduct
from sklearn.gaussian_process.kernels import Exponentiation as sk_Exponentiation
from sklearn.gaussian_process.kernels import ExpSineSquared as sk_ExpSineSquared
from sklearn.gaussian_process.kernels import Hyperparameter
from sklearn.gaussian_process.kernels import Matern as sk_Matern
from sklearn.gaussian_process.kernels import NormalizedKernelMixin as sk_NormalizedKernelMixin
from sklearn.gaussian_process.kernels import Product as sk_Product
from sklearn.gaussian_process.kernels import RationalQuadratic as sk_RationalQuadratic
from sklearn.gaussian_process.kernels import RBF as sk_RBF
from sklearn.gaussian_process.kernels import StationaryKernelMixin as sk_StationaryKernelMixin
from sklearn.gaussian_process.kernels import Sum as sk_Sum
from sklearn.gaussian_process.kernels import WhiteKernel as sk_WhiteKernel

class Kernel(sk_Kernel):
        def __add__(self, b):
        if not isinstance(b, Kernel):
            return Sum(self, ConstantKernel(b))
        return Sum(self, b)
    def __radd__(self, b):
        if not isinstance(b, Kernel):
            return Sum(ConstantKernel(b), self)
        return Sum(b, self)
    def __mul__(self, b):
        if not isinstance(b, Kernel):
            return Product(self, ConstantKernel(b))
        return Product(self, b)
    def __rmul__(self, b):
        if not isinstance(b, Kernel):
            return Product(ConstantKernel(b), self)
        return Product(b, self)
    def __pow__(self, b):
        return Exponentiation(self, b)
    def gradient_x(self, x, X_train):
                raise NotImplementedError

class RBF(Kernel, sk_RBF):
    def gradient_x(self, x, X_train):
                        x = np.asarray(x)
        X_train = np.asarray(X_train)
        length_scale = np.asarray(self.length_scale)
        diff = x - X_train
        diff /= length_scale
                        exp_diff_squared = np.sum(diff**2, axis=1)
        exp_diff_squared *= -0.5
        exp_diff_squared = np.exp(exp_diff_squared, exp_diff_squared)
        exp_diff_squared = np.expand_dims(exp_diff_squared, axis=1)
        exp_diff_squared *= -1
                gradient = exp_diff_squared * diff
        gradient /= length_scale
        return gradient

class Matern(Kernel, sk_Matern):
    def gradient_x(self, x, X_train):
        x = np.asarray(x)
        X_train = np.asarray(X_train)
        length_scale = np.asarray(self.length_scale)
                        diff = x - X_train
        diff /= length_scale
                                dist_sq = np.sum(diff**2, axis=1)
        dist = np.sqrt(dist_sq)
        if self.nu == 0.5:
                                    scaled_exp_dist = -dist
            scaled_exp_dist = np.exp(scaled_exp_dist, scaled_exp_dist)
            scaled_exp_dist *= -1
                                                                                    gradient = -np.ones((X_train.shape[0], x.shape[0]))
            mask = dist != 0.0
            scaled_exp_dist[mask] /= dist[mask]
            scaled_exp_dist = np.expand_dims(scaled_exp_dist, axis=1)
            gradient[mask] = scaled_exp_dist[mask] * diff[mask]
            gradient /= length_scale
            return gradient
        elif self.nu == 1.5:
                                                sqrt_3_dist = sqrt(3) * dist
            f = np.expand_dims(1 + sqrt_3_dist, axis=1)
                                                                                                sqrt_3_by_dist = np.zeros_like(dist)
            nzd = dist != 0.0
            sqrt_3_by_dist[nzd] = sqrt(3) / dist[nzd]
            dist_expand = np.expand_dims(sqrt_3_by_dist, axis=1)
            f_grad = diff / length_scale
            f_grad *= dist_expand
            sqrt_3_dist *= -1
            exp_sqrt_3_dist = np.exp(sqrt_3_dist, sqrt_3_dist)
            g = np.expand_dims(exp_sqrt_3_dist, axis=1)
            g_grad = -g * f_grad
                        f *= -1
            f += 1
            return g * f_grad * f
        elif self.nu == 2.5:
                                                            sqrt_5_dist = sqrt(5) * dist
            f2 = (5.0 / 3.0) * dist_sq
            f2 += sqrt_5_dist
            f2 += 1
            f = np.expand_dims(f2, axis=1)
                                                                                                                                                nzd_mask = dist != 0.0
            nzd = dist[nzd_mask]
            dist[nzd_mask] = np.reciprocal(nzd, nzd)
            dist *= sqrt(5)
            dist = np.expand_dims(dist, axis=1)
            diff /= length_scale
            f1_grad = dist * diff
            f2_grad = (10.0 / 3.0) * diff
            f_grad = f1_grad + f2_grad
            sqrt_5_dist *= -1
            g = np.exp(sqrt_5_dist, sqrt_5_dist)
            g = np.expand_dims(g, axis=1)
            g_grad = -g * f1_grad
            return f * g_grad + g * f_grad

class RationalQuadratic(Kernel, sk_RationalQuadratic):
    def gradient_x(self, x, X_train):
        x = np.asarray(x)
        X_train = np.asarray(X_train)
        alpha = self.alpha
        length_scale = self.length_scale
                        diff = x - X_train
        diff /= length_scale
                        scaled_dist = np.sum(diff**2, axis=1)
        scaled_dist /= (2 * self.alpha)
        scaled_dist += 1
        scaled_dist **= (-alpha - 1)
        scaled_dist *= -1
        scaled_dist = np.expand_dims(scaled_dist, axis=1)
        diff_by_ls = diff / length_scale
        return scaled_dist * diff_by_ls

class ExpSineSquared(Kernel, sk_ExpSineSquared):
    def gradient_x(self, x, X_train):
        x = np.asarray(x)
        X_train = np.asarray(X_train)
        length_scale = self.length_scale
        periodicity = self.periodicity
        diff = x - X_train
        sq_dist = np.sum(diff**2, axis=1)
        dist = np.sqrt(sq_dist)
        pi_by_period = dist * (np.pi / periodicity)
        sine = np.sin(pi_by_period) / length_scale
        sine_squared = -2 * sine**2
        exp_sine_squared = np.exp(sine_squared)
        grad_wrt_exp = -2 * np.sin(2 * pi_by_period) / length_scale**2
                                                grad_wrt_theta = np.zeros_like(dist)
        nzd = dist != 0.0
        grad_wrt_theta[nzd] = np.pi / (periodicity * dist[nzd])
        return np.expand_dims(
            grad_wrt_theta * exp_sine_squared * grad_wrt_exp, axis=1) * diff

class ConstantKernel(Kernel, sk_ConstantKernel):
    def gradient_x(self, x, X_train):
        return np.zeros_like(X_train)

class WhiteKernel(Kernel, sk_WhiteKernel):
    def gradient_x(self, x, X_train):
        return np.zeros_like(X_train)

class Exponentiation(Kernel, sk_Exponentiation):
    def gradient_x(self, x, X_train):
        x = np.asarray(x)
        X_train = np.asarray(X_train)
        expo = self.exponent
        kernel = self.kernel
        K = np.expand_dims(
            kernel(np.expand_dims(x, axis=0), X_train)[0], axis=1)
        return expo * K ** (expo - 1) * kernel.gradient_x(x, X_train)

class Sum(Kernel, sk_Sum):
    def gradient_x(self, x, X_train):
        return (
            self.k1.gradient_x(x, X_train) +
            self.k2.gradient_x(x, X_train)
        )

class Product(Kernel, sk_Product):
    def gradient_x(self, x, X_train):
        x = np.asarray(x)
        x = np.expand_dims(x, axis=0)
        X_train = np.asarray(X_train)
        f_ggrad = (
            np.expand_dims(self.k1(x, X_train)[0], axis=1) *
            self.k2.gradient_x(x, X_train)
        )
        fgrad_g = (
            np.expand_dims(self.k2(x, X_train)[0], axis=1) *
            self.k1.gradient_x(x, X_train)
        )
        return f_ggrad + fgrad_g

class DotProduct(Kernel, sk_DotProduct):
    def gradient_x(self, x, X_train):
        return np.asarray(X_train)

class HammingKernel(sk_StationaryKernelMixin, sk_NormalizedKernelMixin, Kernel):
    
    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):
        self.length_scale = length_scale
        self.length_scale_bounds = length_scale_bounds
    @property
    def hyperparameter_length_scale(self):
        length_scale = self.length_scale
        anisotropic = np.iterable(length_scale) and len(length_scale) > 1
        if anisotropic:
            return Hyperparameter("length_scale", "numeric",
                                  self.length_scale_bounds,
                                  len(length_scale))
        return Hyperparameter(
            "length_scale", "numeric", self.length_scale_bounds)
    def __call__(self, X, Y=None, eval_gradient=False):
                length_scale = self.length_scale
        anisotropic = np.iterable(length_scale) and len(length_scale) > 1
        if np.iterable(length_scale):
            if len(length_scale) > 1:
                length_scale = np.asarray(length_scale, dtype=np.float)
            else:
                length_scale = float(length_scale[0])
        else:
            length_scale = float(length_scale)
        X = np.atleast_2d(X)
        if anisotropic and X.shape[1] != len(length_scale):
            raise ValueError(
                "Expected X to have %d features, got %d" %
                (X.shape, len(length_scale)))
        n_samples, n_dim = X.shape
        Y_is_None = Y is None
        if Y_is_None:
            Y = X
        elif eval_gradient:
            raise ValueError("gradient can be evaluated only when Y != X")
        else:
            Y = np.atleast_2d(Y)
        indicator = np.expand_dims(X, axis=1) != Y
        kernel_prod = np.exp(-np.sum(length_scale * indicator, axis=2))
                        
                if anisotropic:
            grad = -np.expand_dims(kernel_prod, axis=-1) * np.array(indicator, dtype=np.float32)
        else:
            grad = -np.expand_dims(kernel_prod * np.sum(indicator, axis=2), axis=-1)
        grad *= length_scale
        if eval_gradient:
            return kernel_prod, grad
        return kernel_prod
from .gpr import GaussianProcessRegressor
__all__ = ("GaussianProcessRegressor")
import copy
import inspect
import numbers
from collections import Iterable
import numpy as np
from ..callbacks import check_callback
from ..callbacks import VerboseCallback
from ..utils import create_result
from .optimizer import Optimizer

def _eval_callbacks(callbacks, optimizer, specs):
    if callbacks:
        result = create_result(optimizer.Xi, optimizer.yi,
                               optimizer.space, optimizer.rng,
                               specs, optimizer.models)
        for c in callbacks:
            c(result)

def base_minimize(func, dimensions, base_estimator,
                  n_calls=100, n_random_starts=10,
                  acq_func="EI", acq_optimizer="lbfgs",
                  x0=None, y0=None, random_state=None, verbose=False,
                  callback=None, n_points=10000, n_restarts_optimizer=5,
                  xi=0.01, kappa=1.96, n_jobs=1):
        specs = {"args": copy.copy(inspect.currentframe().f_locals),
             "function": inspect.currentframe().f_code.co_name}
    acq_optimizer_kwargs = {
        "n_points": n_points, "n_restarts_optimizer": n_restarts_optimizer,
        "n_jobs": n_jobs}
    acq_func_kwargs = {"xi": xi, "kappa": kappa}
    optimizer = Optimizer(dimensions, base_estimator, n_random_starts,
                          acq_func, acq_optimizer, random_state,
                          acq_optimizer_kwargs=acq_optimizer_kwargs,
                          acq_func_kwargs=acq_func_kwargs)
        if x0 is None:
        x0 = []
    elif not isinstance(x0[0], list):
        x0 = [x0]
    if not isinstance(x0, list):
        raise ValueError("`x0` should be a list, but got %s" % type(x0))
    n_init_func_calls = len(x0) if y0 is None else 0
    n_total_init_calls = n_random_starts + n_init_func_calls
    if n_calls < n_total_init_calls:
        raise ValueError(
            "Expected `n_calls` >= %d, got %d" % (n_total_init_calls,
                                                  n_calls))
    if n_random_starts == 0 and not x0:
        raise ValueError("Either set `n_random_starts` > 0,"
                         " or provide `x0`")
    callbacks = check_callback(callback)
    if verbose:
        callbacks.append(VerboseCallback(
            n_init=n_init_func_calls, n_random=n_random_starts,
            n_total=n_calls))
        if x0 and y0 is None:
        y0 = map(func, x0)
        res = None
        if x0 and y0 is not None:
        if not (isinstance(y0, Iterable) or isinstance(y0, numbers.Number)):
            raise ValueError(
                "`y0` should be an iterable or a scalar, got %s" % type(y0))
        if isinstance(y0, Iterable):
            y0 = list(y0)
        elif isinstance(y0, numbers.Number):
            y0 = [y0]
        if len(x0) != len(y0):
            raise ValueError("`x0` and `y0` should have the same length")
        if not all(map(np.isscalar, y0)):
            raise ValueError(
                "`y0` elements should be scalars")
        res = optimizer.tell(x0, y0)
        res.specs = specs
        _eval_callbacks(callbacks, optimizer, specs)
        n_iterations = n_calls - n_init_func_calls
    for n in range(n_iterations):
        next_x = optimizer.ask()
                fit_model = n < n_iterations - 1
        next_y = func(next_x)
        if not np.isscalar(next_y):
            raise ValueError("`func` should return a scalar")
        res = optimizer.tell(next_x, next_y, fit=fit_model)
        res.specs = specs
        _eval_callbacks(callbacks, optimizer, specs)
    return res
import copy
import inspect
import numbers
import numpy as np
from collections import Iterable
from sklearn.utils import check_random_state
from ..callbacks import check_callback
from ..callbacks import VerboseCallback
from ..space import Space
from ..utils import create_result

def dummy_minimize(func, dimensions, n_calls=100,
                   x0=None, y0=None, random_state=None, verbose=False,
                   callback=None):
            specs = {"args": copy.copy(inspect.currentframe().f_locals),
             "function": inspect.currentframe().f_code.co_name}
        rng = check_random_state(random_state)
    space = Space(dimensions)
    if x0 is None:
        x0 = []
    elif not isinstance(x0[0], list):
        x0 = [x0]
    if not isinstance(x0, list):
        raise ValueError("`x0` should be a list, got %s" % type(x0))
    n_init_func_calls = 0
    if len(x0) > 0 and y0 is not None:
        if isinstance(y0, Iterable):
            y0 = list(y0)
        elif isinstance(y0, numbers.Number):
            y0 = [y0]
        else:
            raise ValueError("`y0` should be an iterable or a scalar, got %s"
                             % type(y0))
        if len(x0) != len(y0):
            raise ValueError("`x0` and `y0` should have the same length")
        if not all(map(np.isscalar, y0)):
            raise ValueError("`y0` elements should be scalars")
    elif len(x0) > 0 and y0 is None:
        y0 = []
        n_calls -= len(x0)
        n_init_func_calls = len(x0)
    elif len(x0) == 0 and y0 is not None:
        raise ValueError("`x0`cannot be `None` when `y0` is provided")
    else:          y0 = []
    callbacks = check_callback(callback)
    if verbose:
        callbacks.append(VerboseCallback(
            n_init=n_init_func_calls, n_total=n_calls))
    X = x0
    y = y0
        X = X + space.rvs(n_samples=n_calls, random_state=rng)
    first = True
    for i in range(len(y0), len(X)):
        y_i = func(X[i])
        if first:
            first = False
            if not np.isscalar(y_i):
                raise ValueError("`func` should return a scalar")
        y.append(y_i)
        if callbacks:
            curr_res = create_result(X[: i + 1], y, space, rng, specs)
            for c in callbacks:
                c(curr_res)
    y = np.array(y)
    return create_result(X, y, space, rng, specs)
from sklearn.utils import check_random_state
from .base import base_minimize
from ..learning import ExtraTreesRegressor
from ..learning import RandomForestRegressor

def forest_minimize(func, dimensions, base_estimator="ET",
                    n_calls=100, n_random_starts=10,
                    acq_func="EI", acq_optimizer="auto",
                    x0=None, y0=None, random_state=None, verbose=False,
                    callback=None, n_points=10000, xi=0.01, kappa=1.96,
                    n_jobs=1):
        rng = check_random_state(random_state)
        if isinstance(base_estimator, str):
        if base_estimator not in ("RF", "ET"):
            raise ValueError(
                "Valid strings for the base_estimator parameter"
                " are: 'RF' or 'ET', not '%s'" % base_estimator)
        if base_estimator == "RF":
            base_estimator = RandomForestRegressor(n_estimators=100,
                                                   min_samples_leaf=3,
                                                   n_jobs=n_jobs,
                                                   random_state=rng)
        elif base_estimator == "ET":
            base_estimator = ExtraTreesRegressor(n_estimators=100,
                                                 min_samples_leaf=3,
                                                 n_jobs=n_jobs,
                                                 random_state=rng)
    return base_minimize(func, dimensions, base_estimator,
                         n_calls=n_calls, n_points=n_points,
                         n_random_starts=n_random_starts,
                         x0=x0, y0=y0, random_state=random_state,
                         acq_func=acq_func,
                         xi=xi, kappa=kappa, verbose=verbose,
                         callback=callback, acq_optimizer="sampling")
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.utils import check_random_state
from .base import base_minimize
from ..learning import GradientBoostingQuantileRegressor

def gbrt_minimize(func, dimensions, base_estimator=None,
                  n_calls=100, n_random_starts=10,
                  acq_func="EI", acq_optimizer="auto",
                  x0=None, y0=None, random_state=None, verbose=False,
                  callback=None, n_points=10000, xi=0.01, kappa=1.96,
                  n_jobs=1):
            rng = check_random_state(random_state)
        if base_estimator is None:
        gbrt = GradientBoostingRegressor(n_estimators=30, loss="quantile")
        base_estimator = GradientBoostingQuantileRegressor(base_estimator=gbrt,
                                                           n_jobs=n_jobs,
                                                           random_state=rng)
    return base_minimize(func, dimensions, base_estimator,
                         n_calls=n_calls, n_points=n_points,
                         n_random_starts=n_random_starts,
                         x0=x0, y0=y0, random_state=random_state, xi=xi,
                         kappa=kappa, acq_func=acq_func,
                         callback=callback, acq_optimizer="sampling")
import numpy as np
from sklearn.utils import check_random_state
from .base import base_minimize
from ..learning import GaussianProcessRegressor
from ..learning.gaussian_process.kernels import ConstantKernel
from ..learning.gaussian_process.kernels import HammingKernel
from ..learning.gaussian_process.kernels import Matern
from ..space import check_dimension
from ..space import Categorical
from ..space import Space

def gp_minimize(func, dimensions, base_estimator=None,
                n_calls=100, n_random_starts=10,
                acq_func="gp_hedge", acq_optimizer="lbfgs", x0=None, y0=None,
                random_state=None, verbose=False, callback=None,
                n_points=10000, n_restarts_optimizer=5, xi=0.01, kappa=1.96,
                noise="gaussian", n_jobs=1):
            rng = check_random_state(random_state)
    dim_types = [check_dimension(d) for d in dimensions]
    is_cat = all([isinstance(check_dimension(d), Categorical) for d in dim_types])
    if is_cat:
        transformed_dims = [check_dimension(d,
                                      transform="identity") for d in dimensions]
    else:
        transformed_dims = []
        for dim_type, dim in zip(dim_types, dimensions):
            if isinstance(dim_type, Categorical):
                transformed_dims.append(check_dimension(dim, transform="onehot"))
                        else:
                transformed_dims.append(check_dimension(dim, transform="normalize"))
    space = Space(transformed_dims)
        if base_estimator is None:
        cov_amplitude = ConstantKernel(1.0, (0.01, 1000.0))
        if is_cat:
            other_kernel = HammingKernel(
                length_scale=np.ones(space.transformed_n_dims))
            acq_optimizer = "sampling"
        else:
            other_kernel = Matern(
                length_scale=np.ones(space.transformed_n_dims),
                length_scale_bounds=[(0.01, 100)] * space.transformed_n_dims,
                nu=2.5)
    base_estimator = GaussianProcessRegressor(
        kernel=cov_amplitude * other_kernel,
        normalize_y=True, random_state=rng, alpha=0.0, noise=noise,
        n_restarts_optimizer=2)
    return base_minimize(
        func, dimensions, base_estimator=base_estimator,
        acq_func=acq_func,
        xi=xi, kappa=kappa, acq_optimizer=acq_optimizer, n_calls=n_calls,
        n_points=n_points, n_random_starts=n_random_starts,
        n_restarts_optimizer=n_restarts_optimizer,
        x0=x0, y0=y0, random_state=random_state, verbose=verbose,
        callback=callback, n_jobs=n_jobs)
import warnings
from collections import Iterable
from numbers import Number
import numpy as np
from scipy.optimize import fmin_l_bfgs_b
from sklearn.base import clone
from sklearn.base import is_regressor
from sklearn.externals.joblib import Parallel, delayed
from sklearn.utils import check_random_state
from ..acquisition import _gaussian_acquisition
from ..acquisition import gaussian_acquisition_1D
from ..space import Categorical
from ..space import Space
from ..utils import create_result

class Optimizer(object):
        def __init__(self, dimensions, base_estimator,
                 n_random_starts=10, acq_func="gp_hedge",
                 acq_optimizer="lbfgs",
                 random_state=None, acq_func_kwargs=None,
                 acq_optimizer_kwargs=None):
                self.acq_func = acq_func
        self.rng = check_random_state(random_state)
        self.acq_func_kwargs = acq_func_kwargs
        if self.acq_func == "gp_hedge":
            self.cand_acq_funcs_ = ["EI", "LCB", "PI"]
            self.gains_ = np.zeros(3)
        else:
            self.cand_acq_funcs_ = [self.acq_func]
        if acq_func_kwargs is None:
            acq_func_kwargs = dict()
        self.eta = acq_func_kwargs.get("eta", 1.0)
        if acq_optimizer_kwargs is None:
            acq_optimizer_kwargs = dict()
        self.n_points = acq_optimizer_kwargs.get("n_points", 10000)
        self.n_restarts_optimizer = acq_optimizer_kwargs.get(
            "n_restarts_optimizer", 5)
        n_jobs = acq_optimizer_kwargs.get("n_jobs", 1)
        self.space = Space(dimensions)
        self.models = []
        self.Xi = []
        self.yi = []
        self._cat_inds = []
        self._non_cat_inds = []
        for ind, dim in enumerate(self.space.dimensions):
            if isinstance(dim, Categorical):
                self._cat_inds.append(ind)
            else:
                self._non_cat_inds.append(ind)
        self._check_arguments(base_estimator, n_random_starts, acq_optimizer)
        self.n_jobs = n_jobs
    def _check_arguments(self, base_estimator, n_random_starts, acq_optimizer):
                if not is_regressor(base_estimator):
            raise ValueError(
                "%s has to be a regressor." % base_estimator)
        self.base_estimator = base_estimator
        if n_random_starts < 0:
            raise ValueError(
                "Expected `n_random_starts` >= 0, got %d" % n_random_starts)
        self._n_random_starts = n_random_starts
        if acq_optimizer == "auto":
            warnings.warn("The 'auto' option for the acq_optimizer will be "
                          "removed in 0.4.")
            acq_optimizer = "lbfgs"
        self.acq_optimizer = acq_optimizer
        if self.acq_optimizer not in ["lbfgs", "sampling"]:
            raise ValueError(
                "Expected acq_optimizer to be 'lbfgs' or 'sampling', "
                "got %s" % acq_optimizer)
    def ask(self):
                if self._n_random_starts > 0:
            self._n_random_starts -= 1
                                    return self.space.rvs(random_state=self.rng)[0]
        else:
            if not self.models:
                raise ValueError("Random evaluations exhausted and no "
                                 "model has been fit.")
            cat_inds = self._cat_inds
            non_cat_inds = self._non_cat_inds
            next_x = self._next_x
            if len(cat_inds) == 0:
                close_to_next_x = lambda x: np.allclose(x, next_x)
                if np.any(np.apply_along_axis(close_to_next_x, 1, self.Xi)):
                    warnings.warn("The objective has been evaluated "
                                  "at this point before.")
            else:
                next_x_arr = np.array(next_x)
                next_x_non_cat = np.array(
                    next_x_arr[non_cat_inds], dtype=np.float32)
                for x in self.Xi:
                    x_arr = np.array(x)
                    cat_eq = np.all(x_arr[cat_inds] == next_x_arr[cat_inds])
                    non_cat_eq = np.allclose(
                        np.array(x_arr[non_cat_inds], dtype=np.float32),
                        next_x_non_cat)
                    if cat_eq and non_cat_eq:
                        warnings.warn("The objective has been evaluated "
                                      "at this point before.")
                        return next_x
    def tell(self, x, y, fit=True):
                        if (isinstance(y, Iterable) and all(isinstance(point, Iterable)
                                            for point in x)):
            if not np.all([p in self.space for p in x]):
                raise ValueError("Not all points are within the bounds of"
                                 " the space.")
            self.Xi.extend(x)
            self.yi.extend(y)
        elif isinstance(x, Iterable) and isinstance(y, Number):
            if x not in self.space:
                raise ValueError("Point (%s) is not within the bounds of"
                                 " the space (%s)."
                                 % (x, self.space.bounds))
            self.Xi.append(x)
            self.yi.append(y)
        else:
            raise ValueError("Type of arguments `x` (%s) and `y` (%s) "
                             "not compatible." % (type(x), type(y)))
        if fit and self._n_random_starts == 0:
            transformed_bounds = np.array(self.space.transformed_bounds)
            est = clone(self.base_estimator)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                est.fit(self.space.transform(self.Xi), self.yi)
            if hasattr(self, "next_xs_") and self.acq_func == "gp_hedge":
                self.gains_ -= est.predict(np.vstack(self.next_xs_))
            self.models.append(est)
            X = self.space.transform(self.space.rvs(
                n_samples=self.n_points, random_state=self.rng))
            self.next_xs_ = []
            for cand_acq_func in self.cand_acq_funcs_:
                values = _gaussian_acquisition(
                    X=X, model=est, y_opt=np.min(self.yi),
                    acq_func=cand_acq_func,
                    acq_func_kwargs=self.acq_func_kwargs)
                                                if self.acq_optimizer == "sampling":
                    next_x = X[np.argmin(values)]
                                                                elif self.acq_optimizer == "lbfgs":
                    x0 = X[np.argsort(values)[:self.n_restarts_optimizer]]
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        results = Parallel(n_jobs=self.n_jobs)(
                            delayed(fmin_l_bfgs_b)(
                                gaussian_acquisition_1D, x,
                                args=(est, np.min(self.yi), cand_acq_func,
                                      self.acq_func_kwargs),
                                bounds=self.space.transformed_bounds,
                                approx_grad=False,
                                maxiter=20)
                            for x in x0)
                    cand_xs = np.array([r[0] for r in results])
                    cand_acqs = np.array([r[1] for r in results])
                    next_x = cand_xs[np.argmin(cand_acqs)]
                                                if not self.space.is_categorical:
                    next_x = np.clip(
                        next_x, transformed_bounds[:, 0],
                        transformed_bounds[:, 1])
                self.next_xs_.append(next_x)
            if self.acq_func == "gp_hedge":
                logits = np.array(self.gains_)
                logits -= np.max(logits)
                exp_logits = np.exp(self.eta * logits)
                probs = exp_logits / np.sum(exp_logits)
                next_x = self.next_xs_[np.argmax(np.random.multinomial(1,
                                                                       probs))]
            else:
                next_x = self.next_xs_[0]
                        self._next_x = self.space.inverse_transform(
                next_x.reshape((1, -1)))[0]
                return create_result(self.Xi, self.yi, self.space, self.rng,
                             models=self.models)
    def run(self, func, n_iter=1):
                for _ in range(n_iter):
            x = self.ask()
            self.tell(x, func(x))
        return create_result(self.Xi, self.yi, self.space, self.rng,
                             models=self.models)
from .base import base_minimize
from .dummy import dummy_minimize
from .forest import forest_minimize
from .gbrt import gbrt_minimize
from .gp import gp_minimize
from .optimizer import Optimizer

__all__ = [
    "base_minimize", "dummy_minimize",
    "forest_minimize", "gbrt_minimize", "gp_minimize",
    "Optimizer"
]
import numbers
import numpy as np
from scipy.stats.distributions import randint
from scipy.stats.distributions import rv_discrete
from scipy.stats.distributions import uniform
from sklearn.utils import check_random_state
from sklearn.utils.fixes import sp_version
from .transformers import CategoricalEncoder
from .transformers import Normalize
from .transformers import Identity
from .transformers import Log10
from .transformers import Pipeline

class _Ellipsis:
    def __repr__(self):
        return '...'

def check_dimension(dimension, transform=None):
        if isinstance(dimension, Dimension):
        return dimension
    if not isinstance(dimension, (list, tuple, np.ndarray)):
        raise ValueError("Dimension has to be a list or tuple.")
    if (len(dimension) == 3 and
            isinstance(dimension[0], numbers.Real) and
            isinstance(dimension[2], str)):
        return Real(*dimension, transform=transform)
    if len(dimension) > 2 or isinstance(dimension[0], str):
        return Categorical(dimension, transform=transform)
    if len(dimension) == 2 and isinstance(dimension[0], numbers.Integral):
        return Integer(*dimension, transform=transform)
    if len(dimension) == 2 and isinstance(dimension[0], numbers.Real):
        return Real(*dimension, transform=transform)
    raise ValueError("Invalid dimension %s. Read the documentation for "
                     "supported types." % dimension)

class Dimension(object):
    
    def rvs(self, n_samples=1, random_state=None):
                rng = check_random_state(random_state)
        samples = self._rvs.rvs(size=n_samples, random_state=rng)
        return self.inverse_transform(samples)
    def transform(self, X):
                return self.transformer.transform(X)
    def inverse_transform(self, Xt):
                return self.transformer.inverse_transform(Xt)
    @property
    def size(self):
        return 1
    @property
    def transformed_size(self):
        return 1
    @property
    def bounds(self):
        raise NotImplementedError
    @property
    def transformed_bounds(self):
        raise NotImplementedError

def _uniform_inclusive(loc=0.0, scale=1.0):
                return uniform(loc=loc, scale=np.nextafter(scale, scale + 1.))

class Real(Dimension):
    def __init__(self, low, high, prior="uniform", transform=None):
                self.low = low
        self.high = high
        self.prior = prior
        if transform is None:
            transform = "identity"
        self.transform_ = transform
        if self.transform_ not in ["normalize", "identity"]:
            raise ValueError(
                "transform should be 'normalize' or 'identity' got %s" %
                self.transform_)
                                        if self.transform_ == "normalize":
                                    self._rvs = _uniform_inclusive(0., 1.)
            if self.prior == "uniform":
                self.transformer = Pipeline(
                    [Identity(), Normalize(low, high)])
            else:
                self.transformer = Pipeline(
                    [Log10(), Normalize(np.log10(low), np.log10(high))]
                )
        else:
            if self.prior == "uniform":
                self._rvs = _uniform_inclusive(self.low, self.high - self.low)
                self.transformer = Identity()
            else:
                self._rvs = _uniform_inclusive(
                    np.log10(self.low),
                    np.log10(self.high) - np.log10(self.low))
                self.transformer = Log10()
    def __eq__(self, other):
        return (type(self) is type(other) and
                np.allclose([self.low], [other.low]) and
                np.allclose([self.high], [other.high]) and
                self.prior == other.prior and
                self.transform_ == other.transform_)
    def __repr__(self):
        return "Real(low={}, high={}, prior={}, transform={})".format(
            self.low, self.high, self.prior, self.transform_)
    def inverse_transform(self, Xt):
                return super(Real, self).inverse_transform(Xt).astype(np.float)
    @property
    def bounds(self):
        return (self.low, self.high)
    def __contains__(self, point):
        return self.low <= point <= self.high
    @property
    def transformed_bounds(self):
        if self.transform_ == "normalize":
            return 0.0, 1.0
        else:
            if self.prior == "uniform":
                return self.low, self.high
            else:
                return np.log10(self.low), np.log10(self.high)

class Integer(Dimension):
    def __init__(self, low, high, transform=None):
                self.low = low
        self.high = high
        if transform is None:
            transform = "identity"
        self.transform_ = transform
        if transform not in ["normalize", "identity"]:
            raise ValueError(
                "transform should be 'normalize' or 'identity' got %s" %
                self.transform_)
        if transform == "normalize":
            self._rvs = uniform(0, 1)
            self.transformer = Normalize(low, high, is_int=True)
        else:
            self._rvs = randint(self.low, self.high + 1)
            self.transformer = Identity()
    def __eq__(self, other):
        return (type(self) is type(other) and
                np.allclose([self.low], [other.low]) and
                np.allclose([self.high], [other.high]))
    def __repr__(self):
        return "Integer(low={}, high={})".format(self.low, self.high)
    def inverse_transform(self, Xt):
                                return super(Integer, self).inverse_transform(Xt).astype(np.int)
    @property
    def bounds(self):
        return (self.low, self.high)
    def __contains__(self, point):
        return self.low <= point <= self.high
    @property
    def transformed_bounds(self):
        if self.transform_ == "normalize":
            return 0, 1
        else:
            return (self.low, self.high)

class Categorical(Dimension):
    def __init__(self, categories, prior=None, transform=None):
                self.categories = categories
        if transform is None:
            transform = "onehot"
        self.transform_ = transform
        if transform not in ["identity", "onehot"]:
            raise ValueError("Expected transform to be 'identity' or 'onehot' "
                             "got %s" % transform)
        if transform == "onehot":
            self.transformer = CategoricalEncoder()
            self.transformer.fit(self.categories)
        else:
            self.transformer = Identity()
        self.prior = prior
        if prior is None:
            self.prior_ = np.tile(1. / len(self.categories),
                                  len(self.categories))
        else:
            self.prior_ = prior
                self._rvs = rv_discrete(
            values=(range(len(self.categories)), self.prior_)
            )
    def __eq__(self, other):
        return (type(self) is type(other) and
                self.categories == other.categories and
                np.allclose(self.prior_, other.prior_))
    def __repr__(self):
        if len(self.categories) > 7:
            cats = self.categories[:3] + [_Ellipsis()] + self.categories[-3:]
        else:
            cats = self.categories
        if self.prior is not None and len(self.prior) > 7:
            prior = self.prior[:3] + [_Ellipsis()] + self.prior[-3:]
        else:
            prior = self.prior
        return "Categorical(categories={}, prior={})".format(
            cats, prior)
    def rvs(self, n_samples=None, random_state=None):
        choices = self._rvs.rvs(size=n_samples, random_state=random_state)
        if isinstance(choices, numbers.Integral):
            return self.categories[choices]
        else:
            return [self.categories[c] for c in choices]
    @property
    def transformed_size(self):
        if self.transform_ == "onehot":
            size = len(self.categories)
                                    return size if size != 2 else 1
        return 1
    @property
    def bounds(self):
        return self.categories
    def __contains__(self, point):
        return point in self.categories
    @property
    def transformed_bounds(self):
        if self.transformed_size == 1:
            return (0.0, 1.0)
        else:
            return [(0.0, 1.0) for i in range(self.transformed_size)]

class Space:
    
    def __init__(self, dimensions):
                self.dimensions = [check_dimension(dim) for dim in dimensions]
    def __eq__(self, other):
        return all([a == b for a, b in zip(self.dimensions, other.dimensions)])
    def __repr__(self):
        if len(self.dimensions) > 31:
            dims = self.dimensions[:15] + [_Ellipsis()] + self.dimensions[-15:]
        else:
            dims = self.dimensions
        return "Space([{}])".format(
            ',\n       '.join(map(str, dims)))
    @property
    def is_real(self):
                return all([isinstance(dim, Real) for dim in self.dimensions])
    def rvs(self, n_samples=1, random_state=None):
                rng = check_random_state(random_state)
                columns = []
        for dim in self.dimensions:
            if sp_version < (0, 16):
                columns.append(dim.rvs(n_samples=n_samples))
            else:
                columns.append(dim.rvs(n_samples=n_samples, random_state=rng))
                rows = []
        for i in range(n_samples):
            r = []
            for j in range(self.n_dims):
                r.append(columns[j][i])
            rows.append(r)
        return rows
    def transform(self, X):
                        columns = []
        for dim in self.dimensions:
            columns.append([])
        for i in range(len(X)):
            for j in range(self.n_dims):
                columns[j].append(X[i][j])
                for j in range(self.n_dims):
            columns[j] = self.dimensions[j].transform(columns[j])
                Xt = np.hstack([np.asarray(c).reshape((len(X), -1)) for c in columns])
        return Xt
    def inverse_transform(self, Xt):
                        columns = []
        start = 0
        for j in range(self.n_dims):
            dim = self.dimensions[j]
            offset = dim.transformed_size
            if offset == 1:
                columns.append(dim.inverse_transform(Xt[:, start]))
            else:
                columns.append(
                    dim.inverse_transform(Xt[:, start:start+offset]))
            start += offset
                rows = []
        for i in range(len(Xt)):
            r = []
            for j in range(self.n_dims):
                r.append(columns[j][i])
            rows.append(r)
        return rows
    @property
    def n_dims(self):
                return len(self.dimensions)
    @property
    def transformed_n_dims(self):
                return sum([dim.transformed_size for dim in self.dimensions])
    @property
    def bounds(self):
                b = []
        for dim in self.dimensions:
            if dim.size == 1:
                b.append(dim.bounds)
            else:
                b.extend(dim.bounds)
        return b
    def __contains__(self, point):
                for component, dim in zip(point, self.dimensions):
            if component not in dim:
                return False
        return True
    @property
    def transformed_bounds(self):
                b = []
        for dim in self.dimensions:
            if dim.transformed_size == 1:
                b.append(dim.transformed_bounds)
            else:
                b.extend(dim.transformed_bounds)
        return b
    @property
    def is_categorical(self):
        return all([isinstance(dim, Categorical) for dim in self.dimensions])
import numpy as np
from sklearn.preprocessing import LabelBinarizer

class Transformer(object):
    def fit(self, X):
        return self
    def transform(self, X):
        raise NotImplementedError
    def inverse_transform(self, X):
        raise NotImplementedError

class Identity(Transformer):
    
    def transform(self, X):
        return X
    def inverse_transform(self, Xt):
        return Xt

class Log10(Transformer):
    
    def transform(self, X):
        return np.log10(np.asarray(X, dtype=np.float))
    def inverse_transform(self, Xt):
        return 10.0 ** np.asarray(Xt, dtype=np.float)

class CategoricalEncoder(Transformer):
    
    def __init__(self):
                self._lb = LabelBinarizer()
    def fit(self, X):
                self.mapping_ = {v: i for i, v in enumerate(X)}
        self.inverse_mapping_ = {i: v for v, i in self.mapping_.items()}
        self._lb.fit([self.mapping_[v] for v in X])
        self.n_classes = len(self._lb.classes_)
        return self
    def transform(self, X):
                return self._lb.transform([self.mapping_[v] for v in X])
    def inverse_transform(self, Xt):
                Xt = np.asarray(Xt)
        return [
            self.inverse_mapping_[i] for i in self._lb.inverse_transform(Xt)
        ]

class Normalize(Transformer):
        def __init__(self, low, high, is_int=False):
        self.low = low
        self.high = high
        self.is_int = is_int
    def transform(self, X):
        X = np.asarray(X)
        if np.any(X > self.high):
            raise ValueError("All values should be less than %f" % self.high)
        if np.any(X < self.low):
            raise ValueError("All values should be greater than %f" % self.low)
        return (X - self.low) / (self.high - self.low)
    def inverse_transform(self, X):
        X = np.asarray(X)
        if np.any(X > 1.0):
            raise ValueError("All values should be less than 1.0")
        if np.any(X < 0.0):
            raise ValueError("All values should be greater than 0.0")
        X_orig = X * (self.high - self.low) + self.low
        if self.is_int:
            return np.round(X_orig).astype(np.int)
        return X_orig

class Pipeline(Transformer):
        def __init__(self, transformers):
        self.transformers = list(transformers)
        for transformer in self.transformers:
            if not isinstance(transformer, Transformer):
                raise ValueError(
                    "Provided transformers should be a Transformer "
                    "instance. Got %s" % transformer
                )
    def fit(self, X):
        for transformer in self.transformers:
            transformer.fit(X)
        return self
    def transform(self, X):
        for transformer in self.transformers:
            X = transformer.transform(X)
        return X
    def inverse_transform(self, X):
        for transformer in self.transformers[::-1]:
            X = transformer.inverse_transform(X)
        return X
from .space import *
from __future__ import print_function
from setuptools import setup, find_packages
from setuptools.command.test import test as TestCommand
import io
import codecs
import os
import sys
import scikitplot
here = os.path.abspath(os.path.dirname(__file__))

def read(*filenames, **kwargs):
    encoding = kwargs.get('encoding', 'utf-8')
    sep = kwargs.get('sep', '\n')
    buf = []
    for filename in filenames:
        with io.open(filename, encoding=encoding) as f:
            buf.append(f.read())
    return sep.join(buf)
long_description = read('README.md')

class PyTest(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True
    def run_tests(self):
        import pytest
        errcode = pytest.main(self.test_args)
        sys.exit(errcode)
setup(
    name='scikit-plot',
    version=scikitplot.__version__,
    url='https://github.com/reiinakano/scikit-plot',
    license='MIT License',
    author='Reiichiro Nakano',
    tests_require=['pytest'],
    install_requires=[
        'matplotlib>=1.3.1',
        'scikit-learn>=0.18',
        'scipy>=0.9'
    ],
    cmdclass={'test': PyTest},
    author_email='reiichiro.s.nakano@gmail.com',
    description='An intuitive library to add plotting functionality to scikit-learn objects.',
    long_description=long_description,
    packages=['scikitplot'],
    include_package_data=True,
    platforms='any',
    test_suite='scikitplot.tests.test_scikitplot',
    classifiers = [
        'Programming Language :: Python',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Natural Language :: English',
        'Intended Audience :: Developers',
        'Intended Audience :: Science/Research',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
        'Topic :: Scientific/Engineering :: Visualization',
        ],
    extras_require={
        'testing': ['pytest'],
    }
)
import os
import sys
sys.path.insert(0, os.path.abspath('../'))

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.doctest',
    'sphinx.ext.napoleon'
]
templates_path = ['_templates']
source_suffix = '.rst'
master_doc = 'index'
project = u'Scikit-plot'
copyright = u'2017, Reiichiro S. Nakano'
author = u'Reiichiro S. Nakano'
version = u''
release = u''
language = None
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']
pygments_style = 'sphinx'
todo_include_todos = False

html_theme = 'sphinx_rtd_theme'

html_static_path = ['_static']

htmlhelp_basename = 'Scikit-plotdoc'

latex_elements = {
            
            
            
            }
latex_documents = [
    (master_doc, 'Scikit-plot.tex', u'Scikit-plot Documentation',
     u'Reiichiro S. Nakano', 'manual'),
]

man_pages = [
    (master_doc, 'scikit-plot', u'Scikit-plot Documentation',
     [author], 1)
]

texinfo_documents = [
    (master_doc, 'Scikit-plot', u'Scikit-plot Documentation',
     author, 'Scikit-plot', 'One line description of project.',
     'Miscellaneous'),
]

from sklearn.decomposition import PCA
from sklearn.datasets import load_digits as load_data
import scikitplot.plotters as skplt
import matplotlib.pyplot as plt

X, y = load_data(return_X_y=True)
pca = PCA(random_state=1)
pca.fit(X)
skplt.plot_pca_component_variance(pca)
plt.show()
    y = np.array(y)
    if not do_cv:
        y_pred = clf.predict(X)
        y_true = y
    else:
        if cv is None:
            cv = StratifiedKFold(shuffle=shuffle, random_state=random_state)
        elif isinstance(cv, int):
            cv = StratifiedKFold(n_splits=cv, shuffle=shuffle, random_state=random_state)
        else:
            pass
        clf_clone = clone(clf)
        preds_list = []
        trues_list = []
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
            clf_clone.fit(X_train, y_train)
            preds = clf_clone.predict(X_test)
            preds_list.append(preds)
            trues_list.append(y_test)
        y_pred = np.concatenate(preds_list)
        y_true = np.concatenate(trues_list)
    ax = plotters.plot_confusion_matrix(y_true=y_true, y_pred=y_pred,
                                        title=title, normalize=normalize, ax=ax, figsize=figsize, 
                                        title_fontsize=title_fontsize, text_fontsize=text_fontsize)
    return ax

def plot_roc_curve(clf, X, y, title='ROC Curves', do_cv=True, cv=None,
                   shuffle=True, random_state=None, curves=('micro', 'macro', 'each_class'),
                   ax=None, figsize=None, title_fontsize="large", text_fontsize="medium"):
        y = np.array(y)
    if not hasattr(clf, 'predict_proba'):
        raise TypeError('"predict_proba" method not in classifier. Cannot calculate ROC Curve.')
    if not do_cv:
        probas = clf.predict_proba(X)
        y_true = y
    else:
        if cv is None:
            cv = StratifiedKFold(shuffle=shuffle, random_state=random_state)
        elif isinstance(cv, int):
            cv = StratifiedKFold(n_splits=cv, shuffle=shuffle, random_state=random_state)
        else:
            pass
        clf_clone = clone(clf)
        preds_list = []
        trues_list = []
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
            clf_clone.fit(X_train, y_train)
            preds = clf_clone.predict_proba(X_test)
            preds_list.append(preds)
            trues_list.append(y_test)
        probas = np.concatenate(preds_list, axis=0)
        y_true = np.concatenate(trues_list)
        ax = plotters.plot_roc_curve(y_true=y_true, y_probas=probas, title=title, curves=curves, 
                                 ax=ax, figsize=figsize, title_fontsize=title_fontsize,
                                 text_fontsize=text_fontsize)
    return ax

def plot_ks_statistic(clf, X, y, title='KS Statistic Plot', do_cv=True, cv=None,
                      shuffle=True, random_state=None, ax=None, figsize=None,
                      title_fontsize="large", text_fontsize="medium"):
        y = np.array(y)
    if not hasattr(clf, 'predict_proba'):
        raise TypeError('"predict_proba" method not in classifier. Cannot calculate ROC Curve.')
    if not do_cv:
        probas = clf.predict_proba(X)
        y_true = y
    else:
        if cv is None:
            cv = StratifiedKFold(shuffle=shuffle, random_state=random_state)
        elif isinstance(cv, int):
            cv = StratifiedKFold(n_splits=cv, shuffle=shuffle, random_state=random_state)
        else:
            pass
        clf_clone = clone(clf)
        preds_list = []
        trues_list = []
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
            clf_clone.fit(X_train, y_train)
            preds = clf_clone.predict_proba(X_test)
            preds_list.append(preds)
            trues_list.append(y_test)
        probas = np.concatenate(preds_list, axis=0)
        y_true = np.concatenate(trues_list)
    ax = plotters.plot_ks_statistic(y_true, probas, title=title, ax=ax, figsize=figsize,
                                    title_fontsize=title_fontsize, text_fontsize=text_fontsize)
    return ax

def plot_precision_recall_curve(clf, X, y, title='Precision-Recall Curve', do_cv=True,
                                cv=None, shuffle=True, random_state=None, curves=('micro', 'each_class'),
                                ax=None, figsize=None, title_fontsize="large", text_fontsize="medium"):
        y = np.array(y)
    if not hasattr(clf, 'predict_proba'):
        raise TypeError('"predict_proba" method not in classifier. '
                        'Cannot calculate Precision-Recall Curve.')
    if not do_cv:
        probas = clf.predict_proba(X)
        y_true = y
    else:
        if cv is None:
            cv = StratifiedKFold(shuffle=shuffle, random_state=random_state)
        elif isinstance(cv, int):
            cv = StratifiedKFold(n_splits=cv, shuffle=shuffle, random_state=random_state)
        else:
            pass
        clf_clone = clone(clf)
        preds_list = []
        trues_list = []
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
            clf_clone.fit(X_train, y_train)
            preds = clf_clone.predict_proba(X_test)
            preds_list.append(preds)
            trues_list.append(y_test)
        probas = np.concatenate(preds_list, axis=0)
        y_true = np.concatenate(trues_list)
        ax = plotters.plot_precision_recall_curve(y_true, probas, title=title, curves=curves, ax=ax,
                                              figsize=figsize, title_fontsize=title_fontsize,
                                              text_fontsize=text_fontsize)
    return ax
from __future__ import absolute_import, division, print_function, unicode_literals
import six
import warnings
import types
from scikitplot.plotters import plot_silhouette, plot_elbow_curve

def clustering_factory(clf):
        required_methods = ['fit', 'fit_predict']
    for method in required_methods:
        if not hasattr(clf, method):
            raise TypeError('"{}" is not in clf. Did you pass a clusterer instance?'.format(method))
    additional_methods = {
        'plot_silhouette': plot_silhouette,
        'plot_elbow_curve': plot_elbow_curve
    }
    for key, fn in six.iteritems(additional_methods):
        if hasattr(clf, key):
            warnings.warn('"{}" method already in clf. '
                          'Overriding anyway. This may result in unintended behavior.'.format(key))
        setattr(clf, key, types.MethodType(fn, clf))
    return clf
from __future__ import absolute_import, division, print_function, unicode_literals
import numpy as np
from sklearn.preprocessing import LabelEncoder

def binary_ks_curve(y_true, y_probas):
        y_true, y_probas = np.asarray(y_true), np.asarray(y_probas)
    lb = LabelEncoder()
    encoded_labels = lb.fit_transform(y_true)
    if len(lb.classes_) != 2:
        raise ValueError('Cannot calculate KS statistic for data with '
                         '{} category/ies'.format(len(lb.classes_)))
    idx = encoded_labels == 0
    data1 = np.sort(y_probas[idx])
    data2 = np.sort(y_probas[-idx])
    ctr1, ctr2 = 0, 0
    thresholds, pct1, pct2 = [], [], []
    while ctr1 < len(data1) or ctr2 < len(data2):
                if ctr1 >= len(data1):
            current = data2[ctr2]
            while ctr2 < len(data2) and current == data2[ctr2]:
                ctr2 += 1
                elif ctr2 >= len(data2):
            current = data1[ctr1]
            while ctr1 < len(data1) and current == data1[ctr1]:
                ctr1 += 1
        else:
            if data1[ctr1] > data2[ctr2]:
                current = data2[ctr2]
                while ctr2 < len(data2) and current == data2[ctr2]:
                    ctr2 += 1
            elif data1[ctr1] < data2[ctr2]:
                current = data1[ctr1]
                while ctr1 < len(data1) and current == data1[ctr1]:
                    ctr1 += 1
            else:
                current = data2[ctr2]
                while ctr2 < len(data2) and current == data2[ctr2]:
                    ctr2 += 1
                while ctr1 < len(data1) and current == data1[ctr1]:
                    ctr1 += 1
        thresholds.append(current)
        pct1.append(ctr1)
        pct2.append(ctr2)
    thresholds = np.asarray(thresholds)
    pct1 = np.asarray(pct1) / float(len(data1))
    pct2 = np.asarray(pct2) / float(len(data2))
    if thresholds[0] != 0:
        thresholds = np.insert(thresholds, 0, [0.0])
        pct1 = np.insert(pct1, 0, [0.0])
        pct2 = np.insert(pct2, 0, [0.0])
    if thresholds[-1] != 1:
        thresholds = np.append(thresholds, [1.0])
        pct1 = np.append(pct1, [1.0])
        pct2 = np.append(pct2, [1.0])
    differences = pct1 - pct2
    ks_statistic, max_distance_at = np.max(differences), thresholds[np.argmax(differences)]
    return thresholds, pct1, pct2, ks_statistic, max_distance_at, lb.classes_
from __future__ import absolute_import, division, print_function, unicode_literals
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.model_selection import learning_curve
from scipy import interp
import itertools
from scikitplot.helpers import binary_ks_curve
from sklearn.base import clone
from sklearn.metrics import silhouette_score
from sklearn.metrics import silhouette_samples
from scipy.spatial.distance import cdist, pdist

def plot_confusion_matrix(y_true, y_pred, title=None, normalize=False, ax=None, figsize=None, 
                          title_fontsize="large", text_fontsize="medium"):
        if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    cm = confusion_matrix(y_true, y_pred)
    classes = np.unique(y_true)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm = np.around(cm, decimals=2)
    if title:
        ax.set_title(title, fontsize=title_fontsize)
    elif normalize:
        ax.set_title('Normalized Confusion Matrix', fontsize=title_fontsize)
    else:
        ax.set_title('Confusion Matrix', fontsize=title_fontsize)
    image = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.colorbar(mappable=image)
    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks)
    ax.set_xticklabels(classes, fontsize=text_fontsize)
    ax.set_yticks(tick_marks)
    ax.set_yticklabels(classes, fontsize=text_fontsize)
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax.text(j, i, cm[i, j],
                horizontalalignment="center",
                verticalalignment="center",
                fontsize=text_fontsize,
                color="white" if cm[i, j] > thresh else "black")
    ax.set_ylabel('True label', fontsize=text_fontsize)
    ax.set_xlabel('Predicted label', fontsize=text_fontsize)
    return ax

def plot_roc_curve(y_true, y_probas, title='ROC Curves', curves=('micro', 'macro', 'each_class'),
                   ax=None, figsize=None, title_fontsize="large", text_fontsize="medium"):
    
    if 'micro' not in curves and 'macro' not in curves and 'each_class' not in curves:
        raise ValueError('Invalid argument for curves as it only takes "micro", "macro", or "each_class"')
    classes = np.unique(y_true)
    probas = y_probas
    
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(y_true, probas[:, i], pos_label=classes[i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        micro_key = 'micro'
    i = 0
    while micro_key in fpr:
        i += 1
        micro_key += str(i)
    y_true = label_binarize(y_true, classes=classes)
    if len(classes) == 2:
        y_true = np.hstack((1 - y_true, y_true))
    fpr[micro_key], tpr[micro_key], _ = roc_curve(y_true.ravel(), probas.ravel())
    roc_auc[micro_key] = auc(fpr[micro_key], tpr[micro_key])
    
        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classes))]))
        mean_tpr = np.zeros_like(all_fpr)
    for i in range(len(classes)):
        mean_tpr += interp(all_fpr, fpr[i], tpr[i])
        mean_tpr /= len(classes)
    macro_key = 'macro'
    i = 0
    while macro_key in fpr:
        i += 1
        macro_key += str(i)
    fpr[macro_key] = all_fpr
    tpr[macro_key] = mean_tpr
    roc_auc[macro_key] = auc(fpr[macro_key], tpr[macro_key])
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_title(title, fontsize=title_fontsize)
    if 'each_class' in curves:
        for i in range(len(classes)):
            ax.plot(fpr[i], tpr[i], lw=2,
                    label='ROC curve of class {0} (area = {1:0.2f})'
                    ''.format(classes[i], roc_auc[i]))
        
    if 'micro' in curves:
        ax.plot(fpr[micro_key], tpr[micro_key],
                label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[micro_key]),
                color='deeppink', linestyle=':', linewidth=4)
      
    if 'macro' in curves:
        ax.plot(fpr[macro_key], tpr[macro_key],
                label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[macro_key]),
                color='navy', linestyle=':', linewidth=4)
    ax.plot([0, 1], [0, 1], 'k--', lw=2)
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate', fontsize=text_fontsize)
    ax.set_ylabel('True Positive Rate', fontsize=text_fontsize)
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc='lower right', fontsize=text_fontsize)
    return ax

def plot_ks_statistic(y_true, y_probas, title='KS Statistic Plot', ax=None, figsize=None,
                      title_fontsize="large", text_fontsize="medium"):
        classes = np.unique(y_true)
    if len(classes) != 2:
        raise ValueError('Cannot calculate KS statistic for data with '
                         '{} category/ies'.format(len(classes)))
    probas = y_probas
        thresholds, pct1, pct2, ks_statistic, \
        max_distance_at, classes = binary_ks_curve(y_true, probas[:, 1].ravel())
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_title(title, fontsize=title_fontsize)
    ax.plot(thresholds, pct1, lw=3, label='Class {}'.format(classes[0]))
    ax.plot(thresholds, pct2, lw=3, label='Class {}'.format(classes[1]))
    idx = np.where(thresholds == max_distance_at)[0][0]
    ax.axvline(max_distance_at, *sorted([pct1[idx], pct2[idx]]),
               label='KS Statistic: {:.3f} at {:.3f}'.format(ks_statistic, max_distance_at),
               linestyle=':', lw=3, color='black')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.0])
    ax.set_xlabel('Threshold', fontsize=text_fontsize)
    ax.set_ylabel('Percentage below threshold', fontsize=text_fontsize)
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc='lower right', fontsize=text_fontsize)
    return ax

def plot_precision_recall_curve(y_true, y_probas, title='Precision-Recall Curve',
                                curves=('micro', 'each_class'), ax=None,
                                figsize=None, title_fontsize="large", text_fontsize="medium"):
        classes = np.unique(y_true)
    probas = y_probas
    if 'micro' not in curves and 'each_class' not in curves:
        raise ValueError('Invalid argument for curves as it only takes "micro" or "each_class"')
        precision = dict()
    recall = dict()
    average_precision = dict()
    for i in range(len(classes)):
        precision[i], recall[i], _ = precision_recall_curve(y_true, probas[:, i],
                                                            pos_label=classes[i])
    y_true = label_binarize(y_true, classes=classes)
    if len(classes) == 2:
        y_true = np.hstack((1 - y_true, y_true))
    for i in range(len(classes)):
        average_precision[i] = average_precision_score(y_true[:, i], probas[:, i])
        micro_key = 'micro'
    i = 0
    while micro_key in precision:
        i += 1
        micro_key += str(i)
    precision[micro_key], recall[micro_key], _ = precision_recall_curve(y_true.ravel(),
                                                                        probas.ravel())
    average_precision[micro_key] = average_precision_score(y_true, probas, average='micro')
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_title(title, fontsize=title_fontsize)
    if 'each_class' in curves:
        for i in range(len(classes)):
            ax.plot(recall[i], precision[i], lw=2,
                    label='Precision-recall curve of class {0} '
                          '(area = {1:0.3f})'.format(classes[i], average_precision[i]))
    if 'micro' in curves:
        ax.plot(recall[micro_key], precision[micro_key],
                label='micro-average Precision-recall curve '
                      '(area = {0:0.3f})'.format(average_precision[micro_key]),
                color='navy', linestyle=':', linewidth=4)
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Recall')
    ax.set_ylabel('Precision')
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc='best', fontsize=text_fontsize)
    return ax

def plot_feature_importances(clf, title='Feature Importance', feature_names=None,
                             max_num_features=20, order='descending', ax=None,
                             figsize=None, title_fontsize="large", text_fontsize="medium"):
        if not hasattr(clf, 'feature_importances_'):
        raise TypeError('"feature_importances_" attribute not in classifier. '
                        'Cannot plot feature importances.')
    importances = clf.feature_importances_
    if hasattr(clf, 'estimators_')\
            and isinstance(clf.estimators_, list)\
            and hasattr(clf.estimators_[0], 'feature_importances_'):
        std = np.std([tree.feature_importances_ for tree in clf.estimators_],
                     axis=0)
    else:
        std = None
    if order == 'descending':
        indices = np.argsort(importances)[::-1]
    elif order == 'ascending':
        indices = np.argsort(importances)
    elif order is None:
        indices = np.array(range(len(importances)))
    else:
        raise ValueError('Invalid argument {} for "order"'.format(order))
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    if feature_names is None:
        feature_names = indices
    else:
        feature_names = np.array(feature_names)[indices]
    max_num_features = min(max_num_features, len(importances))
    ax.set_title(title, fontsize=title_fontsize)
    if std is not None:
        ax.bar(range(max_num_features), importances[indices][:max_num_features], color='r',
               yerr=std[indices][:max_num_features], align='center')
    else:
        ax.bar(range(max_num_features), importances[indices][:max_num_features],
               color='r', align='center')
    ax.set_xticks(range(max_num_features))
    ax.set_xticklabels(feature_names[:max_num_features])
    ax.set_xlim([-1, max_num_features])
    ax.tick_params(labelsize=text_fontsize)
    return ax

def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None, train_sizes=None, n_jobs=1,
                        ax=None, figsize=None, title_fontsize="large", text_fontsize="medium"):
        if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    if train_sizes is None:
        train_sizes = np.linspace(.1, 1.0, 5)
    ax.set_title(title, fontsize=title_fontsize)
    ax.set_xlabel("Training examples", fontsize=text_fontsize)
    ax.set_ylabel("Score", fontsize=text_fontsize)
    train_sizes, train_scores, test_scores = learning_curve(
        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    ax.grid()
    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.1, color="r")
    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.1, color="g")
    ax.plot(train_sizes, train_scores_mean, 'o-', color="r",
            label="Training score")
    ax.plot(train_sizes, test_scores_mean, 'o-', color="g",
            label="Cross-validation score")
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc="best", fontsize=text_fontsize)
    return ax

def plot_silhouette(clf, X, title='Silhouette Analysis', metric='euclidean', copy=True, ax=None,
                    figsize=None, title_fontsize="large", text_fontsize="medium"):
        if copy:
        clf = clone(clf)
    cluster_labels = clf.fit_predict(X)
    n_clusters = len(set(cluster_labels))
    silhouette_avg = silhouette_score(X, cluster_labels, metric=metric)
    sample_silhouette_values = silhouette_samples(X, cluster_labels, metric=metric)
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_title(title, fontsize=title_fontsize)
    ax.set_xlim([-0.1, 1])
    ax.set_ylim([0, len(X) + (n_clusters + 1) * 10 + 10])
    ax.set_xlabel('Silhouette coefficient values', fontsize=text_fontsize)
    ax.set_ylabel('Cluster label', fontsize=text_fontsize)
    y_lower = 10
    for i in range(n_clusters):
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
        color = cm.spectral(float(i) / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, ith_cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)
        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=text_fontsize)
        y_lower = y_upper + 10
    ax.axvline(x=silhouette_avg, color="red", linestyle="--",
               label='Silhouette score: {0:0.3f}'.format(silhouette_avg))
    ax.set_yticks([])      ax.set_xticks(np.arange(-0.1, 1.0, 0.2))
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc='best', fontsize=text_fontsize)
    return ax

def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, ax=None,
                     figsize=None, title_fontsize="large", text_fontsize="medium"):
        if cluster_ranges is None:
        cluster_ranges = range(1, 12, 2)
    else:
        cluster_ranges = sorted(cluster_ranges)
    if not hasattr(clf, 'n_clusters'):
        raise TypeError('"n_clusters" attribute not in classifier. '
                        'Cannot plot elbow method.')
    clfs = []
    for i in cluster_ranges:
        current_clf = clone(clf)
        setattr(current_clf, "n_clusters", i)
        clfs.append(current_clf.fit(X))
    centroids = [k.cluster_centers_ for k in clfs]
    D_k = [cdist(X, cent, 'euclidean') for cent in centroids]
    dist = [np.min(D, axis=1) for D in D_k]
    
    wcss = [np.sum(d**2) for d in dist]
    tss = np.sum(pdist(X)**2)/X.shape[0]
    bss = tss - wcss
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_title(title, fontsize=title_fontsize)
    ax.plot(cluster_ranges, bss/tss*100, 'b*-')
    ax.grid(True)
    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)
    ax.set_ylabel('Percent variance explained', fontsize=text_fontsize)
    ax.tick_params(labelsize=text_fontsize)
    return ax

def plot_pca_component_variance(clf, title='PCA Component Explained Variances',
                                target_explained_variance=0.75, ax=None, figsize=None,
                                title_fontsize="large", text_fontsize="medium"):
        if not hasattr(clf, 'explained_variance_ratio_'):
        raise TypeError('"clf" does not have explained_variance_ratio_ '
                        'attribute. Has the PCA been fitted?')
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_title(title, fontsize=title_fontsize)
    cumulative_sum_ratios = np.cumsum(clf.explained_variance_ratio_)
        idx = np.searchsorted(cumulative_sum_ratios, target_explained_variance)
    ax.plot(range(len(clf.explained_variance_ratio_) + 1),
            np.concatenate(([0], np.cumsum(clf.explained_variance_ratio_))), '*-')
    ax.grid(True)
    ax.set_xlabel('First n principal components', fontsize=text_fontsize)
    ax.set_ylabel('Explained variance ratio of first n components', fontsize=text_fontsize)
    ax.set_ylim([-0.02, 1.02])
    if idx < len(cumulative_sum_ratios):
        ax.plot(idx+1, cumulative_sum_ratios[idx], 'ro',
                label='{0:0.3f} Explained variance ratio for '
                'first {1} components'.format(cumulative_sum_ratios[idx], idx+1),
                markersize=4, markeredgewidth=4)
        ax.axhline(cumulative_sum_ratios[idx],
                   linestyle=':', lw=3, color='black')
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc="best", fontsize=text_fontsize)
    return ax

def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None, figsize=None,
                                title_fontsize="large", text_fontsize="medium"):
        transformed_X = clf.transform(X)
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_title(title, fontsize=title_fontsize)
    classes = np.unique(np.array(y))
    for label in classes:
        ax.scatter(transformed_X[y == label, 0], transformed_X[y == label, 1],
                   alpha=0.8, lw=2, label=label)
    ax.legend(loc='best', shadow=False, scatterpoints=1, fontsize=text_fontsize)
    ax.set_xlabel('First Principal Component', fontsize=text_fontsize)
    ax.set_ylabel('Second Principal Component', fontsize=text_fontsize)
    ax.tick_params(labelsize=text_fontsize)
    return ax
from __future__ import absolute_import, division, print_function, unicode_literals
__version__ = '0.2.3'

from scikitplot.classifiers import classifier_factory
from scikitplot.clustering import clustering_factory
descr = 
import os
import sys
DISTNAME = 'scikit-tensor'
DESCRIPTION = descr
with open(os.path.join(os.path.dirname(__file__), 'README.md')) as f:
    LONG_DESCRIPTION = f.read()
MAINTAINER = 'Maximilian Nickel',
MAINTAINER_EMAIL = 'mnick@mit.edu',
URL = 'http://github.com/mnick/scikit-tensor'
LICENSE = 'GPLv3'
DOWNLOAD_URL = URL
PACKAGE_NAME = 'sktensor'
EXTRA_INFO = dict(
    classifiers=[
        "Development Status :: 3 - Alpha",
        'Intended Audience :: Developers',
        'Intended Audience :: Science/Research',
        'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
        'Topic :: Scientific/Engineering',
        'Topic :: Software Development',
        'Operating System :: Microsoft :: Windows',
        'Operating System :: POSIX',
        'Operating System :: Unix',
        'Operating System :: MacOS',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.6',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.3',
    ]
)
try:
    import setuptools      EXTRA_INFO.update(dict(
        zip_safe=False,           include_package_data=True,
    ))
except:
    print('setuptools module not found.')
    print("Install setuptools if you want to enable 'python setup.py develop'.")

def configuration(parent_package='', top_path=None, package_name=DISTNAME):
    if os.path.exists('MANIFEST'):
        os.remove('MANIFEST')
    from numpy.distutils.misc_util import Configuration
    config = Configuration(None, parent_package, top_path)
        config.set_options(
        ignore_setup_xxx_py=True,
        assume_default_configuration=True,
        delegate_options_to_subpackages=True,
        quiet=True
    )
    config.add_subpackage(PACKAGE_NAME)
    return config

def get_version():
    
    __metaclass__ = ABCMeta
    def ttm(self, V, mode=None, transp=False, without=False):
                if mode is None:
            mode = range(self.ndim)
        if isinstance(V, np.ndarray):
            Y = self._ttm_compute(V, mode, transp)
        elif is_sequence(V):
            dims, vidx = check_multiplication_dims(mode, self.ndim, len(V), vidx=True, without=without)
            Y = self._ttm_compute(V[vidx[0]], dims[0], transp)
            for i in xrange(1, len(dims)):
                Y = Y._ttm_compute(V[vidx[i]], dims[i], transp)
        return Y
    def ttv(self, v, modes=[], without=False):
                if not isinstance(v, tuple):
            v = (v, )
        dims, vidx = check_multiplication_dims(modes, self.ndim, len(v), vidx=True, without=without)
        for i in range(len(dims)):
            if not len(v[vidx[i]]) == self.shape[dims[i]]:
                raise ValueError('Multiplicant is wrong size')
        remdims = np.setdiff1d(range(self.ndim), dims)
        return self._ttv_compute(v, dims, vidx, remdims)
            
    @abstractmethod
    def _ttm_compute(self, V, mode, transp):
        pass
    @abstractmethod
    def _ttv_compute(self, v, dims, vidx, remdims):
        pass
    @abstractmethod
    def unfold(self, rdims, cdims=None, transp=False):
        pass
    @abstractmethod
    def uttkrp(self, U, mode):
                pass
    @abstractmethod
    def transpose(self, axes=None):
                pass

def istensor(X):
    return isinstance(X, tensor_mixin)

conv_funcs = [
    'norm',
    'transpose',
    'ttm',
    'ttv',
    'unfold',
]
for fname in conv_funcs:
    def call_on_me(obj, *args, **kwargs):
        if not istensor(obj):
            raise ValueError('%s() object must be tensor (%s)' % (fname, type(obj)))
        func = getattr(obj, fname)
        return func(*args, **kwargs)
    nfunc = types.FunctionType(
        func_attr(call_on_me, 'code'),
        {
            'getattr': getattr,
            'fname': fname,
            'istensor': istensor,
            'ValueError': ValueError,
            'type': type
        },
        name=fname,
        argdefs=func_attr(call_on_me, 'defaults'),
        closure=func_attr(call_on_me, 'closure')
    )
    setattr(sys.modules[__name__], fname, nfunc)

def check_multiplication_dims(dims, N, M, vidx=False, without=False):
    dims = array(dims, ndmin=1)
    if len(dims) == 0:
        dims = arange(N)
    if without:
        dims = setdiff1d(range(N), dims)
    if not np.in1d(dims, arange(N)).all():
        raise ValueError('Invalid dimensions')
    P = len(dims)
    sidx = np.argsort(dims)
    sdims = dims[sidx]
    if vidx:
        if M > N:
            raise ValueError('More multiplicants than dimensions')
        if M != N and M != P:
            raise ValueError('Invalid number of multiplicants')
        if P == M:
            vidx = sidx
        else:
            vidx = sdims
        return sdims, vidx
    else:
        return sdims

def innerprod(X, Y):
        return dot(X.flatten(), Y.flatten())

def nvecs(X, n, rank, do_flipsign=True, dtype=np.float):
        Xn = X.unfold(n)
    if issparse_mat(Xn):
        Xn = csr_matrix(Xn, dtype=dtype)
        Y = Xn.dot(Xn.T)
        _, U = eigsh(Y, rank, which='LM')
    else:
        Y = Xn.dot(Xn.T)
        N = Y.shape[0]
        _, U = eigh(Y, eigvals=(N - rank, N - 1))
                U = array(U[:, ::-1])
        if do_flipsign:
        U = flipsign(U)
    return U

def flipsign(U):
        midx = abs(U).argmax(axis=0)
    for i in range(U.shape[1]):
        if U[midx[i], i] < 0:
            U[:, i] = -U[:, i]
    return U

def center(X, n):
    Xn = unfold(X, n)
    N = Xn.shape[0]
    m = Xn.sum(axis=0) / N
    m = kron(m, ones((N, 1)))
    Xn = Xn - m
    return fold(Xn, n)

def center_matrix(X):
    m = X.mean(axis=0)
    return X - m

def scale(X, n):
    Xn = unfold(X, n)
    m = np.float_(np.sqrt((Xn ** 2).sum(axis=1)))
    m[m == 0] = 1
    for i in range(Xn.shape[0]):
        Xn[i, :] = Xn[i] / m[i]
    return fold(Xn, n, X.shape)

def khatrirao(A, reverse=False):
    
    if not isinstance(A, tuple):
        raise ValueError('A must be a tuple of array likes')
    N = A[0].shape[1]
    M = 1
    for i in range(len(A)):
        if A[i].ndim != 2:
            raise ValueError('A must be a tuple of matrices (A[%d].ndim = %d)' % (i, A[i].ndim))
        elif N != A[i].shape[1]:
            raise ValueError('All matrices must have same number of columns')
        M *= A[i].shape[0]
    matorder = arange(len(A))
    if reverse:
        matorder = matorder[::-1]
        P = np.zeros((M, N), dtype=A[0].dtype)
    for n in range(N):
        ab = A[matorder[0]][:, n]
        for j in range(1, len(matorder)):
            ab = np.kron(ab, A[matorder[j]][:, n])
        P[:, n] = ab
    return P

def teneye(dim, order):
        I = zeros(dim ** order)
    for f in range(dim):
        idd = f
        for i in range(1, order):
            idd = idd + dim ** (i - 1) * (f - 1)
        I[idd] = 1
    return I.reshape(ones(order) * dim)

def tvecmat(m, n):
    d = m * n
    i2 = arange(d).reshape(m, n).T.flatten()
    Tmn = zeros((d, d))
    Tmn[arange(d), i2] = 1
    return Tmn
                            
import logging
import time
import numpy as np
from numpy import array, dot, ones, sqrt
from scipy.linalg import pinv
from numpy.random import rand
from .core import nvecs, norm
from .ktensor import ktensor
_log = logging.getLogger('CP')
_DEF_MAXITER = 500
_DEF_INIT = 'nvecs'
_DEF_CONV = 1e-5
_DEF_FIT_METHOD = 'full'
_DEF_TYPE = np.float
__all__ = [
    'als',
    'opt',
    'wopt'
]

def als(X, rank, **kwargs):
    
        ainit = kwargs.pop('init', _DEF_INIT)
    maxiter = kwargs.pop('max_iter', _DEF_MAXITER)
    fit_method = kwargs.pop('fit_method', _DEF_FIT_METHOD)
    conv = kwargs.pop('conv', _DEF_CONV)
    dtype = kwargs.pop('dtype', _DEF_TYPE)
    if not len(kwargs) == 0:
        raise ValueError('Unknown keywords (%s)' % (kwargs.keys()))
    N = X.ndim
    normX = norm(X)
    U = _init(ainit, X, N, rank, dtype)
    fit = 0
    exectimes = []
    for itr in range(maxiter):
        tic = time.clock()
        fitold = fit
        for n in range(N):
            Unew = X.uttkrp(U, n)
            Y = ones((rank, rank), dtype=dtype)
            for i in (list(range(n)) + list(range(n + 1, N))):
                Y = Y * dot(U[i].T, U[i])
            Unew = Unew.dot(pinv(Y))
                        if itr == 0:
                lmbda = sqrt((Unew ** 2).sum(axis=0))
            else:
                lmbda = Unew.max(axis=0)
                lmbda[lmbda < 1] = 1
            U[n] = Unew / lmbda
        P = ktensor(U, lmbda)
        if fit_method == 'full':
            normresidual = normX ** 2 + P.norm() ** 2 - 2 * P.innerprod(X)
            fit = 1 - (normresidual / normX ** 2)
        else:
            fit = itr
        fitchange = abs(fitold - fit)
        exectimes.append(time.clock() - tic)
        _log.debug(
            '[%3d] fit: %.5f | delta: %7.1e | secs: %.5f' %
            (itr, fit, fitchange, exectimes[-1])
        )
        if itr > 0 and fitchange < conv:
            break
    return P, fit, itr, array(exectimes)

def opt(X, rank, **kwargs):
    ainit = kwargs.pop('init', _DEF_INIT)
    maxiter = kwargs.pop('maxIter', _DEF_MAXITER)
    conv = kwargs.pop('conv', _DEF_CONV)
    dtype = kwargs.pop('dtype', _DEF_TYPE)
    if not len(kwargs) == 0:
        raise ValueError('Unknown keywords (%s)' % (kwargs.keys()))
    N = X.ndim
    U = _init(ainit, X, N, rank, dtype)

def wopt(X, rank, **kwargs):
    raise NotImplementedError()

def _init(init, X, N, rank, dtype):
        Uinit = [None for _ in range(N)]
    if isinstance(init, list):
        Uinit = init
    elif init == 'random':
        for n in range(1, N):
            Uinit[n] = array(rand(X.shape[n], rank), dtype=dtype)
    elif init == 'nvecs':
        for n in range(1, N):
            Uinit[n] = array(nvecs(X, n, rank), dtype=dtype)
    else:
        raise 'Unknown option (init=%s)' % str(init)
    return Uinit

import logging
import time
import numpy as np
from numpy import dot, ones, zeros, diag, kron, outer, array, prod, eye
from numpy.linalg import norm, solve, eigvals
from numpy.random import rand
from scipy.linalg import qr
from scipy.sparse.linalg import eigsh
from scipy.optimize import fmin_l_bfgs_b, fmin_ncg, fmin_tnc
from scipy.sparse import issparse
_DEF_MAXITER = 500
_DEF_INIT = 'nvecs'
_DEF_PROJ = True
_DEF_CONV = 1e-5
_DEF_NNE = -1
_DEF_OPTFUNC = 'lbfgs'
_log = logging.getLogger('DEDICOM')
np.seterr(invalid='raise')

def asalsan(X, rank, **kwargs):
            ainit = kwargs.pop('init', _DEF_INIT)
    proj = kwargs.pop('proj', _DEF_PROJ)
    maxIter = kwargs.pop('maxIter', _DEF_MAXITER)
    conv = kwargs.pop('conv', _DEF_CONV)
    nne = kwargs.pop('nne', _DEF_NNE)
    optfunc = kwargs.pop('optfunc', _DEF_OPTFUNC)
    if not len(kwargs) == 0:
        raise BaseException('Unknown keywords (%s)' % (kwargs.keys()))
        D = ones((len(X), rank))
    sz = X[0].shape
    n = sz[0]
    R = rand(rank, rank)
    if ainit == 'random':
        A = rand(n, rank)
    elif ainit == 'nvecs':
        S = zeros((n, n))
        T = zeros((n, n))
        for i in range(len(X)):
            T = X[i]
            S = S + T + T.T
        evals, A = eigsh(S, rank)
        if nne > 0:
            A[A < 0] = 0
        if proj:
            Q, A2 = qr(A)
            X2 = __projectSlices(X, Q)
            R = __updateR(X2, A2, D, R, nne)
        else:
            R = __updateR(X, A, D, R, nne)
    elif isinstance(ainit, np.ndarray):
        A = ainit
    else:
        raise 'Unknown init option ("%s")' % ainit
        if issparse(X[0]):
        normX = [norm(M.data) ** 2 for M in X]
        Xflat = [M.tolil().reshape((1, prod(M.shape))).tocsr() for M in X]
    else:
        normX = [norm(M) ** 2 for M in X]
        Xflat = [M.flatten() for M in X]
    M = zeros((n, n))
    normXSum = sum(normX)
        fit = fitold = f = fitchange = 0
    exectimes = []
    for iters in xrange(maxIter):
        tic = time.clock()
        fitold = fit
        A = __updateA(X, A, D, R, nne)
        if proj:
            Q, A2 = qr(A)
            X2 = __projectSlices(X, Q)
            R = __updateR(X2, A2, D, R, nne)
            D, f = __updateD(X2, A2, D, R, nne, optfunc)
        else:
            R = __updateR(X, A, D, R, nne)
            D, f = __updateD(X, A, D, R, nne, optfunc)
                f = 0
        for i in xrange(len(X)):
            AD = dot(A, diag(D[i, :]))
            M = dot(dot(AD, R), AD.T)
            f += normX[i] + norm(M) ** 2 - 2 * Xflat[i].dot(M.flatten())
        f *= 0.5
        fit = 1 - (f / normXSum)
        fitchange = abs(fitold - fit)
        exectimes.append(time.clock() - tic)
                _log.debug('[%3d] fit: %.5f | delta: %7.1e | secs: %.5f' % (
            iters, fit, fitchange, exectimes[-1]
        ))
        if iters > 1 and fitchange < conv:
            break
    return A, R, D, fit, iters, array(exectimes)

def __updateA(X, A, D, R, nne):
    rank = A.shape[1]
    F = zeros((X[0].shape[0], rank))
    E = zeros((rank, rank))
    AtA = dot(A.T, A)
    for i in range(len(X)):
        Dk = diag(D[i, :])
        DRD = dot(Dk, dot(R, Dk))
        DRtD = DRD.T
        F += X[i].dot(dot(A, DRtD)) + X[i].T.dot(dot(A, DRD))
        E += dot(DRD, dot(AtA, DRtD)) + dot(DRtD, dot(AtA, DRD))
    if nne > 0:
        E = dot(A, E) + nne
        A = A * (F / E)
    else:
        A = solve(E.T, F.T).T
    return A

def __updateR(X, A, D, R, nne):
    r = A.shape[1] ** 2
    T = zeros((r, r))
    t = zeros(r)
    for i in range(len(X)):
        AD = dot(A, diag(D[i, :]))
        ADt = AD.T
        tmp = dot(ADt, AD)
        T = T + kron(tmp, tmp)
        tmp = dot(ADt, X[i].dot(AD))
        t = t + tmp.flatten()
    r = A.shape[1]
    if nne > 0:
        Rflat = R.flatten()
        T = dot(T, Rflat) + nne
        R = (Rflat * t / T).reshape(r, r)
    else:
                R = solve(T, t).reshape(r, r)
            return R

def __updateD(X, A, D, R, nne, optfunc):
    f = 0
    for i in range(len(X)):
        d = D[i, :]
        u = Updater(X[i], A, R)
        if nne > 0:
            bounds = len(d) * [(0, None)]
            res = fmin_l_bfgs_b(
                u.updateD_F, d, u.updateD_G, factr=1e12, bounds=bounds
            )
        else:
            if optfunc == 'lbfgs':
                res = fmin_l_bfgs_b(u.updateD_F, d, u.updateD_G, factr=1e12)
                D[i, :] = res[0]
                f += res[1]
            elif optfunc == 'ncg':
                res = fmin_ncg(
                    u.updateD_F, d, u.updateD_G, fhess=u.updateD_H,
                    full_output=True, disp=False
                )
                                raise NotImplementedError()
            elif optfunc == 'tnc':
                res = fmin_tnc(u.updateD_F, d, u.updateD_G, disp=False)
                                raise NotImplementedError()
    return D, f

class Updater:
    def __init__(self, Z, A, R):
        self.Z = Z
        self.A = A
        self.R = R
        self.x = None
    def precompute(self, x, cache=True):
        if not cache or self.x is None or (x != self.x).any():
            self.AD = dot(self.A, diag(x))
            self.ADt = self.AD.T
            self.E = self.Z - dot(self.AD, dot(self.R, self.ADt))
    def updateD_F(self, x):
        self.precompute(x)
        return norm(self.E, 'fro') ** 2
    def updateD_G(self, x):
                self.precompute(x)
        g = zeros(len(x))
        Ai = zeros(self.A.shape[0])
        for i in range(len(g)):
            Ai = self.A[:, i]
            g[i] = (self.E * (dot(self.AD, outer(self.R[:, i], Ai)) +
                    dot(outer(Ai, self.R[i, :]), self.ADt))).sum()
        return -2 * g
    def updateD_H(self, x):
                self.precompute(x)
        H = zeros((len(x), len(x)))
        Ai = zeros(self.A.shape[0])
        Aj = zeros(Ai.shape)
        for i in range(len(x)):
            Ai = self.A[:, i]
            ti = dot(self.AD, outer(self.R[:, i], Ai)) + dot(outer(Ai, self.R[i, :]), self.ADt)
            for j in range(i, len(x)):
                Aj = self.A[:, j]
                tj = outer(Ai, Aj)
                H[i, j] = (
                    self.E * (self.R[i, j] * tj + self.R[j, i] * tj.T) -
                    ti * (
                        dot(self.AD, outer(self.R[:, j], Aj)) +
                        dot(outer(Aj, self.R[j, :]), self.ADt)
                    )
                ).sum()
                H[j, i] = H[i, j]
        H *= -2
        e = eigvals(H).min()
        H = H + (eye(H.shape[0]) * e)
        return H

def __projectSlices(X, Q):
    X2 = []
    for i in range(len(X)):
        X2.append(Q.T.dot(X[i].dot(Q)))
    return X2
import numpy as np
from numpy import array, prod, argsort
from .core import tensor_mixin, khatrirao
from .pyutils import inherit_docstring_from, from_to_without

__all__ = [
    'dtensor',
    'unfolded_dtensor',
]

class dtensor(tensor_mixin, np.ndarray):
    
    def __new__(cls, input_array):
        obj = np.asarray(input_array).view(cls)
        return obj
    def __array_wrap__(self, out_arr, context=None):
        return np.ndarray.__array_wrap__(self, out_arr, context)
    def __eq__(self, other):
        return np.equal(self, other)
    def _ttm_compute(self, V, mode, transp):
        sz = array(self.shape)
        r1, r2 = from_to_without(0, self.ndim, mode, separate=True)
                        order = [mode] + r1 + r2
        newT = self.transpose(axes=order)
        newT = newT.reshape(sz[mode], prod(sz[r1 + list(range(mode + 1, len(sz)))]))
        if transp:
            newT = V.T.dot(newT)
            p = V.shape[1]
        else:
            newT = V.dot(newT)
            p = V.shape[0]
        newsz = [p] + list(sz[:mode]) + list(sz[mode + 1:])
        newT = newT.reshape(newsz)
                newT = newT.transpose(argsort(order))
        return dtensor(newT)
    def _ttv_compute(self, v, dims, vidx, remdims):
                if not isinstance(v, tuple):
            raise ValueError('v must be a tuple of vectors')
        ndim = self.ndim
        order = list(remdims) + list(dims)
        if ndim > 1:
            T = self.transpose(order)
        sz = array(self.shape)[order]
        for i in np.arange(len(dims), 0, -1):
            T = T.reshape((sz[:ndim - 1].prod(), sz[ndim - 1]))
            T = T.dot(v[vidx[i - 1]])
            ndim -= 1
        if ndim > 0:
            T = T.reshape(sz[:ndim])
        return T
    def ttt(self, other, modes=None):
        pass
    def unfold(self, mode):
        
        sz = array(self.shape)
        N = len(sz)
        order = ([mode], from_to_without(N - 1, -1, mode, step=-1, skip=-1))
        newsz = (sz[order[0]][0], prod(sz[order[1]]))
        arr = self.transpose(axes=(order[0] + order[1]))
        arr = arr.reshape(newsz)
        return unfolded_dtensor(arr, mode, self.shape)
    def norm(self):
                return np.linalg.norm(self)
    @inherit_docstring_from(tensor_mixin)
    def uttkrp(self, U, n):
        order = list(range(n)) + list(range(n + 1, self.ndim))
        Z = khatrirao(tuple(U[i] for i in order), reverse=True)
        return self.unfold(n).dot(Z)
    @inherit_docstring_from(tensor_mixin)
    def transpose(self, axes=None):
        return dtensor(np.transpose(array(self), axes=axes))

class unfolded_dtensor(np.ndarray):
    def __new__(cls, input_array, mode, ten_shape):
        obj = np.asarray(input_array).view(cls)
        obj.ten_shape = ten_shape
        obj.mode = mode
        return obj
    def __array_finalize__(self, obj):
        if obj is None:
            return
        self.ten_shape = getattr(obj, 'ten_shape', None)
        self.mode = getattr(obj, 'mode', None)
    def fold(self):
        shape = array(self.ten_shape)
        N = len(shape)
        order = ([self.mode], from_to_without(0, N, self.mode, reverse=True))
        arr = self.reshape(tuple(shape[order[0]],) + tuple(shape[order[1]]))
        arr = np.transpose(arr, argsort(order[0] + order[1]))
        return dtensor(arr)
from numpy import zeros, dot, diag
from numpy.random import rand
from scipy.linalg import svd, norm, orth
from scipy.sparse.linalg import eigsh
import time
import logging
_log = logging.getLogger('INDSCAL')
_DEF_MAXITER = 50
_DEF_INIT = 'random'
_DEF_CONV = 1e-7

def orth_als(X, ncomp, **kwargs):
    ainit = kwargs.pop('init', _DEF_INIT)
    maxiter = kwargs.pop('max_iter', _DEF_MAXITER)
    conv = kwargs.pop('conv', _DEF_CONV)
    if not len(kwargs) == 0:
        raise ValueError('Unknown keywords (%s)' % (kwargs.keys()))
    K = len(X)
    normX = sum([norm(Xk)**2 for Xk in X])
    A = init(X, ainit, ncomp)
    fit = 0
    exectimes = []
    for itr in range(maxiter):
        tic = time.time()
        fitold = fit
        D = _updateD(X, A)
        A = _updateA(X, A, D)
        fit = sum([norm(X[k] - dot(A, dot(diag(D[k, :]), A.T)))**2 for k in range(K)])
        fit = 1 - fit / normX
        fitchange = abs(fitold - fit)
        exectimes.append(time.time() - tic)
        _log.info('[%3d] fit: %0.5f | delta: %7.1e | secs: %.5f' % (
            itr, fit, fitchange, exectimes[-1]
        ))
        if itr > 0 and fitchange < conv:
            break
    return A, D

def _updateA(X, A, D):
    G = zeros(A.shape)
    for k in range(len(X)):
        G = G + dot(X[k], dot(A, diag(D[k, :])))
    U, _, Vt = svd(G, full_matrices=0)
    A = dot(U, Vt)
    return A

def _updateD(X, A):
    K, R = len(X), A.shape[1]
    D = zeros((K, R))
    for k in range(K):
        D[k, :] = diag(dot(A.T, dot(X[k], A)))
    D[D < 0] = 0
    return D

def init(X, init, ncomp):
    N, K = X[0].shape[0], len(X)
    if init == 'random':
        A = orth(rand(N, ncomp))
    elif init == 'nvecs':
        S = zeros(N, N)
        for k in range(K):
            S = S + X[k] + X[k].T
        _, A = eigsh(S, ncomp)
    return A
import numpy as np
from numpy import dot, ones, array, outer, zeros, prod, sum
from sktensor.core import khatrirao, tensor_mixin
from sktensor.dtensor import dtensor
__all__ = [
    'ktensor',
    'vectorized_ktensor',
]

class ktensor(object):
    
    def __init__(self, U, lmbda=None):
        self.U = U
        self.shape = tuple(Ui.shape[0] for Ui in U)
        self.ndim = len(self.shape)
        self.rank = U[0].shape[1]
        self.lmbda = lmbda
        if not all(array([Ui.shape[1] for Ui in U]) == self.rank):
            raise ValueError('Dimension mismatch of factor matrices')
        if lmbda is None:
            self.lmbda = ones(self.rank)
    def __eq__(self, other):
        if isinstance(other, ktensor):
                        if self.ndim != other.ndim or self.shape != other.shape:
                return False
                        return all(
                [(self.U[i] == other.U[i]).all() for i in range(self.ndim)] +
                [(self.lmbda == other.lmbda).all()]
            )
        else:
                        raise NotImplementedError()
    def uttkrp(self, U, mode):
                N = self.ndim
        if mode == 1:
            R = U[1].shape[1]
        else:
            R = U[0].shape[1]
        W = np.tile(self.lmbda, 1, R)
        for i in range(mode) + range(mode + 1, N):
            W = W * dot(self.U[i].T, U[i])
        return dot(self.U[mode], W)
    def norm(self):
                N = len(self.shape)
        coef = outer(self.lmbda, self.lmbda)
        for i in range(N):
            coef = coef * dot(self.U[i].T, self.U[i])
        return np.sqrt(coef.sum())
    def innerprod(self, X):
                N = len(self.shape)
        R = len(self.lmbda)
        res = 0
        for r in range(R):
            vecs = []
            for n in range(N):
                vecs.append(self.U[n][:, r])
            res += self.lmbda[r] * X.ttv(tuple(vecs))
        return res
    def toarray(self):
                A = dot(self.lmbda, khatrirao(tuple(self.U)).T)
        return A.reshape(self.shape)
    def totensor(self):
                return dtensor(self.toarray())
    def tovec(self):
        v = zeros(sum([s * self.rank for s in self.shape]))
        offset = 0
        for M in self.U:
            noff = offset + prod(M.shape)
            v[offset:noff] = M.flatten()
            offset = noff
        return vectorized_ktensor(v, self.shape, self.lmbda)

class vectorized_ktensor(object):
    def __init__(self, v, shape, lmbda):
        self.v = v
        self.shape = shape
        self.lmbda = lmbda
    def toktensor(self):
        order = len(self.shape)
        rank = len(self.v) / sum(self.shape)
        U = [None for _ in range(order)]
        offset = 0
        for i in range(order):
            noff = offset + self.shape[i] * rank
            U[i] = self.v[offset:noff].reshape((self.shape[i], rank))
            offset = noff
        return ktensor(U, self.lmbda)
def inherit_docstring_from(cls):
    def docstring_inheriting_decorator(fn):
        fn.__doc__ = getattr(cls, fn.__name__).__doc__
        return fn
    return docstring_inheriting_decorator

def is_sequence(obj):
        try:
        from collections import Sequence
    except ImportError:
        from operator import isSequenceType
        return isSequenceType(obj)
    else:
        return isinstance(obj, Sequence)

def is_number(obj):
        try:
        from numbers import Number
    except ImportError:
        from operator import isNumberType
        return isNumberType(obj)
    else:
        return isinstance(obj, Number)

def func_attr(f, attr):
        if hasattr(f, 'func_%s' % attr):
        return getattr(f, 'func_%s' % attr)
    elif hasattr(f, '__%s__' % attr):
        return getattr(f, '__%s__' % attr)
    else:
        raise ValueError('Object %s has no attr' % (str(f), attr))

def from_to_without(frm, to, without, step=1, skip=1, reverse=False, separate=False):
        if reverse:
        frm, to = (to - 1), (frm - 1)
        step *= -1
        skip *= -1
    a = list(range(frm, without, step))
    b = list(range(without + skip, to, step))
    if separate:
        return a, b
    else:
        return a + b
import logging
import time
import numpy as np
from numpy import dot, zeros, array, eye, kron, prod
from numpy.linalg import norm, solve, inv, svd
from scipy.sparse import csr_matrix, issparse
from scipy.sparse.linalg import eigsh
from numpy.random import rand
__version__ = "0.5"
__all__ = ['als']
_DEF_MAXITER = 100
_DEF_INIT = 'nvecs'
_DEF_CONV = 1e-4
_DEF_LMBDA = 0
_DEF_ATTR = []
_DEF_NO_FIT = 1e9
_DEF_FIT_METHOD = None
_log = logging.getLogger('RESCAL')

def als(X, rank, **kwargs):
    
        ainit = kwargs.pop('init', _DEF_INIT)
    maxIter = kwargs.pop('maxIter', _DEF_MAXITER)
    conv = kwargs.pop('conv', _DEF_CONV)
    lmbdaA = kwargs.pop('lambda_A', _DEF_LMBDA)
    lmbdaR = kwargs.pop('lambda_R', _DEF_LMBDA)
    lmbdaV = kwargs.pop('lambda_V', _DEF_LMBDA)
    func_compute_fval = kwargs.pop('compute_fval', _DEF_FIT_METHOD)
    orthogonalize = kwargs.pop('orthogonalize', False)
    P = kwargs.pop('attr', _DEF_ATTR)
    dtype = kwargs.pop('dtype', np.float)
        if not len(kwargs) == 0:
        raise ValueError('Unknown keywords (%s)' % (kwargs.keys()))
        sz = X[0].shape
    for i in range(len(X)):
        if X[i].ndim != 2:
            raise ValueError('Frontal slices of X must be matrices')
        if X[i].shape != sz:
            raise ValueError('Frontal slices of X must be all of same shape')
                    
    if func_compute_fval is None:
        if orthogonalize:
            func_compute_fval = _compute_fval_orth
        elif prod(X[0].shape) * len(X) > _DEF_NO_FIT:
            _log.warn('For large tensors automatic computation of fit is disabled by default\nTo compute the fit, call rescal.als with "compute_fit=True"\nPlease note that this might cause memory and runtime problems')
            func_compute_fval = None
        else:
            func_compute_fval = _compute_fval
    n = sz[0]
    k = len(X)
    _log.debug(
        '[Config] rank: %d | maxIter: %d | conv: %7.1e | lmbda: %7.1e' %
        (rank, maxIter, conv, lmbdaA)
    )
    _log.debug('[Config] dtype: %s / %s' % (dtype, X[0].dtype))
        for i in range(k):
        if issparse(X[i]):
            X[i] = X[i].tocsr()
            X[i].sort_indices()
    for i in range(len(P)):
        if issparse(P[i]):
            P[i] = P[i].tocoo().tocsr()
            P[i].sort_indices()
        _log.debug('Initializing A')
    if ainit == 'random':
        A = array(rand(n, rank), dtype=dtype)
    elif ainit == 'nvecs':
        S = csr_matrix((n, n), dtype=dtype)
        for i in range(k):
            S = S + X[i]
            S = S + X[i].T
        _, A = eigsh(csr_matrix(S, dtype=dtype, shape=(n, n)), rank)
        A = array(A, dtype=dtype)
    else:
        raise ValueError('Unknown init option ("%s")' % ainit)
        R = _updateR(X, A, lmbdaR)
    Z = _updateZ(A, P, lmbdaV)
        normX = [sum(M.data ** 2) for M in X]
        fit = fitchange = fitold = f = 0
    exectimes = []
    for itr in range(maxIter):
        tic = time.time()
        fitold = fit
        A = _updateA(X, A, R, P, Z, lmbdaA, orthogonalize)
        R = _updateR(X, A, lmbdaR)
        Z = _updateZ(A, P, lmbdaV)
                if func_compute_fval is not None:
            fit = func_compute_fval(X, A, R, P, Z, lmbdaA, lmbdaR, lmbdaV, normX)
        else:
            fit = np.Inf
        fitchange = abs(fitold - fit)
        toc = time.time()
        exectimes.append(toc - tic)
        _log.debug('[%3d] fval: %0.5f | delta: %7.1e | secs: %.5f' % (
            itr, fit, fitchange, exectimes[-1]
        ))
        if itr > 0 and fitchange < conv:
            break
    return A, R, f, itr + 1, array(exectimes)

def _updateA(X, A, R, P, Z, lmbdaA, orthogonalize):
        n, rank = A.shape
    F = zeros((n, rank), dtype=A.dtype)
    E = zeros((rank, rank), dtype=A.dtype)
    AtA = dot(A.T, A)
    for i in range(len(X)):
        F += X[i].dot(dot(A, R[i].T)) + X[i].T.dot(dot(A, R[i]))
        E += dot(R[i], dot(AtA, R[i].T)) + dot(R[i].T, dot(AtA, R[i]))
        I = lmbdaA * eye(rank, dtype=A.dtype)
        for i in range(len(Z)):
        F += P[i].dot(Z[i].T)
        E += dot(Z[i], Z[i].T)
        A = solve(I + E.T, F.T).T
    return orth(A) if orthogonalize else A

def _updateR(X, A, lmbdaR):
    rank = A.shape[1]
    U, S, Vt = svd(A, full_matrices=False)
    Shat = kron(S, S)
    Shat = (Shat / (Shat ** 2 + lmbdaR)).reshape(rank, rank)
    R = []
    for i in range(len(X)):
        Rn = Shat * dot(U.T, X[i].dot(U))
        Rn = dot(Vt.T, dot(Rn, Vt))
        R.append(Rn)
    return R

def _updateZ(A, P, lmbdaZ):
    Z = []
    if len(P) == 0:
        return Z
        pinvAt = inv(dot(A.T, A) + lmbdaZ * eye(A.shape[1], dtype=A.dtype))
    pinvAt = dot(pinvAt, A.T).T
    for i in range(len(P)):
        if issparse(P[i]):
            Zn = P[i].tocoo().T.tocsr().dot(pinvAt).T
        else:
            Zn = dot(pinvAt.T, P[i])
        Z.append(Zn)
    return Z

def _compute_fval(X, A, R, P, Z, lmbdaA, lmbdaR, lmbdaZ, normX):
        f = lmbdaA * norm(A) ** 2
    for i in range(len(X)):
        ARAt = dot(A, dot(R[i], A.T))
        f += (norm(X[i] - ARAt) ** 2) / normX[i] + lmbdaR * norm(R[i]) ** 2
    return f

def _compute_fval_orth(X, A, R, P, Z, lmbdaA, lmbdaR, lmbdaZ, normX):
    f = lmbdaA * norm(A) ** 2
    for i in range(len(X)):
        f += (normX[i] - norm(R[i]) ** 2) / normX[i] + lmbdaR * norm(R[i]) ** 2
    return f

def sptensor_to_list(X):
    from scipy.sparse import lil_matrix
    if X.ndim != 3:
        raise ValueError('Only third-order tensors are supported (ndim=%d)' % X.ndim)
    if X.shape[0] != X.shape[1]:
        raise ValueError('First and second mode must be of identical length')
    N = X.shape[0]
    K = X.shape[2]
    res = [lil_matrix((N, N)) for _ in range(K)]
    for n in range(X.nnz()):
        res[X.subs[2][n]][X.subs[0][n], X.subs[1][n]] = X.vals[n]
    return res
def orth(A):
    [U, _, Vt] = svd(A, full_matrices=0)
    return dot(U, Vt)
def configuration(parent_package='', top_path=None):
    from numpy.distutils.misc_util import Configuration
    config = Configuration('sktensor', parent_package, top_path)
    config.add_subpackage('tests')
    return config
import numpy as np
from numpy import zeros, ones, array, arange, copy, ravel_multi_index, unravel_index
from numpy import setdiff1d, hstack, hsplit, vsplit, sort, prod, lexsort, unique, bincount
from scipy.sparse import coo_matrix
from scipy.sparse import issparse as issparse_mat
from sktensor.core import tensor_mixin
from sktensor.utils import accum
from sktensor.dtensor import unfolded_dtensor
from sktensor.pyutils import inherit_docstring_from, from_to_without

__all__ = [
    'concatenate',
    'fromarray',
    'sptensor',
    'unfolded_sptensor',
]

class sptensor(tensor_mixin):
    
    def __init__(self, subs, vals, shape=None, dtype=None, accumfun=None, issorted=False):
        if not isinstance(subs, tuple):
            raise ValueError('Subscripts must be a tuple of array-likes')
        if len(subs[0]) != len(vals):
            raise ValueError('Subscripts and values must be of equal length')
        if dtype is None:
            dtype = array(vals).dtype
        for i in range(len(subs)):
            if array(subs[i]).dtype.kind != 'i':
                raise ValueError('Subscripts must be integers')
        vals = array(vals, dtype=dtype)
        if accumfun is not None:
            vals, subs = accum(
                subs, vals,
                issorted=False, with_subs=True, func=accumfun
            )
        self.subs = subs
        self.vals = vals
        self.dtype = dtype
        self.issorted = issorted
        self.accumfun = accumfun
        if shape is None:
            self.shape = tuple(array(subs).max(axis=1).flatten() + 1)
        else:
            self.shape = tuple(int(d) for d in shape)
        self.ndim = len(subs)
    def __eq__(self, other):
        if isinstance(other, sptensor):
            self._sort()
            other._sort()
            return (self.vals == other.vals).all() and (array(self.subs) == array(other.subs)).all()
        elif isinstance(other, np.ndarray):
            return (self.toarray() == other).all()
        else:
            raise NotImplementedError('Unsupported object class for sptensor.__eq__ (%s)' % type(other))
    def __getitem__(self, idx):
                if len(idx) != self.ndim:
            raise ValueError('subscripts must be complete')
        sidx = ones(len(self.vals))
        for i in range(self.ndim):
            sidx = np.logical_and(self.subs[i] == idx[i], sidx)
        vals = self.vals[sidx]
        if len(vals) == 0:
            vals = 0
        elif len(vals) > 1:
            if self.accumfun is None:
                raise ValueError('Duplicate entries without specified accumulation function')
            vals = self.accumfun(vals)
        return vals
    def __sub__(self, other):
        if isinstance(other, np.ndarray):
            res = -other
            res[self.subs] += self.vals
        else:
            raise NotImplementedError()
        return res
    def _sort(self):
                subs = array(self.subs)
        sidx = lexsort(subs)
        self.subs = tuple(z.flatten()[sidx] for z in vsplit(subs, len(self.shape)))
        self.vals = self.vals[sidx]
        self.issorted = True
    def _ttm_compute(self, V, mode, transp):
        Z = self.unfold(mode, transp=True).tocsr()
        if transp:
            V = V.T
        Z = Z.dot(V.T)
        shape = copy(self.shape)
        shape[mode] = V.shape[0]
        if issparse_mat(Z):
            newT = unfolded_sptensor((Z.data, (Z.row, Z.col)), [mode], None, shape=shape).fold()
        else:
            newT = unfolded_dtensor(Z.T, mode, shape).fold()
        return newT
    def _ttv_compute(self, v, dims, vidx, remdims):
        nvals = self.vals
        nsubs = self.subs
        for i in range(len(dims)):
            idx = nsubs[dims[i]]
            w = v[vidx[i]]
            nvals = nvals * w[idx]
                if len(remdims) == 0:
            return nvals.sum()
        nsubs = tuple(self.subs[i] for i in remdims)
        nshp = tuple(self.shape[i] for i in remdims)
                if len(remdims) == 1:
            usubs = unique(nsubs[0])
            bins = usubs.searchsorted(nsubs[0])
            c = bincount(bins, weights=nvals)
            (nz,) = c.nonzero()
            return sptensor((usubs[nz],), c[nz], nshp)
                return sptensor(nsubs, nvals, shape=nshp, accumfun=np.sum)
    def _ttm_me_compute(self, V, edims, sdims, transp):
                shapeY = np.copy(self.shape)
                for n in np.union1d(edims, sdims):
            shapeY[n] = V[n].shape[1] if transp else V[n].shape[0]
                Y = zeros(shapeY)
        shapeY = array(shapeY)
        v = [None for _ in range(len(edims))]
        for i in range(np.prod(shapeY[edims])):
            rsubs = unravel_index(shapeY[edims], i)
    def unfold(self, rdims, cdims=None, transp=False):
        if isinstance(rdims, type(1)):
            rdims = [rdims]
        if transp:
            cdims = rdims
            rdims = setdiff1d(range(self.ndim), cdims)[::-1]
        elif cdims is None:
            cdims = setdiff1d(range(self.ndim), rdims)[::-1]
        if not (arange(self.ndim) == sort(hstack((rdims, cdims)))).all():
            raise ValueError(
                'Incorrect specification of dimensions (rdims: %s, cdims: %s)'
                % (str(rdims), str(cdims))
            )
        M = prod([self.shape[r] for r in rdims])
        N = prod([self.shape[c] for c in cdims])
        ridx = _build_idx(self.subs, self.vals, rdims, self.shape)
        cidx = _build_idx(self.subs, self.vals, cdims, self.shape)
        return unfolded_sptensor((self.vals, (ridx, cidx)), (M, N), rdims, cdims, self.shape)
    @inherit_docstring_from(tensor_mixin)
    def uttkrp(self, U, mode):
        R = U[1].shape[1] if mode == 0 else U[0].shape[1]
                dims = from_to_without(0, self.ndim, mode)
        V = zeros((self.shape[mode], R))
        for r in range(R):
            Z = tuple(U[n][:, r] for n in dims)
            TZ = self.ttv(Z, mode, without=True)
            if isinstance(TZ, sptensor):
                V[TZ.subs, r] = TZ.vals
            else:
                V[:, r] = self.ttv(Z, mode, without=True)
        return V
    @inherit_docstring_from(tensor_mixin)
    def transpose(self, axes=None):
                if axes is None:
            raise NotImplementedError(
                'Sparse tensor transposition without axes argument is not supported'
            )
        nsubs = tuple([self.subs[idx] for idx in axes])
        nshape = [self.shape[idx] for idx in axes]
        return sptensor(nsubs, self.vals, nshape)
    def concatenate(self, tpl, axis=None):
                if axis is None:
            raise NotImplementedError(
                'Sparse tensor concatenation without axis argument is not supported'
            )
        T = self
        for i in range(1, len(tpl)):
            T = _single_concatenate(T, tpl[i], axis=axis)
        return T
    def norm(self):
                return np.linalg.norm(self.vals)
    def toarray(self):
        A = zeros(self.shape)
        A.put(ravel_multi_index(self.subs, tuple(self.shape)), self.vals)
        return A

class unfolded_sptensor(coo_matrix):
    
    def __init__(self, tpl, shape, rdims, cdims, ten_shape, dtype=None, copy=False):
        self.ten_shape = array(ten_shape)
        if isinstance(rdims, int):
            rdims = [rdims]
        if cdims is None:
            cdims = setdiff1d(range(len(self.ten_shape)), rdims)[::-1]
        self.rdims = rdims
        self.cdims = cdims
        super(unfolded_sptensor, self).__init__(tpl, shape=shape, dtype=dtype, copy=copy)
    def fold(self):
                nsubs = zeros((len(self.data), len(self.ten_shape)), dtype=np.int)
        if len(self.rdims) > 0:
            nidx = unravel_index(self.row, self.ten_shape[self.rdims])
            for i in range(len(self.rdims)):
                nsubs[:, self.rdims[i]] = nidx[i]
        if len(self.cdims) > 0:
            nidx = unravel_index(self.col, self.ten_shape[self.cdims])
            for i in range(len(self.cdims)):
                nsubs[:, self.cdims[i]] = nidx[i]
        nsubs = [z.flatten() for z in hsplit(nsubs, len(self.ten_shape))]
        return sptensor(tuple(nsubs), self.data, self.ten_shape)

def fromarray(A):
        subs = np.nonzero(A)
    vals = A[subs]
    return sptensor(subs, vals, shape=A.shape, dtype=A.dtype)

def _single_concatenate(ten, other, axis):
    tshape = ten.shape
    oshape = other.shape
    if len(tshape) != len(oshape):
        raise ValueError("len(tshape) != len(oshape")
    oaxes = setdiff1d(range(len(tshape)), [axis])
    for i in oaxes:
        if tshape[i] != oshape[i]:
            raise ValueError("Dimensions must match")
    nsubs = [None for _ in range(len(tshape))]
    for i in oaxes:
        nsubs[i] = np.concatenate((ten.subs[i], other.subs[i]))
    nsubs[axis] = np.concatenate((
        ten.subs[axis], other.subs[axis] + tshape[axis]
    ))
    nvals = np.concatenate((ten.vals, other.vals))
    nshape = np.copy(tshape)
    nshape[axis] = tshape[axis] + oshape[axis]
    return sptensor(nsubs, nvals, nshape)

def _build_idx(subs, vals, dims, tshape):
    shape = array([tshape[d] for d in dims], ndmin=1)
    dims = array(dims, ndmin=1)
    if len(shape) == 0:
        idx = ones(len(vals), dtype=vals.dtype)
    elif len(subs) == 0:
        idx = array(tuple())
    else:
        idx = ravel_multi_index(tuple(subs[i] for i in dims), shape)
    return idx
import logging
import time
import numpy as np
from numpy import array, ones, sqrt
from numpy.random import rand
from .pyutils import is_number
from .core import ttm, nvecs, norm
__all__ = [
    'hooi',
    'hosvd',
]
_log = logging.getLogger('TUCKER')
__DEF_MAXITER = 500
__DEF_INIT = 'nvecs'
__DEF_CONV = 1e-7

def hooi(X, rank, **kwargs):
            ainit = kwargs.pop('init', __DEF_INIT)
    maxIter = kwargs.pop('maxIter', __DEF_MAXITER)
    conv = kwargs.pop('conv', __DEF_CONV)
    dtype = kwargs.pop('dtype', X.dtype)
    if not len(kwargs) == 0:
        raise ValueError('Unknown keywords (%s)' % (kwargs.keys()))
    ndims = X.ndim
    if is_number(rank):
        rank = rank * ones(ndims)
    normX = norm(X)
    U = __init(ainit, X, ndims, rank, dtype)
    fit = 0
    exectimes = []
    for itr in range(maxIter):
        tic = time.clock()
        fitold = fit
        for n in range(ndims):
            Utilde = ttm(X, U, n, transp=True, without=True)
            U[n] = nvecs(Utilde, n, rank[n])
                core = ttm(Utilde, U, n, transp=True)
                normresidual = sqrt(normX ** 2 - norm(core) ** 2)
                fit = 1 - (normresidual / normX)
        fitchange = abs(fitold - fit)
        exectimes.append(time.clock() - tic)
        _log.debug(
            '[%3d] fit: %.5f | delta: %7.1e | secs: %.5f'
            % (itr, fit, fitchange, exectimes[-1])
        )
        if itr > 1 and fitchange < conv:
            break
    return core, U
def hosvd(X, rank, dims=None, dtype=None, compute_core=True):
    U = [None for _ in range(X.ndim)]
    if dims is None:
        dims = range(X.ndim)
    if dtype is None:
        dtype = X.dtype
    for d in dims:
        U[d] = array(nvecs(X, d, rank[d]), dtype=dtype)
    if compute_core:
        core = X.ttm(U, transp=True)
        return U, core
    else:
        return U
def __init(init, X, N, rank, dtype):
            Uinit = [None]
    if isinstance(init, list):
        Uinit = init
    elif init == 'random':
        for n in range(1, N):
            Uinit.append(array(rand(X.shape[n], rank[n]), dtype=dtype))
    elif init == 'nvecs':
        Uinit = hosvd(X, rank, range(1, N), dtype=dtype, compute_core=False)
    return Uinit
import numpy as np
from numpy import cumprod, array, arange, zeros, floor, lexsort

def accum(subs, vals, func=np.sum, issorted=False, with_subs=False):
            if not issorted:
        sidx = lexsort(subs, axis=0)
        subs = [sub[sidx] for sub in subs]
        vals = vals[sidx]
    idx = np.where(np.diff(subs).any(axis=0))[0] + 1
    idx = np.concatenate(([0], idx, [subs[0].shape[0]]))
        nvals = np.zeros(len(idx) - 1)
    for i in range(len(idx) - 1):
        nvals[i] = func(vals[idx[i]:idx[i + 1]])
        if with_subs:
        return nvals, tuple(sub[idx[:-1]] for sub in subs)
    else:
        return nvals

def unravel_dimension(shape, idx):
    if isinstance(idx, type(1)):
        idx = array([idx])
    k = [1] + list(cumprod(shape[:-1]))
    n = len(shape)
    subs = zeros((len(idx), n), dtype=np.int)
    for i in arange(n - 1, -1, -1):
        subs[:, i] = floor(idx / k[i])
        idx = idx % k[i]
    return subs
__version__ = '0.1'
from .version import __version__
from .utils import *
from .core import *
from .sptensor import sptensor, unfolded_sptensor
from .dtensor import dtensor, unfolded_dtensor
from .ktensor import ktensor
from .cp import als as cp_als
from .tucker import hooi as tucker_hooi
from .tucker import hooi as tucker_hosvd
from numpy import array
import pytest

@pytest.fixture
def subs():
    return (
        array([0, 1, 0, 5, 7, 8]),
        array([2, 0, 4, 5, 3, 9]),
        array([0, 1, 2, 2, 1, 0])
    )

@pytest.fixture
def vals():
    return array([1, 2, 3, 4, 5, 6.1])

@pytest.fixture
def shape():
    return (10, 12, 3)
from numpy.random import randint, seed
import pytest

@pytest.fixture
def sptensor_seed():
    return seed(5)

@pytest.fixture
def sz():
    return 100

@pytest.fixture
def vals(sptensor_seed, sz):
    return randint(0, 100, sz)

@pytest.fixture
def shape():
    return (25, 11, 18, 7, 2)

@pytest.fixture
def subs(sptensor_seed, shape, sz):
    return tuple(randint(0, shape[i], sz) for i in range(len(shape)))
from numpy import array, zeros
import pytest

@pytest.fixture
def T():
    T = zeros((3, 4, 2))
    T[:, :, 0] = array([[1, 4, 7, 10], [2, 5, 8, 11], [3, 6, 9, 12]])
    T[:, :, 1] = array([[13, 16, 19, 22], [14, 17, 20, 23], [15, 18, 21, 24]])
    return T

@pytest.fixture
def Y():
    Y = zeros((2, 4, 2))
    Y[:, :, 0] = array([[22, 49, 76, 103], [28, 64, 100, 136]])
    Y[:, :, 1] = array([[130, 157, 184, 211], [172, 208, 244, 280]])
    return Y

@pytest.fixture
def U():
    return array([[1, 3, 5], [2, 4, 6]])
from setuptools import setup, find_packages
setup(
    name='sklearn-deap',
    version='0.1.8',
    author='Rodrigo',
    author_email='',
    description='Use evolutionary algorithms instead of gridsearch in scikit-learn.',
    url='https://github.com/rsteca/sklearn-deap',
    download_url='https://github.com/rsteca/sklearn-deap/tarball/0.1.8',
    classifiers=[
        'Development Status :: 4 - Beta',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.6',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python',
        'Intended Audience :: Science/Research',
        'Intended Audience :: Developers',
    ],
    package_dir={'': '.'},
    packages=find_packages('.'),
    install_requires=[
        'numpy>=1.9.3',
        'scipy>=0.16.0',
        'deap>=1.0.2',
        'scikit-learn>=0.16.1,<0.20',
    ],
)
import numpy as np
import random
from deap import base, creator, tools, algorithms
from multiprocessing import Pool
from sklearn.base import clone, is_classifier
from sklearn.cross_validation import check_cv, _fit_and_score
from sklearn.grid_search import BaseSearchCV
from sklearn.metrics.scorer import check_scoring
from sklearn.utils.validation import _num_samples, indexable

def enum(**enums):
    return type('Enum', (), enums)

param_types = enum(Categorical=1, Numerical=2)

def _get_param_types_maxint(params):
        name_values = list(params.items())
    types = []
    for _, possible_values in name_values:
        if isinstance(possible_values[0], float):
            types.append(param_types.Numerical)
        else:
            types.append(param_types.Categorical)
    maxints = [len(possible_values) - 1 for _, possible_values in name_values]
    return name_values, types, maxints

def _initIndividual(pcls, maxints):
    part = pcls(random.randint(0, maxint) for maxint in maxints)
    return part

def _mutIndividual(individual, up, indpb, gene_type=None):
    for i, up, rn in zip(range(len(up)), up, [random.random() for _ in range(len(up))]):
        if rn < indpb:
            individual[i] = random.randint(0, up)
    return individual,

def _cxIndividual(ind1, ind2, indpb, gene_type):
    for i, gt, rn in zip(range(len(ind1)), gene_type, [random.random() for _ in range(len(ind1))]):
        if rn > indpb:
            continue
        if gt is param_types.Categorical:
            ind1[i], ind2[i] = ind2[i], ind1[i]
        else:
                        if ind1[i] <= ind2[i]:
                ind1[i] = random.randint(ind1[i], ind2[i])
                ind2[i] = random.randint(ind1[i], ind2[i])
            else:
                ind1[i] = random.randint(ind2[i], ind1[i])
                ind2[i] = random.randint(ind2[i], ind1[i])
    return ind1, ind2

def _individual_to_params(individual, name_values):
    return dict((name, values[gene]) for gene, (name, values) in zip(individual, name_values))

score_cache = {}
def _evalFunction(individual, name_values, X, y, scorer, cv, iid, fit_params,
                  verbose=0, error_score='raise'):
    parameters = _individual_to_params(individual, name_values)
    score = 0
    n_test = 0
    paramkey = str(individual)
    if paramkey in score_cache:
        score = score_cache[paramkey]
    else:
        for train, test in cv:
            _score, _, _ = _fit_and_score(estimator=individual.est, X=X, y=y, scorer=scorer,
                                    train=train, test=test, verbose=verbose,
                                    parameters=parameters, fit_params=fit_params,
                                    error_score=error_score)
            if iid:
                score += _score * len(test)
                n_test += len(test)
            else:
                score += _score
                n_test += 1
        score /= float(n_test)
        score_cache[paramkey] = score
    return (score,)

class EvolutionaryAlgorithmSearchCV(BaseSearchCV):
        def __init__(self, estimator, params, scoring=None, cv=4,
                 refit=True, verbose=False, population_size=50,
                 gene_mutation_prob=0.1, gene_crossover_prob=0.5,
                 tournament_size=3, generations_number=10, gene_type=None,
                 n_jobs=1, iid=True, pre_dispatch='2*n_jobs', error_score='raise',
                 fit_params=None):
        super(EvolutionaryAlgorithmSearchCV, self).__init__(
            estimator=estimator, scoring=scoring, fit_params=fit_params,
            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
            pre_dispatch=pre_dispatch, error_score=error_score)
        self.params = params
        self.possible_params = params if isinstance(params, list) else [params]
        self.population_size = population_size
        self.generations_number = generations_number
        self._individual_evals = {}
        self.gene_mutation_prob = gene_mutation_prob
        self.gene_crossover_prob = gene_crossover_prob
        self.tournament_size = tournament_size
        self.gene_type = gene_type
    def fit(self, X, y=None):
        self.best_estimator_ = None
        self.best_score_ = float("-inf")
        self.best_params_ = None
        for possible_params in self.possible_params:
            self._fit(X, y, possible_params)
        if self.refit:
            self.best_estimator_ = clone(self.estimator)
            self.best_estimator_.set_params(**self.best_params_)
            self.best_estimator_.fit(X, y)
    def _fit(self, X, y, parameter_dict):
        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)
        n_samples = _num_samples(X)
        X, y = indexable(X, y)
        if y is not None:
            if len(y) != n_samples:
                raise ValueError('Target variable (y) has a different number '
                                 'of samples (%i) than data (X: %i samples)'
                                 % (len(y), n_samples))
        cv = check_cv(self.cv, y=y, classifier=is_classifier(self.estimator))
        creator.create("FitnessMax", base.Fitness, weights=(1.0,))
        creator.create("Individual", list, est=clone(self.estimator), fitness=creator.FitnessMax)
        toolbox = base.Toolbox()
        name_values, gene_type, maxints = _get_param_types_maxint(parameter_dict)
        if self.gene_type is None:
            self.gene_type = gene_type
        if self.verbose:
            print("Types %s and maxint %s detected" % (self.gene_type, maxints))
        toolbox.register("individual", _initIndividual, creator.Individual, maxints=maxints)
        toolbox.register("population", tools.initRepeat, list, toolbox.individual)
        toolbox.register("evaluate", _evalFunction,
                         name_values=name_values, X=X, y=y,
                         scorer=self.scorer_, cv=cv, iid=self.iid, verbose=self.verbose,
                         error_score=self.error_score, fit_params=self.fit_params)
        toolbox.register("mate", _cxIndividual, indpb=self.gene_crossover_prob, gene_type=self.gene_type)
        toolbox.register("mutate", _mutIndividual, indpb=self.gene_mutation_prob, up=maxints)
        toolbox.register("select", tools.selTournament, tournsize=self.tournament_size)
        if self.n_jobs > 1:
            pool = Pool(processes=self.n_jobs)
            toolbox.register("map", pool.map)
        pop = toolbox.population(n=self.population_size)
        hof = tools.HallOfFame(1)
        stats = tools.Statistics(lambda ind: ind.fitness.values)
        stats.register("avg", np.mean)
        stats.register("min", np.min)
        stats.register("max", np.max)
        if self.verbose:
            print('--- Evolve in {0} possible combinations ---'.format(np.prod(np.array(maxints) + 1)))
        pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,
                                           ngen=self.generations_number, stats=stats,
                                           halloffame=hof, verbose=self.verbose)
        current_best_score_ = hof[0].fitness.values[0]
        current_best_params_ = _individual_to_params(hof[0], name_values)
        if self.verbose:
            print("Best individual is: %s\nwith fitness: %s" % (
                current_best_params_, current_best_score_))
        if current_best_score_ > self.best_score_:
            self.best_score_ = current_best_score_
            self.best_params_ = current_best_params_
import re
import ast
from setuptools import setup, find_packages
_version_re = re.compile(r'__version__\s+=\s+(.*)')
with open('sklearn_evaluation/__init__.py', 'rb') as f:
    VERSION = str(ast.literal_eval(_version_re.search(
        f.read().decode('utf-8')).group(1)))
DOWNLOAD_URL = ('https://github.com/edublancas/sklearn-evaluation/tarball/{}'
                .format(VERSION))
setup(name='sklearn-evaluation',
      packages=find_packages(exclude=['tests']),
      version=VERSION,
      description=('scikit-learn model evaluation made easy: plots, tables and'
                   'markdown reports.'),
      url='http://github.com/edublancas/sklearn-evaluation',
      download_url=DOWNLOAD_URL,
      author='Eduardo Blancas Reyes',
      author_email='edu.blancas@gmail.com',
      license='MIT',
      keywords=['datascience', 'machinelearning'],
      classifiers=[],
      install_requires=[
          'scikit-learn',
          'matplotlib',
          'six',
          'decorator'
      ],
      test_suite='nose.collector',
      tests_require=['nose'],        )
import sys
import os
sys.path.insert(0, os.path.abspath('../sphinxext'))
sys.path.insert(0, os.path.abspath('../..'))

extensions = [
    'ipython_sphinxext.ipython_console_highlighting',
    'ipython_sphinxext.ipython_directive',
    'sphinx.ext.autodoc',
    'sphinx.ext.autosummary',
    'sphinx.ext.intersphinx',
    'numpydoc',
    'plot_directive'
]
plot_include_source = True
plot_html_show_formats = False
plot_html_show_source_link = False

templates_path = ['_templates']
source_suffix = '.rst'

master_doc = 'index'
project = u'sklearn-evaluation'
copyright = u'2016, Eduardo Blancas Reyes'
author = u'Eduardo Blancas Reyes'

import re
import ast
_version_re = re.compile(r'__version__\s+=\s+(.*)')
with open('../../sklearn_evaluation/__init__.py', 'rb') as f:
    VERSION = str(ast.literal_eval(_version_re.search(
        f.read().decode('utf-8')).group(1)))
version = VERSION
release = VERSION
language = None

exclude_patterns = []


pygments_style = 'sphinx'

todo_include_todos = False

html_theme = 'sphinx_rtd_theme'



html_static_path = ['_static']








htmlhelp_basename = 'sklearn-evaluationdoc'

latex_elements = {

}
latex_documents = [
    (master_doc, 'sklearn-evaluation.tex', u'sklearn-evaluation Documentation',
     u'Eduardo Blancas Reyes', 'manual'),
]




man_pages = [
    (master_doc, 'sklearn-evaluation', u'sklearn-evaluation Documentation',
     [author], 1)
]


texinfo_documents = [
    (master_doc, 'sklearn-evaluation', u'sklearn-evaluation Documentation',
     author, 'sklearn-evaluation', 'One line description of project.',
     'Miscellaneous'),
]


intersphinx_mapping = {
    'sklearn': ('http://scikit-learn.org/stable', None),
    'matplotlib': ('http://matplotlib.org/', None),
}

numpydoc_show_class_members = False
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import six
from six.moves import xrange
import sys, os, shutil, io, re, textwrap
from os.path import relpath
import traceback
if not six.PY3:
    import cStringIO
from docutils.parsers.rst import directives
from docutils.parsers.rst.directives.images import Image
align = Image.align
import sphinx
sphinx_version = sphinx.__version__.split(".")
sphinx_version = tuple([int(re.split('[^0-9]', x)[0])
                        for x in sphinx_version[:2]])
try:
        import jinja2
    def format_template(template, **kw):
        return jinja2.Template(template).render(**kw)
except ImportError:
    import jinja
    def format_template(template, **kw):
        return jinja.from_string(template, **kw)
import matplotlib
import matplotlib.cbook as cbook
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib import _pylab_helpers
__version__ = 2

def plot_directive(name, arguments, options, content, lineno,
                   content_offset, block_text, state, state_machine):
    return run(arguments, content, options, state_machine, state, lineno)
plot_directive.__doc__ = __doc__

def _option_boolean(arg):
    if not arg or not arg.strip():
                return True
    elif arg.strip().lower() in ('no', '0', 'false'):
        return False
    elif arg.strip().lower() in ('yes', '1', 'true'):
        return True
    else:
        raise ValueError('"%s" unknown boolean' % arg)

def _option_context(arg):
    if arg in [None, 'reset', 'close-figs']:
        return arg
    raise ValueError("argument should be None or 'reset' or 'close-figs'")

def _option_format(arg):
    return directives.choice(arg, ('python', 'doctest'))

def _option_align(arg):
    return directives.choice(arg, ("top", "middle", "bottom", "left", "center",
                                   "right"))

def mark_plot_labels(app, document):
        for name, explicit in six.iteritems(document.nametypes):
        if not explicit:
            continue
        labelid = document.nameids[name]
        if labelid is None:
            continue
        node = document.ids[labelid]
        if node.tagname in ('html_only', 'latex_only'):
            for n in node:
                if n.tagname == 'figure':
                    sectname = name
                    for c in n:
                        if c.tagname == 'caption':
                            sectname = c.astext()
                            break
                    node['ids'].remove(labelid)
                    node['names'].remove(name)
                    n['ids'].append(labelid)
                    n['names'].append(name)
                    document.settings.env.labels[name] = \
                        document.settings.env.docname, labelid, sectname
                    break

def setup(app):
    setup.app = app
    setup.config = app.config
    setup.confdir = app.confdir
    options = {'alt': directives.unchanged,
               'height': directives.length_or_unitless,
               'width': directives.length_or_percentage_or_unitless,
               'scale': directives.nonnegative_int,
               'align': _option_align,
               'class': directives.class_option,
               'include-source': _option_boolean,
               'format': _option_format,
               'context': _option_context,
               'nofigs': directives.flag,
               'encoding': directives.encoding
               }
    app.add_directive('plot', plot_directive, True, (0, 2, False), **options)
    app.add_config_value('plot_pre_code', None, True)
    app.add_config_value('plot_include_source', False, True)
    app.add_config_value('plot_html_show_source_link', True, True)
    app.add_config_value('plot_formats', ['png', 'hires.png', 'pdf'], True)
    app.add_config_value('plot_basedir', None, True)
    app.add_config_value('plot_html_show_formats', True, True)
    app.add_config_value('plot_rcparams', {}, True)
    app.add_config_value('plot_apply_rcparams', False, True)
    app.add_config_value('plot_working_directory', None, True)
    app.add_config_value('plot_template', None, True)
    app.connect(str('doctree-read'), mark_plot_labels)

def contains_doctest(text):
    try:
                compile(text, '<string>', 'exec')
        return False
    except SyntaxError:
        pass
    r = re.compile(r'^\s*>>>', re.M)
    m = r.search(text)
    return bool(m)

def unescape_doctest(text):
        if not contains_doctest(text):
        return text
    code = ""
    for line in text.split("\n"):
        m = re.match(r'^\s*(>>>|\.\.\.) (.*)$', line)
        if m:
            code += m.group(2) + "\n"
        elif line.strip():
            code += "        else:
            code += "\n"
    return code

def split_code_at_show(text):
    
    parts = []
    is_doctest = contains_doctest(text)
    part = []
    for line in text.split("\n"):
        if (not is_doctest and line.strip() == 'plt.show()') or \
               (is_doctest and line.strip() == '>>> plt.show()'):
            part.append(line)
            parts.append("\n".join(part))
            part = []
        else:
            part.append(line)
    if "\n".join(part).strip():
        parts.append("\n".join(part))
    return parts

def remove_coding(text):
        sub_re = re.compile("^    return sub_re.sub("", text)

TEMPLATE = 
exception_template = 
plot_context = dict()
class ImageFile(object):
    def __init__(self, basename, dirname):
        self.basename = basename
        self.dirname = dirname
        self.formats = []
    def filename(self, format):
        return os.path.join(self.dirname, "%s.%s" % (self.basename, format))
    def filenames(self):
        return [self.filename(fmt) for fmt in self.formats]

def out_of_date(original, derived):
        return (not os.path.exists(derived) or
            (os.path.exists(original) and
             os.stat(derived).st_mtime < os.stat(original).st_mtime))

class PlotError(RuntimeError):
    pass

def run_code(code, code_path, ns=None, function_name=None):
    
                if six.PY2:
        pwd = os.getcwdu()
    else:
        pwd = os.getcwd()
    old_sys_path = list(sys.path)
    if setup.config.plot_working_directory is not None:
        try:
            os.chdir(setup.config.plot_working_directory)
        except OSError as err:
            raise OSError(str(err) + '\n`plot_working_directory` option in'
                          'Sphinx configuration file must be a valid '
                          'directory path')
        except TypeError as err:
            raise TypeError(str(err) + '\n`plot_working_directory` option in '
                            'Sphinx configuration file must be a string or '
                            'None')
        sys.path.insert(0, setup.config.plot_working_directory)
    elif code_path is not None:
        dirname = os.path.abspath(os.path.dirname(code_path))
        os.chdir(dirname)
        sys.path.insert(0, dirname)
        old_sys_argv = sys.argv
    sys.argv = [code_path]
        stdout = sys.stdout
    if six.PY3:
        sys.stdout = io.StringIO()
    else:
        sys.stdout = cStringIO.StringIO()
                def _dummy_print(*arg, **kwarg):
        pass
    try:
        try:
            code = unescape_doctest(code)
            if ns is None:
                ns = {}
            if not ns:
                if setup.config.plot_pre_code is None:
                    six.exec_(six.text_type("import numpy as np\n" +
                    "from matplotlib import pyplot as plt\n"), ns)
                else:
                    six.exec_(six.text_type(setup.config.plot_pre_code), ns)
            ns['print'] = _dummy_print
            if "__main__" in code:
                six.exec_("__name__ = '__main__'", ns)
            code = remove_coding(code)
            six.exec_(code, ns)
            if function_name is not None:
                six.exec_(function_name + "()", ns)
        except (Exception, SystemExit) as err:
            raise PlotError(traceback.format_exc())
    finally:
        os.chdir(pwd)
        sys.argv = old_sys_argv
        sys.path[:] = old_sys_path
        sys.stdout = stdout
    return ns

def clear_state(plot_rcparams, close=True):
    if close:
        plt.close('all')
    matplotlib.rc_file_defaults()
    matplotlib.rcParams.update(plot_rcparams)

def render_figures(code, code_path, output_dir, output_base, context,
                   function_name, config, context_reset=False,
                   close_figs=False):
            default_dpi = {'png': 80, 'hires.png': 200, 'pdf': 200}
    formats = []
    plot_formats = config.plot_formats
    if isinstance(plot_formats, six.string_types):
        plot_formats = eval(plot_formats)
    for fmt in plot_formats:
        if isinstance(fmt, six.string_types):
            formats.append((fmt, default_dpi.get(fmt, 80)))
        elif type(fmt) in (tuple, list) and len(fmt)==2:
            formats.append((str(fmt[0]), int(fmt[1])))
        else:
            raise PlotError('invalid image format "%r" in plot_formats' % fmt)
    
    code_pieces = split_code_at_show(code)
        all_exists = True
    img = ImageFile(output_base, output_dir)
    for format, dpi in formats:
        if out_of_date(code_path, img.filename(format)):
            all_exists = False
            break
        img.formats.append(format)
    if all_exists:
        return [(code, [img])]
        results = []
    all_exists = True
    for i, code_piece in enumerate(code_pieces):
        images = []
        for j in xrange(1000):
            if len(code_pieces) > 1:
                img = ImageFile('%s_%02d_%02d' % (output_base, i, j), output_dir)
            else:
                img = ImageFile('%s_%02d' % (output_base, j), output_dir)
            for format, dpi in formats:
                if out_of_date(code_path, img.filename(format)):
                    all_exists = False
                    break
                img.formats.append(format)
                        if not all_exists:
                all_exists = (j > 0)
                break
            images.append(img)
        if not all_exists:
            break
        results.append((code_piece, images))
    if all_exists:
        return results
    
    results = []
    if context:
        ns = plot_context
    else:
        ns = {}
    if context_reset:
        clear_state(config.plot_rcparams)
        plot_context.clear()
    close_figs = not context or close_figs
    for i, code_piece in enumerate(code_pieces):
        if not context or config.plot_apply_rcparams:
            clear_state(config.plot_rcparams, close_figs)
        elif close_figs:
            plt.close('all')
        run_code(code_piece, code_path, ns, function_name)
        images = []
        fig_managers = _pylab_helpers.Gcf.get_all_fig_managers()
        for j, figman in enumerate(fig_managers):
            if len(fig_managers) == 1 and len(code_pieces) == 1:
                img = ImageFile(output_base, output_dir)
            elif len(code_pieces) == 1:
                img = ImageFile("%s_%02d" % (output_base, j), output_dir)
            else:
                img = ImageFile("%s_%02d_%02d" % (output_base, i, j),
                                output_dir)
            images.append(img)
            for format, dpi in formats:
                try:
                    figman.canvas.figure.savefig(img.filename(format),
                                                 dpi=dpi,
                                                 bbox_inches="tight")
                except Exception as err:
                    raise PlotError(traceback.format_exc())
                img.formats.append(format)
        results.append((code_piece, images))
    if not context or config.plot_apply_rcparams:
        clear_state(config.plot_rcparams, close=not context)
    return results

def run(arguments, content, options, state_machine, state, lineno):
        if arguments and content:
        raise RuntimeError("plot:: directive can't have both args and content")
    document = state_machine.document
    config = document.settings.env.config
    nofigs = 'nofigs' in options
    options.setdefault('include-source', config.plot_include_source)
    keep_context = 'context' in options
    context_opt = None if not keep_context else options['context']
    rst_file = document.attributes['source']
    rst_dir = os.path.dirname(rst_file)
    if len(arguments):
        if not config.plot_basedir:
            source_file_name = os.path.join(setup.app.builder.srcdir,
                                            directives.uri(arguments[0]))
        else:
            source_file_name = os.path.join(setup.confdir, config.plot_basedir,
                                            directives.uri(arguments[0]))
                caption = '\n'.join(content)
                if len(arguments) == 2:
            function_name = arguments[1]
        else:
            function_name = None
        with io.open(source_file_name, 'r', encoding='utf-8') as fd:
            code = fd.read()
        output_base = os.path.basename(source_file_name)
    else:
        source_file_name = rst_file
        code = textwrap.dedent("\n".join(map(str, content)))
        counter = document.attributes.get('_plot_counter', 0) + 1
        document.attributes['_plot_counter'] = counter
        base, ext = os.path.splitext(os.path.basename(source_file_name))
        output_base = '%s-%d.py' % (base, counter)
        function_name = None
        caption = ''
    base, source_ext = os.path.splitext(output_base)
    if source_ext in ('.py', '.rst', '.txt'):
        output_base = base
    else:
        source_ext = ''
        output_base = output_base.replace('.', '-')
        is_doctest = contains_doctest(code)
    if 'format' in options:
        if options['format'] == 'python':
            is_doctest = False
        else:
            is_doctest = True
        source_rel_name = relpath(source_file_name, setup.confdir)
    source_rel_dir = os.path.dirname(source_rel_name)
    while source_rel_dir.startswith(os.path.sep):
        source_rel_dir = source_rel_dir[1:]
        build_dir = os.path.join(os.path.dirname(setup.app.doctreedir),
                             'plot_directive',
                             source_rel_dir)
                build_dir = os.path.normpath(build_dir)
    if not os.path.exists(build_dir):
        os.makedirs(build_dir)
        dest_dir = os.path.abspath(os.path.join(setup.app.builder.outdir,
                                            source_rel_dir))
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir) 
        dest_dir_link = os.path.join(relpath(setup.confdir, rst_dir),
                                 source_rel_dir).replace(os.path.sep, '/')
    build_dir_link = relpath(build_dir, rst_dir).replace(os.path.sep, '/')
    source_link = dest_dir_link + '/' + output_base + source_ext
        try:
        results = render_figures(code,
                                 source_file_name,
                                 build_dir,
                                 output_base,
                                 keep_context,
                                 function_name,
                                 config,
                                 context_reset=context_opt == 'reset',
                                 close_figs=context_opt == 'close-figs')
        errors = []
    except PlotError as err:
        reporter = state.memo.reporter
        sm = reporter.system_message(
            2, "Exception occurred in plotting %s\n from %s:\n%s" % (output_base,
                                                source_file_name, err),
            line=lineno)
        results = [(code, [])]
        errors = [sm]
        caption = '\n'.join('      ' + line.strip()
                        for line in caption.split('\n'))
        total_lines = []
    for j, (code_piece, images) in enumerate(results):
        if options['include-source']:
            if is_doctest:
                lines = ['']
                lines += [row.rstrip() for row in code_piece.split('\n')]
            else:
                lines = ['.. code-block:: python', '']
                lines += ['    %s' % row.rstrip()
                          for row in code_piece.split('\n')]
            source_code = "\n".join(lines)
        else:
            source_code = ""
        if nofigs:
            images = []
        opts = [':%s: %s' % (key, val) for key, val in six.iteritems(options)
                if key in ('alt', 'height', 'width', 'scale', 'align', 'class')]
        only_html = ".. only:: html"
        only_latex = ".. only:: latex"
        only_texinfo = ".. only:: texinfo"
                        if j == 0 and config.plot_html_show_source_link:
            src_link = source_link
        else:
            src_link = None
        result = format_template(
            config.plot_template or TEMPLATE,
            dest_dir=dest_dir_link,
            build_dir=build_dir_link,
            source_link=src_link,
            multi_image=len(images) > 1,
            only_html=only_html,
            only_latex=only_latex,
            only_texinfo=only_texinfo,
            options=opts,
            images=images,
            source_code=source_code,
            html_show_formats=config.plot_html_show_formats and not nofigs,
            caption=caption)
        total_lines.extend(result.split("\n"))
        total_lines.extend("\n")
    if total_lines:
        state_machine.insert_input(total_lines, source=source_file_name)
        if not os.path.exists(dest_dir):
        cbook.mkdirs(dest_dir)
    for code_piece, images in results:
        for img in images:
            for fn in img.filenames():
                destimg = os.path.join(dest_dir, os.path.basename(fn))
                if fn != destimg:
                    shutil.copyfile(fn, destimg)
        target_name = os.path.join(dest_dir, output_base + source_ext)
    with io.open(target_name, 'w', encoding="utf-8") as f:
        if source_file_name == rst_file:
            code_escaped = unescape_doctest(code)
        else:
            code_escaped = code
        f.write(code_escaped)
    return errors
from sphinx import highlighting
from IPython.lib.lexers import IPyLexer
def setup(app):
    
                    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}
    return metadata

ipy2 = IPyLexer(python3=False)
ipy3 = IPyLexer(python3=True)
highlighting.lexers['ipython'] = ipy2
highlighting.lexers['ipython2'] = ipy2
highlighting.lexers['ipython3'] = ipy3
from __future__ import print_function

import atexit
import os
import re
import sys
import tempfile
import ast
import warnings
import shutil

from docutils.parsers.rst import directives
from sphinx.util.compat import Directive
from traitlets.config import Config
from IPython import InteractiveShell
from IPython.core.profiledir import ProfileDir
from IPython.utils import io
from IPython.utils.py3compat import PY3
if PY3:
    from io import StringIO
else:
    from StringIO import StringIO
COMMENT, INPUT, OUTPUT =  range(3)

def block_parser(part, rgxin, rgxout, fmtin, fmtout):
        block = []
    lines = part.split('\n')
    N = len(lines)
    i = 0
    decorator = None
    while 1:
        if i==N:
                        break
        line = lines[i]
        i += 1
        line_stripped = line.strip()
        if line_stripped.startswith('            block.append((COMMENT, line))
            continue
        if line_stripped.startswith('@'):
                                    decorator = line_stripped
            continue
                matchin = rgxin.match(line)
        if matchin:
            lineno, inputline = int(matchin.group(1)), matchin.group(2)
                        continuation = '   %s:'%''.join(['.']*(len(str(lineno))+2))
            Nc = len(continuation)
                                                                        
            rest = []
            while i<N:
                                
                nextline = lines[i]
                matchout = rgxout.match(nextline)
                                if matchout or nextline.startswith('                    break
                elif nextline.startswith(continuation):
                                                                                                                                            nextline = nextline[Nc:]
                    if nextline and nextline[0] == ' ':
                        nextline = nextline[1:]
                    inputline += '\n' +  nextline
                else:
                    rest.append(nextline)
                i+= 1
            block.append((INPUT, (decorator, inputline, '\n'.join(rest))))
            continue
                        matchout = rgxout.match(line)
        if matchout:
            lineno, output = int(matchout.group(1)), matchout.group(2)
            if i<N-1:
                output = '\n'.join([output] + lines[i:])
            block.append((OUTPUT, output))
            break
    return block

class EmbeddedSphinxShell(object):
    
    def __init__(self, exec_lines=None):
        self.cout = StringIO()
        if exec_lines is None:
            exec_lines = []
                config = Config()
        config.HistoryManager.hist_file = ':memory:'
        config.InteractiveShell.autocall = False
        config.InteractiveShell.autoindent = False
        config.InteractiveShell.colors = 'NoColor'
                tmp_profile_dir = tempfile.mkdtemp(prefix='profile_')
        profname = 'auto_profile_sphinx_build'
        pdir = os.path.join(tmp_profile_dir,profname)
        profile = ProfileDir.create_profile_dir(pdir)
                        IP = InteractiveShell.instance(config=config, profile_dir=profile)
        atexit.register(self.cleanup)
                io.stdout = self.cout
        io.stderr = self.cout
                                
                self.IP = IP
        self.user_ns = self.IP.user_ns
        self.user_global_ns = self.IP.user_global_ns
        self.input = ''
        self.output = ''
        self.tmp_profile_dir = tmp_profile_dir
        self.is_verbatim = False
        self.is_doctest = False
        self.is_suppress = False
                                                self.directive = None
                        self._pyplot_imported = False
                for line in exec_lines:
            self.process_input_line(line, store_history=False)
    def cleanup(self):
        shutil.rmtree(self.tmp_profile_dir, ignore_errors=True)
    def clear_cout(self):
        self.cout.seek(0)
        self.cout.truncate(0)
    def process_input_line(self, line, store_history=True):
        
        stdout = sys.stdout
        splitter = self.IP.input_splitter
        try:
            sys.stdout = self.cout
            splitter.push(line)
            more = splitter.push_accepts_more()
            if not more:
                source_raw = splitter.raw_reset()
                self.IP.run_cell(source_raw, store_history=store_history)
        finally:
            sys.stdout = stdout
    def process_image(self, decorator):
                savefig_dir = self.savefig_dir
        source_dir = self.source_dir
        saveargs = decorator.split(' ')
        filename = saveargs[1]
                outfile = os.path.relpath(os.path.join(savefig_dir,filename),
                    source_dir)
        
        imagerows = ['.. image:: %s'%outfile]
        for kwarg in saveargs[2:]:
            arg, val = kwarg.split('=')
            arg = arg.strip()
            val = val.strip()
            imagerows.append('   :%s: %s'%(arg, val))
        image_file = os.path.basename(outfile)         image_directive = '\n'.join(imagerows)
        return image_file, image_directive
        def process_input(self, data, input_prompt, lineno):
                decorator, input, rest = data
        image_file = None
        image_directive = None
        is_verbatim = decorator=='@verbatim' or self.is_verbatim
        is_doctest = (decorator is not None and \
                     decorator.startswith('@doctest')) or self.is_doctest
        is_suppress = decorator=='@suppress' or self.is_suppress
        is_okexcept = decorator=='@okexcept' or self.is_okexcept
        is_okwarning = decorator=='@okwarning' or self.is_okwarning
        is_savefig = decorator is not None and \
                     decorator.startswith('@savefig')
        input_lines = input.split('\n')
        if len(input_lines) > 1:
            if input_lines[-1] != "":
                input_lines.append('')                                        
        continuation = '   %s:'%''.join(['.']*(len(str(lineno))+2))
        if is_savefig:
            image_file, image_directive = self.process_image(decorator)
        ret = []
        is_semicolon = False
                if is_suppress and self.hold_count:
            store_history = False
        else:
            store_history = True
                with warnings.catch_warnings(record=True) as ws:
            for i, line in enumerate(input_lines):
                if line.endswith(';'):
                    is_semicolon = True
                if i == 0:
                                        if is_verbatim:
                        self.process_input_line('')
                        self.IP.execution_count += 1                     else:
                                                self.process_input_line(line, store_history=store_history)
                    formatted_line = '%s %s'%(input_prompt, line)
                else:
                                        if not is_verbatim:
                        self.process_input_line(line, store_history=store_history)
                    formatted_line = '%s %s'%(continuation, line)
                if not is_suppress:
                    ret.append(formatted_line)
        if not is_suppress and len(rest.strip()) and is_verbatim:
                                                ret.append(rest)
                self.cout.seek(0)
        processed_output = self.cout.read()
        if not is_suppress and not is_semicolon:
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ret.append(processed_output)
        elif is_semicolon:
                        ret.append('')
                filename = "Unknown"
        lineno = 0
        if self.directive.state:
            filename = self.directive.state.document.current_source
            lineno = self.directive.state.document.current_line
                        if not is_okexcept and "Traceback" in processed_output:
            s =  "\nException in %s at block ending on line %s\n" % (filename, lineno)
            s += "Specify :okexcept: as an option in the ipython:: block to suppress this message\n"
            sys.stdout.write('\n\n>>>' + ('-' * 73))
            sys.stdout.write(s)
            sys.stdout.write(processed_output)
            sys.stdout.write('<<<' + ('-' * 73) + '\n\n')
                        if not is_okwarning:
            for w in ws:
                s =  "\nWarning in %s at block ending on line %s\n" % (filename, lineno)
                s += "Specify :okwarning: as an option in the ipython:: block to suppress this message\n"
                sys.stdout.write('\n\n>>>' + ('-' * 73))
                sys.stdout.write(s)
                sys.stdout.write(('-' * 76) + '\n')
                s=warnings.formatwarning(w.message, w.category,
                                         w.filename, w.lineno, w.line)
                sys.stdout.write(s)
                sys.stdout.write('<<<' + ('-' * 73) + '\n')
        self.cout.truncate(0)
        return (ret, input_lines, processed_output,
                is_doctest, decorator, image_file, image_directive)

    def process_output(self, data, output_prompt, input_lines, output,
                       is_doctest, decorator, image_file):
                        
        TAB = ' ' * 4
        if is_doctest and output is not None:
            found = output             found = found.strip()
            submitted = data.strip()
            if self.directive is None:
                source = 'Unavailable'
                content = 'Unavailable'
            else:
                source = self.directive.state.document.current_source
                content = self.directive.content
                                content = '\n'.join([TAB + line for line in content])
                        ind = found.find(output_prompt)
            if ind < 0:
                e = ('output does not contain output prompt\n\n'
                     'Document source: {0}\n\n'
                     'Raw content: \n{1}\n\n'
                     'Input line(s):\n{TAB}{2}\n\n'
                     'Output line(s):\n{TAB}{3}\n\n')
                e = e.format(source, content, '\n'.join(input_lines),
                             repr(found), TAB=TAB)
                raise RuntimeError(e)
            found = found[len(output_prompt):].strip()
                        if decorator.strip() == '@doctest':
                                if found != submitted:
                    e = ('doctest failure\n\n'
                         'Document source: {0}\n\n'
                         'Raw content: \n{1}\n\n'
                         'On input line(s):\n{TAB}{2}\n\n'
                         'we found output:\n{TAB}{3}\n\n'
                         'instead of the expected:\n{TAB}{4}\n\n')
                    e = e.format(source, content, '\n'.join(input_lines),
                                 repr(found), repr(submitted), TAB=TAB)
                    raise RuntimeError(e)
            else:
                self.custom_doctest(decorator, input_lines, found, submitted)
                                out_data = []
        is_verbatim = decorator=='@verbatim' or self.is_verbatim
        if is_verbatim and data.strip():
                                                            
                                                                                                            out_data.append("{0} {1}\n".format(output_prompt, data))
        return out_data
    def process_comment(self, data):
                if not self.is_suppress:
            return [data]
    def save_image(self, image_file):
                self.ensure_pyplot()
        command = 'plt.gcf().savefig("%s")'%image_file
                self.process_input_line('bookmark ipy_thisdir', store_history=False)
        self.process_input_line('cd -b ipy_savedir', store_history=False)
        self.process_input_line(command, store_history=False)
        self.process_input_line('cd -b ipy_thisdir', store_history=False)
        self.process_input_line('bookmark -d ipy_thisdir', store_history=False)
        self.clear_cout()
    def process_block(self, block):
                ret = []
        output = None
        input_lines = None
        lineno = self.IP.execution_count
        input_prompt = self.promptin % lineno
        output_prompt = self.promptout % lineno
        image_file = None
        image_directive = None
        found_input = False
        for token, data in block:
            if token == COMMENT:
                out_data = self.process_comment(data)
            elif token == INPUT:
                found_input = True
                (out_data, input_lines, output, is_doctest,
                 decorator, image_file, image_directive) = \
                          self.process_input(data, input_prompt, lineno)
            elif token == OUTPUT:
                if not found_input:
                    TAB = ' ' * 4
                    linenumber = 0
                    source = 'Unavailable'
                    content = 'Unavailable'
                    if self.directive:
                        linenumber = self.directive.state.document.current_line
                        source = self.directive.state.document.current_source
                        content = self.directive.content
                                                content = '\n'.join([TAB + line for line in content])
                    e = ('\n\nInvalid block: Block contains an output prompt '
                         'without an input prompt.\n\n'
                         'Document source: {0}\n\n'
                         'Content begins at line {1}: \n\n{2}\n\n'
                         'Problematic block within content: \n\n{TAB}{3}\n\n')
                    e = e.format(source, linenumber, content, block, TAB=TAB)
                                                            sys.stdout.write(e)
                    raise RuntimeError('An invalid block was detected.')
                out_data = \
                    self.process_output(data, output_prompt, input_lines,
                                        output, is_doctest, decorator,
                                        image_file)
                if out_data:
                                                                                                    assert(ret[-1] == '')
                    del ret[-1]
            if out_data:
                ret.extend(out_data)
                if image_file is not None:
            self.save_image(image_file)
        return ret, image_directive
    def ensure_pyplot(self):
                                        
        if not self._pyplot_imported:
            if 'matplotlib.backends' not in sys.modules:
                                                                                import matplotlib
                matplotlib.use('agg')
                        self.process_input_line('import matplotlib.pyplot as plt',
                                    store_history=False)
            self._pyplot_imported = True
    def process_pure_python(self, content):
                output = []
        savefig = False         multiline = False         multiline_start = None
        fmtin = self.promptin
        ct = 0
        for lineno, line in enumerate(content):
            line_stripped = line.strip()
            if not len(line):
                output.append(line)
                continue
                        if line_stripped.startswith('@'):
                output.extend([line])
                if 'savefig' in line:
                    savefig = True                 continue
                        if line_stripped.startswith('                output.extend([line])
                continue
                        continuation  = u'   %s:'% ''.join(['.']*(len(str(ct))+2))
            if not multiline:
                modified = u"%s %s" % (fmtin % ct, line_stripped)
                output.append(modified)
                ct += 1
                try:
                    ast.parse(line_stripped)
                    output.append(u'')
                except Exception:                     multiline = True
                    multiline_start = lineno
            else:                 modified = u'%s %s' % (continuation, line)
                output.append(modified)
                                if len(content) > lineno + 1:
                    nextline = content[lineno + 1]
                    if len(nextline) - len(nextline.lstrip()) > 3:
                        continue
                try:
                    mod = ast.parse(
                            '\n'.join(content[multiline_start:lineno+1]))
                    if isinstance(mod.body[0], ast.FunctionDef):
                                                for element in mod.body[0].body:
                            if isinstance(element, ast.Return):
                                multiline = False
                    else:
                        output.append(u'')
                        multiline = False
                except Exception:
                    pass
            if savefig:                 self.ensure_pyplot()
                self.process_input_line('plt.clf()', store_history=False)
                self.clear_cout()
                savefig = False
        return output
    def custom_doctest(self, decorator, input_lines, found, submitted):
                from .custom_doctests import doctests
        args = decorator.split()
        doctest_type = args[1]
        if doctest_type in doctests:
            doctests[doctest_type](self, args, input_lines, found, submitted)
        else:
            e = "Invalid option to @doctest: {0}".format(doctest_type)
            raise Exception(e)

class IPythonDirective(Directive):
    has_content = True
    required_arguments = 0
    optional_arguments = 4     final_argumuent_whitespace = True
    option_spec = { 'python': directives.unchanged,
                    'suppress' : directives.flag,
                    'verbatim' : directives.flag,
                    'doctest' : directives.flag,
                    'okexcept': directives.flag,
                    'okwarning': directives.flag
                  }
    shell = None
    seen_docs = set()
    def get_config_options(self):
                config = self.state.document.settings.env.config
                outdir = self.state.document.settings.env.app.outdir
        savefig_dir = config.ipython_savefig_dir
        source_dir = os.path.dirname(self.state.document.current_source)
        if savefig_dir is None:
            savefig_dir = config.html_static_path or '_static'
        if isinstance(savefig_dir, list):
            savefig_dir = os.path.join(*savefig_dir)
        savefig_dir = os.path.join(outdir, savefig_dir)
                rgxin      = config.ipython_rgxin
        rgxout     = config.ipython_rgxout
        promptin   = config.ipython_promptin
        promptout  = config.ipython_promptout
        mplbackend = config.ipython_mplbackend
        exec_lines = config.ipython_execlines
        hold_count = config.ipython_holdcount
        return (savefig_dir, source_dir, rgxin, rgxout,
                promptin, promptout, mplbackend, exec_lines, hold_count)
    def setup(self):
                (savefig_dir, source_dir, rgxin, rgxout, promptin, promptout,
         mplbackend, exec_lines, hold_count) = self.get_config_options()
        if not os.path.exists(savefig_dir):
            os.makedirs(savefig_dir)
        if self.shell is None:
                                    
            if mplbackend and 'matplotlib.backends' not in sys.modules:
                import matplotlib
                matplotlib.use(mplbackend)
                                    self.shell = EmbeddedSphinxShell(exec_lines)
                        self.shell.directive = self
                                if not self.state.document.current_source in self.seen_docs:
            self.shell.IP.history_manager.reset()
            self.shell.IP.execution_count = 1
            self.seen_docs.add(self.state.document.current_source)
                self.shell.rgxin = rgxin
        self.shell.rgxout = rgxout
        self.shell.promptin = promptin
        self.shell.promptout = promptout
        self.shell.savefig_dir = savefig_dir
        self.shell.source_dir = source_dir
        self.shell.hold_count = hold_count
                self.shell.process_input_line('bookmark ipy_savedir %s'%savefig_dir,
                                      store_history=False)
        self.shell.clear_cout()
        return rgxin, rgxout, promptin, promptout
    def teardown(self):
                self.shell.process_input_line('bookmark -d ipy_savedir',
                                      store_history=False)
        self.shell.clear_cout()
    def run(self):
        debug = False
                        rgxin, rgxout, promptin, promptout = self.setup()
        options = self.options
        self.shell.is_suppress = 'suppress' in options
        self.shell.is_doctest = 'doctest' in options
        self.shell.is_verbatim = 'verbatim' in options
        self.shell.is_okexcept = 'okexcept' in options
        self.shell.is_okwarning = 'okwarning' in options
                if 'python' in self.arguments:
            content = self.content
            self.content = self.shell.process_pure_python(content)
                        parts = '\n'.join(self.content).split('\n\n')
        lines = ['.. code-block:: ipython', '']
        figures = []
        for part in parts:
            block = block_parser(part, rgxin, rgxout, promptin, promptout)
            if len(block):
                rows, figure = self.shell.process_block(block)
                for row in rows:
                    lines.extend(['   {0}'.format(line)
                                  for line in row.split('\n')])
                if figure is not None:
                    figures.append(figure)
        for figure in figures:
            lines.append('')
            lines.extend(figure.split('\n'))
            lines.append('')
        if len(lines) > 2:
            if debug:
                print('\n'.join(lines))
            else:
                                                                self.state_machine.insert_input(
                    lines, self.state_machine.input_lines.source(0))
                self.teardown()
        return []
def setup(app):
    setup.app = app
    app.add_directive('ipython', IPythonDirective)
    app.add_config_value('ipython_savefig_dir', None, 'env')
    app.add_config_value('ipython_rgxin',
                         re.compile('In \[(\d+)\]:\s?(.*)\s*'), 'env')
    app.add_config_value('ipython_rgxout',
                         re.compile('Out\[(\d+)\]:\s?(.*)\s*'), 'env')
    app.add_config_value('ipython_promptin', 'In [%d]:', 'env')
    app.add_config_value('ipython_promptout', 'Out[%d]:', 'env')
                    app.add_config_value('ipython_mplbackend', 'agg', 'env')
            execlines = ['import numpy as np', 'import matplotlib.pyplot as plt']
    app.add_config_value('ipython_execlines', execlines, 'env')
    app.add_config_value('ipython_holdcount', True, 'env')
    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}
    return metadata
def test():
    examples = [
        
In [1]: x = 'hello world'
@doctest
In [2]: x.upper()
Out[2]: 'HELLO WORLD'
@verbatim
In [3]: x.st<TAB>
x.startswith  x.strip
            try:
        imp = data.feature_importances_
    except:
        imp = np.array(data)
                try:
        sub_imp = np.array([e.feature_importances_ for e in data.estimators_])
                std = np.std(sub_imp, axis=0)
    except:
        std = None
        n_features = len(imp)
        if top_n and top_n > n_features:
        raise ValueError(('top_n ({}) cannot be greater than the number of'
                          ' features ({})'.format(top_n, n_features)))
    if top_n and top_n < 1:
        raise ValueError('top_n cannot be less than 1')
    if feature_names and len(feature_names) != n_features:
        raise ValueError(('feature_names ({}) must match the number of'
                          ' features ({})'.format(len(feature_names),
                                                  n_features)))
        if feature_names is None:
        feature_names = ['Feature {}'.format(n) for n in range(1, n_features+1)]
        feature_names = np.array(feature_names)
    else:
        feature_names = np.array(feature_names)
        idx = np.argsort(imp)[::-1]
    imp = imp[idx]
    feature_names = feature_names[idx]
    if std is not None:
        std = std[idx]
        if std is not None:
        names = 'feature_name,importance,std_'
        res = np.core.records.fromarrays([feature_names, imp, std],
                                         names=names)
    else:
        names = 'feature_name,importance'
        res = np.core.records.fromarrays([feature_names, imp],
                                         names=names)
        if top_n:
        res = res[:top_n]
    return res
import matplotlib.pyplot as plt
from .util import estimator_type, class_name
from . import plot

class ClassifierEvaluator(object):
    
    def __init__(self, estimator=None, y_true=None, y_pred=None, y_score=None,
                 feature_names=None, target_names=None, estimator_name=None):
        self._estimator = estimator
        self._y_true = y_true
        self._y_pred = y_pred
        self._y_score = y_score
        self._feature_names = feature_names
        self._target_names = target_names
        self._estimator_name = estimator_name
                
    @property
    def estimator_type(self):
                return estimator_type(self.estimator)
    @property
    def estimator_class(self):
                return class_name(self.estimator)
    @property
    def estimator(self):
        return self._estimator
    @property
    def y_true(self):
        return self._y_true
    @property
    def y_pred(self):
        return self._y_pred
    @property
    def y_score(self):
        return self._y_score
    @property
    def feature_names(self):
        return self._feature_names
    @property
    def target_names(self):
        return self._target_names
    @property
    def estimator_name(self):
        return self._estimator_name
    @property
    def confusion_matrix(self):
                return plot.confusion_matrix(self.y_true, self.y_pred,
                                     self.target_names, ax=_gen_ax())
    @property
    def roc(self):
                return plot.roc(self.y_true, self.y_score, ax=_gen_ax())
    @property
    def precision_recall(self):
                return plot.precision_recall(self.y_true, self.y_score, ax=_gen_ax())
    @property
    def feature_importances(self):
                return plot.feature_importances(self.estimator,
                                        feature_names=self.feature_names,
                                        ax=_gen_ax())
    @property
    def feature_importances_table(self):
                from . import table
        return table.feature_importances(self.estimator,
                                         feature_names=self.feature_names)
    @property
    def precision_at_proportions(self):
                return plot.precision_at_proportions(self.y_true, self.y_score,
                                             ax=_gen_ax())
    def generate_report(self, template, path=None, style=None):
                from .report import generate
        return generate(self, template, path, style)

def _gen_ax():
    fig = plt.figure()
    ax = fig.add_subplot(111)
    return ax
import numpy as np
from sklearn.metrics import precision_score
from . import validate

@validate.proportion
def precision_at(y_true, y_score, proportion, ignore_nas=False):
    '''
    Calculates precision at a given proportion.
    Only supports binary classification.
    '''
        scores_sorted = np.sort(y_score)[::-1]
            cutoff_index = max(int(len(y_true) * proportion) - 1, 0)
        cutoff_value = scores_sorted[cutoff_index]
        scores_binary = np.array([int(y >= cutoff_value) for y in y_score])
        if ignore_nas:
        precision = __precision(y_true, scores_binary)
    else:
        precision = precision_score(y_true, scores_binary)
    return precision, cutoff_value

@validate.proportion
def __threshold_at(y_score, proportion):
        scores_sorted = np.sort(y_score)[::-1]
            threshold_index = max(int(len(y_score) * proportion) - 1, 0)
        threshold_value = scores_sorted[threshold_index]
    return threshold_value

@validate.proportion
def __binarize_scores_at(y_score, proportion):
    threshold_value = __threshold_at(y_score, proportion)
    y_score_binary = np.array([int(y >= threshold_value) for y in y_score])
    return y_score_binary

def __precision(y_true, y_pred):
    '''
        Precision metric tolerant to unlabeled data in y_true,
        NA values are ignored for the precision calculation
    '''
        y_true = np.copy(y_true)
    y_pred = np.copy(y_pred)
                    is_nan = np.isnan(y_true)
    y_true[is_nan] = 0
    y_pred[is_nan] = 0
    precision = precision_score(y_true, y_pred)
    return precision

@validate.proportion
def tp_at(y_true, y_score, proportion):
    y_pred = __binarize_scores_at(y_score, proportion)
    tp = (y_pred == 1) & (y_true == 1)
    return tp.sum()

@validate.proportion
def fp_at(y_true, y_score, proportion):
    y_pred = __binarize_scores_at(y_score, proportion)
    fp = (y_pred == 1) & (y_true == 0)
    return fp.sum()

@validate.proportion
def tn_at(y_true, y_score, proportion):
    y_pred = __binarize_scores_at(y_score, proportion)
    tn = (y_pred == 0) & (y_true == 0)
    return tn.sum()

@validate.proportion
def fn_at(y_true, y_score, proportion):
    y_pred = __binarize_scores_at(y_score, proportion)
    fn = (y_pred == 0) & (y_true == 1)
    return fn.sum()

@validate.proportion
def labels_at(y_true, y_score, proportion, normalize=False):
    '''
        Return the number of labels encountered in the top  X proportion
    '''
        indexes = np.argsort(y_score)[::-1]
        y_true_sorted = y_true[indexes]
        cutoff_index = max(int(len(y_true_sorted) * proportion) - 1, 0)
        y_true_top = y_true_sorted[:cutoff_index+1]
            values = int((~np.isnan(y_true_top)).sum())
    if normalize:
        values = float(values)/(~np.isnan(y_true)).sum()
    return values
import base64
import re
from datetime import datetime
from functools import reduce
from .table import Table
from six import BytesIO
import matplotlib
try:
    import mistune
except:
    raise ImportError('You need to install mistune to generate reports')

def generate(evaluator, template, path=None, style=None):
        
    date_utc = '{} UTC'.format(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'))
    _extra_tags = {
        'date_utc':  date_utc,
        'date': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
    }
            if template.endswith('.md'):
        with open(template, 'r') as f:
            template = f.read()
        lines = [l.strip() for l in template.splitlines()]
    template = reduce(lambda x, y: x+'\n'+y, lines)
        tags = parse_tags(template)
            tags = filter(lambda t: t not in _extra_tags.keys(), tags)
        attrs = getattr_from_list(evaluator, tags)
            for k, v in attrs.items():
        if isinstance(v, matplotlib.axes.Axes):
            attrs[k] = figure2html(v.get_figure())
        if isinstance(v, Table):
            attrs[k] = v.html
        attrs.update(_extra_tags)
        report = template.format(**attrs)
        report = to_html(report, style)
    if path is not None:
        report_file = open(path, 'w')
        report_file.write(report)
        report_file.close()
    else:
        return report

def to_html(template, style):
        markdown = mistune.Markdown()
    html = markdown(template)
        if style is not None:
        f = open(style, 'r')
        css = f.read()
        html = '<style>'+css+'</style>'+html
    return html

def figure2html(fig):
    return base64_2_html(figure2base64(fig))

def base64_2_html(img):
    try:
        html = '<img src="data:image/png;base64,'+img+'"></img>'      except:
        img = img.decode("utf-8")
        html = '<img src="data:image/png;base64,'+img+'"></img>'      return html

def figure2base64(fig):
    io = BytesIO()
    fig.savefig(io, format='png')
    try:
        fig_base64 = base64.encodebytes(io.getvalue())      except:
        fig_base64 = base64.encodestring(io.getvalue())      return fig_base64

def prettify_list(l):
    l = [str(idx+1)+'. '+str(el) for idx, el in enumerate(l)]
    return reduce(lambda x, y: x+'<br>'+y, l)

def prettify_dict(d):
    return prettify_list([key+': '+str(d[key]) for key in d.keys()])

def parse_tags(s):
        return re.findall('{(\w+)\}*', s)

def getattr_from_list(obj, attr_names):
    return {attr: getattr(obj, attr) for attr in attr_names}
from . import compute

__all__ = ['feature_importances']

class Table():
    def __init__(self, content, header):
        try:
            self._tabulate = __import__('tabulate').tabulate
        except:
            raise ImportError('tabulate is required to use the table module')
        self.content = content
        self.header = header
    @property
    def html(self):
        return self._tabulate(self.content, headers=self.header,
                              tablefmt='html')
    def __str__(self):
        return self._tabulate(self.content, headers=self.header,
                              tablefmt='grid')
    def _repr_html_(self):
        return self.html

def feature_importances(data, top_n=None, feature_names=None):
        res = compute.feature_importances(data, top_n, feature_names)
    return Table(res, res.dtype.names)
import re
import collections
from collections import defaultdict, namedtuple
from itertools import product
from six import string_types
import numpy as np

def estimator_type(model):
    s = str(type(model))
    model_name = re.search(".*'(.+?)'.*", s).group(1).split(".")[-1]
    return model_name

def class_name(obj):
    class_name = str(type(obj))
    class_name = re.search(".*'(.+?)'.*", class_name).group(1)
    return class_name

def _can_iterate(obj):
    is_string = isinstance(obj, string_types)
    is_iterable = isinstance(obj, collections.Iterable)
    return is_iterable and not is_string

def is_column_vector(x):
    return len(x.shape) == 2 and x.shape[1] == 1

def is_row_vector(x):
    return len(x.shape) == 1

def _group_by(data, criteria):
        if isinstance(criteria, str):
        criteria_str = criteria
        def criteria(x):
            return x[criteria_str]
    res = defaultdict(list)
    for element in data:
        key = criteria(element)
        res[key].append(element)
    return res

def _get_params_value(params):
            ord_params = sorted(params)
    def fn(obj):
        l = []
        for p in ord_params:
            try:
                l.append((p, obj.parameters[p]))
            except:
                raise ValueError('{} is not a valid parameter'.format(p))
        return tuple(l)
    return fn

def _sorted_map_iter(d):
    ord_keys = sorted(d.keys())
    for k in ord_keys:
        yield (k, d[k])

def _product(k, v):
        if not _can_iterate(k):
        k = [k]
    if not _can_iterate(v):
        v = [v]
    return list(product(k, v))

def _mapping_to_tuple_pairs(d):
                    t = []
    ord_keys = sorted(d.keys())
    for k in ord_keys:
        t.append(_product(k, d[k]))
    return tuple(product(*t))

def _flatten_list(l):
    return [item for sublist in l for item in sublist]

def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):
    import matplotlib.colors as colors
    import numpy as np
    new_cmap = colors.LinearSegmentedColormap.from_list(
        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),
        cmap(np.linspace(minval, maxval, n)))
    return new_cmap

def default_heatmap():
    import matplotlib.pyplot as plt
    return truncate_colormap(plt.cm.OrRd, 0.1, 0.7)

def _dict2named_tuple(d):
    return namedtuple('NamedTupleFromDict', d.keys())(**d)

def _grid_scores_from_dicts(grid_scores):
            for score in grid_scores:
        val_scores_key = 'cv_validation_scores'
        score[val_scores_key] = np.array(score[val_scores_key])
        grid_scores = [_dict2named_tuple(d) for d in grid_scores]
    return grid_scores
import inspect
from decorator import decorator

@decorator
def proportion(func, *args, **kwargs):
        try:
        proportion = kwargs['proportion']
    except:
        try:
            fn_args = inspect.getargspec(func).args
            idx = fn_args.index('proportion')
            proportion = args[idx]
        except:
            proportion = None
        if (not (0 <= proportion <= 1.0)) and proportion is not None:
        raise ValueError('Proportion must be between 0 and 1.0')
    return func(*args, **kwargs)
__version__ = '0.4'
from .evaluator import ClassifierEvaluator
__all__ = ['ClassifierEvaluator']
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix as sk_confusion_matrix
from ..metrics import precision_at
from .. import compute
from ..util import is_column_vector, is_row_vector, default_heatmap

def confusion_matrix(y_true, y_pred, target_names=None, normalize=False,
                     cmap=None, ax=None):
            values = set(y_true).union(set(y_pred))
    expected_len = len(values)
    if target_names and (expected_len != len(target_names)):
        raise ValueError(('Data cointains {} different values, but target'
                         ' names contains {} values.'.format(expected_len,
                                                             len(target_names)
                                                             )))
        if not target_names:
        values = list(values)
        values.sort()
        target_names = ['Class {}'.format(v) for v in values]
    cm = sk_confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    np.set_printoptions(precision=2)
    if ax is None:
        ax = plt.gca()
                        for (y, x), v in np.ndenumerate(cm):
        try:
            label = '{:.2}'.format(v)
        except:
            label = v
        ax.text(x, y, label, horizontalalignment='center',
                verticalalignment='center')
    if cmap is None:
        cmap = default_heatmap()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.colorbar(im, ax=ax)
    tick_marks = np.arange(len(target_names))
    ax.set_xticks(tick_marks)
    ax.set_xticklabels(target_names)
    ax.set_yticks(tick_marks)
    ax.set_yticklabels(target_names)
    title = 'Confusion matrix'
    if normalize:
        title += ' (normalized)'
    ax.set_title(title)
    ax.set_ylabel('True label')
    ax.set_xlabel('Predicted label')
    return ax


def feature_importances(data, top_n=None, feature_names=None, ax=None):
            res = compute.feature_importances(data, top_n, feature_names)
        n_feats = len(res)
    if ax is None:
        ax = plt.gca()
    ax.set_title("Feature importances")
    try:
        ax.bar(range(n_feats), res.importance, yerr=res.std_, color='red',
               align="center")
    except:
        ax.bar(range(n_feats), res.importance, color='red',
               align="center")
    ax.set_xticks(range(n_feats))
    ax.set_xticklabels(res.feature_name)
    ax.set_xlim([-1, n_feats])
    return ax

def precision_at_proportions(y_true, y_score, ax=None):
        if ax is None:
        ax = plt.gca()
    y_score_is_vector = is_column_vector(y_score) or is_row_vector(y_score)
    if not y_score_is_vector:
        y_score = y_score[:, 1]
        proportions = [0.01 * i for i in range(1, 101)]
    precs_and_cutoffs = [precision_at(y_true, y_score, p) for p in proportions]
    precs, cutoffs = zip(*precs_and_cutoffs)
        ax.plot(proportions, precs)
    ax.set_title('Precision at various proportions')
    ax.set_ylabel('Precision')
    ax.set_xlabel('Proportion')
    ticks = [0.1 * i for i in range(1, 11)]
    ax.set_xticks(ticks)
    ax.set_xticklabels(ticks)
    ax.set_yticks(ticks)
    ax.set_yticklabels(ticks)
    ax.set_ylim([0, 1.0])
    ax.set_xlim([0, 1.0])
    return ax
from functools import reduce
from operator import itemgetter
import matplotlib.pyplot as plt
import numpy as np
from six import string_types
from ..util import (_group_by, _get_params_value, _mapping_to_tuple_pairs,
                    default_heatmap, _sorted_map_iter, _flatten_list)

def grid_search(grid_scores, change, subset=None, kind='line', cmap=None,
                ax=None):
        if change is None:
        raise ValueError(('change can\'t be None, you need to select at least'
                          ' one value to make the plot.'))
    if ax is None:
        ax = plt.gca()
    if cmap is None:
        cmap = default_heatmap()
    if isinstance(change, string_types) or len(change) == 1:
        return _grid_search_single(grid_scores, change, subset, kind, ax)
    elif len(change) == 2:
        return _grid_search_double(grid_scores, change, subset, cmap, ax)
    else:
        raise ValueError('change must have length 1 or 2 or be a string')

def _grid_search_single(grid_scores, change, subset, kind, ax):
                
        params = set(grid_scores[0].parameters.keys())
            try:
        params.remove(change)
    except KeyError:
        raise ValueError('{} is not a valid parameter'.format(change))
                if subset:
                        groups = _group_by(grid_scores, _get_params_value(subset.keys()))
        keys = _mapping_to_tuple_pairs(subset)
        groups = {k: v for k, v in _sorted_map_iter(groups) if k in keys}
        grid_scores = _flatten_list(groups.values())
        groups = _group_by(grid_scores, _get_params_value(params))
        if not groups:
            raise ValueError(('Your subset didn\'t match any data'
                              ' verify that the values are correct.'))
                else:
        groups = _group_by(grid_scores, _get_params_value(params))
    if kind == 'bar':
        change_unique = len(set([g.parameters[change] for g in grid_scores]))
                        bar_shifter = BarShifter(g_number=change_unique, g_size=len(groups),
                                 ax=ax)
    for params_kv, group in _sorted_map_iter(groups):
                        x = [element.parameters[change] for element in group]
        y = [element.mean_validation_score for element in group]
        stds = [element.cv_validation_scores.std() for element in group]
                label = ['{}: {}'.format(*t) for t in params_kv]
                label = reduce(lambda x, y: x+', '+y, label)
        if kind == 'bar':
            bar_shifter(y, yerr=stds, label=label)
        elif kind == 'line':
            is_categorical = isinstance(x[0], string_types)
            if is_categorical:
                ints = range(len(x))
                ax.errorbar(ints, y, yerr=stds, label=label)
                ax.set_xticks(ints)
            else:
                ax.errorbar(x, y, yerr=stds, label=label)
                ax.set_xticks(x)
    ax.set_xticklabels(x)
    ax.set_title('Grid search results')
    ax.set_ylabel('Mean score')
    ax.set_xlabel(change)
    ax.legend(loc="best")
    ax.margins(0.05)
    return ax

def _grid_search_double(grid_scores, change, subset, cmap, ax):
        if len(set(change)) == 1:
        raise ValueError('You need to pass two different parameters')
        if subset is not None:
        groups = _group_by(grid_scores, _get_params_value(subset.keys()))
        keys = _mapping_to_tuple_pairs(subset)
        groups = {k: v for k, v in _sorted_map_iter(groups) if k in keys}
        grid_scores = _flatten_list(groups.values())
        if not groups:
            raise ValueError(('Your subset didn\'t match any data'
                              ' verify that the values are correct.'))
        matrix_elements = _group_by(grid_scores, _get_params_value(change))
    for k, v in matrix_elements.items():
        if len(v) > 1:
            raise ValueError(('More than one result matched your criteria.'
                              ' Make sure you specify parameters using change'
                              ' and subset so only one group matches.'))
        matrix_elements = {k: v[0] for k, v in matrix_elements.items()}
                row_names = sorted(set([t[0] for t in matrix_elements.keys()]),
                       key=itemgetter(1))
    col_names = sorted(set([t[1] for t in matrix_elements.keys()]),
                       key=itemgetter(1))
        cols = len(col_names)
    rows = len(row_names)
        x_coord = {k: v for k, v in zip(col_names, range(cols))}
    y_coord = {k: v for k, v in zip(row_names, range(rows))}
            m = {(x_coord[k[1]], y_coord[k[0]]): v for k, v in matrix_elements.items()}
    matrix = np.zeros((rows, cols))
    for (j, i), v in m.items():
        matrix[i][j] = v.mean_validation_score
        row_labels = ['{}={}'.format(*x) for x in row_names]
    col_labels = ['{}={}'.format(*y) for y in col_names]
    im = ax.imshow(matrix, interpolation='nearest', cmap=cmap)
        for (x, y), v in m.items():
        label = '{:.3}'.format(v.mean_validation_score)
        ax.text(x, y, label, horizontalalignment='center',
                verticalalignment='center')
    ax.set_xticks(range(cols))
    ax.set_xticklabels(col_labels, rotation=45)
    ax.set_yticks(range(rows))
    ax.set_yticklabels(row_labels)
    plt.colorbar(im, ax=ax)
    ax.get_figure().tight_layout()
    return ax

class BarShifter:
                            def __init__(self, g_number, g_size, ax, scale=0.8):
        self.g_number = g_number
        self.g_size = g_size
        self.ax = ax
        self.i = 0
        self.width = (1.0/g_size)*scale
        self.colors = plt.get_cmap()(np.linspace(0, 1, self.g_size))
    def __call__(self, height, bottom=None, hold=None, **kwargs):
        left = [x+self.i*self.width for x in range(self.g_number)]
        self.ax.bar(left, height, self.width, color=self.colors[self.i],
                    ecolor=self.colors[self.i],
                    **kwargs)
        self.i += 1
        if self.i == self.g_size:
            n = range(self.g_number)
            ticks_pos = [x+(self.width*self.g_size)/2.0 for x in n]
            self.ax.set_xticks(ticks_pos)
import numpy as np
import matplotlib.pyplot as plt

def learning_curve(train_scores, test_scores, train_sizes, ax=None):
        if ax is None:
        ax = plt.gca()
    ax.grid()
    ax.set_title("Learning Curve")
    ax.set_xlabel("Training examples")
    ax.set_ylabel("Score mean")
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.1,
                    color="r")
    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.1, color="g")
    ax.plot(train_sizes, train_scores_mean, 'o-', color="r",
            label="Training score")
    ax.plot(train_sizes, test_scores_mean, 'o-', color="g",
            label="Cross-validation score")
    ax.legend(loc="best")
    ax.margins(0.05)
    return ax
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.preprocessing import label_binarize
from ..util import is_column_vector, is_row_vector

def precision_recall(y_true, y_score, ax=None):
        if ax is None:
        ax = plt.gca()
        y_score_is_vector = is_column_vector(y_score) or is_row_vector(y_score)
    if y_score_is_vector:
        n_classes = 2
    else:
        _, n_classes = y_score.shape
    
    if n_classes > 2:
                y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
        _precision_recall_multi(y_true_bin, y_score, ax=ax)
        for i in range(n_classes):
            _precision_recall(y_true_bin[:, i], y_score[:, i], ax=ax)
    else:
        if y_score_is_vector:
            _precision_recall(y_true, y_score, ax)
        else:
            _precision_recall(y_true, y_score[:, 1], ax)
        return ax

def _precision_recall(y_true, y_score, ax=None):
        precision, recall, _ = precision_recall_curve(y_true, y_score)
    average_precision = average_precision_score(y_true, y_score)
    if ax is None:
        ax = plt.gca()
    ax.plot(recall, precision, label=('Precision-Recall curve: AUC={0:0.2f}'
                                      .format(average_precision)))
    _set_ax_settings(ax)
    return ax

def _precision_recall_multi(y_true, y_score, ax=None):
            precision, recall, _ = precision_recall_curve(y_true.ravel(),
                                                  y_score.ravel())
    avg_prec = average_precision_score(y_true, y_score, average="micro")
    if ax is None:
        ax = plt.gca()
    ax.plot(recall, precision,
            label=('micro-average Precision-recall curve (area = {0:0.2f})'
                   .format(avg_prec)))
    _set_ax_settings(ax)
    return ax

def _set_ax_settings(ax):
    ax.set_title('Precision-Recall')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Recall')
    ax.set_ylabel('Precision')
    ax.legend(loc="best")
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from ..util import is_column_vector, is_row_vector

def roc(y_true, y_score, ax=None):
        if ax is None:
        ax = plt.gca()
        y_score_is_vector = is_column_vector(y_score) or is_row_vector(y_score)
    if y_score_is_vector:
        n_classes = 2
    else:
        _, n_classes = y_score.shape
    
    if n_classes > 2:
                y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
        _roc_multi(y_true_bin, y_score, ax=ax)
        for i in range(n_classes):
            _roc(y_true_bin[:, i], y_score[:, i], ax=ax)
    else:
        if y_score_is_vector:
            _roc(y_true, y_score, ax)
        else:
            _roc(y_true, y_score[:, 1], ax)
        return ax

def _roc(y_true, y_score, ax=None):
        
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    ax.plot(fpr, tpr, label=('ROC curve (area = {0:0.2f})'.format(roc_auc)))
    _set_ax_settings(ax)
    return ax

def _roc_multi(y_true, y_score, ax=None):
            fpr, tpr, _ = roc_curve(y_true.ravel(), y_score.ravel())
    roc_auc = auc(fpr, tpr)
    if ax is None:
        ax = plt.gca()
    ax.plot(fpr, tpr, label=('micro-average ROC curve (area = {0:0.2f})'
                             .format(roc_auc)))
    _set_ax_settings(ax)
    return ax

def _set_ax_settings(ax):
    ax.plot([0, 1], [0, 1], 'k--')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('ROC')
    ax.legend(loc="best")
import numpy as np
import matplotlib.pyplot as plt

def validation_curve(train_scores, test_scores, param_range, param_name=None,
                     semilogx=False, ax=None):
    
    if ax is None:
        ax = plt.gca()
    if semilogx:
        ax.set_xscale('log')
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    ax.set_title("Validation Curve")
    ax.set_ylabel("Score mean")
    if param_name:
        ax.set_xlabel(param_name)
    ax.plot(param_range, train_scores_mean, label="Training score", color="r")
    ax.plot(param_range, test_scores_mean, label="Cross-validation score",
            color="g")
    ax.fill_between(param_range, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.2, color="r")
    ax.fill_between(param_range, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.2, color="g")
    ax.legend(loc="best")
    ax.margins(0.05)
    return ax
from .classification import (confusion_matrix,
                             feature_importances,
                             precision_at_proportions)
from .roc import roc
from .precision_recall import precision_recall
from .grid_search import grid_search
from .validation_curve import validation_curve
from .learning_curve import learning_curve

__all__ = ['confusion_matrix', 'feature_importances', 'precision_recall',
           'roc', 'precision_at_proportions', 'grid_search',
           'validation_curve', 'learning_curve']
import sys
from setuptools import find_packages, setup
exec(compile(open('skll/version.py').read(), 'skll/version.py', 'exec'))

def readme():
    with open('README.rst') as f:
        return f.read()

def requirements():
        if sys.version_info < (3, 0):
        req_path = 'requirements_rtd.txt'
        else:
        req_path = 'requirements.txt'
    with open(req_path) as f:
        reqs = f.read().splitlines()
    return reqs

setup(name='skll',
      version=__version__,
      description=('SciKit-Learn Laboratory makes it easier to run machine'
                   'learning experiments with scikit-learn.'),
      long_description=readme(),
      keywords='learning scikit-learn',
      url='http://github.com/EducationalTestingService/skll',
      author='Daniel Blanchard',
      author_email='dblanchard@ets.org',
      license='BSD 3 clause',
      packages=find_packages(),
      include_package_data=True,
      entry_points={'console_scripts':
                    ['filter_features = skll.utilities.filter_features:main',
                     'generate_predictions = skll.utilities.generate_predictions:main',
                     'join_features = skll.utilities.join_features:main',
                     'print_model_weights = skll.utilities.print_model_weights:main',
                     'run_experiment = skll.utilities.run_experiment:main',
                     'skll_convert = skll.utilities.skll_convert:main',
                     'summarize_results = skll.utilities.summarize_results:main',
                     'compute_eval_from_predictions = skll.utilities.compute_eval_from_predictions:main']},
      install_requires=requirements(),
      classifiers=['Intended Audience :: Science/Research',
                   'Intended Audience :: Developers',
                   'License :: OSI Approved :: BSD License',
                   'Programming Language :: Python',
                   'Topic :: Software Development',
                   'Topic :: Scientific/Engineering',
                   'Operating System :: Microsoft :: Windows',
                   'Operating System :: POSIX',
                   'Operating System :: Unix',
                   'Operating System :: MacOS',
                   'Programming Language :: Python :: 2',
                   'Programming Language :: Python :: 2.7',
                   'Programming Language :: Python :: 3',
                   'Programming Language :: Python :: 3.3',
                   'Programming Language :: Python :: 3.4',
                   ],
      zip_safe=False)
import sys, os
sys.path.insert(0, os.path.abspath('..'))
import skll

extensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode', 'sphinx.ext.mathjax']
templates_path = ['_templates']
source_suffix = '.rst'

master_doc = 'index'
project = u'SciKit-Learn Laboratory'
copyright = u'2012-2017, Educational Testing Service'
version = skll.version.__version__
release = version

exclude_patterns = ['_build']


pygments_style = 'sphinx'


on_rtd = os.environ.get('READTHEDOCS', None) == 'True'
if not on_rtd:
    import sphinx_rtd_theme
    html_theme = 'sphinx_rtd_theme'
    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]









htmlhelp_basename = 'SKLLdoc'

latex_elements = {
'papersize': 'letterpaper',
'pointsize': '12pt'
}
latex_documents = [
  ('index', 'SKLL.tex', u'SciKit-Learn Laboratory Documentation',
   u'Daniel Blanchard \\and Michael Heilman \\and Nitin Madnani', 'manual'),
]




man_pages = [
    ('index', 'SKLL', u'SciKit-Learn Laboratory Documentation',
     [u'Daniel Blanchard, Michael Heilman, and Nitin Madnani'], 1)
]


texinfo_documents = [
  ('index', 'SKLL', u'SciKit-Learn Laboratory Documentation',
   u'Daniel Blanchard, Michael Heilman, and Nitin Madnani', 'SciKit-LearnLab', 'One line description of project.',
   'Miscellaneous'),
]



epub_title = u'SciKit-Learn Laboratory'
epub_author = u'Daniel Blanchard, Michael Heilman, and Nitin Madnani'
epub_publisher = u'Educational Testing Service'
epub_copyright = u'2012-2014, Educational Testing Service'








from __future__ import print_function, unicode_literals
import json
import os
import sys
import sklearn.datasets
from sklearn.cross_validation import train_test_split

def main():
        print('Retrieving boston data from servers...', end='')
    boston_data = sklearn.datasets.load_boston()
    sys.stdout.flush()
    print('done')
    sys.stdout.flush()

    X = boston_data['data']
    Y = boston_data['target']
    examples = [{'id': 'EXAMPLE_{}'.format(i),
                 'y': y,
                 'x': {'f{}'.format(j): x_val for j, x_val in enumerate(x)}}
                for i, (x, y) in enumerate(zip(X, Y))]
    examples_train, examples_test = train_test_split(examples, test_size=0.33,
                                                     random_state=42)
    print('Writing training and testing files...', end='')
    for examples, suffix in [(examples_train, 'train'), (examples_test,
                                                         'test')]:
        boston_dir = os.path.join('boston', suffix)
        if not os.path.exists(boston_dir):
            os.makedirs(boston_dir)
        jsonlines_path = os.path.join(boston_dir,
                                      'example_boston_features.jsonlines')
        with open(jsonlines_path, 'w') as f:
            for ex in examples:
                f.write('{}\n'.format(json.dumps(ex)))
    print('done')

if __name__ == '__main__':
    main()

from __future__ import print_function, unicode_literals
import json
import os
import sys
import sklearn.datasets
from sklearn.cross_validation import train_test_split

def main():
        print('Retrieving iris data from servers...', end='')
    iris_data = sklearn.datasets.load_iris()
    print('done')
    sys.stdout.flush()

    X = iris_data['data']
    Y = [iris_data['target_names'][label] for label in iris_data['target']]
    examples = [{'id': 'EXAMPLE_{}'.format(i),
                 'y': y,
                 'x': {'f{}'.format(j): x_val for j, x_val in enumerate(x)}}
                for i, (x, y) in enumerate(zip(X, Y))]
    examples_train, examples_test = train_test_split(examples, test_size=0.33,
                                                     random_state=42)
    print('Writing training and testing files...', end='')
    for examples, suffix in [(examples_train, 'train'), (examples_test,
                                                         'test')]:
        iris_dir = os.path.join('iris', suffix)
        if not os.path.exists(iris_dir):
            os.makedirs(iris_dir)
        jsonlines_path = os.path.join(iris_dir,
                                      'example_iris_features.jsonlines')
        with open(jsonlines_path, 'w') as f:
            for ex in examples:
                f.write('{}\n'.format(json.dumps(ex)))
    print('done')

if __name__ == '__main__':
    main()
from __future__ import division, print_function, unicode_literals
import logging
import os
import sys
from itertools import chain
from skll import Writer, Reader
def main():
        logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'), level=logging.INFO)
    logger = logging.getLogger(__name__)
    if not (os.path.exists('train.csv') and os.path.exists('test.csv')):
        logger.error('This script requires the train.csv and test.csv files ' +
                     'from http://www.kaggle.com/c/titanic-gettingStarted/' +
                     'data to be in the current directory in order to work. ' +
                     'Please download them and try again.')
        sys.exit(1)
        subset_dict = {'vitals': ['Sex', 'Age'],
                   'socioeconomic': ['Pclass', 'Fare'],
                   'family': ['SibSp', 'Parch'],
                   'misc': ['Embarked']}
    features_to_keep = list(chain(*subset_dict.values()))
        if not os.path.exists('titanic/train'):
        logger.info('Creating titanic/train directory')
        os.makedirs('titanic/train')
    if not os.path.exists('titanic/dev'):
        logger.info('Creating titanic/dev directory')
        os.makedirs('titanic/dev')
    if not os.path.exists('titanic/train+dev'):
        logger.info('Creating titanic/train+dev directory')
        os.makedirs('titanic/train+dev')
    if not os.path.exists('titanic/test'):
        logger.info('Creating titanic/test directory')
        os.makedirs('titanic/test')
        train_fs = Reader.for_path('train.csv', label_col='Survived',
                               id_col='PassengerId', quiet=False,
                               sparse=False).read()
    train_fs.filter(features=features_to_keep)
    num_train_dev = len(train_fs)
    num_train = int((num_train_dev / 5) * 4)
    writer = Writer.for_path('titanic/train/.csv',
                                       train_fs[:num_train],
                                       id_col='PassengerId',
                                       label_col='Survived',
                                       quiet=False,
                                       subsets=subset_dict)
    writer.write()
            writer = Writer.for_path('titanic/train+dev/.csv',
                                       train_fs,
                                       label_col='Survived',
                                       id_col='PassengerId',
                                       quiet=False,
                                       subsets=subset_dict)
    writer.write()
        writer = Writer.for_path('titanic/dev/.csv',
                                       train_fs[num_train:],
                                       label_col='Survived',
                                       id_col='PassengerId',
                                       quiet=False,
                                       subsets=subset_dict)
    writer.write()
        test_fs = Reader.for_path('test.csv', label_col='Survived', quiet=False,
                              sparse=False).read()
    test_fs.filter(features=features_to_keep)
    num_test = len(test_fs)
    test_fs.ids = list(range(num_train_dev + 1, num_test + num_train_dev + 1))
    writer = Writer.for_path('titanic/test/.csv',
                                       test_fs,
                                       label_col='Survived',
                                       id_col='PassengerId',
                                       quiet=False,
                                       subsets=subset_dict)
    writer.write()

if __name__ == '__main__':
    main()
from __future__ import absolute_import, print_function, unicode_literals
import csv
import errno
import itertools
import logging
import os
from io import open
from os.path import (basename, dirname, exists,
                     isabs, join, normpath, realpath)
import configparser  import numpy as np
import ruamel.yaml as yaml
from six import string_types, iteritems  from sklearn.metrics import SCORERS

_VALID_TASKS = frozenset(['predict', 'train', 'evaluate',
                          'cross_validate', 'learning_curve'])
_VALID_SAMPLERS = frozenset(['Nystroem', 'RBFSampler', 'SkewedChi2Sampler',
                             'AdditiveChi2Sampler', ''])
_VALID_FEATURE_SCALING_OPTIONS = frozenset(['with_std', 'with_mean', 'both',
                                            'none'])

class SKLLConfigParser(configparser.ConfigParser):
    
    def __init__(self):
                        required = ['experiment_name', 'task', 'learners']
                        defaults = {'class_map': '{}',
                    'custom_learner_path': '',
                    'cv_folds_file': '',
                    'feature_hasher': 'False',
                    'feature_scaling': 'none',
                    'featuresets': '[]',
                    'featureset_names': '[]',
                    'fixed_parameters': '[]',
                    'grid_search': 'False',
                    'grid_search_folds': '3',
                    'grid_search_jobs': '0',
                    'hasher_features': '0',
                    'id_col': 'id',
                    'ids_to_floats': 'False',
                    'label_col': 'y',
                    'log': '',
                    'learning_curve_cv_folds_list': '[]',
                    'learning_curve_train_sizes': '[]',
                    'min_feature_count': '1',
                    'models': '',
                    'num_cv_folds': '10',
                    'objectives': "['f1_score_micro']",
                    'objective': "f1_score_micro",
                    'param_grids': '[]',
                    'pos_label_str': '',
                    'predictions': '',
                    'probability': 'False',
                    'random_folds': 'False',
                    'results': '',
                    'sampler': '',
                    'sampler_parameters': '[]',
                    'save_cv_folds': 'False',
                    'shuffle': 'False',
                    'suffix': '',
                    'test_directory': '',
                    'test_file': '',
                    'train_directory': '',
                    'train_file': ''}
        correct_section_mapping = {'class_map': 'Input',
                                   'custom_learner_path': 'Input',
                                   'cv_folds_file': 'Input',
                                   'feature_hasher': 'Input',
                                   'feature_scaling': 'Input',
                                   'featuresets': 'Input',
                                   'featureset_names': 'Input',
                                   'fixed_parameters': 'Input',
                                   'grid_search': 'Tuning',
                                   'grid_search_folds': 'Tuning',
                                   'grid_search_jobs': 'Tuning',
                                   'hasher_features': 'Input',
                                   'id_col': 'Input',
                                   'ids_to_floats': 'Input',
                                   'label_col': 'Input',
                                   'log': 'Output',
                                   'learning_curve_cv_folds_list': 'Input',
                                   'learning_curve_train_sizes': 'Input',
                                   'min_feature_count': 'Tuning',
                                   'models': 'Output',
                                   'num_cv_folds': 'Input',
                                   'objectives': 'Tuning',
                                   'objective': 'Tuning',
                                   'param_grids': 'Tuning',
                                   'pos_label_str': 'Tuning',
                                   'predictions': 'Output',
                                   'probability': 'Output',
                                   'random_folds': 'Input',
                                   'results': 'Output',
                                   'sampler': 'Input',
                                   'sampler_parameters': 'Input',
                                   'save_cv_folds': 'Output',
                                   'shuffle': 'Input',
                                   'suffix': 'Input',
                                   'test_directory': 'Input',
                                   'test_file': 'Input',
                                   'train_directory': 'Input',
                                   'train_file': 'Input'}
                        assert defaults.keys() == correct_section_mapping.keys()
        super(SKLLConfigParser, self).__init__(defaults=defaults)
        self._required_options = required
        self._section_mapping = correct_section_mapping
    def _find_invalid_options(self):
        
                valid_options = list(self._defaults.keys()) + self._required_options
                specified_options = set(itertools.chain(*[self.options(section)
                                                  for section in self.sections()]))
                invalid_options = set(specified_options).difference(valid_options)
        return invalid_options
    def _find_ill_specified_options(self):
                incorrectly_specified_options = []
        multiply_specified_options = []
        for option_name, default_value in self._defaults.items():
            used_sections = [section for section in self.sections()
                             if self.get(section, option_name) != default_value]
                        if len(used_sections) == 0:
                pass
                        elif len(used_sections) > 1:
                multiply_specified_options.append((option_name, used_sections))
                        elif used_sections[0] != self._section_mapping[option_name]:
                incorrectly_specified_options.append((option_name,
                                                      used_sections[0]))
        return (incorrectly_specified_options, multiply_specified_options)
    def validate(self):
        
        invalid_options = self._find_invalid_options()
        if invalid_options:
            raise KeyError('Configuration file contains the following '
                           'unrecognized options: {}'
                           .format(list(invalid_options)))
        incorrectly_specified_options, multiply_specified_options = self._find_ill_specified_options()
        if multiply_specified_options:
            raise KeyError('The following are defined in multiple sections: '
                           '{}'.format([t[0] for t in
                                        multiply_specified_options]))
        if incorrectly_specified_options:
            raise KeyError('The following are not defined in the appropriate '
                           'sections: {}'.format([t[0] for t in
                                                  incorrectly_specified_options]))

def _locate_file(file_path, config_dir):
    if not file_path:
        return ''
    path_to_check = file_path if isabs(file_path) else normpath(join(config_dir,
                                                                     file_path))
    ans = exists(path_to_check)
    if not ans:
        raise IOError(errno.ENOENT, "File does not exist", path_to_check)
    else:
        return path_to_check

def _setup_config_parser(config_path, validate=True):
            config = SKLLConfigParser()
        if not exists(config_path):
        raise IOError(errno.ENOENT, "Configuration file does not exist",
                      config_path)
    config.read(config_path)
        objective_value = config.get('Tuning', 'objective')
    objectives_value = config.get('Tuning', 'objectives')
    objective_default = config._defaults['objective']
    objectives_default = config._defaults['objectives']
        if (objectives_value != objectives_default and objective_value != objective_default):
        raise ValueError("The configuration file can specify "
                         "either 'objective' or 'objectives', "
                         "not both")
    else:
                if objective_value == objective_default:
            config.remove_option('Tuning', 'objective')
        else:
                        objective_value = yaml.safe_load(_fix_json(objective_value), )
            if isinstance(objective_value, string_types):
                config.set(
                    'Tuning', 'objectives', "['{}']".format(objective_value))
                config.remove_option('Tuning', 'objective')
            else:
                raise TypeError("objective should be a string")
    if validate:
        config.validate()
    return config

def _parse_config_file(config_path):
    
        logger = logging.getLogger(__name__)
        if config_path == "":
        raise IOError("The name of the configuration file is empty")
        config_path = realpath(config_path)
    config_dir = dirname(config_path)
        config = _setup_config_parser(config_path)
    
        if config.has_option("General", "experiment_name"):
        experiment_name = config.get("General", "experiment_name")
    else:
        raise ValueError("Configuration file does not contain experiment_name "
                         "in the [General] section.")
    if config.has_option("General", "task"):
        task = config.get("General", "task")
    else:
        raise ValueError("Configuration file does not contain task in the "
                         "[General] section.")
    if task not in _VALID_TASKS:
        raise ValueError('An invalid task was specified: {}.  Valid tasks are:'
                         ' {}'.format(task, ', '.join(_VALID_TASKS)))
        sampler = config.get("Input", "sampler")
    if sampler not in _VALID_SAMPLERS:
        raise ValueError('An invalid sampler was specified: {}.  Valid '
                         'samplers are: {}'.format(sampler,
                                                   ', '.join(_VALID_SAMPLERS)))
            feature_hasher = config.getboolean("Input", "feature_hasher")
    hasher_features = config.getint("Input", "hasher_features")
    if feature_hasher:
        if hasher_features <= 0:
            raise ValueError("Configuration file must specify a non-zero value "
                             "for the option hasher_features when "
                             "feature_hasher is True.")
            elif hasher_features > 0:
        logger.warning("Ignoring hasher_features since feature_hasher is either"
                       " missing or set to False.")
    if config.has_option("Input", "learners"):
        learners_string = config.get("Input", "learners")
    else:
        raise ValueError("Configuration file does not contain list of learners "
                         "in [Input] section.")
    learners = yaml.safe_load(_fix_json(learners_string))
    if len(learners) == 0:
        raise ValueError("Configuration file contains an empty list of learners"
                         " in the [Input] section.")
    elif len(set(learners)) < len(learners):
        raise ValueError('Configuration file contains the same learner multiple'
                         ' times, which is not currently supported.  Please use'
                         ' param_grids with tuning to find the optimal settings'
                         ' for the learner.')
    custom_learner_path = _locate_file(config.get("Input", "custom_learner_path"),
                                       config_dir)
        featuresets_string = config.get("Input", "featuresets")
    featuresets = yaml.safe_load(_fix_json(featuresets_string))
            if not isinstance(featuresets, list) or not all(isinstance(fs, list) for fs
                                                    in featuresets):
        raise ValueError("The featuresets parameter should be a list of "
                         "features or a list of lists of features. You "
                         "specified: {}".format(featuresets))
    featureset_names = yaml.safe_load(_fix_json(config.get("Input",
                                                      "featureset_names")))
        if featureset_names:
        if (not isinstance(featureset_names, list) or
                not all([isinstance(fs, string_types) for fs in
                         featureset_names])):
            raise ValueError("The featureset_names parameter should be a list "
                             "of strings. You specified: {}"
                             .format(featureset_names))
                    learning_curve_cv_folds_list_string = config.get("Input",
                                                     "learning_curve_cv_folds_list")
    learning_curve_cv_folds_list = yaml.safe_load(_fix_json(learning_curve_cv_folds_list_string))
    if len(learning_curve_cv_folds_list) == 0:
        learning_curve_cv_folds_list = [10] * len(learners)
    else:
        if (not isinstance(learning_curve_cv_folds_list, list) or
            not all([isinstance(fold, int) for fold in learning_curve_cv_folds_list]) or
            not len(learning_curve_cv_folds_list) == len(learners)):
            raise ValueError("The learning_curve_cv_folds parameter should "
                             "be a list of integers of the same length as "
                             "the number of learners. You specified: {}"
                             .format(learning_curve_cv_folds_list))
                    learning_curve_train_sizes_string = config.get("Input", "learning_curve_train_sizes")
    learning_curve_train_sizes = yaml.safe_load(_fix_json(learning_curve_train_sizes_string))
    if len(learning_curve_train_sizes) == 0:
        learning_curve_train_sizes = np.linspace(0.1, 1.0, 5).tolist()
    else:
        if (not isinstance(learning_curve_train_sizes, list) or
            not all([isinstance(size, int) or isinstance(size, float) for size in
                         learning_curve_train_sizes])):
            raise ValueError("The learning_curve_train_sizes parameter should "
                             "be a list of integers or floats. You specified: {}"
                             .format(learning_curve_train_sizes))
        do_shuffle = config.getboolean("Input", "shuffle")
    fixed_parameter_list = yaml.safe_load(_fix_json(config.get("Input",
                                                          "fixed_parameters")))
    fixed_sampler_parameters = _fix_json(config.get("Input",
                                                    "sampler_parameters"))
    fixed_sampler_parameters = yaml.safe_load(fixed_sampler_parameters)
    param_grid_list = yaml.safe_load(_fix_json(config.get("Tuning", "param_grids")))
    pos_label_str = config.get("Tuning", "pos_label_str")
            feature_scaling = config.get("Input", "feature_scaling")
    if feature_scaling not in _VALID_FEATURE_SCALING_OPTIONS:
        raise ValueError("Invalid value for feature_scaling parameter: {}"
                         .format(feature_scaling))
    suffix = config.get("Input", "suffix")
    label_col = config.get("Input", "label_col")
    id_col = config.get("Input", "id_col")
    ids_to_floats = config.getboolean("Input", "ids_to_floats")
        cv_folds_file = _locate_file(config.get("Input", "cv_folds_file"),
                                 config_dir)
    num_cv_folds = config.getint("Input", "num_cv_folds")
    if cv_folds_file:
        cv_folds = _load_cv_folds(cv_folds_file,
                                  ids_to_floats=ids_to_floats)
    else:
                cv_folds = num_cv_folds if num_cv_folds else 10
        save_cv_folds = config.get("Output", "save_cv_folds")
        random_folds = config.getboolean("Input", "random_folds")
    if random_folds:
        if cv_folds_file:
            logger.warning('Specifying cv_folds_file overrides random_folds')
        do_stratified_folds = False
    else:
        do_stratified_folds = True
        train_path = config.get("Input", "train_directory").rstrip(os.sep)
    test_path = config.get("Input", "test_directory").rstrip(os.sep)
    train_file = config.get("Input", "train_file")
    test_file = config.get("Input", "test_file")
            if not train_file and not test_file and (isinstance(featuresets, list) and
                                             len(featuresets) == 0):
        raise ValueError(
            "The 'featuresets' parameters cannot be an empty list.")
        if not train_file and not train_path:
        raise ValueError('Invalid [Input] parameters: either "train_file" or '
                         '"train_directory" must be specified in the '
                         'configuration file.')
        if train_file and train_path:
        raise ValueError('Invalid [Input] parameters: only either "train_file"'
                         ' or "train_directory" can be specified in the '
                         'configuration file, not both.')
        if test_file and test_path:
        raise ValueError('Invalid [Input] parameters: only either "test_file" '
                         'or "test_directory" can be specified in the '
                         'configuration file, not both.')
                    if train_file:
        train_path = train_file
        featuresets = [['train_{}'.format(basename(train_file))]]
        suffix = ''
            if test_file:
        test_path = test_file
        featuresets[0][0] += '_test_{}'.format(basename(test_file))
        train_path = _locate_file(train_path, config_dir)
    test_path = _locate_file(test_path, config_dir)
    train_file = _locate_file(train_file, config_dir)
    test_file = _locate_file(test_file, config_dir)
        class_map_string = config.get("Input", "class_map")
    original_class_map = yaml.safe_load(_fix_json(class_map_string))
    if original_class_map:
                        class_map = {}
        for replacement, original_list in iteritems(original_class_map):
            for original in original_list:
                class_map[original] = replacement
        del original_class_map
    else:
        class_map = None
        probability = config.getboolean("Output", "probability")
        prediction_dir = _locate_file(config.get("Output", "predictions"),
                                  config_dir)
    if prediction_dir:
        if not exists(prediction_dir):
            os.makedirs(prediction_dir)
        log_path = _locate_file(config.get("Output", "log"), config_dir)
    if log_path:
        log_path = join(config_dir, log_path)
        if not exists(log_path):
            os.makedirs(log_path)
        model_path = _locate_file(config.get("Output", "models"), config_dir)
    if model_path:
        model_path = join(config_dir, model_path)
        if not exists(model_path):
            os.makedirs(model_path)
        results_path = _locate_file(config.get("Output", "results"), config_dir)
    if results_path:
        results_path = join(config_dir, results_path)
        if not exists(results_path):
            os.makedirs(results_path)
                do_grid_search = config.getboolean("Tuning", "grid_search")
        min_feature_count = config.getint("Tuning", "min_feature_count")
        grid_search_jobs = config.getint("Tuning", "grid_search_jobs")
    if not grid_search_jobs:
        grid_search_jobs = None
        grid_search_folds = config.getint("Tuning", "grid_search_folds")
        grid_objectives = config.get("Tuning", "objectives")
    grid_objectives = yaml.safe_load(_fix_json(grid_objectives))
    if not isinstance(grid_objectives, list):
        raise TypeError("objectives should be a "
                        "list of objectives")
        if 'mean_squared_error' in grid_objectives:
        logger.warning("The objective function \"mean_squared_error\" "
                       "is deprecated and will be removed in the next "
                       "release, please use the function "
                       "\"neg_mean_squared_error\" instead.")
        grid_objectives[grid_objectives.index('mean_squared_error')] = 'neg_mean_squared_error'
    if not all([objective in SCORERS for objective in grid_objectives]):
        raise ValueError('Invalid grid objective function(s): {}'
                         .format(grid_objectives))
        if (task == 'evaluate' or task == 'predict') and not test_path:
        raise ValueError('The test set must be set when task is evaluate or '
                         'predict.')
    if task in ['cross_validate', 'train', 'learning_curve'] and test_path:
        raise ValueError('The test set should not be set when task is '
                         '{}.'.format(task))
    if task in ['train', 'predict'] and results_path:
        raise ValueError('The results path should not be set when task is '
                         '{}.'.format(task))
    if task == 'train' and not model_path:
        raise ValueError('The model path should be set when task is train.')
    if task in ['learning_curve', 'train'] and prediction_dir:
        raise ValueError('The predictions path should not be set when task is '
                         '{}.'.format(task))
    if task in ['cross_validate', 'learning_curve'] and model_path:
        raise ValueError('The models path should not be set when task is '
                         '{}.'.format(task))
        if not featureset_names:
        featureset_names = [_munge_featureset_name(x) for x in featuresets]
    if len(featureset_names) != len(featuresets):
        raise ValueError(('Number of feature set names (%s) does not match '
                          'number of feature sets (%s).') %
                         (len(featureset_names), len(featuresets)))
        train_set_name = basename(train_path)
    test_set_name = basename(test_path) if test_path else "cv"
    return (experiment_name, task, sampler, fixed_sampler_parameters,
            feature_hasher, hasher_features, id_col, label_col, train_set_name,
            test_set_name, suffix, featuresets, do_shuffle, model_path,
            do_grid_search, grid_objectives, probability, results_path,
            pos_label_str, feature_scaling, min_feature_count,
            grid_search_jobs, grid_search_folds, cv_folds, save_cv_folds,
            do_stratified_folds, fixed_parameter_list, param_grid_list,
            featureset_names, learners, prediction_dir, log_path, train_path,
            test_path, ids_to_floats, class_map, custom_learner_path,
            learning_curve_cv_folds_list, learning_curve_train_sizes)

def _munge_featureset_name(featureset):
        if isinstance(featureset, string_types):
        return featureset
    res = '+'.join(sorted(featureset))
    return res

def _fix_json(json_string):
        json_string = json_string.replace('True', 'true')
    json_string = json_string.replace('False', 'false')
    json_string = json_string.replace("'", '"')
    return json_string

def _load_cv_folds(cv_folds_file, ids_to_floats=False):
        with open(cv_folds_file, 'r') as f:
        reader = csv.reader(f)
        next(reader)          res = {}
        for row in reader:
            if ids_to_floats:
                try:
                    row[0] = float(row[0])
                except ValueError:
                    raise ValueError('You set ids_to_floats to true, but ID {}'
                                     ' could not be converted to float'
                                     .format(row[0]))
            res[row[0]] = row[1]
    return res
from __future__ import absolute_import, print_function, unicode_literals
import csv
import datetime
import json
import logging
import math
import numpy as np
import os
import sys
from collections import defaultdict
from io import open
from itertools import combinations
from os.path import basename, exists, isfile, join
import ruamel.yaml as yaml
from prettytable import PrettyTable, ALL
from six import iterkeys, iteritems  from six.moves import zip
from sklearn import __version__ as SCIKIT_VERSION
from skll.config import _munge_featureset_name, _parse_config_file
from skll.data.readers import Reader
from skll.learner import (Learner, MAX_CONCURRENT_PROCESSES,
                          _import_custom_learner)
from skll.version import __version__
try:
    from gridmap import Job, process_jobs
except ImportError:
    _HAVE_GRIDMAP = False
else:
    _HAVE_GRIDMAP = True
try:
    import pandas as pd
except ImportError:
    _HAVE_PANDAS = False
else:
    _HAVE_PANDAS = True
try:
    import matplotlib
    import seaborn as sns
except ImportError:
    _HAVE_SEABORN = False
else:
    import matplotlib.pyplot as plt
    plt.ioff()
    _HAVE_SEABORN = True
_VALID_TASKS = frozenset(['predict', 'train', 'evaluate', 'cross_validate'])
_VALID_SAMPLERS = frozenset(['Nystroem', 'RBFSampler', 'SkewedChi2Sampler',
                             'AdditiveChi2Sampler', ''])
class NumpyTypeEncoder(json.JSONEncoder):
    '''
    This class is used when serializing results, particularly the input label
    values if the input has int-valued labels.  Numpy int64 objects can't
    be serialized by the json module, so we must convert them to int objects.
    A related issue where this was adapted from:
    http://stackoverflow.com/questions/11561932/why-does-json-dumpslistnp-arange5-fail-while-json-dumpsnp-arange5-tolis
    '''
    def default(self, obj):
        if isinstance(obj, np.int64):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)

def _get_stat_float(label_result_dict, stat):
        if stat in label_result_dict and label_result_dict[stat] is not None:
        return label_result_dict[stat]
    else:
        return float('nan')

def _write_skll_folds(skll_fold_ids, skll_fold_ids_file):
    
    f = csv.writer(skll_fold_ids_file)
    f.writerow(['id', 'cv_test_fold'])
    for example_id in skll_fold_ids:
        f.writerow([example_id, skll_fold_ids[example_id]])
    skll_fold_ids_file.flush()

def _write_summary_file(result_json_paths, output_file, ablation=0):
        learner_result_dicts = []
        all_features = defaultdict(set)
    logger = logging.getLogger(__name__)
    for json_path in result_json_paths:
        if not exists(json_path):
            logger.error(('JSON results file %s not found. Skipping summary '
                          'creation. You can manually create the summary file'
                          ' after the fact by using the summarize_results '
                          'script.'), json_path)
            return
        else:
            with open(json_path, 'r') as json_file:
                obj = json.load(json_file)
                featureset_name = obj[0]['featureset_name']
                if ablation != 0 and '_minus_' in featureset_name:
                    parent_set = featureset_name.split('_minus_', 1)[0]
                    all_features[parent_set].update(
                        yaml.safe_load(obj[0]['featureset']))
                learner_result_dicts.extend(obj)
        header = set(learner_result_dicts[0].keys()) - {'result_table',
                                                    'descriptive'}
    if ablation != 0:
        header.add('ablated_features')
    header = sorted(header)
    writer = csv.DictWriter(output_file, header, extrasaction='ignore',
                            dialect=csv.excel_tab)
    writer.writeheader()
        for lrd in learner_result_dicts:
        featureset_name = lrd['featureset_name']
        if ablation != 0:
            parent_set = featureset_name.split('_minus_', 1)[0]
            ablated_features = all_features[parent_set].difference(
                yaml.safe_load(lrd['featureset']))
            lrd['ablated_features'] = ''
            if ablated_features:
                lrd['ablated_features'] = json.dumps(sorted(ablated_features))
                writer.writerow(lrd)
    output_file.flush()

def _write_learning_curve_file(result_json_paths, output_file):
    
    learner_result_dicts = []
        logger = logging.getLogger(__name__)
    for json_path in result_json_paths:
        if not exists(json_path):
            logger.error(('JSON results file %s not found. Skipping summary '
                          'creation. You can manually create the summary file'
                          ' after the fact by using the summarize_results '
                          'script.'), json_path)
            return
        else:
            with open(json_path, 'r') as json_file:
                obj = json.load(json_file)
                learner_result_dicts.extend(obj)
        header = ['featureset_name', 'learner_name', 'objective',
              'train_set_name', 'training_set_size', 'train_score_mean',
              'test_score_mean', 'train_score_std', 'test_score_std',
              'scikit_learn_version', 'version']
    writer = csv.DictWriter(output_file,
                            header,
                            extrasaction='ignore',
                            dialect=csv.excel_tab)
    writer.writeheader()
                for lrd in learner_result_dicts:
        training_set_sizes = lrd['computed_curve_train_sizes']
        train_scores_means_by_size = lrd['learning_curve_train_scores_means']
        test_scores_means_by_size = lrd['learning_curve_test_scores_means']
        train_scores_stds_by_size = lrd['learning_curve_train_scores_stds']
        test_scores_stds_by_size = lrd['learning_curve_test_scores_stds']
                lrd['objective'] = lrd['grid_objective']
        for (size,
             train_score_mean,
             test_score_mean,
             train_score_std,
             test_score_std) in zip(training_set_sizes,
                                    train_scores_means_by_size,
                                    test_scores_means_by_size,
                                    train_scores_stds_by_size,
                                    test_scores_stds_by_size):
            lrd['training_set_size'] = size
            lrd['train_score_mean'] = train_score_mean
            lrd['test_score_mean'] = test_score_mean
            lrd['train_score_std'] = train_score_std
            lrd['test_score_std'] = test_score_std
            writer.writerow(lrd)
    output_file.flush()

def _print_fancy_output(learner_result_dicts, output_file=sys.stdout):
        if not learner_result_dicts:
        raise ValueError('Result dictionary list is empty!')
    lrd = learner_result_dicts[0]
    print('Experiment Name: {}'.format(lrd['experiment_name']),
          file=output_file)
    print('SKLL Version: {}'.format(lrd['version']), file=output_file)
    print('Training Set: {}'.format(lrd['train_set_name']), file=output_file)
    print('Training Set Size: {}'.format(
        lrd['train_set_size']), file=output_file)
    print('Test Set: {}'.format(lrd['test_set_name']), file=output_file)
    print('Test Set Size: {}'.format(lrd['test_set_size']), file=output_file)
    print('Shuffle: {}'.format(lrd['shuffle']), file=output_file)
    print('Feature Set: {}'.format(lrd['featureset']), file=output_file)
    print('Learner: {}'.format(lrd['learner_name']), file=output_file)
    print('Task: {}'.format(lrd['task']), file=output_file)
    if lrd['task'] == 'cross_validate':
        print('Number of Folds: {}'.format(lrd['cv_folds']),
              file=output_file)
        print('Stratified Folds: {}'.format(lrd['stratified_folds']),
              file=output_file)
    print('Feature Scaling: {}'.format(lrd['feature_scaling']),
          file=output_file)
    print('Grid Search: {}'.format(lrd['grid_search']), file=output_file)
    print('Grid Search Folds: {}'.format(lrd['grid_search_folds']),
          file=output_file)
    print('Grid Objective Function: {}'.format(lrd['grid_objective']),
          file=output_file)
    print('Using Folds File: {}'.format(isinstance(lrd['cv_folds'], dict)),
          file=output_file)
    print('Scikit-learn Version: {}'.format(lrd['scikit_learn_version']),
          file=output_file)
    print('Start Timestamp: {}'.format(
        lrd['start_timestamp']), file=output_file)
    print('End Timestamp: {}'.format(lrd['end_timestamp']), file=output_file)
    print('Total Time: {}'.format(lrd['total_time']), file=output_file)
    print('\n', file=output_file)
    for lrd in learner_result_dicts:
        print('Fold: {}'.format(lrd['fold']), file=output_file)
        print('Model Parameters: {}'.format(lrd.get('model_params', '')),
              file=output_file)
        print('Grid Objective Score (Train) = {}'.format(lrd.get('grid_score',
                                                                 '')),
              file=output_file)
        if 'result_table' in lrd:
            print(lrd['result_table'], file=output_file)
            print('Accuracy = {}'.format(lrd['accuracy']),
                  file=output_file)
        if 'descriptive' in lrd:
            print('Descriptive statistics:', file=output_file)
            for desc_stat in ['min', 'max', 'avg', 'std']:
                actual = lrd['descriptive']['actual'][desc_stat]
                predicted = lrd['descriptive']['predicted'][desc_stat]
                print((' {} = {: .4f} (actual), {: .4f} '
                       '(predicted)').format(desc_stat.title(), actual,
                                             predicted),
                      file=output_file)
            print('Pearson = {: f}'.format(lrd['pearson']),
                  file=output_file)
        print('Objective Function Score (Test) = {}'.format(lrd['score']),
              file=output_file)
        print('', file=output_file)

def _load_featureset(dir_path, feat_files, suffix, id_col='id', label_col='y',
                     ids_to_floats=False, quiet=False, class_map=None,
                     feature_hasher=False, num_features=None):
                if isfile(dir_path):
        return Reader.for_path(dir_path, label_col=label_col, id_col=id_col,
                               ids_to_floats=ids_to_floats, quiet=quiet,
                               class_map=class_map,
                               feature_hasher=feature_hasher,
                               num_features=num_features).read()
    else:
        merged_set = None
        for file_name in sorted(join(dir_path, featfile + suffix) for
                                featfile in feat_files):
            fs = Reader.for_path(file_name, label_col=label_col, id_col=id_col,
                                 ids_to_floats=ids_to_floats, quiet=quiet,
                                 class_map=class_map,
                                 feature_hasher=feature_hasher,
                                 num_features=num_features).read()
            if merged_set is None:
                merged_set = fs
            else:
                merged_set += fs
        return merged_set

def _classify_featureset(args):
                    experiment_name = args.pop("experiment_name")
    task = args.pop("task")
    sampler = args.pop("sampler")
    feature_hasher = args.pop("feature_hasher")
    hasher_features = args.pop("hasher_features")
    job_name = args.pop("job_name")
    featureset = args.pop("featureset")
    featureset_name = args.pop("featureset_name")
    learner_name = args.pop("learner_name")
    train_path = args.pop("train_path")
    test_path = args.pop("test_path")
    train_set_name = args.pop("train_set_name")
    test_set_name = args.pop("test_set_name")
    shuffle = args.pop('shuffle')
    model_path = args.pop("model_path")
    prediction_prefix = args.pop("prediction_prefix")
    grid_search = args.pop("grid_search")
    grid_objective = args.pop("grid_objective")
    suffix = args.pop("suffix")
    log_path = args.pop("log_path")
    probability = args.pop("probability")
    results_path = args.pop("results_path")
    fixed_parameters = args.pop("fixed_parameters")
    sampler_parameters = args.pop("sampler_parameters")
    param_grid = args.pop("param_grid")
    pos_label_str = args.pop("pos_label_str")
    overwrite = args.pop("overwrite")
    feature_scaling = args.pop("feature_scaling")
    min_feature_count = args.pop("min_feature_count")
    grid_search_jobs = args.pop("grid_search_jobs")
    grid_search_folds = args.pop("grid_search_folds")
    cv_folds = args.pop("cv_folds")
    save_cv_folds = args.pop("save_cv_folds")
    stratified_folds = args.pop("do_stratified_folds")
    label_col = args.pop("label_col")
    id_col = args.pop("id_col")
    ids_to_floats = args.pop("ids_to_floats")
    class_map = args.pop("class_map")
    custom_learner_path = args.pop("custom_learner_path")
    quiet = args.pop('quiet', False)
    learning_curve_cv_folds = args.pop("learning_curve_cv_folds")
    learning_curve_train_sizes = args.pop("learning_curve_train_sizes")
    if args:
        raise ValueError(("Extra arguments passed to _classify_featureset: "
                          "{}").format(args.keys()))
    start_timestamp = datetime.datetime.now()
    with open(log_path, 'w') as log_file:
                print("Task:", task, file=log_file)
        if task == 'cross_validate':
            print(("Cross-validating ({} folds) on {}, feature " +
                   "set {} ...").format(cv_folds, train_set_name, featureset),
                  file=log_file)
        elif task == 'evaluate':
            print(("Training on {}, Test on {}, " +
                   "feature set {} ...").format(train_set_name, test_set_name,
                                                featureset),
                  file=log_file)
        elif task == 'train':
            print("Training on {}, feature set {} ...".format(train_set_name,
                                                              featureset),
                  file=log_file)
        elif task == 'learning_curve':
            print(("Generating learning curve "
                   "({} 80/20 folds, sizes={}, objective={}) on {}, "
                   "feature set {} ...").format(learning_curve_cv_folds,
                                                learning_curve_train_sizes,
                                                grid_objective,
                                                train_set_name,
                                                featureset),
                  file=log_file)
        else:              print(("Training on {}, Making predictions about {}, " +
                   "feature set {} ...").format(train_set_name, test_set_name,
                                                featureset),
                  file=log_file)
                        modelfile = join(model_path, '{}.model'.format(job_name))
        if (task in ['cross_validate', 'learning_curve'] or
            not exists(modelfile) or
            overwrite):
            train_examples = _load_featureset(train_path, featureset, suffix,
                                              label_col=label_col,
                                              id_col=id_col,
                                              ids_to_floats=ids_to_floats,
                                              quiet=quiet, class_map=class_map,
                                              feature_hasher=feature_hasher,
                                              num_features=hasher_features)
            train_set_size = len(train_examples.ids)
            if not train_examples.has_labels:
                raise ValueError('Training examples do not have labels')
                        learner = Learner(learner_name,
                              probability=probability,
                              feature_scaling=feature_scaling,
                              model_kwargs=fixed_parameters,
                              pos_label_str=pos_label_str,
                              min_feature_count=min_feature_count,
                              sampler=sampler,
                              sampler_kwargs=sampler_parameters,
                              custom_learner_path=custom_learner_path)
                else:
                                    if custom_learner_path:
                _import_custom_learner(custom_learner_path, learner_name)
            train_set_size = 'unknown'
            if exists(modelfile) and not overwrite:
                print(('\tloading pre-existing %s model: %s') % (learner_name,
                                                                 modelfile))
            learner = Learner.from_file(modelfile)
                if task == 'evaluate' or task == 'predict':
            test_examples = _load_featureset(test_path, featureset, suffix,
                                             label_col=label_col,
                                             id_col=id_col,
                                             ids_to_floats=ids_to_floats,
                                             quiet=quiet, class_map=class_map,
                                             feature_hasher=feature_hasher,
                                             num_features=hasher_features)
            test_set_size = len(test_examples.ids)
        else:
            test_set_size = 'n/a'
                learner_result_dict_base = {'experiment_name': experiment_name,
                                    'train_set_name': train_set_name,
                                    'train_set_size': train_set_size,
                                    'test_set_name': test_set_name,
                                    'test_set_size': test_set_size,
                                    'featureset': json.dumps(featureset),
                                    'featureset_name': featureset_name,
                                    'shuffle': shuffle,
                                    'learner_name': learner_name,
                                    'task': task,
                                    'start_timestamp':
                                    start_timestamp.strftime('%d %b %Y %H:%M:'
                                                             '%S.%f'),
                                    'version': __version__,
                                    'feature_scaling': feature_scaling,
                                    'grid_search': grid_search,
                                    'grid_objective': grid_objective,
                                    'grid_search_folds': grid_search_folds,
                                    'min_feature_count': min_feature_count,
                                    'cv_folds': cv_folds,
                                    'save_cv_folds': save_cv_folds,
                                    'stratified_folds': stratified_folds,
                                    'scikit_learn_version': SCIKIT_VERSION}
                        task_results = None
        if task == 'cross_validate':
            print('\tcross-validating', file=log_file)
            task_results, grid_scores, skll_fold_ids = learner.cross_validate(
                train_examples, shuffle=shuffle, stratified=stratified_folds,
                prediction_prefix=prediction_prefix, grid_search=grid_search,
                grid_search_folds=grid_search_folds, cv_folds=cv_folds,
                grid_objective=grid_objective, param_grid=param_grid,
                grid_jobs=grid_search_jobs, save_cv_folds=save_cv_folds)
        elif task == 'learning_curve':
            print('\tgenerating learning curve', file=log_file)
            (curve_train_scores,
             curve_test_scores,
             computed_curve_train_sizes) = learner.learning_curve(train_examples,
                                                                  cv_folds=learning_curve_cv_folds,
                                                                  train_sizes=learning_curve_train_sizes,
                                                                  objective=grid_objective)
        else:
                        if not exists(modelfile) or overwrite:
                print(('\tfeaturizing and training new ' +
                       '{} model').format(learner_name),
                      file=log_file)
                if not isinstance(cv_folds, int):
                    grid_search_folds = cv_folds
                best_score = learner.train(train_examples,
                                           shuffle=shuffle,
                                           grid_search=grid_search,
                                           grid_search_folds=grid_search_folds,
                                           grid_objective=grid_objective,
                                           param_grid=param_grid,
                                           grid_jobs=grid_search_jobs)
                grid_scores = [best_score]
                                if model_path:
                    learner.save(modelfile)
                if grid_search:
                                                                                print('\tbest {} grid search score: {}'
                          .format(grid_objective, round(best_score, 3)),
                          file=log_file)
            else:
                grid_scores = [None]
                        param_out = ('{}: {}'.format(param_name, param_value)
                         for param_name, param_value in
                         iteritems(learner.model.get_params()))
            print('\thyperparameters: {}'.format(', '.join(param_out)),
                  file=log_file)
                                    if task == 'evaluate':
                print('\tevaluating predictions', file=log_file)
                task_results = [learner.evaluate(test_examples,
                                                 prediction_prefix=prediction_prefix,
                                                 grid_objective=grid_objective)]
            elif task == 'predict':
                print('\twriting predictions', file=log_file)
                learner.predict(test_examples,
                                prediction_prefix=prediction_prefix)
            
        end_timestamp = datetime.datetime.now()
        learner_result_dict_base['end_timestamp'] = end_timestamp.strftime(
            '%d %b %Y %H:%M:%S.%f')
        total_time = end_timestamp - start_timestamp
        learner_result_dict_base['total_time'] = str(total_time)
        if task == 'cross_validate' or task == 'evaluate':
            results_json_path = join(results_path,
                                     '{}.results.json'.format(job_name))
            res = _create_learner_result_dicts(task_results,
                                               grid_scores,
                                               learner_result_dict_base)
                        file_mode = 'w' if sys.version_info >= (3, 0) else 'wb'
            with open(results_json_path, file_mode) as json_file:
                json.dump(res, json_file, cls=NumpyTypeEncoder)
            with open(join(results_path,
                           '{}.results'.format(job_name)),
                      'w') as output_file:
                _print_fancy_output(res, output_file)
        elif task == 'learning_curve':
            results_json_path = join(results_path,
                                     '{}.results.json'.format(job_name))
            res = {}
            res.update(learner_result_dict_base)
            res.update({'learning_curve_cv_folds': learning_curve_cv_folds,
                        'given_curve_train_sizes': learning_curve_train_sizes,
                        'learning_curve_train_scores_means': np.mean(curve_train_scores, axis=1),
                        'learning_curve_test_scores_means': np.mean(curve_test_scores, axis=1),
                        'learning_curve_train_scores_stds': np.std(curve_train_scores, axis=1, ddof=1),
                        'learning_curve_test_scores_stds': np.std(curve_test_scores, axis=1, ddof=1),
                        'computed_curve_train_sizes': computed_curve_train_sizes})
                        file_mode = 'w' if sys.version_info >= (3, 0) else 'wb'
            with open(results_json_path, file_mode) as json_file:
                json.dump([res], json_file, cls=NumpyTypeEncoder)
        else:
            res = [learner_result_dict_base]
                if task == 'cross_validate' and save_cv_folds:
            skll_fold_ids_file = experiment_name + '_skll_fold_ids.csv'
            file_mode = 'w' if sys.version_info >= (3, 0) else 'wb'
            with open(join(results_path, skll_fold_ids_file),
                      file_mode) as output_file:
                _write_skll_folds(skll_fold_ids, output_file)
    return res

def _create_learner_result_dicts(task_results,
                                 grid_scores,
                                 learner_result_dict_base):
        res = []
    num_folds = len(task_results)
    accuracy_sum = 0.0
    pearson_sum = 0.0
    score_sum = None
    prec_sum_dict = defaultdict(float)
    recall_sum_dict = defaultdict(float)
    f_sum_dict = defaultdict(float)
    result_table = None
    for k, ((conf_matrix,
             fold_accuracy,
             result_dict,
             model_params,
             score), grid_score) in enumerate(zip(task_results,
                                                  grid_scores),
                                              start=1):
                learner_result_dict = {}
        learner_result_dict.update(learner_result_dict_base)
                        learner_result_dict['result_table'] = ''
        learner_result_dict['accuracy'] = ''
        learner_result_dict['pearson'] = ''
        learner_result_dict['score'] = ''
        learner_result_dict['fold'] = ''
        if learner_result_dict_base['task'] == 'cross_validate':
            learner_result_dict['fold'] = k
        learner_result_dict['model_params'] = json.dumps(model_params)
        if grid_score is not None:
            learner_result_dict['grid_score'] = grid_score
        if conf_matrix:
            labels = sorted(iterkeys(task_results[0][2]))
            result_table = PrettyTable([""] + labels + ["Precision", "Recall",
                                                        "F-measure"],
                                       header=True, hrules=ALL)
            result_table.align = 'r'
            result_table.float_format = '.3'
            for i, actual_label in enumerate(labels):
                conf_matrix[i][i] = "[{}]".format(conf_matrix[i][i])
                label_prec = _get_stat_float(result_dict[actual_label],
                                             "Precision")
                label_recall = _get_stat_float(result_dict[actual_label],
                                               "Recall")
                label_f = _get_stat_float(result_dict[actual_label],
                                          "F-measure")
                if not math.isnan(label_prec):
                    prec_sum_dict[actual_label] += float(label_prec)
                if not math.isnan(label_recall):
                    recall_sum_dict[actual_label] += float(label_recall)
                if not math.isnan(label_f):
                    f_sum_dict[actual_label] += float(label_f)
                result_row = ([actual_label] + conf_matrix[i] +
                              [label_prec, label_recall, label_f])
                result_table.add_row(result_row)
            result_table_str = '{}'.format(result_table)
            result_table_str += '\n(row = reference; column = predicted)'
            learner_result_dict['result_table'] = result_table_str
            learner_result_dict['accuracy'] = fold_accuracy
            accuracy_sum += fold_accuracy
                        else:
            learner_result_dict.update(result_dict)
            pearson_sum += float(learner_result_dict['pearson'])
        if score is not None:
            if score_sum is None:
                score_sum = score
            else:
                score_sum += score
            learner_result_dict['score'] = score
        res.append(learner_result_dict)
    if num_folds > 1:
        learner_result_dict = {}
        learner_result_dict.update(learner_result_dict_base)
        learner_result_dict['fold'] = 'average'
        if result_table:
            result_table = PrettyTable(["Label", "Precision", "Recall",
                                        "F-measure"],
                                       header=True)
            result_table.align = "r"
            result_table.align["Label"] = "l"
            result_table.float_format = '.3'
            for actual_label in labels:
                                prec_mean = prec_sum_dict[actual_label] / num_folds
                recall_mean = recall_sum_dict[actual_label] / num_folds
                f_mean = f_sum_dict[actual_label] / num_folds
                result_table.add_row([actual_label] +
                                     [prec_mean, recall_mean, f_mean])
            learner_result_dict['result_table'] = '{}'.format(result_table)
            learner_result_dict['accuracy'] = accuracy_sum / num_folds
        else:
            learner_result_dict['pearson'] = pearson_sum / num_folds
        if score_sum is not None:
            learner_result_dict['score'] = score_sum / num_folds
        res.append(learner_result_dict)
    return res

def run_configuration(config_file, local=False, overwrite=True, queue='all.q',
                      hosts=None, write_summary=True, quiet=False,
                      ablation=0, resume=False):
            logger = logging.getLogger(__name__)
        (experiment_name, task, sampler, fixed_sampler_parameters, feature_hasher,
     hasher_features, id_col, label_col, train_set_name, test_set_name, suffix,
     featuresets, do_shuffle, model_path, do_grid_search, grid_objectives,
     probability, results_path, pos_label_str, feature_scaling,
     min_feature_count, grid_search_jobs, grid_search_folds, cv_folds, save_cv_folds,
     do_stratified_folds, fixed_parameter_list, param_grid_list, featureset_names,
     learners, prediction_dir, log_path, train_path, test_path, ids_to_floats,
     class_map, custom_learner_path, learning_curve_cv_folds_list,
     learning_curve_train_sizes) = _parse_config_file(config_file)
        if not local and not _HAVE_GRIDMAP:
        local = True
        logger.warning('gridmap 0.10.1+ not available. Forcing local '
                       'mode.  To run things on a DRMAA-compatible '
                       'cluster, install gridmap>=0.10.1 via pip.')
        if task == 'learning_curve':
        if do_grid_search:
            do_grid_search = False
            logger.warning("Grid search is not supported during "
                       "learning curve generation. Ignoring.")
        if ablation is None or ablation > 0:
            ablation = 0
            logger.warning("Ablating features is not supported during "
                           "learning curve generation. Ignoring.")
            if ablation is None or ablation > 0:
                expanded_fs = []
        expanded_fs_names = []
        for features, featureset_name in zip(featuresets, featureset_names):
            features = sorted(features)
            featureset = set(features)
                        if ablation is None:
                for i in range(1, len(features)):
                    for excluded_features in combinations(features, i):
                        expanded_fs.append(sorted(featureset -
                                                  set(excluded_features)))
                        expanded_fs_names.append(
                            featureset_name +
                            '_minus_' +
                            _munge_featureset_name(excluded_features))
                        else:
                for excluded_features in combinations(features, ablation):
                    expanded_fs.append(sorted(featureset -
                                              set(excluded_features)))
                    expanded_fs_names.append(
                        featureset_name +
                        '_minus_' +
                        _munge_featureset_name(excluded_features))
                        expanded_fs.append(features)
            expanded_fs_names.append(featureset_name + '_all')
                featuresets = expanded_fs
        featureset_names = expanded_fs_names
    elif ablation < 0:
        raise ValueError('Value for "ablation" argument must be either '
                         'positive integer or None.')
        if not local:
        jobs = []
        result_json_paths = []
            for featureset_name in featureset_names:
        if len(featureset_name) > 210:
            raise OSError('System generated file length "{}" exceeds the '
                          'maximum length supported.  Please specify names of '
                          'your datasets with "featureset_names".  If you are '
                          'running ablation experiment, please reduce the '
                          'length of the features in "featuresets" because the'
                          ' auto-generated name would be longer than the file '
                          'system can handle'.format(featureset_name))
        for featureset, featureset_name in zip(featuresets, featureset_names):
        for learner_num, learner_name in enumerate(learners):
            for grid_objective in grid_objectives:
                                                if len(grid_objectives) == 1:
                    job_name_components = [experiment_name, featureset_name,
                                           learner_name]
                else:
                    job_name_components = [experiment_name, featureset_name,
                                           learner_name, grid_objective]
                job_name = '_'.join(job_name_components)
                                prediction_prefix = join(prediction_dir, job_name)
                                                temp_logfile = join(log_path, '{}.log'.format(job_name))
                                result_json_path = join(results_path,
                                        '{}.results.json'.format(job_name))
                                result_json_paths.append(result_json_path)
                                if resume and (exists(result_json_path) and
                               os.path.getsize(result_json_path)):
                    logger.info('Running in resume mode and %s exists, '
                                'so skipping job.', result_json_path)
                    continue
                                job_args = {}
                job_args["experiment_name"] = experiment_name
                job_args["task"] = task
                job_args["sampler"] = sampler
                job_args["feature_hasher"] = feature_hasher
                job_args["hasher_features"] = hasher_features
                job_args["job_name"] = job_name
                job_args["featureset"] = featureset
                job_args["featureset_name"] = featureset_name
                job_args["learner_name"] = learner_name
                job_args["train_path"] = train_path
                job_args["test_path"] = test_path
                job_args["train_set_name"] = train_set_name
                job_args["test_set_name"] = test_set_name
                job_args["shuffle"] = do_shuffle
                job_args["model_path"] = model_path
                job_args["prediction_prefix"] = prediction_prefix
                job_args["grid_search"] = do_grid_search
                job_args["grid_objective"] = grid_objective
                job_args["suffix"] = suffix
                job_args["log_path"] = temp_logfile
                job_args["probability"] = probability
                job_args["results_path"] = results_path
                job_args["sampler_parameters"] = (fixed_sampler_parameters
                                                  if fixed_sampler_parameters
                                                  else dict())
                job_args["fixed_parameters"] = (fixed_parameter_list[learner_num]
                                                if fixed_parameter_list
                                                else dict())
                job_args["param_grid"] = (param_grid_list[learner_num]
                                          if param_grid_list else None)
                job_args["pos_label_str"] = pos_label_str
                job_args["overwrite"] = overwrite
                job_args["feature_scaling"] = feature_scaling
                job_args["min_feature_count"] = min_feature_count
                job_args["grid_search_jobs"] = grid_search_jobs
                job_args["grid_search_folds"] = grid_search_folds
                job_args["cv_folds"] = cv_folds
                job_args["save_cv_folds"] = save_cv_folds
                job_args["do_stratified_folds"] = do_stratified_folds
                job_args["label_col"] = label_col
                job_args["id_col"] = id_col
                job_args["ids_to_floats"] = ids_to_floats
                job_args["quiet"] = quiet
                job_args["class_map"] = class_map
                job_args["custom_learner_path"] = custom_learner_path
                job_args["learning_curve_cv_folds"] = learning_curve_cv_folds_list[learner_num]
                job_args["learning_curve_train_sizes"] = learning_curve_train_sizes
                if not local:
                    jobs.append(Job(_classify_featureset, [job_args],
                                    num_slots=(MAX_CONCURRENT_PROCESSES if
                                               (do_grid_search or
                                                task == 'learning_curve') else 1),
                                    name=job_name, queue=queue))
                else:
                    _classify_featureset(job_args)
    test_set_name = basename(test_path)
        if not local and _HAVE_GRIDMAP:
        if log_path:
            job_results = process_jobs(jobs, white_list=hosts,
                                       temp_dir=log_path)
        else:
            job_results = process_jobs(jobs, white_list=hosts)
        _check_job_results(job_results)
        if (task == 'cross_validate' or task == 'evaluate') and write_summary:
        summary_file_name = experiment_name + '_summary.tsv'
        file_mode = 'w' if sys.version_info >= (3, 0) else 'wb'
        with open(join(results_path, summary_file_name),
                  file_mode) as output_file:
            _write_summary_file(result_json_paths,
                                output_file,
                                ablation=ablation)
    elif task == 'learning_curve':
        output_file_name = experiment_name + '_summary.tsv'
        file_mode = 'w' if sys.version_info >= (3, 0) else 'wb'
        output_file_path = join(results_path, output_file_name)
        with open(output_file_path, file_mode) as output_file:
            _write_learning_curve_file(result_json_paths, output_file)
                if _HAVE_PANDAS and _HAVE_SEABORN:
            _generate_learning_curve_plots(experiment_name,
                                           results_path,
                                           output_file_path)
        else:
            logger.warning("Raw data for the learning curve saved in "
                           "{}. No plots were generated since pandas and "
                           "seaborn are not installed. ".format(output_file_path))
    return result_json_paths

def _check_job_results(job_results):
        logger = logging.getLogger(__name__)
    logger.info('Checking job results')
    for result_dicts in job_results:
        if not result_dicts or 'task' not in result_dicts[0]:
            logger.error('There was an error running the experiment:\n%s',
                         result_dicts)

def _compute_ylimits_for_featureset(df, objectives):
    
            ylimits = {}
    for objective in objectives:
                df_train = df[(df['variable'] == 'train_score_mean') & (df['objective'] == objective)]
        df_test = df[(df['variable'] == 'test_score_mean') & (df['objective'] == objective)]
        train_values_lower = df_train['value'].values - df_train['train_score_std'].values
        test_values_lower = df_test['value'].values - df_test['test_score_std'].values
        min_score = np.min(np.concatenate([train_values_lower,
                                           test_values_lower]))
        train_values_upper = df_train['value'].values + df_train['train_score_std'].values
        test_values_upper = df_test['value'].values + df_test['test_score_std'].values
        max_score = np.max(np.concatenate([train_values_upper,
                                           test_values_upper]))
                if min_score < 0:
            lower_limit = -1.1 if min_score >= -1 else math.floor(min_score)
        else:
            lower_limit = 0
        if max_score > 0:
            upper_limit = 1.1 if max_score <= 1 else math.ceil(max_score)
        else:
            upper_limit = 0
        ylimits[objective] = (lower_limit, upper_limit)
    return ylimits

def _generate_learning_curve_plots(experiment_name,
                                   output_dir,
                                   learning_curve_tsv_file):
    
            df = pd.read_csv(learning_curve_tsv_file, sep='\t')
    num_learners = len(df['learner_name'].unique())
    num_objectives = len(df['objective'].unique())
    df_melted = pd.melt(df, id_vars=[c for c in df.columns
                                     if c not in ['train_score_mean', 'test_score_mean']])
                rotate_labels = np.any([size >= 1000 for size in df['training_set_size'].unique()])
            for fs_name, df_fs in df_melted.groupby('featureset_name'):
        fig = plt.figure();
        fig.set_size_inches(2.5*num_learners, 2.5*num_objectives);
                with sns.axes_style('whitegrid', {"grid.linestyle": ':',
                                          "xtick.major.size": 3.0}):
            g = sns.FacetGrid(df_fs, row="objective", col="learner_name",
                              hue="variable", size=2.5, aspect=1,
                              margin_titles=True, despine=True, sharex=False,
                              sharey=False, legend_out=False, palette="Set1")
            colors = train_color, test_color = sns.color_palette("Set1")[:2]
            g = g.map_dataframe(sns.pointplot, "training_set_size", "value",
                                scale=.5, ci=None)
            ylimits = _compute_ylimits_for_featureset(df_fs, g.row_names)
            for ax in g.axes.flat:
                plt.setp(ax.texts, text="")
            g = (g.set_titles(row_template='', col_template='{col_name}')
                 .set_axis_labels('Training Examples', 'Score'))
            if rotate_labels:
                g = g.set_xticklabels(rotation=60)
            for i, row_name in enumerate(g.row_names):
                for j, col_name in enumerate(g.col_names):
                    ax = g.axes[i][j]
                    ax.set(ylim=ylimits[row_name])
                    df_ax_train = df_fs[(df_fs['learner_name'] == col_name) &
                                        (df_fs['objective'] == row_name) &
                                        (df_fs['variable'] == 'train_score_mean')]
                    df_ax_test = df_fs[(df_fs['learner_name'] == col_name) &
                                       (df_fs['objective'] == row_name) &
                                       (df_fs['variable'] == 'test_score_mean')]
                    ax.fill_between(list(range(len(df_ax_train))),
                                    df_ax_train['value'] - df_ax_train['train_score_std'],
                                    df_ax_train['value'] + df_ax_train['train_score_std'],
                                    alpha=0.1,
                                    color=train_color)
                    ax.fill_between(list(range(len(df_ax_test))),
                                    df_ax_test['value'] - df_ax_test['test_score_std'],
                                    df_ax_test['value'] + df_ax_test['test_score_std'],
                                    alpha=0.1,
                                    color=test_color)
                    if j == 0:
                        ax.set_ylabel(row_name)
                        if i == 0:
                            ax.legend(handles=[matplotlib.lines.Line2D([], [], color=c, label=l, linestyle='-') for c, l in zip(colors, ['Training', 'Cross-validation'])],
                                      loc=4,
                                      fancybox=True,
                                      fontsize='x-small',
                                      ncol=1,
                                      frameon=True)
            g.fig.tight_layout(w_pad=1)
            plt.savefig(join(output_dir,'{}_{}.png'.format(experiment_name, fs_name)), dpi=300);
from __future__ import absolute_import, print_function, unicode_literals
import copy
import inspect
import logging
import os
import sys
from collections import Counter, defaultdict
from functools import wraps
from importlib import import_module
from multiprocessing import cpu_count
import joblib
import numpy as np
import scipy.sparse as sp
from six import iteritems, itervalues
from six import string_types
from six.moves import xrange as range
from six.moves import zip
from sklearn.model_selection import (GridSearchCV,
                                     KFold,
                                     LeaveOneGroupOut,
                                     ShuffleSplit,
                                     StratifiedKFold)
from sklearn.ensemble import (AdaBoostClassifier,
                              AdaBoostRegressor,
                              GradientBoostingClassifier,
                              GradientBoostingRegressor,
                              RandomForestClassifier,
                              RandomForestRegressor)
from sklearn.feature_extraction import FeatureHasher
from sklearn.feature_selection import SelectKBest
from sklearn.kernel_approximation import (AdditiveChi2Sampler,
                                          Nystroem,
                                          RBFSampler,
                                          SkewedChi2Sampler)
from sklearn.linear_model import (ElasticNet, Lasso, LinearRegression,
                                  LogisticRegression, Ridge, SGDClassifier,
                                  SGDRegressor)
from sklearn.linear_model.base import LinearModel
from sklearn.metrics import (accuracy_score,
                             confusion_matrix,
                             precision_recall_fscore_support,
                             SCORERS)
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.utils import shuffle as sk_shuffle
from skll.data import FeatureSet
from skll.metrics import _CORRELATION_METRICS, use_score_func
from skll.version import VERSION
_DEFAULT_PARAM_GRIDS = {AdaBoostClassifier:
                        [{'learning_rate': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        AdaBoostRegressor:
                        [{'learning_rate': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        DecisionTreeClassifier:
                        [{'max_features': ["auto", None]}],
                        DecisionTreeRegressor:
                        [{'max_features': ["auto", None]}],
                        ElasticNet:
                        [{'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        GradientBoostingClassifier:
                        [{'max_depth': [1, 3, 5]}],
                        GradientBoostingRegressor:
                        [{'max_depth': [1, 3, 5]}],
                        KNeighborsClassifier:
                        [{'n_neighbors': [1, 5, 10, 100],
                          'weights': ['uniform', 'distance']}],
                        KNeighborsRegressor:
                        [{'n_neighbors': [1, 5, 10, 100],
                          'weights': ['uniform', 'distance']}],
                        Lasso:
                        [{'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        LinearRegression:
                        [{}],
                        LinearSVC:
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        LogisticRegression:
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        SVC: [{'C': [0.01, 0.1, 1.0, 10.0, 100.0],
                               'gamma': ['auto', 0.01, 0.1, 1.0, 10.0, 100.0]}],
                        MultinomialNB:
                        [{'alpha': [0.1, 0.25, 0.5, 0.75, 1.0]}],
                        RandomForestClassifier:
                        [{'max_depth': [1, 5, 10, None]}],
                        RandomForestRegressor:
                        [{'max_depth': [1, 5, 10, None]}],
                        Ridge:
                        [{'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        SGDClassifier:
                        [{'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01],
                          'penalty': ['l1', 'l2', 'elasticnet']}],
                        SGDRegressor:
                        [{'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01],
                          'penalty': ['l1', 'l2', 'elasticnet']}],
                        LinearSVR:
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        SVR:
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0],
                          'gamma': ['auto', 0.01, 0.1, 1.0, 10.0, 100.0]}]}

_BINARY_CLASS_OBJ_FUNCS = frozenset(['unweighted_kappa',
                                     'linear_weighted_kappa',
                                     'quadratic_weighted_kappa',
                                     'uwk_off_by_one',
                                     'lwk_off_by_one',
                                     'qwk_off_by_one',
                                     'kendall_tau',
                                     'pearson',
                                     'spearman'])
_REGRESSION_ONLY_OBJ_FUNCS = frozenset(['r2',
                                        'neg_mean_squared_error'])
_CLASSIFICATION_ONLY_OBJ_FUNCS = frozenset(['accuracy',
                                            'precision',
                                            'recall',
                                            'f1',
                                            'f1_score_micro',
                                            'f1_score_macro',
                                            'f1_score_weighted',
                                            'f1_score_least_frequent',
                                            'average_precision',
                                            'roc_auc'])
_INT_CLASS_OBJ_FUNCS = frozenset(['unweighted_kappa',
                                  'linear_weighted_kappa',
                                  'quadratic_weighted_kappa',
                                  'uwk_off_by_one',
                                  'lwk_off_by_one',
                                  'qwk_off_by_one'])
_REQUIRES_DENSE = (GradientBoostingClassifier, GradientBoostingRegressor)
MAX_CONCURRENT_PROCESSES = int(os.getenv('SKLL_MAX_CONCURRENT_PROCESSES', '5'))

class FilteredLeaveOneGroupOut(LeaveOneGroupOut):
    
    def __init__(self, keep, example_ids):
        super(FilteredLeaveOneGroupOut, self).__init__()
        self.keep = keep
        self.example_ids = example_ids
        self._warned = False
        self.logger = logging.getLogger(__name__)
    def split(self, X, y, groups):
        for train_index, test_index in super(FilteredLeaveOneGroupOut,
                                             self).split(X, y, groups):
            train_len = len(train_index)
            test_len = len(test_index)
            train_index = [i for i in train_index if self.example_ids[i] in
                           self.keep]
            test_index = [i for i in test_index if self.example_ids[i] in
                          self.keep]
            if not self._warned and (train_len != len(train_index) or
                                     test_len != len(test_index)):
                self.logger.warning('Feature set contains IDs that are not ' +
                                    'in folds dictionary. Skipping those IDs.')
                self._warned = True
            yield train_index, test_index

def _find_default_param_grid(cls):
        for key_cls, grid in _DEFAULT_PARAM_GRIDS.items():
        if issubclass(cls, key_cls):
            return grid
    return None

def _import_custom_learner(custom_learner_path, custom_learner_name):
        if not custom_learner_path:
        raise ValueError('custom_learner_path was not set and learner {} '
                         'was not found.'.format(custom_learner_name))
    if not custom_learner_path.endswith('.py'):
        raise ValueError('custom_learner_path must end in .py ({})'
                         .format(custom_learner_path))
    custom_learner_module_name = os.path.basename(custom_learner_path)[:-3]
    sys.path.append(os.path.dirname(os.path.abspath(custom_learner_path)))
    import_module(custom_learner_module_name)
    globals()[custom_learner_name] = \
        getattr(sys.modules[custom_learner_module_name], custom_learner_name)

def _train_and_score(learner,
                     train_examples,
                     test_examples,
                     objective='f1_score_micro'):
    
    _ = learner.train(train_examples, grid_search=False, shuffle=False)
    train_predictions = learner.predict(train_examples)
    test_predictions = learner.predict(test_examples)
    if learner.model_type._estimator_type == 'classifier':
        test_label_list = np.unique(test_examples.labels).tolist()
        unseen_test_label_list = [label for label in test_label_list
                                  if not label in learner.label_list]
        unseen_label_dict = {label: i for i, label in enumerate(unseen_test_label_list,
                                                                start=len(learner.label_list))}
                train_and_test_label_dict = learner.label_dict.copy()
        train_and_test_label_dict.update(unseen_label_dict)
        train_labels = np.array([train_and_test_label_dict[label]
                                 for label in train_examples.labels])
        test_labels = np.array([train_and_test_label_dict[label]
                                 for label in test_examples.labels])
    else:
        train_labels = train_examples.labels
        test_labels = test_examples.labels
    train_score = use_score_func(objective, train_labels, train_predictions)
    test_score = use_score_func(objective, test_labels, test_predictions)
    return train_score, test_score

def _predict_binary(self, X):
    
    if self.coef_.shape[0] == 1:
        res = self.predict_proba(X)[:, 1]
    else:
        res = self.predict_normal(X)
    return res

class SelectByMinCount(SelectKBest):
    
    def __init__(self, min_count=1):
        self.min_count = min_count
        self.scores_ = None
    def fit(self, X, y=None):
                col_counts = [0 for _ in range(X.shape[1])]
        if sp.issparse(X):
                        _, col_indices, _ = sp.find(X)
        else:
                        col_indices = X.nonzero()[1].tolist()
        for i in col_indices:
            col_counts[i] += 1
        self.scores_ = np.array(col_counts)
        return self
    def _get_support_mask(self):
                mask = np.zeros(self.scores_.shape, dtype=bool)
        mask[self.scores_ >= self.min_count] = True
        return mask

def rescaled(cls):
            if hasattr(cls, 'rescale'):
        return cls
        orig_init = cls.__init__
    orig_fit = cls.fit
    orig_predict = cls.predict
    if cls._estimator_type == 'classifier':
        raise ValueError('Classifiers cannot be rescaled. ' +
                         'Only regressors can.')
        @wraps(cls.fit)
    def fit(self, X, y=None):
        
                orig_fit(self, X, y=y)
        if self.constrain:
                        self.y_min = min(y)
            self.y_max = max(y)
        if self.rescale:
                        y_hat = orig_predict(self, X)
            self.yhat_mean = np.mean(y_hat)
            self.yhat_sd = np.std(y_hat)
            self.y_mean = np.mean(y)
            self.y_sd = np.std(y)
        return self
    @wraps(cls.predict)
    def predict(self, X):
                        res = orig_predict(self, X)
        if self.rescale:
                                    res = (((res - self.yhat_mean) / self.yhat_sd)
                   * self.y_sd) + self.y_mean
        if self.constrain:
                        res = np.array([max(self.y_min, min(self.y_max, pred))
                            for pred in res])
        return res
    @classmethod
    @wraps(cls._get_param_names)
    def _get_param_names(class_x):
                try:
            init = getattr(orig_init, 'deprecated_original', orig_init)
            args, varargs, _, _ = inspect.getargspec(init)
            if varargs is not None:
                raise RuntimeError('scikit-learn estimators should always '
                                   'specify their parameters in the signature'
                                   ' of their init (no varargs).')
                        args.pop(0)
        except TypeError:
            args = []
        rescale_args = inspect.getargspec(class_x.__init__)[0]
                rescale_args.pop(0)
        args += rescale_args
        args.sort()
        return args
    @wraps(cls.__init__)
    def init(self, constrain=True, rescale=True, **kwargs):
                        self.constrain = constrain
        self.rescale = rescale
        self.y_min = None
        self.y_max = None
        self.yhat_mean = None
        self.yhat_sd = None
        self.y_mean = None
        self.y_sd = None
        orig_init(self, **kwargs)
        cls.__init__ = init
    cls.fit = fit
    cls.predict = predict
    cls._get_param_names = _get_param_names
    cls.rescale = True
        return cls

@rescaled
class RescaledAdaBoostRegressor(AdaBoostRegressor):
    pass

@rescaled
class RescaledDecisionTreeRegressor(DecisionTreeRegressor):
    pass

@rescaled
class RescaledElasticNet(ElasticNet):
    pass

@rescaled
class RescaledGradientBoostingRegressor(GradientBoostingRegressor):
    pass

@rescaled
class RescaledKNeighborsRegressor(KNeighborsRegressor):
    pass

@rescaled
class RescaledLasso(Lasso):
    pass

@rescaled
class RescaledLinearRegression(LinearRegression):
    pass

@rescaled
class RescaledRandomForestRegressor(RandomForestRegressor):
    pass

@rescaled
class RescaledRidge(Ridge):
    pass

@rescaled
class RescaledSVR(SVR):
    pass

@rescaled
class RescaledLinearSVR(LinearSVR):
    pass

@rescaled
class RescaledSGDRegressor(SGDRegressor):
    pass

class Learner(object):
    
    def __init__(self, model_type, probability=False, feature_scaling='none',
                 model_kwargs=None, pos_label_str=None, min_feature_count=1,
                 sampler=None, sampler_kwargs=None, custom_learner_path=None):
                super(Learner, self).__init__()
        self.feat_vectorizer = None
        self.scaler = None
        self.label_dict = None
        self.label_list = None
        self.pos_label_str = pos_label_str
        self._model = None
        self._feature_scaling = feature_scaling
        self.feat_selector = None
        self._min_feature_count = min_feature_count
        self._model_kwargs = {}
        self._sampler_kwargs = {}
        if model_type not in globals():
                                    _import_custom_learner(custom_learner_path, model_type)
            model_class = globals()[model_type]
            default_param_grid = (model_class.default_param_grid()
                                  if hasattr(model_class, 'default_param_grid')
                                  else [{}])
                        global _REQUIRES_DENSE
            _DEFAULT_PARAM_GRIDS.update({model_class: default_param_grid})
            if hasattr(model_class, 'requires_dense') and \
                    model_class.requires_dense():
                _REQUIRES_DENSE = _REQUIRES_DENSE + (model_class,)
        self._model_type = globals()[model_type]
        self._probability = None
                self.probability = probability
        self._use_dense_features = \
            (issubclass(self._model_type, _REQUIRES_DENSE) or
             self._feature_scaling in {'with_mean', 'both'})
                if issubclass(self._model_type, SVC):
            self._model_kwargs['cache_size'] = 1000
            self._model_kwargs['probability'] = self.probability
            if self.probability:
                logger = logging.getLogger(__name__)
                logger.warning('Because LibSVM does an internal ' +
                               'cross-validation to produce probabilities, ' +
                               'results will not be exactly replicable when ' +
                               'using SVC and probability mode.')
        elif issubclass(self._model_type,
                        (RandomForestClassifier, RandomForestRegressor,
                         GradientBoostingClassifier, GradientBoostingRegressor,
                         AdaBoostClassifier, AdaBoostRegressor)):
            self._model_kwargs['n_estimators'] = 500
        elif issubclass(self._model_type, SVR):
            self._model_kwargs['cache_size'] = 1000
        elif issubclass(self._model_type, SGDClassifier):
            self._model_kwargs['loss'] = 'log'
        if issubclass(self._model_type,
                      (RandomForestClassifier, LinearSVC, LogisticRegression,
                       DecisionTreeClassifier, GradientBoostingClassifier,
                       GradientBoostingRegressor, DecisionTreeRegressor,
                       RandomForestRegressor, SGDClassifier, SGDRegressor,
                       AdaBoostRegressor, AdaBoostClassifier, LinearSVR,
                       Lasso, Ridge, ElasticNet, SVC)):
            self._model_kwargs['random_state'] = 123456789
        if sampler_kwargs:
            self._sampler_kwargs.update(sampler_kwargs)
        if sampler:
            sampler_type = globals()[sampler]
            if issubclass(sampler_type, (Nystroem, RBFSampler,
                                         SkewedChi2Sampler)):
                self._sampler_kwargs['random_state'] = 123456789
            self.sampler = sampler_type(**self._sampler_kwargs)
        else:
            self.sampler = None
        if model_kwargs:
                                                                                    if issubclass(self._model_type,
                          (AdaBoostRegressor, AdaBoostClassifier)) and ('base_estimator' in model_kwargs):
                base_estimator_name = model_kwargs['base_estimator']
                base_estimator_kwargs = {} if base_estimator_name in ['MultinomialNB', 'SVR'] else {'random_state': 123456789}
                base_estimator = globals()[base_estimator_name](**base_estimator_kwargs)
                model_kwargs['base_estimator'] = base_estimator
            self._model_kwargs.update(model_kwargs)
    @classmethod
    def from_file(cls, learner_path):
                skll_version, learner = joblib.load(learner_path)
                if isinstance(learner._model_type, string_types):
            learner._model_type = globals()[learner._model_type]
                if not isinstance(learner, cls):
            raise ValueError(('The pickle stored at {} does not contain ' +
                              'a {} object.').format(learner_path, cls))
                        elif skll_version >= (0, 9, 17):
            if not hasattr(learner, 'sampler'):
                learner.sampler = None
                                                if hasattr(learner, 'scaler'):
                new_scaler = copy.copy(learner.scaler)
                                                                if (not hasattr(new_scaler, 'scale_') and
                        'std_' in new_scaler.__dict__):
                    new_scaler.scale_ =  new_scaler.__dict__['std_']
                    learner.scaler = new_scaler
            return learner
        else:
            raise ValueError(("{} stored in pickle file {} was " +
                              "created with version {} of SKLL, which is " +
                              "incompatible with the current version " +
                              "{}").format(cls, learner_path,
                                           '.'.join(skll_version),
                                           '.'.join(VERSION)))
    @property
    def model_type(self):
                return self._model_type
    @property
    def model_kwargs(self):
                return self._model_kwargs
    @property
    def model(self):
                return self._model
    def load(self, learner_path):
                del self.__dict__
        self.__dict__ = Learner.from_file(learner_path).__dict__
    @property
    def model_params(self):
                res = {}
        intercept = None
        if (isinstance(self._model, LinearModel) or
            (isinstance(self._model, SVR) and
                 self._model.kernel == 'linear') or
            isinstance(self._model, SGDRegressor)):
            
            coef = self.model.coef_
            intercept = {'_intercept_': self.model.intercept_}
                        if isinstance(self._model, SVR):
                coef = coef.toarray()[0]
                        coef = coef.reshape(1, -1)
            coef = self.feat_selector.inverse_transform(coef)[0]
            for feat, idx in iteritems(self.feat_vectorizer.vocabulary_):
                if coef[idx]:
                    res[feat] = coef[idx]
        elif isinstance(self._model, LinearSVC) or isinstance(self._model, LogisticRegression):
            label_list = self.label_list
                                                if len(self.label_list) == 2:
                label_list = self.label_list[-1:]
            for i, label in enumerate(label_list):
                coef = self.model.coef_[i]
                coef = self.feat_selector.inverse_transform(coef)[0]
                for feat, idx in iteritems(self.feat_vectorizer.vocabulary_):
                    if coef[idx]:
                        res['{}\t{}'.format(label, feat)] = coef[idx]
            if isinstance(self.model.intercept_, float):
                intercept = {'_intercept_': self.model.intercept_}
            elif self.model.intercept_.any():
                intercept = dict(zip(label_list, self.model.intercept_))
        else:
                        raise ValueError(("{} is not supported by" +
                              " model_params with its current settings."
                              ).format(self._model_type))
        return res, intercept
    @property
    def probability(self):
                return self._probability
    @probability.setter
    def probability(self, value):
                self._probability = value
        if not hasattr(self.model_type, "predict_proba") and value:
            logger = logging.getLogger(__name__)
            logger.warning(("probability was set to True, but {} does not have"
                            " a predict_proba() method.")
                           .format(self.model_type))
            self._probability = False
    def save(self, learner_path):
                        learner_dir = os.path.dirname(learner_path)
        if not os.path.exists(learner_dir):
            os.makedirs(learner_dir)
                joblib.dump((VERSION, self), learner_path)
    def _create_estimator(self):
                estimator = None
        default_param_grid = _find_default_param_grid(self._model_type)
        if default_param_grid is None:
            raise ValueError("%s is not a valid learner type." %
                             (self._model_type,))
        estimator = self._model_type(**self._model_kwargs)
        return estimator, default_param_grid
    def _check_input_formatting(self, examples):
                        if self.model_type._estimator_type == 'regressor':
            for label in examples.labels:
                if isinstance(label, string_types):
                    raise TypeError("You are doing regression with string "
                                    "labels.  Convert them to integers or "
                                    "floats.")
                for val in examples.features.data:
            if isinstance(val, string_types):
                raise TypeError("You have feature values that are strings.  "
                                "Convert them to floats.")
    @staticmethod
    def _check_max_feature_value(feat_array):
                max_feat_abs = np.max(np.abs(feat_array.data))
        if max_feat_abs > 1000.0:
            logger = logging.getLogger(__name__)
            logger.warning(("You have a feature with a very large absolute " +
                            "value (%s).  That may cause the learning " +
                            "algorithm to crash or perform " +
                            "poorly."), max_feat_abs)
    def _create_label_dict(self, examples):
                        if self.model_type._estimator_type == 'regressor':
            return
                self.label_list = np.unique(examples.labels).tolist()
                        if self.pos_label_str:
            self.label_list = sorted(self.label_list,
                                     key=lambda x: (x == self.pos_label_str,
                                                    x))
                                self.label_dict = {label: i for i, label in enumerate(self.label_list)}
    def _train_setup(self, examples):
                        self._check_input_formatting(examples)
                self.feat_vectorizer = examples.vectorizer
                self.feat_selector = SelectByMinCount(
            min_count=self._min_feature_count)
                if not issubclass(self._model_type, MultinomialNB):
            if self._feature_scaling != 'none':
                scale_with_mean = self._feature_scaling in {
                    'with_mean', 'both'}
                scale_with_std = self._feature_scaling in {'with_std', 'both'}
                self.scaler = StandardScaler(copy=True,
                                             with_mean=scale_with_mean,
                                             with_std=scale_with_std)
            else:
                                                self.scaler = StandardScaler(copy=False,
                                             with_mean=False,
                                             with_std=False)
    def train(self, examples, param_grid=None, grid_search_folds=3,
              grid_search=True, grid_objective='f1_score_micro',
              grid_jobs=None, shuffle=False, create_label_dict=True):
                logger = logging.getLogger(__name__)
                        if grid_search:
            if self.model_type._estimator_type == 'regressor':
                                if grid_objective in _CLASSIFICATION_ONLY_OBJ_FUNCS:
                    raise ValueError("{} is not a valid grid objective "
                                     "function for the {} learner"
                                     .format(grid_objective,
                                             self._model_type))
            elif grid_objective not in _CLASSIFICATION_ONLY_OBJ_FUNCS:
                                
                if issubclass(examples.labels.dtype.type, int):
                                        if grid_objective not in _INT_CLASS_OBJ_FUNCS:
                        raise ValueError("{} is not a valid grid objective "
                                         "function for the {} learner with "
                                         "integer labels"
                                         .format(grid_objective,
                                                 self._model_type))
                elif issubclass(examples.labels.dtype.type, str):
                                                            raise ValueError("{} is not a valid grid objective "
                                     "function for the {} learner with string "
                                     "labels".format(grid_objective,
                                                     self._model_type))
                elif len(set(examples.labels)) == 2:
                                                            if grid_objective not in _BINARY_CLASS_OBJ_FUNCS:
                        raise ValueError("{} is not a valid grid objective "
                                         "function for the {} learner with "
                                         "binary labels"
                                         .format(grid_objective,
                                                 self._model_type))
                elif grid_objective in _REGRESSION_ONLY_OBJ_FUNCS:
                                        raise ValueError("{} is not a valid grid objective "
                                     "function for the {} learner"
                                     .format(grid_objective,
                                             self._model_type))
                                        if grid_search or shuffle:
            if grid_search and not shuffle:
                logger.warning('Training data will be shuffled to randomize '
                               'grid search folds.  Shuffling may yield '
                               'different results compared to scikit-learn.')
            ids, labels, features = sk_shuffle(examples.ids, examples.labels,
                                               examples.features,
                                               random_state=123456789)
            examples = FeatureSet(examples.name, ids, labels=labels,
                                  features=features,
                                  vectorizer=examples.vectorizer)
                        if create_label_dict:
            self._create_label_dict(examples)
        self._train_setup(examples)
                xtrain = self.feat_selector.fit_transform(examples.features)
                if self._use_dense_features:
            try:
                xtrain = xtrain.todense()
            except MemoryError:
                if issubclass(self._model_type, _REQUIRES_DENSE):
                    reason = ('{} does not support sparse ' +
                              'matrices.').format(self._model_type)
                else:
                    reason = ('{} feature scaling requires a dense ' +
                              'matrix.').format(self._feature_scaling)
                raise MemoryError('Ran out of memory when converting training '
                                  'data to dense. This was required because ' +
                                  reason)
        if isinstance(self.feat_vectorizer, FeatureHasher) and \
                issubclass(self._model_type, MultinomialNB):
            raise ValueError('Cannot use FeatureHasher with MultinomialNB, '
                             'because MultinomialNB cannot handle negative '
                             'feature values.')
                if not issubclass(self._model_type, MultinomialNB):
            xtrain = self.scaler.fit_transform(xtrain)
                self._check_max_feature_value(xtrain)
                if self.sampler:
            logger.warning('Sampler converts sparse matrix to dense')
            if isinstance(self.sampler, SkewedChi2Sampler):
                logger.warning('SkewedChi2Sampler uses a dense matrix')
                xtrain = self.sampler.fit_transform(xtrain.todense())
            else:
                xtrain = self.sampler.fit_transform(xtrain)
                estimator, default_param_grid = self._create_estimator()
                        if self.model_type._estimator_type == 'classifier':
            labels = np.array([self.label_dict[label] for label in
                               examples.labels])
        else:
            labels = examples.labels
                if grid_search:
                        if isinstance(grid_search_folds, int):
                grid_search_folds = \
                    self._compute_num_folds_from_example_counts(
                        grid_search_folds, labels)
                if not grid_jobs:
                    grid_jobs = grid_search_folds
                else:
                    grid_jobs = min(grid_search_folds, grid_jobs)
                folds = grid_search_folds
            else:
                                if not grid_jobs:
                    grid_jobs = len(np.unique(grid_search_folds))
                else:
                    grid_jobs = min(len(np.unique(grid_search_folds)),
                                    grid_jobs)
                                dummy_label = next(itervalues(grid_search_folds))
                fold_groups = [grid_search_folds.get(curr_id, dummy_label) for
                               curr_id in examples.ids]
                folds = FilteredLeaveOneGroupOut(grid_search_folds, examples.ids).split(examples.features, examples.labels, fold_groups)
                        if not param_grid:
                param_grid = default_param_grid
                                    if (grid_objective in _CORRELATION_METRICS and
                    self.model_type._estimator_type == 'classifier'):
                estimator.predict_normal = estimator.predict
                estimator.predict = _predict_binary
                                    grid_jobs = min(grid_jobs, cpu_count(), MAX_CONCURRENT_PROCESSES)
            grid_searcher = GridSearchCV(estimator, param_grid,
                                         scoring=grid_objective,
                                         cv=folds,
                                         n_jobs=grid_jobs,
                                         pre_dispatch=grid_jobs)
                        grid_searcher.fit(xtrain, labels)
            self._model = grid_searcher.best_estimator_
            grid_score = grid_searcher.best_score_
        else:
            self._model = estimator.fit(xtrain, labels)
            grid_score = 0.0
        return grid_score
    def evaluate(self, examples, prediction_prefix=None, append=False,
                 grid_objective=None):
                        grid_score = None
                yhat = self.predict(examples, prediction_prefix=prediction_prefix,
                            append=append)
                if self.model_type._estimator_type == 'classifier':
            test_label_list = np.unique(examples.labels).tolist()
                                    unseen_test_label_list = [label for label in test_label_list
                                      if not label in self.label_list]
            unseen_label_dict = {label: i for i, label in enumerate(unseen_test_label_list,
                                                                    start=len(self.label_list))}
                        train_and_test_label_dict = self.label_dict.copy()
            train_and_test_label_dict.update(unseen_label_dict)
            ytest = np.array([train_and_test_label_dict[label]
                              for label in examples.labels])
        else:
            ytest = examples.labels
                if self.probability:
                        if grid_objective and grid_objective in _CORRELATION_METRICS:
                try:
                    grid_score = use_score_func(grid_objective, ytest,
                                                yhat[:, 1])
                except ValueError:
                    grid_score = float('NaN')
            yhat = np.array([max(range(len(row)),
                                 key=lambda i: row[i])
                             for row in yhat])
                if (grid_objective and (grid_objective not in _CORRELATION_METRICS or
                                not self.probability)):
            try:
                grid_score = use_score_func(grid_objective, ytest, yhat)
            except ValueError:
                grid_score = float('NaN')
        if self.model_type._estimator_type == 'regressor':
            result_dict = {'descriptive': defaultdict(dict)}
            for table_label, y in zip(['actual', 'predicted'], [ytest, yhat]):
                result_dict['descriptive'][table_label]['min'] = min(y)
                result_dict['descriptive'][table_label]['max'] = max(y)
                result_dict['descriptive'][table_label]['avg'] = np.mean(y)
                result_dict['descriptive'][table_label]['std'] = np.std(y)
            result_dict['pearson'] = use_score_func('pearson', ytest, yhat)
            res = (None, None, result_dict, self._model.get_params(),
                   grid_score)
        else:
                        num_labels = len(train_and_test_label_dict)
            conf_mat = confusion_matrix(ytest, yhat,
                                        labels=list(range(num_labels)))
                        overall_accuracy = accuracy_score(ytest, yhat)
            result_matrix = precision_recall_fscore_support(
                ytest, yhat, labels=list(range(num_labels)), average=None)
                        result_dict = defaultdict(dict)
            for actual_label in sorted(train_and_test_label_dict):
                col = train_and_test_label_dict[actual_label]
                result_dict[actual_label]["Precision"] = result_matrix[0][col]
                result_dict[actual_label]["Recall"] = result_matrix[1][col]
                result_dict[actual_label]["F-measure"] = result_matrix[2][col]
            res = (conf_mat.tolist(), overall_accuracy, result_dict,
                   self._model.get_params(), grid_score)
        return res
    def predict(self, examples, prediction_prefix=None, append=False,
                class_labels=False):
                logger = logging.getLogger(__name__)
        example_ids = examples.ids
                                if isinstance(self.feat_vectorizer, FeatureHasher):
            if (self.feat_vectorizer.n_features !=
                    examples.vectorizer.n_features):
                logger.warning("There is mismatch between the training model "
                               "features and the data passed to predict.")
            self_feat_vec_tuple = (self.feat_vectorizer.dtype,
                                   self.feat_vectorizer.input_type,
                                   self.feat_vectorizer.n_features,
                                   self.feat_vectorizer.non_negative)
            example_feat_vec_tuple = (examples.vectorizer.dtype,
                                      examples.vectorizer.input_type,
                                      examples.vectorizer.n_features,
                                      examples.vectorizer.non_negative)
            if self_feat_vec_tuple == example_feat_vec_tuple:
                xtest = examples.features
            else:
                xtest = self.feat_vectorizer.transform(
                    examples.vectorizer.inverse_transform(
                        examples.features))
        else:
            if (set(self.feat_vectorizer.feature_names_) !=
                    set(examples.vectorizer.feature_names_)):
                logger.warning("There is mismatch between the training model "
                               "features and the data passed to predict.")
            if self.feat_vectorizer == examples.vectorizer:
                xtest = examples.features
            else:
                xtest = self.feat_vectorizer.transform(
                    examples.vectorizer.inverse_transform(
                        examples.features))
                xtest = self.feat_selector.transform(xtest)
                if self.sampler:
            logger.warning('Sampler converts sparse matrix to dense')
            if isinstance(self.sampler, SkewedChi2Sampler):
                logger.warning('SkewedChi2Sampler uses a dense matrix')
                xtest = self.sampler.fit_transform(xtest.todense())
            else:
                xtest = self.sampler.fit_transform(xtest)
                if self._use_dense_features and not isinstance(xtest, np.ndarray):
            try:
                xtest = xtest.todense()
            except MemoryError:
                if issubclass(self._model_type, _REQUIRES_DENSE):
                    reason = ('{} does not support sparse ' +
                              'matrices.').format(self._model_type)
                else:
                    reason = ('{} feature scaling requires a dense ' +
                              'matrix.').format(self._feature_scaling)
                raise MemoryError('Ran out of memory when converting test ' +
                                  'data to dense. This was required because ' +
                                  reason)
                if not issubclass(self._model_type, MultinomialNB):
            xtest = self.scaler.transform(xtest)
                try:
            yhat = (self._model.predict_proba(xtest)
                    if (self.probability and
                        not class_labels)
                    else self._model.predict(xtest))
        except NotImplementedError as e:
            logger.error("Model type: %s\nModel: %s\nProbability: %s\n",
                         self._model_type, self._model, self.probability)
            raise e
                if prediction_prefix is not None:
            prediction_file = '{}.predictions'.format(prediction_prefix)
            with open(prediction_file,
                      "w" if not append else "a") as predictionfh:
                                if not append:
                                        if self.probability:
                        print('\t'.join(["id"] +
                                        [str(x) for x in self.label_list]),
                              file=predictionfh)
                    else:
                        print('id\tprediction', file=predictionfh)
                if self.probability:
                    for example_id, class_probs in zip(example_ids, yhat):
                        print('\t'.join([str(example_id)] +
                                        [str(x) for x in class_probs]),
                              file=predictionfh)
                else:
                    if self.model_type._estimator_type == 'regressor':
                        for example_id, pred in zip(example_ids, yhat):
                            print('{0}\t{1}'.format(example_id, pred),
                                  file=predictionfh)
                    else:
                        for example_id, pred in zip(example_ids, yhat):
                            print('%s\t%s' % (example_id,
                                              self.label_list[int(pred)]),
                                  file=predictionfh)
        if (class_labels and
                self.model_type._estimator_type == 'classifier'):
            yhat = np.array([self.label_list[int(pred)] for pred in yhat])
        return yhat
    def _compute_num_folds_from_example_counts(self, cv_folds, labels):
                assert isinstance(cv_folds, int)
                if self.model_type._estimator_type == 'regressor':
            return cv_folds
        min_examples_per_label = min(Counter(labels).values())
        if min_examples_per_label <= 1:
            raise ValueError(('The training set has only {} example for a' +
                              ' label.').format(min_examples_per_label))
        if min_examples_per_label < cv_folds:
            logger = logging.getLogger(__name__)
            logger.warning('The minimum number of examples per label was %s.  '
                           'Setting the number of cross-validation folds to '
                           'that value.', min_examples_per_label)
            cv_folds = min_examples_per_label
        return cv_folds
    def cross_validate(self, examples, stratified=True, cv_folds=10,
                       grid_search=False, grid_search_folds=3, grid_jobs=None,
                       grid_objective='f1_score_micro', prediction_prefix=None,
                       param_grid=None, shuffle=False, save_cv_folds=False):
        
                        random_state = np.random.RandomState(123456789)
                logger = logging.getLogger(__name__)
                                        if grid_search or shuffle:
            if grid_search and not shuffle:
                logger.warning('Training data will be shuffled to randomize '
                               'grid search folds. Shuffling may yield '
                               'different results compared to scikit-learn.')
            ids, labels, features = sk_shuffle(examples.ids, examples.labels,
                                               examples.features,
                                               random_state=random_state)
            examples = FeatureSet(examples.name, ids, labels=labels,
                                  features=features,
                                  vectorizer=examples.vectorizer)
                self._create_label_dict(examples)
        self._train_setup(examples)
                if isinstance(cv_folds, int):
            cv_folds = self._compute_num_folds_from_example_counts(
                cv_folds, examples.labels)
            stratified = (stratified and
                          self.model_type._estimator_type == 'classifier')
            if stratified:
                kfold = StratifiedKFold(n_splits=cv_folds)
                cv_groups = None
            else:
                kfold = KFold(n_splits=cv_folds, random_state=random_state)
                cv_groups = None
                else:
                                                                                    dummy_label = next(itervalues(cv_folds))
            fold_groups = [cv_folds.get(curr_id, dummy_label) for curr_id in
                           examples.ids]
                        kfold = FilteredLeaveOneGroupOut(cv_folds, examples.ids)
            cv_groups = fold_groups
            grid_search_folds = cv_folds
                        skll_fold_ids = None
        if save_cv_folds:
            skll_fold_ids = {}
            for fold_num, (_, test_indices) in enumerate(kfold.split(examples.features,
                                                                     examples.labels,
                                                                     cv_groups)):
                for index in test_indices:
                    skll_fold_ids[examples.ids[index]] = str(fold_num)
                        results = []
        grid_search_scores = []
        append_predictions = False
        for train_index, test_index in kfold.split(examples.features,
                                                   examples.labels,
                                                   cv_groups):
                        self._model = None              train_set = FeatureSet(examples.name,
                                   examples.ids[train_index],
                                   labels=examples.labels[train_index],
                                   features=examples.features[train_index],
                                   vectorizer=examples.vectorizer)
                                    grid_search_score = self.train(train_set,
                                           grid_search_folds=grid_search_folds,
                                           grid_search=grid_search,
                                           grid_objective=grid_objective,
                                           param_grid=param_grid,
                                           grid_jobs=grid_jobs,
                                           shuffle=grid_search,
                                           create_label_dict=False)
            grid_search_scores.append(grid_search_score)
                        
                        test_tuple = FeatureSet(examples.name,
                                    examples.ids[test_index],
                                    labels=examples.labels[test_index],
                                    features=examples.features[test_index],
                                    vectorizer=examples.vectorizer)
            results.append(self.evaluate(test_tuple,
                                         prediction_prefix=prediction_prefix,
                                         append=append_predictions,
                                         grid_objective=grid_objective))
            append_predictions = True
                return results, grid_search_scores, skll_fold_ids
    def learning_curve(self,
                       examples,
                       cv_folds=10,
                       train_sizes=np.linspace(0.1, 1.0, 5),
                       objective='f1_score_micro'):
        
                        random_state = np.random.RandomState(123456789)
                logger = logging.getLogger(__name__)
                        self._create_label_dict(examples)
        self._train_setup(examples)
                        cv = ShuffleSplit(n_splits=cv_folds,
                          test_size=0.2,
                          random_state=random_state)
        cv_iter = list(cv.split(examples.features, examples.labels, None))
        n_max_training_samples = len(cv_iter[0][0])
                        _module = import_module('sklearn.model_selection._validation')
        _translate_train_sizes = getattr(_module, '_translate_train_sizes')
        train_sizes_abs = _translate_train_sizes(train_sizes,
                                                 n_max_training_samples)
        n_unique_ticks = train_sizes_abs.shape[0]
                        featureset_iter = (FeatureSet.split_by_ids(examples, train, test) for train, test in cv_iter)
                                n_jobs = min(cpu_count(), MAX_CONCURRENT_PROCESSES)
                        parallel = joblib.Parallel(n_jobs=n_jobs, pre_dispatch=n_jobs)
        out = parallel(joblib.delayed(_train_and_score)(self,
                                                        train_fs[:n_train_samples],
                                                        test_fs,
                                                        objective)
                       for train_fs, test_fs in featureset_iter
                       for n_train_samples in train_sizes_abs)
                out = np.array(out)
        n_cv_folds = out.shape[0] // n_unique_ticks
        out = out.reshape(n_cv_folds, n_unique_ticks, 2)
        out = np.asarray(out).transpose((2, 1, 0))
        return list(out[0]), list(out[1]), list(train_sizes_abs)

from __future__ import print_function, unicode_literals
import logging
import numpy as np
from scipy.stats import kendalltau, spearmanr, pearsonr
from six import string_types
from six.moves import xrange as range
from sklearn.metrics import confusion_matrix, f1_score, SCORERS

_CORRELATION_METRICS = frozenset(['kendall_tau', 'spearman', 'pearson'])

def kappa(y_true, y_pred, weights=None, allow_off_by_one=False):
        logger = logging.getLogger(__name__)
        assert(len(y_true) == len(y_pred))
                                    try:
        y_true = [int(np.round(float(y))) for y in y_true]
        y_pred = [int(np.round(float(y))) for y in y_pred]
    except ValueError as e:
        logger.error("For kappa, the labels should be integers or strings "
                     "that can be converted to ints (E.g., '4.0' or '3').")
        raise e
        min_rating = min(min(y_true), min(y_pred))
    max_rating = max(max(y_true), max(y_pred))
            y_true = [y - min_rating for y in y_true]
    y_pred = [y - min_rating for y in y_pred]
        num_ratings = max_rating - min_rating + 1
    observed = confusion_matrix(y_true, y_pred,
                                labels=list(range(num_ratings)))
    num_scored_items = float(len(y_true))
        if isinstance(weights, string_types):
        wt_scheme = weights
        weights = None
    else:
        wt_scheme = ''
    if weights is None:
        weights = np.empty((num_ratings, num_ratings))
        for i in range(num_ratings):
            for j in range(num_ratings):
                diff = abs(i - j)
                if allow_off_by_one and diff:
                    diff -= 1
                if wt_scheme == 'linear':
                    weights[i, j] = diff
                elif wt_scheme == 'quadratic':
                    weights[i, j] = diff ** 2
                elif not wt_scheme:                      weights[i, j] = bool(diff)
                else:
                    raise ValueError('Invalid weight scheme specified for '
                                     'kappa: {}'.format(wt_scheme))
    hist_true = np.bincount(y_true, minlength=num_ratings)
    hist_true = hist_true[: num_ratings] / num_scored_items
    hist_pred = np.bincount(y_pred, minlength=num_ratings)
    hist_pred = hist_pred[: num_ratings] / num_scored_items
    expected = np.outer(hist_true, hist_pred)
        observed = observed / num_scored_items
        k = 1.0
    if np.count_nonzero(weights):
        k -= (sum(sum(weights * observed)) / sum(sum(weights * expected)))
    return k

def kendall_tau(y_true, y_pred):
        ret_score = kendalltau(y_true, y_pred)[0]
    return ret_score if not np.isnan(ret_score) else 0.0

def spearman(y_true, y_pred):
        ret_score = spearmanr(y_true, y_pred)[0]
    return ret_score if not np.isnan(ret_score) else 0.0

def pearson(y_true, y_pred):
        ret_score = pearsonr(y_true, y_pred)[0]
    return ret_score if not np.isnan(ret_score) else 0.0

def f1_score_least_frequent(y_true, y_pred):
        least_frequent = np.bincount(y_true).argmin()
    return f1_score(y_true, y_pred, average=None)[least_frequent]

def use_score_func(func_name, y_true, y_pred):
        scorer = SCORERS[func_name]
    return scorer._sign * scorer._score_func(y_true, y_pred, **scorer._kwargs)
__version__ = '1.3'
VERSION = tuple(int(x) for x in __version__.split('.'))
from __future__ import absolute_import, print_function, unicode_literals
from sklearn.metrics import f1_score, make_scorer, SCORERS
from .data import FeatureSet, Reader, Writer
from .experiments import run_configuration
from .learner import Learner
from .metrics import (kappa, kendall_tau, spearman, pearson,
                      f1_score_least_frequent)

__all__ = ['FeatureSet', 'Learner', 'Reader', 'kappa', 'kendall_tau',
           'spearman', 'pearson', 'f1_score_least_frequent',
           'run_configuration', 'Writer']
_scorers = {'f1_score_micro': make_scorer(f1_score,
                                          average='micro'),
            'f1_score_macro': make_scorer(f1_score,
                                          average='macro'),
            'f1_score_weighted': make_scorer(f1_score,
                                             average='weighted'),
            'f1_score_least_frequent': make_scorer(f1_score_least_frequent),
            'pearson': make_scorer(pearson),
            'spearman': make_scorer(spearman),
            'kendall_tau': make_scorer(kendall_tau),
            'unweighted_kappa': make_scorer(kappa),
            'quadratic_weighted_kappa': make_scorer(kappa,
                                                    weights='quadratic'),
            'linear_weighted_kappa': make_scorer(kappa, weights='linear'),
            'qwk_off_by_one': make_scorer(kappa, weights='quadratic',
                                          allow_off_by_one=True),
            'lwk_off_by_one': make_scorer(kappa, weights='linear',
                                          allow_off_by_one=True),
            'uwk_off_by_one': make_scorer(kappa, allow_off_by_one=True)}
SCORERS.update(_scorers)

from array import array
from collections import Mapping
import numpy as np
import scipy.sparse as sp
import six
from sklearn.feature_extraction import DictVectorizer as OldDictVectorizer

class DictVectorizer(OldDictVectorizer):
        def __init__(self, dtype=np.float64, separator="=", sparse=True,
                 sort=True):
        self.dtype = dtype
        self.separator = separator
        self.sparse = sparse
        self.sort = sort
        self.feature_names_ = []
        self.vocabulary_ = {}
    def __eq__(self, other):
                return (self.dtype == other.dtype and
                self.vocabulary_ == other.vocabulary_)
    def fit(self, X, y=None):
                        self.feature_names_ = []
        self.vocabulary_ = {}
        vocab = self.vocabulary_
        for x in X:
            for f, v in six.iteritems(x):
                if isinstance(v, six.string_types):
                    f = "%s%s%s" % (f, self.separator, v)
                if f not in vocab:
                    self.feature_names_.append(f)
                    vocab[f] = len(vocab)
        if self.sort:
            self.feature_names_.sort()
            self.vocabulary_ = dict((f, i) for i, f in
                                    enumerate(self.feature_names_))
        return self
    def fit_transform(self, X, y=None):
                                                assert array("i").itemsize == 4, (
            "sizeof(int) != 4 on your platform; please report this at"
            " https://github.com/scikit-learn/scikit-learn/issues and"
            " include the output from platform.platform() in your bug report")
        self.vocabulary_ = {}
        self.feature_names_ = []
        dtype = self.dtype
        vocab = self.vocabulary_
                X = [X] if isinstance(X, Mapping) else X
        indices = array("i")
        indptr = array("i", [0])
                        values = []
                        for x in X:
            for f, v in six.iteritems(x):
                if isinstance(v, six.string_types):
                    f = "%s%s%s" % (f, self.separator, v)
                    v = 1
                if f not in vocab:
                    self.feature_names_.append(f)
                    vocab[f] = len(vocab)
                indices.append(vocab[f])
                values.append(dtype(v))
            indptr.append(len(indices))
        if len(indptr) == 1:
            raise ValueError("Sample sequence X is empty.")
        if len(indices) > 0:
                                    indices = np.frombuffer(indices, dtype=np.intc)
        indptr = np.frombuffer(indptr, dtype=np.intc)
        shape = (len(indptr) - 1, len(vocab))
        result_matrix = sp.csr_matrix((values, indices, indptr),
                                      shape=shape, dtype=dtype)
                if self.sort:
            self.feature_names_.sort()
            map_index = np.empty(len(self.feature_names_), dtype=np.int32)
            for new_val, f in enumerate(self.feature_names_):
                map_index[new_val] = self.vocabulary_[f]
                self.vocabulary_[f] = new_val
            result_matrix = result_matrix[:, map_index]
                if not self.sparse:
            result_matrix = result_matrix.toarray()
        return result_matrix
from __future__ import absolute_import, print_function, unicode_literals
from copy import deepcopy
import numpy as np
import scipy.sparse as sp
from six import iteritems
from six.moves import zip
from sklearn.feature_extraction import DictVectorizer, FeatureHasher
from skll.data.dict_vectorizer import DictVectorizer as NewDictVectorizer

class FeatureSet(object):
    
    def __init__(self, name, ids, labels=None, features=None,
                 vectorizer=None):
        super(FeatureSet, self).__init__()
        self.name = name
        if isinstance(ids, list):
            ids = np.array(ids)
        self.ids = ids
        if isinstance(labels, list):
            labels = np.array(labels)
        self.labels = labels
        self.features = features
        self.vectorizer = vectorizer
                if isinstance(self.features, list):
            if self.vectorizer is None:
                self.vectorizer = NewDictVectorizer(sparse=True)
            self.features = self.vectorizer.fit_transform(self.features)
        if self.features is not None:
            num_feats = self.features.shape[0]
            if self.ids is None:
                raise ValueError('A list of IDs is required')
            num_ids = self.ids.shape[0]
            if num_feats != num_ids:
                raise ValueError(('Number of IDs (%s) does not equal '
                                  'number of feature rows (%s)') % (num_ids,
                                                                    num_feats))
            if self.labels is None:
                self.labels = np.empty(num_feats)
                self.labels.fill(None)
            num_labels = self.labels.shape[0]
            if num_feats != num_labels:
                raise ValueError(('Number of labels (%s) does not equal '
                                  'number of feature rows (%s)') % (num_labels,
                                                                    num_feats))
    def __contains__(self, value):
                return value in self.ids
    def __eq__(self, other):
        
                                if not self.features.has_sorted_indices:
            self.features.sort_indices()
        if not other.features.has_sorted_indices:
            other.features.sort_indices()
        return (self.ids.shape == other.ids.shape and
                self.labels.shape == other.labels.shape and
                self.features.shape == other.features.shape and
                (self.ids == other.ids).all() and
                (self.labels == other.labels).all() and
                np.allclose(self.features.data, other.features.data,
                            rtol=1e-6) and
                (self.features.indices == other.features.indices).all() and
                (self.features.indptr == other.features.indptr).all() and
                self.vectorizer == other.vectorizer)
    def __iter__(self):
                if self.features is not None:
            if not isinstance(self.vectorizer, DictVectorizer):
                raise ValueError('FeatureSets can only be iterated through if '
                                 'they use a DictVectorizer for their feature '
                                 'vectorizer.')
            for id_, label_, feats in zip(self.ids, self.labels,
                                          self.features):
                                                                yield (id_, label_,
                       self.vectorizer.inverse_transform(feats)[0])
        else:
            return
    def __len__(self):
        return self.features.shape[0]
    def __add__(self, other):
        
                if set(self.ids) != set(other.ids):
            raise ValueError('IDs are not in the same order in each '
                             'feature set')
                        ids_indices = dict((y, x) for x, y in enumerate(other.ids))
        relative_order = [ids_indices[self_id] for self_id in self.ids]
                new_set = FeatureSet('+'.join(sorted([self.name, other.name])),
                             deepcopy(self.ids))
                if not isinstance(self.vectorizer, type(other.vectorizer)):
            raise ValueError('Cannot combine FeatureSets because they are '
                             'not both using the same type of feature '
                             'vectorizer (e.g., DictVectorizer, '
                             'FeatureHasher)')
        uses_feature_hasher = isinstance(self.vectorizer, FeatureHasher)
        if uses_feature_hasher:
            if (self.vectorizer.n_features !=
                    other.vectorizer.n_features):
                raise ValueError('Cannot combine FeatureSets that uses '
                                 'FeatureHashers with different values of '
                                 'n_features setting.')
        else:
                        if (set(self.vectorizer.feature_names_) &
                    set(other.vectorizer.feature_names_)):
                raise ValueError('Cannot combine FeatureSets because they '
                                 'have duplicate feature names.')
        num_feats = self.features.shape[1]
        new_set.features = sp.hstack([self.features,
                                      other.features[relative_order]],
                                     'csr')
        new_set.vectorizer = deepcopy(self.vectorizer)
        if not uses_feature_hasher:
            for feat_name, index in other.vectorizer.vocabulary_.items():
                new_set.vectorizer.vocabulary_[feat_name] = (index +
                                                             num_feats)
            other_names = other.vectorizer.feature_names_
            new_set.vectorizer.feature_names_.extend(other_names)
                if self.has_labels:
                        if other.has_labels and \
                    not np.all(self.labels == other.labels[relative_order]):
                raise ValueError('Feature sets have conflicting labels for '
                                 'examples with the same ID.')
            new_set.labels = deepcopy(self.labels)
        else:
            new_set.labels = deepcopy(other.labels[relative_order])
        return new_set
    def filter(self, ids=None, labels=None, features=None, inverse=False):
                        mask = np.ones(len(self), dtype=bool)
        if ids is not None:
            mask = np.logical_and(mask, np.in1d(self.ids, ids))
        if labels is not None:
            mask = np.logical_and(mask, np.in1d(self.labels, labels))
        if inverse and (labels is not None or ids is not None):
            mask = np.logical_not(mask)
                self.ids = self.ids[mask]
        self.labels = self.labels[mask]
        self.features = self.features[mask, :]
                if features is not None:
            if isinstance(self.vectorizer, FeatureHasher):
                raise ValueError('FeatureSets with FeatureHasher vectorizers'
                                 ' cannot be filtered by feature.')
            columns = np.array(sorted({feat_num for feat_name, feat_num in
                                       iteritems(self.vectorizer.vocabulary_)
                                       if (feat_name in features or
                                           feat_name.split('=', 1)[0] in
                                           features)}))
            if inverse:
                all_columns = np.arange(self.features.shape[1])
                columns = all_columns[np.logical_not(np.in1d(all_columns,
                                                             columns))]
            self.features = self.features[:, columns]
            self.vectorizer.restrict(columns, indices=True)
    def filtered_iter(self, ids=None, labels=None, features=None,
                      inverse=False):
                if self.features is not None and not isinstance(self.vectorizer,
                                                        DictVectorizer):
            raise ValueError('FeatureSets can only be iterated through if they'
                             ' use a DictVectorizer for their feature '
                             'vectorizer.')
        for id_, label_, feats in zip(self.ids, self.labels, self.features):
                        if ids is not None and (id_ in ids) == inverse:
                continue
                        if labels is not None and (label_ in labels) == inverse:
                continue
            feat_dict = self.vectorizer.inverse_transform(feats)[0]
            if features is not None:
                feat_dict = {name: value for name, value in
                             iteritems(feat_dict) if
                             (inverse != (name in features or
                                          name.split('=', 1)[0] in features))}
            elif not inverse:
                feat_dict = {}
            yield id_, label_, feat_dict
    def __sub__(self, other):
                new_set = deepcopy(self)
        new_set.filter(features=other.vectorizer.feature_names_,
                       inverse=True)
        return new_set
    @property
    def has_labels(self):
                if self.labels is not None:
            return not (np.issubdtype(self.labels.dtype, float) and
                        np.isnan(np.min(self.labels)))
        else:
            return False
    def __str__(self):
                return str(self.__dict__)
    def __repr__(self):
                return repr(self.__dict__)
    def __getitem__(self, value):
                        if isinstance(value, slice):
            sliced_ids = self.ids[value]
            sliced_feats = (self.features[value] if self.features is not None
                            else None)
            sliced_labels = (self.labels[value] if self.labels is not None
                             else None)
            return FeatureSet('{}_{}'.format(self.name, value), sliced_ids,
                              features=sliced_feats, labels=sliced_labels,
                              vectorizer=self.vectorizer)
        else:
            label = self.labels[value] if self.labels is not None else None
            feats = self.features[value, :]
            features = (self.vectorizer.inverse_transform(feats)[0] if
                        self.features is not None else {})
            return self.ids[value], label, features
    @staticmethod
    def split_by_ids(fs, ids_for_split1, ids_for_split2=None):
        
                                                        ids1 = fs.ids[ids_for_split1]
        labels1 = fs.labels[ids_for_split1]
        features1 = fs.features[ids_for_split1]
        if ids_for_split2 is None:
            ids2 = fs.ids[~np.in1d(fs.ids, ids_for_split1)]
            labels2 = fs.labels[~np.in1d(fs.ids, ids_for_split1)]
            features2 = fs.features[~np.in1d(fs.ids, ids_for_split1)]
        else:
            ids2 = fs.ids[ids_for_split2]
            labels2 = fs.labels[ids_for_split2]
            features2 = fs.features[ids_for_split2]
        fs1 = FeatureSet('{}_1'.format(fs.name),
                         ids1,
                         labels=labels1,
                         features=features1,
                         vectorizer=fs.vectorizer)
        fs2 = FeatureSet('{}_2'.format(fs.name),
                         ids2,
                         labels=labels2,
                         features=features2,
                         vectorizer=fs.vectorizer)
        return fs1, fs2
    @staticmethod
    def from_data_frame(df, name, labels_column=None, vectorizer=None):
                if labels_column:
            feature_columns = [column for column in df.columns if column != labels_column]
            labels = df[labels_column].tolist()
        else:
            feature_columns = df.columns
            labels = None
        features = df[feature_columns].to_dict(orient='records')
        return FeatureSet(name,
                          ids=df.index.tolist(),
                          labels=labels,
                          features=features,
                          vectorizer=vectorizer)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import csv
import json
import logging
import re
import sys
from csv import DictReader
from itertools import chain, islice
from io import open, BytesIO, StringIO
import numpy as np
from bs4 import UnicodeDammit
from six import iteritems, PY2, PY3, string_types, text_type
from six.moves import map, zip
from sklearn.feature_extraction import FeatureHasher
from skll.data import FeatureSet
from skll.data.dict_vectorizer import DictVectorizer

class Reader(object):
    
    def __init__(self, path_or_list, quiet=True, ids_to_floats=False,
                 label_col='y', id_col='id', class_map=None, sparse=True,
                 feature_hasher=False, num_features=None):
        super(Reader, self).__init__()
        self.path_or_list = path_or_list
        self.quiet = quiet
        self.ids_to_floats = ids_to_floats
        self.label_col = label_col
        self.id_col = id_col
        self.class_map = class_map
        self._progress_msg = ''
        if feature_hasher:
            self.vectorizer = FeatureHasher(n_features=num_features)
        else:
            self.vectorizer = DictVectorizer(sparse=sparse)
    @classmethod
    def for_path(cls, path_or_list, **kwargs):
                if not isinstance(path_or_list, string_types):
            return DictListReader(path_or_list)
        else:
                        ext = '.' + path_or_list.rsplit('.', 1)[-1].lower()
            if ext not in EXT_TO_READER:
                raise ValueError(('Example files must be in either .arff, '
                                  '.csv, .jsonlines, .megam, .ndj, or .tsv '
                                  'format. You specified: '
                                  '{}').format(path_or_list))
        return EXT_TO_READER[ext](path_or_list, **kwargs)
    def _sub_read(self, f):
                raise NotImplementedError
    def _print_progress(self, progress_num, end="\r"):
                        if not self.quiet:
            print("{}{:>15}".format(self._progress_msg, progress_num),
                  end=end, file=sys.stderr)
            sys.stderr.flush()
    def read(self):
                        logger = logging.getLogger(__name__)
        logger.debug('Path: %s', self.path_or_list)
        if not self.quiet:
            self._progress_msg = "Loading {}...".format(self.path_or_list)
            print(self._progress_msg, end="\r", file=sys.stderr)
            sys.stderr.flush()
                ids = []
        labels = []
        with open(self.path_or_list, 'r' if PY3 else 'rb') as f:
            for ex_num, (id_, class_, _) in enumerate(self._sub_read(f), start=1):
                                if self.ids_to_floats:
                    try:
                        id_ = float(id_)
                    except ValueError:
                        raise ValueError(('You set ids_to_floats to true,'
                                          ' but ID {} could not be '
                                          'converted to float in '
                                          '{}').format(id_,
                                                       self.path_or_list))
                ids.append(id_)
                labels.append(class_)
                if ex_num % 100 == 0:
                    self._print_progress(ex_num)
            self._print_progress(ex_num)
                total = ex_num
                ids = np.array(ids)
        labels = np.array(labels)
        def feat_dict_generator():
            with open(self.path_or_list, 'r' if PY3 else 'rb') as f:
                for ex_num, (_, _, feat_dict) in enumerate(self._sub_read(f)):
                    yield feat_dict
                    if ex_num % 100 == 0:
                        self._print_progress('{:.8}%'.format(100 * ((ex_num /
                                                                    total))))
                self._print_progress("100%")
                features = self.vectorizer.fit_transform(feat_dict_generator())
                self._print_progress("done", end="\n")
                assert ids.shape[0] == labels.shape[0] == features.shape[0]
        if ids.shape[0] != len(set(ids)):
            raise ValueError('The example IDs are not unique in %s.' %
                             self.path_or_list)
        return FeatureSet(self.path_or_list, ids, labels=labels,
                          features=features, vectorizer=self.vectorizer)

class DictListReader(Reader):
    
    def read(self):
        ids = []
        labels = []
        feat_dicts = []
        for example_num, example in enumerate(self.path_or_list):
            curr_id = str(example.get("id",
                                      "EXAMPLE_{}".format(example_num)))
            if self.ids_to_floats:
                try:
                    curr_id = float(curr_id)
                except ValueError:
                    raise ValueError(('You set ids_to_floats to true,' +
                                      ' but ID {} could not be ' +
                                      'converted to float in ' +
                                      '{}').format(curr_id, example))
            class_name = (safe_float(example['y'],
                                     replace_dict=self.class_map)
                          if 'y' in example else None)
            example = example['x']
                        if self.ids_to_floats:
                try:
                    curr_id = float(curr_id)
                except ValueError:
                    raise ValueError(('You set ids_to_floats to true, but ID '
                                      '{} could not be converted to float in '
                                      '{}').format(curr_id, self.path_or_list))
            ids.append(curr_id)
            labels.append(class_name)
            feat_dicts.append(example)
                        if example_num % 100 == 0:
                self._print_progress(example_num)
                ids = np.array(ids)
        labels = np.array(labels)
        features = self.vectorizer.fit_transform(feat_dicts)
        return FeatureSet('converted', ids, labels=labels,
                          features=features, vectorizer=self.vectorizer)

class NDJReader(Reader):
    
    def _sub_read(self, f):
        for example_num, line in enumerate(f):
                        line = line.strip()
                        if line.startswith('//') or not line:
                continue
                        example = json.loads(line)
                                    curr_id = str(example.get("id",
                                      "EXAMPLE_{}".format(example_num)))
            class_name = (safe_float(example['y'],
                                     replace_dict=self.class_map)
                          if 'y' in example else None)
            example = example["x"]
            if self.ids_to_floats:
                try:
                    curr_id = float(curr_id)
                except ValueError:
                    raise ValueError(('You set ids_to_floats to true, but' +
                                      ' ID {} could not be converted to ' +
                                      'float').format(curr_id))
            yield curr_id, class_name, example

class MegaMReader(Reader):
    
    def _sub_read(self, f):
        example_num = 0
        curr_id = 'EXAMPLE_0'
        for line in f:
                        if not isinstance(line, text_type):
                line = UnicodeDammit(line, ['utf-8',
                                            'windows-1252']).unicode_markup
            line = line.strip()
                        if line.startswith('                curr_id = line[1:].strip()
            elif line and line not in ['TRAIN', 'TEST', 'DEV']:
                split_line = line.split()
                num_cols = len(split_line)
                del line
                                if num_cols == 1:
                    class_name = safe_float(split_line[0],
                                            replace_dict=self.class_map)
                    field_pairs = []
                                elif num_cols % 2 == 1:
                    class_name = safe_float(split_line[0],
                                            replace_dict=self.class_map)
                    field_pairs = split_line[1:]
                                elif num_cols % 2 == 0:
                    class_name = None
                    field_pairs = split_line
                curr_info_dict = {}
                if len(field_pairs) > 0:
                                        field_names = islice(field_pairs, 0, None, 2)
                                                            field_values = (safe_float(val) for val in
                                    islice(field_pairs, 1, None, 2))
                                        curr_info_dict.update(zip(field_names, field_values))
                    if len(curr_info_dict) != len(field_pairs) / 2:
                        raise ValueError(('There are duplicate feature ' +
                                          'names in {} for example ' +
                                          '{}.').format(self.path_or_list,
                                                        curr_id))
                yield curr_id, class_name, curr_info_dict
                                                example_num += 1
                curr_id = 'EXAMPLE_{}'.format(example_num)

class LibSVMReader(Reader):
    
    line_regex = re.compile(r'^(?P<label_num>[^ ]+)\s+(?P<features>[^                            r'(?P<comments>                            r'(?P<label_map>[^|]+)\s*\|\s*'
                            r'(?P<feat_map>.*)\s*)?$', flags=re.UNICODE)
    LIBSVM_REPLACE_DICT = {'\u2236': ':',
                           '\uFF03': '                           '\u2002': ' ',
                           '\ua78a': '=',
                           '\u2223': '|'}
    @staticmethod
    def _pair_to_tuple(pair, feat_map):
                name, value = pair.split(':')
        if feat_map is not None:
            name = feat_map[name]
        value = safe_float(value)
        return (name, value)
    def _sub_read(self, f):
        for example_num, line in enumerate(f):
            curr_id = ''
                        if isinstance(line, bytes):
                line = UnicodeDammit(line, ['utf-8',
                                            'windows-1252']).unicode_markup
            match = self.line_regex.search(line.strip())
            if not match:
                raise ValueError('Line does not look like valid libsvm format'
                                 '\n{}'.format(line))
                        if match.group('comments') is not None:
                                if match.group('feat_map'):
                    feat_map = {}
                    for pair in match.group('feat_map').split():
                        number, name = pair.split('=')
                        for orig, replacement in \
                                LibSVMReader.LIBSVM_REPLACE_DICT.items():
                            name = name.replace(orig, replacement)
                        feat_map[number] = name
                else:
                    feat_map = None
                                if match.group('label_map'):
                    label_map = dict(pair.split('=') for pair in
                                     match.group('label_map').strip().split())
                else:
                    label_map = None
                curr_id = match.group('example_id').strip()
            if not curr_id:
                curr_id = 'EXAMPLE_{}'.format(example_num)
            class_num = match.group('label_num')
                        if label_map:
                class_name = label_map[class_num]
            else:
                class_name = class_num
            class_name = safe_float(class_name,
                                    replace_dict=self.class_map)
            curr_info_dict = dict(self._pair_to_tuple(pair, feat_map) for pair
                                  in match.group('features').strip().split())
            yield curr_id, class_name, curr_info_dict

class DelimitedReader(Reader):
    
    def __init__(self, path_or_list, **kwargs):
        self.dialect = kwargs.pop('dialect', 'excel-tab')
        super(DelimitedReader, self).__init__(path_or_list, **kwargs)
    def _sub_read(self, f):
        reader = DictReader(f, dialect=self.dialect)
        for example_num, row in enumerate(reader):
            if self.label_col is not None and self.label_col in row:
                class_name = safe_float(row[self.label_col],
                                        replace_dict=self.class_map)
                del row[self.label_col]
            else:
                class_name = None
            if self.id_col not in row:
                curr_id = "EXAMPLE_{}".format(example_num)
            else:
                curr_id = row[self.id_col]
                del row[self.id_col]
                                                            columns_to_delete = []
            if PY2:
                columns_to_convert_to_unicode = []
            for fname, fval in iteritems(row):
                fval_float = safe_float(fval)
                                if fval_float:
                    row[fname] = fval_float
                    if PY2:
                        columns_to_convert_to_unicode.append(fname)
                else:
                    columns_to_delete.append(fname)
                        for cname in columns_to_delete:
                del row[cname]
                                    if PY2:
                for cname in columns_to_convert_to_unicode:
                    fval = row[cname]
                    del row[cname]
                    row[cname.decode('utf-8')] = fval
                if not self.ids_to_floats:
                    curr_id = curr_id.decode('utf-8')
            yield curr_id, class_name, row

class CSVReader(DelimitedReader):
    
    def __init__(self, path_or_list, **kwargs):
        kwargs['dialect'] = 'excel'
        super(CSVReader, self).__init__(path_or_list, **kwargs)

class ARFFReader(DelimitedReader):
    
    def __init__(self, path_or_list, **kwargs):
        kwargs['dialect'] = 'arff'
        super(ARFFReader, self).__init__(path_or_list, **kwargs)
        self.relation = ''
        self.regression = False
    @staticmethod
    def split_with_quotes(s, delimiter=' ', quote_char="'", escape_char='\\'):
                if PY2:
            delimiter = delimiter.encode()
            quote_char = quote_char.encode()
            escape_char = escape_char.encode()
        return next(csv.reader([s], delimiter=delimiter, quotechar=quote_char,
                               escapechar=escape_char))
    def _sub_read(self, f):
        field_names = []
                for line in f:
                        if not isinstance(line, text_type):
                decoded_line = UnicodeDammit(line,
                                             ['utf-8',
                                              'windows-1252']).unicode_markup
            else:
                decoded_line = line
            line = decoded_line.strip()
                        if line:
                                                split_header = self.split_with_quotes(line)
                row_type = split_header[0].lower()
                if row_type == '@attribute':
                                        field_name = split_header[1]
                    field_names.append(field_name)
                                        if field_name == self.label_col:
                        self.regression = (len(split_header) > 2 and
                                           split_header[2] == 'numeric')
                                elif row_type == '@relation':
                    self.relation = split_header[1]
                                elif row_type == '@data':
                    break
                
                if PY2:
            io_type = BytesIO
        else:
            io_type = StringIO
        with io_type() as field_buffer:
            csv.writer(field_buffer, dialect='arff').writerow(field_names)
            field_str = field_buffer.getvalue()
                        if self.label_col != field_names[-1]:
            self.label_col = None
                return super(ARFFReader, self)._sub_read(chain([field_str], f))

class TSVReader(DelimitedReader):
    
    def __init__(self, path_or_list, **kwargs):
        kwargs['dialect'] = 'excel-tab'
        super(TSVReader, self).__init__(path_or_list, **kwargs)

def safe_float(text, replace_dict=None):
    
        text = text_type(text)
    if replace_dict is not None:
        if text in replace_dict:
            text = replace_dict[text]
        else:
            logging.getLogger(__name__).warning('Encountered value that was '
                                                'not in replacement '
                                                'dictionary (e.g., class_map):'
                                                ' {}'.format(text))
    try:
        return int(text)
    except ValueError:
        try:
            return float(text)
        except ValueError:
            return text.decode('utf-8') if PY2 else text
        except TypeError:
            return 0.0
    except TypeError:
        return 0

EXT_TO_READER = {".arff": ARFFReader,
                 ".csv": CSVReader,
                 ".jsonlines": NDJReader,
                 ".libsvm": LibSVMReader,
                 ".megam": MegaMReader,
                 '.ndj': NDJReader,
                 ".tsv": TSVReader}
from __future__ import absolute_import, print_function, unicode_literals
import json
import logging
import os
import re
import sys
from csv import DictWriter
from decimal import Decimal
from io import open
import numpy as np
from six import iteritems, PY2, string_types, text_type
from six.moves import map
from sklearn.feature_extraction import FeatureHasher

class Writer(object):
    
    def __init__(self, path, feature_set, **kwargs):
        super(Writer, self).__init__()
        self.requires_binary = kwargs.pop('requires_binary', False)
        self.quiet = kwargs.pop('quiet', True)
        self.path = path
        self.feat_set = feature_set
        self.subsets = kwargs.pop('subsets', None)
                        self.root, self.ext = re.search(r'^(.*)(\.[^.]*)$', path).groups()
        self._progress_msg = ''
        if kwargs:
            raise ValueError('Passed extra keyword arguments to '
                             'Writer constructor: {}'.format(kwargs))
    @classmethod
    def for_path(cls, path, feature_set, **kwargs):
                        ext = '.' + path.rsplit('.', 1)[-1].lower()
        return EXT_TO_WRITER[ext](path, feature_set, **kwargs)
    def write(self):
                        logger = logging.getLogger(__name__)
        if isinstance(self.feat_set.vectorizer, FeatureHasher):
            raise ValueError('Writer cannot write sets that use'
                             'FeatureHasher for vectorization.')
                if self.subsets is None:
            self._write_subset(self.path, None)
                else:
            for subset_name, filter_features in iteritems(self.subsets):
                logger.debug('Subset (%s) features: %s', subset_name,
                             filter_features)
                sub_path = os.path.join(self.root, '{}{}'.format(subset_name,
                                                                 self.ext))
                self._write_subset(sub_path, set(filter_features))
    def _write_subset(self, sub_path, filter_features):
                        logger = logging.getLogger(__name__)
        logger.debug('sub_path: %s', sub_path)
        logger.debug('feature_set: %s', self.feat_set.name)
        logger.debug('filter_features: %s', filter_features)
        if not self.quiet:
            self._progress_msg = "Writing {}...".format(sub_path)
            print(self._progress_msg, end="\r", file=sys.stderr)
            sys.stderr.flush()
                filtered_set = (self.feat_set.filtered_iter(features=filter_features)
                        if filter_features is not None else self.feat_set)
                file_mode = 'wb' if (self.requires_binary and PY2) else 'w'
        with open(sub_path, file_mode) as output_file:
                        self._write_header(filtered_set, output_file, filter_features)
                        for ex_num, (id_, label_, feat_dict) in enumerate(filtered_set):
                self._write_line(id_, label_, feat_dict, output_file)
                if not self.quiet and ex_num % 100 == 0:
                    print("{}{:>15}".format(self._progress_msg, ex_num),
                          end="\r", file=sys.stderr)
                    sys.stderr.flush()
            if not self.quiet:
                print("{}{:<15}".format(self._progress_msg, "done"),
                      file=sys.stderr)
                sys.stderr.flush()
    def _write_header(self, feature_set, output_file, filter_features):
                pass
    def _write_line(self, id_, label_, feat_dict, output_file):
                raise NotImplementedError

class DelimitedFileWriter(Writer):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['requires_binary'] = True
        self.dialect = kwargs.pop('dialect', 'excel-tab')
        self.label_col = kwargs.pop('label_col', 'y')
        self.id_col = kwargs.pop('id_col', 'id')
        super(DelimitedFileWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None
    def _get_fieldnames(self, filter_features):
                        if filter_features is not None:
            fieldnames = {feat_name for feat_name in
                          self.feat_set.vectorizer.get_feature_names() if
                          (feat_name in filter_features or
                           feat_name.split('=', 1)[0] in filter_features)}
        else:
            fieldnames = set(self.feat_set.vectorizer.get_feature_names())
        fieldnames.add(self.id_col)
        if self.feat_set.has_labels:
            fieldnames.add(self.label_col)
        return sorted(fieldnames)
    def _write_header(self, feature_set, output_file, filter_features):
                        self._dict_writer = DictWriter(output_file,
                                       self._get_fieldnames(filter_features),
                                       restval=0, dialect=self.dialect)
                self._dict_writer.writeheader()
    def _write_line(self, id_, label_, feat_dict, output_file):
                        if self.label_col not in feat_dict:
            if self.feat_set.has_labels:
                feat_dict[self.label_col] = label_
        else:
            raise ValueError(('Class column name "{}" already used as feature '
                              'name.').format(self.label_col))
                if self.id_col not in feat_dict:
            feat_dict[self.id_col] = id_
        else:
            raise ValueError('ID column name "{}" already used as feature '
                             'name.'.format(self.id_col))
                self._dict_writer.writerow(feat_dict)

class CSVWriter(DelimitedFileWriter):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['dialect'] = 'excel'
        super(CSVWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None

class TSVWriter(DelimitedFileWriter):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['dialect'] = 'excel-tab'
        super(TSVWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None

class ARFFWriter(DelimitedFileWriter):
    
    def __init__(self, path, feature_set, **kwargs):
        self.relation = kwargs.pop('relation', 'skll_relation')
        self.regression = kwargs.pop('regression', False)
        kwargs['dialect'] = 'arff'
        super(ARFFWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None
    def _write_header(self, feature_set, output_file, filter_features):
                fieldnames = self._get_fieldnames(filter_features)
        if self.label_col in fieldnames:
            fieldnames.remove(self.label_col)
                print("@relation '{}'\n".format(self.relation), file=output_file)
                for field in fieldnames:
            print("@attribute '{}' numeric".format(field.replace('\\', '\\\\')
                                                   .replace("'", "\\'")),
                  file=output_file)
                if self.regression:
            print("@attribute {} numeric".format(self.label_col),
                  file=output_file)
        else:
            if self.feat_set.has_labels:
                print("@attribute {} ".format(self.label_col) +
                      "{" + ','.join(map(str,
                                         sorted(set(self.feat_set.labels)))) +
                      "}", file=output_file)
        fieldnames.append(self.label_col)
                        self._dict_writer = DictWriter(output_file, fieldnames, restval=0,
                                       extrasaction='ignore', dialect='arff')
                print("\n@data", file=output_file)

class MegaMWriter(Writer):
        @staticmethod
    def _replace_non_ascii(line):
                char_list = []
        for char in line:
            char_num = ord(char)
            char_list.append(
                '<U{}>'.format(char_num) if char_num > 127 else char)
        return ''.join(char_list)
    def _write_line(self, id_, label_, feat_dict, output_file):
                        print('        if self.feat_set.has_labels:
            print('{}'.format(label_), end='\t', file=output_file)
        print(self._replace_non_ascii(' '.join(('{} {}'.format(field,
                                                               value) for
                                                field, value in
                                                sorted(feat_dict.items()) if
                                                Decimal(value) != 0))),
              file=output_file)

class NDJWriter(Writer):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['requires_binary'] = True
        super(NDJWriter, self).__init__(path, feature_set, **kwargs)
    def _write_line(self, id_, label_, feat_dict, output_file):
                example_dict = {}
                if self.feat_set.has_labels:
            example_dict['y'] = np.asscalar(label_)
        example_dict['id'] = np.asscalar(id_)
        example_dict["x"] = feat_dict
        print(json.dumps(example_dict, sort_keys=True), file=output_file)

class LibSVMWriter(Writer):
    
    LIBSVM_REPLACE_DICT = {':': '\u2236',
                           '                           ' ': '\u2002',
                           '=': '\ua78a',
                           '|': '\u2223'}
    def __init__(self, path, feature_set, **kwargs):
        self.label_map = kwargs.pop('label_map', None)
        super(LibSVMWriter, self).__init__(path, feature_set, **kwargs)
        if self.label_map is None:
            self.label_map = {}
            if feature_set.has_labels:
                self.label_map = {label: num for num, label in
                                  enumerate(sorted({label for label in
                                                    feature_set.labels if
                                                    not isinstance(label,
                                                                   (int,
                                                                    float))}))}
                        self.label_map[None] = '00000'
    @staticmethod
    def _sanitize(name):
                if isinstance(name, string_types):
            for orig, replacement in LibSVMWriter.LIBSVM_REPLACE_DICT.items():
                name = name.replace(orig, replacement)
        return name
    def _write_line(self, id_, label_, feat_dict, output_file):
                field_values = sorted([(self.feat_set.vectorizer.vocabulary_[field] +
                                1, value) for field, value in
                               iteritems(feat_dict) if Decimal(value) != 0])
                if label_ in self.label_map:
            print('{}'.format(self.label_map[label_]), end=' ',
                  file=output_file)
        else:
            print('{}'.format(label_), end=' ', file=output_file)
                print(' '.join(('{}:{}'.format(field, value) for field, value in
                        field_values)), end=' ', file=output_file)
                print('        print(self._sanitize('{}'.format(id_)), end='',
              file=output_file)
        print(' |', end=' ', file=output_file)
        if (PY2 and self.feat_set.has_labels and isinstance(label_,
                                                            text_type)):
            label_ = label_.encode('utf-8')
        if label_ in self.label_map:
            print('%s=%s' % (self._sanitize(self.label_map[label_]),
                             self._sanitize(label_)),
                  end=' | ', file=output_file)
        else:
            print(' |', end=' ', file=output_file)
        line = ' '.join(('%s=%s' % (self.feat_set.vectorizer.vocabulary_[field]
                                    + 1, self._sanitize(field)) for
                         field, value in feat_dict.items() if
                         Decimal(value) != 0))
        print(line, file=output_file)

EXT_TO_WRITER = {".arff": ARFFWriter,
                 ".csv": CSVWriter,
                 ".jsonlines": NDJWriter,
                 ".libsvm": LibSVMWriter,
                 ".megam": MegaMWriter,
                 '.ndj': NDJWriter,
                 ".tsv": TSVWriter}
from __future__ import absolute_import, print_function, unicode_literals
import csv
from six import PY2
from .featureset import FeatureSet
from .readers import (ARFFReader, CSVReader, LibSVMReader, MegaMReader,
                      NDJReader, TSVReader, safe_float, Reader)
from .writers import (ARFFWriter, DelimitedFileWriter, Writer,
                      LibSVMWriter, MegaMWriter, NDJWriter)

if PY2:
    csv.register_dialect('arff', delimiter=b',', quotechar=b"'",
                         escapechar=b'\\', doublequote=False,
                         lineterminator=b'\n', skipinitialspace=True)
else:
    csv.register_dialect('arff', delimiter=',', quotechar="'",
                         escapechar='\\', doublequote=False,
                         lineterminator='\n', skipinitialspace=True)

__all__ = ['Reader', 'safe_float', 'FeatureSet', 'ARFFReader',
           'CSVReader', 'LibSVMReader', 'MegaMReader', 'NDJReader',
           'TSVReader', 'ARFFWriter', 'DelimitedFileWriter', 'LibSVMWriter',
           'MegaMWriter', 'NDJWriter', 'Writer']
from __future__ import print_function, unicode_literals
import argparse
import csv
import logging
from skll.data import Reader, safe_float
from skll.metrics import use_score_func
from skll.version import __version__

def compute_eval_from_predictions(examples_file, predictions_file,
                                  metric_names):
    
        data = Reader.for_path(examples_file).read()
    gold = dict(zip(data.ids, data.labels))
        pred = {}
    with open(predictions_file) as pred_file:
        reader = csv.reader(pred_file, dialect=csv.excel_tab)
        next(reader)          for row in reader:
            pred[row[0]] = safe_float(row[1])
            if set(gold.keys()) != set(pred.keys()):
        raise ValueError('The example and prediction IDs do not match.')
    example_ids = sorted(gold.keys())
    res = {}
    for metric_name in metric_names:
        score = use_score_func(metric_name,
                               [gold[ex_id] for ex_id in example_ids],
                               [pred[ex_id] for ex_id in example_ids])
        res[metric_name] = score
    return res

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Computes evaluation metrics from prediction files after \
                     you have run an experiment.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('examples_file',
                        help='SKLL input file with labeled examples')
    parser.add_argument('predictions_file',
                        help='file with predictions from SKLL')
    parser.add_argument('metric_names',
                        help='metrics to compute',
                        nargs='+')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    scores = compute_eval_from_predictions(args.examples_file,
                                           args.predictions_file,
                                           args.metric_names)
    for metric_name in args.metric_names:
        print("{}\t{}\t{}".format(scores[metric_name],
                                  metric_name, args.predictions_file))

if __name__ == '__main__':
    main()
'''
Script that filters a given feature file to remove unwanted features, labels,
or IDs.
:author: Dan Blanchard (dblanchard@ets.org)
:date: November 2014
'''
from __future__ import print_function, unicode_literals
import argparse
import logging
import os
import sys
from skll.data.readers import EXT_TO_READER
from skll.data.writers import ARFFWriter, DelimitedFileWriter, EXT_TO_WRITER
from skll.version import __version__

def main(argv=None):
    '''
    Handles command line arguments and gets things started.
    :param argv: List of arguments, as if specified on the command-line.
                 If None, ``sys.argv[1:]`` is used instead.
    :type argv: list of str
    '''
        parser = argparse.ArgumentParser(
        description="Takes an input feature file and removes any instances or\
                     features that do not match the specified patterns.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('infile',
                        help='input feature file (ends in .arff, .csv, \
                              .jsonlines, .megam, .ndj, or .tsv)')
    parser.add_argument('outfile',
                        help='output feature file (must have same extension as\
                              input file)')
    parser.add_argument('-f', '--feature',
                        help='A feature in the feature file you would like to \
                              keep.  If unspecified, no features are removed.',
                        nargs='*')
    parser.add_argument('-I', '--id',
                        help='An instance ID in the feature file you would \
                              like to keep.  If unspecified, no instances are \
                              removed based on their IDs.',
                        nargs='*')
    parser.add_argument('--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-i', '--inverse',
                        help='Instead of keeping features and/or examples in \
                              lists, remove them.',
                        action='store_true')
    parser.add_argument('-L', '--label',
                        help='A label in the feature file you would like to \
                              keep.  If unspecified, no instances are removed \
                              based on their labels.',
                        nargs='*')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the class \
                              labels in ARFF, CSV, or TSV files. For ARFF \
                              files, this must be the final column to count as\
                              the label.',
                        default='y')
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - '
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        valid_extensions = {ext for ext in EXT_TO_READER if ext != '.libsvm'}
        input_extension = os.path.splitext(args.infile)[1].lower()
    output_extension = os.path.splitext(args.outfile)[1].lower()
    if input_extension == '.libsvm':
        logger.error('Cannot filter LibSVM files.  Please use skll_convert to '
                     'convert to a different datatype first.')
        sys.exit(1)
    if input_extension not in valid_extensions:
        logger.error(('Input file must be in either .arff, .csv, .jsonlines, '
                      '.megam, .ndj, or .tsv format. You specified: '
                      '{}').format(input_extension))
        sys.exit(1)
    if output_extension != input_extension:
        logger.error(('Output file must be in the same format as the input '
                      'file.  You specified: {}').format(output_extension))
        sys.exit(1)
        reader = EXT_TO_READER[input_extension](args.infile, quiet=args.quiet,
                                            label_col=args.label_col,
                                            id_col=args.id_col)
    feature_set = reader.read()
        feature_set.filter(ids=args.id, labels=args.label, features=args.feature,
                       inverse=args.inverse)
        writer_type = EXT_TO_WRITER[input_extension]
    writer_args = {'quiet': args.quiet}
    if writer_type is DelimitedFileWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
    elif writer_type is ARFFWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
        writer_args['regression'] = reader.regression
        writer_args['relation'] = reader.relation
    writer = writer_type(args.outfile, feature_set, **writer_args)
    writer.write()

if __name__ == '__main__':
    main()
from __future__ import absolute_import, print_function, unicode_literals
import argparse
import logging
import os
from skll.data.readers import EXT_TO_READER
from skll.learner import Learner
from skll.version import __version__

class Predictor(object):
    
    def __init__(self, model_path, threshold=None, positive_label=1):
                self._learner = Learner.from_file(model_path)
        self._pos_index = positive_label
        self.threshold = threshold
    def predict(self, data):
                        preds = self._learner.predict(data)
        preds = preds.tolist()
        if self._learner.probability:
            if self.threshold is None:
                return [pred[self._pos_index] for pred in preds]
            else:
                return [int(pred[self._pos_index] >= self.threshold)
                        for pred in preds]
        elif self._learner.model._estimator_type == 'regressor':
            return preds
        else:
            return [self._learner.label_list[pred if isinstance(pred, int) else
                                             int(pred[0])] for pred in preds]

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Loads a trained model and outputs predictions based \
                     on input feature files.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        conflict_handler='resolve')
    parser.add_argument('model_file',
                        help='Model file to load and use for generating \
                              predictions.')
    parser.add_argument('input_file',
                        help='A csv file, json file, or megam file \
                              (with or without the label column), \
                              with the appropriate suffix.',
                        nargs='+')
    parser.add_argument('-i', '--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the labels\
                              in ARFF, CSV, or TSV files. For ARFF files, this\
                              must be the final column to count as the label.',
                        default='y')
    parser.add_argument('-p', '--positive_label',
                        help="If the model is only being used to predict the \
                              probability of a particular label, this \
                              specifies the index of the label we're \
                              predicting. 1 = second label, which is default \
                              for binary classification. Keep in mind that \
                              labels are sorted lexicographically.",
                        default=1, type=int)
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('-t', '--threshold',
                        help="If the model we're using is generating \
                              probabilities of the positive label, return 1 \
                              if it meets/exceeds the given threshold and 0 \
                              otherwise.",
                        type=float)
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        predictor = Predictor(args.model_file,
                          positive_label=args.positive_label,
                          threshold=args.threshold)
        for input_file in args.input_file:
                input_extension = os.path.splitext(input_file)[1].lower()
        if input_extension not in EXT_TO_READER:
            logger.error(('Input file must be in either .arff, .csv, '
                          '.jsonlines, .libsvm, .megam, .ndj, or .tsv format. '
                          ' Skipping file {}').format(input_file))
            continue
        else:
                        reader = EXT_TO_READER[input_extension](input_file,
                                                    quiet=args.quiet,
                                                    label_col=args.label_col,
                                                    id_col=args.id_col)
            feature_set = reader.read()
            for pred in predictor.predict(feature_set):
                print(pred)

if __name__ == '__main__':
    main()
'''
Script that joins a bunch of feature files together to create one file.
:author: Dan Blanchard (dblanchard@ets.org)
:date: November 2014
'''
from __future__ import print_function, unicode_literals
import argparse
import logging
import os
import sys
from skll.data.readers import EXT_TO_READER
from skll.data.writers import ARFFWriter, DelimitedFileWriter, EXT_TO_WRITER
from skll.version import __version__

def main(argv=None):
    '''
    Handles command line arguments and gets things started.
    :param argv: List of arguments, as if specified on the command-line.
                 If None, ``sys.argv[1:]`` is used instead.
    :type argv: list of str
    '''
        parser = argparse.ArgumentParser(
        description="Joins multiple input feature files together into one \
                     file.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('infile',
                        help='input feature file (ends in .arff, .csv, \
                              .jsonlines, .megam, .ndj, or .tsv)',
                        nargs='+')
    parser.add_argument('outfile',
                        help='output feature file')
    parser.add_argument('-i', '--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the class \
                              labels in ARFF, CSV, or TSV files. For ARFF \
                              files, this must be the final column to count as\
                              the label.',
                        default='y')
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - '
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        valid_extensions = {ext for ext in EXT_TO_READER if ext != '.libsvm'}
        input_extension_set = {os.path.splitext(inf)[1].lower() for inf in
                           args.infile}
    output_extension = os.path.splitext(args.outfile)[1].lower()
        if len(input_extension_set) > 1:
        logger.error('All input files must be in the same format.')
        sys.exit(1)
    input_extension = list(input_extension_set)[0]
    if input_extension not in valid_extensions:
        logger.error(('Input files must be in either .arff, .csv, .jsonlines, '
                      '.megam, .ndj, or .tsv format. You specified: '
                      '{}').format(input_extension))
        sys.exit(1)
    if output_extension != input_extension:
        logger.error(('Output file must be in the same format as the input '
                      'file.  You specified: {}').format(output_extension))
        sys.exit(1)
        merged_set = None
    for infile in args.infile:
        reader = EXT_TO_READER[input_extension](infile, quiet=args.quiet,
                                                label_col=args.label_col,
                                                id_col=args.id_col)
        fs = reader.read()
        if merged_set is None:
            merged_set = fs
        else:
            merged_set += fs
        writer_type = EXT_TO_WRITER[input_extension]
    writer_args = {'quiet': args.quiet}
    if writer_type is DelimitedFileWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
    elif writer_type is ARFFWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
        writer_args['regression'] = reader.regression
        writer_args['relation'] = reader.relation
    writer = writer_type(args.outfile, merged_set, **writer_args)
    writer.write()

if __name__ == '__main__':
    main()
from __future__ import print_function, unicode_literals
import argparse
import logging
import sys
from six import iteritems
import numpy as np
from skll import Learner
from skll.version import __version__

def main(argv=None):
        parser = argparse.ArgumentParser(description="Prints out the weights of a \
                                                  given model.",
                                     conflict_handler='resolve',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('model_file', help='model file to load')
    parser.add_argument('--k',
                        help='number of top features to print (0 for all)',
                        type=int, default=50)
    parser.add_argument('--sign',
                        choices=['positive', 'negative', 'all'],
                        default='all',
                        help='show only positive, only negative, ' +
                             'or all weights')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    k = args.k if args.k > 0 else None
    learner = Learner.from_file(args.model_file)
    (weights, intercept) = learner.model_params
    weight_items = iteritems(weights)
    if args.sign == 'positive':
        weight_items = (x for x in weight_items if x[1] > 0)
    elif args.sign == 'negative':
        weight_items = (x for x in weight_items if x[1] < 0)
    if intercept is not None:
                if '_intercept_' in intercept:
                        if isinstance(intercept['_intercept_'], np.ndarray):
                intercept_list = ["%.12f" % i for i in intercept['_intercept_']]
                print("intercept = {}".format(intercept_list))
            else:
                print("intercept = {:.12f}".format(intercept['_intercept_']))
        else:
            print("== intercept values ==")
            for (label, val) in intercept.items():
                print("{:.12f}\t{}".format(val, label))
        print()
    print("Number of nonzero features:", len(weights), file=sys.stderr)
    for feat, val in sorted(weight_items, key=lambda x: -abs(x[1]))[:k]:
        print("{:.12f}\t{}".format(val, feat))

if __name__ == '__main__':
    main()

from __future__ import print_function, unicode_literals
import argparse
import logging
from skll.experiments import run_configuration
from skll.version import __version__

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Runs the scikit-learn experiments in a given config file.\
                     If Grid Map is installed, jobs will automatically be \
                     created and run on a DRMAA-compatible cluster.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        conflict_handler='resolve')
    parser.add_argument('config_file',
                        help='Configuration file describing the sklearn task\
                              to run.',
                        nargs='+')
    parser.add_argument('-a', '--ablation',
                        help='Runs an ablation study where repeated \
                              experiments are conducted where the specified \
                              number of features in each featureset in the \
                              configuration file are held out.',
                        type=int, default=0,
                        metavar='NUM_FEATURES')
    parser.add_argument('-A', '--ablation_all',
                        help='Runs an ablation study where repeated \
                              experiments are conducted with all combinations \
                              of features in each featureset in the \
                              configuration file. Overrides --ablation \
                              setting.',
                        action='store_true')
    parser.add_argument('-k', '--keep_models',
                        help='If trained models already exists, re-use them\
                              instead of overwriting them.',
                        action='store_true')
    parser.add_argument('-l', '--local',
                        help='Do not use the Grid Engine for running jobs and\
                              just run everything sequentially on the local \
                              machine. This is for debugging.',
                        action='store_true')
    parser.add_argument('-m', '--machines',
                        help="comma-separated list of machines to add to\
                              gridmap's whitelist (if not specified, all\
                              available machines are used). Note that full \
                              names must be specified, e.g., \
                              \"nlp.research.ets.org\"",
                        default=None)
    parser.add_argument('-q', '--queue',
                        help="Use this queue for gridmap.",
                        default='all.q')
    parser.add_argument('-r', '--resume',
                        help='If result files already exist for an experiment, \
                              do not overwrite them. This is very useful when \
                              doing a large ablation experiment and part of it \
                              crashes.',
                        action='store_true')
    parser.add_argument('-v', '--verbose',
                        help='Print more status information. For every ' +
                             'additional time this flag is specified, ' +
                             'output gets more verbose.',
                        default=0, action='count')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        log_level = max(logging.WARNING - (args.verbose * 10), logging.DEBUG)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'), level=log_level)
    machines = None
    if args.machines:
        machines = args.machines.split(',')
    ablation = args.ablation
    if args.ablation_all:
        ablation = None
        for config_file in args.config_file:
        run_configuration(config_file, local=args.local, overwrite=not
                          args.keep_models, queue=args.queue, hosts=machines,
                          ablation=ablation, resume=args.resume)

if __name__ == '__main__':
    main()
from __future__ import print_function, unicode_literals
import argparse
import logging
import os
import sys
from bs4 import UnicodeDammit
from six import PY2
from skll.data.dict_vectorizer import DictVectorizer
from skll.data.readers import EXT_TO_READER
from skll.data.writers import (ARFFWriter, DelimitedFileWriter, LibSVMWriter,
                               EXT_TO_WRITER)
from skll.version import __version__

def _pair_to_dict_tuple(pair):
        number, name = pair.split('=')
    if PY2:
        name = name.encode('utf-8')
    number = int(number)
    return (name, number)

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Takes an input feature file and converts it to another \
                     format. Formats are determined automatically from file \
                     extensions.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('infile',
                        help='input feature file (ends in .arff, .csv, \
                              .jsonlines, .libsvm, .megam, .ndj, or .tsv)')
    parser.add_argument('outfile',
                        help='output feature file (ends in .arff, .csv, \
                              .jsonlines, .libsvm, .megam, .ndj, or .tsv)')
    parser.add_argument('-i', '--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the class \
                              labels in ARFF, CSV, or TSV files. For ARFF \
                              files, this must be the final column to count as\
                              the label.',
                        default='y')
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('--arff_regression',
                        help='Create ARFF files for regression, not \
                              classification.',
                        action='store_true')
    parser.add_argument('--arff_relation',
                        help='Relation name to use for ARFF file.',
                        default='skll_relation')
    parser.add_argument('--reuse_libsvm_map',
                        help='If you want to output multiple files that use \
                              the same mapping from labels and features to \
                              numbers when writing libsvm files, you can \
                              specify an existing .libsvm file to reuse the \
                              mapping from.',
                        type=argparse.FileType('rb'))
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - '
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        input_extension = os.path.splitext(args.infile)[1].lower()
    output_extension = os.path.splitext(args.outfile)[1].lower()
    if input_extension not in EXT_TO_READER:
        logger.error(('Input file must be in either .arff, .csv, .jsonlines, '
                      '.libsvm, .megam, .ndj, or .tsv format. You specified: '
                      '{}').format(input_extension))
        sys.exit(1)
        if args.reuse_libsvm_map and output_extension == '.libsvm':
        feat_map = {}
        label_map = {}
        for line in args.reuse_libsvm_map:
            line = UnicodeDammit(line, ['utf-8',
                                        'windows-1252']).unicode_markup
            if '                logger.error('The LibSVM file you want to reuse the map from '
                             'was not created by SKLL and does not actually '
                             'contain the necessary mapping info.')
                sys.exit(1)
            comments = line.split('            _, label_map_str, feat_map_str = comments.split('|')
            feat_map.update(_pair_to_dict_tuple(pair) for pair in
                            feat_map_str.strip().split())
            label_map.update(_pair_to_dict_tuple(pair) for pair in
                             label_map_str
                             .strip().split())
        feat_vectorizer = DictVectorizer()
        feat_vectorizer.fit([{name: 1} for name in feat_map])
        feat_vectorizer.vocabulary_ = feat_map
    else:
        feat_vectorizer = None
        label_map = None
        reader = EXT_TO_READER[input_extension](args.infile, quiet=args.quiet,
                                            label_col=args.label_col,
                                            id_col=args.id_col)
    feature_set = reader.read()
        writer_type = EXT_TO_WRITER[output_extension]
    writer_args = {'quiet': args.quiet}
    if writer_type is DelimitedFileWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
    elif writer_type is ARFFWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
        writer_args['regression'] = args.arff_regression
        writer_args['relation'] = args.arff_relation
    elif writer_type is LibSVMWriter:
        writer_args['label_map'] = label_map
    writer = writer_type(args.outfile, feature_set, **writer_args)
    writer.write()
if __name__ == '__main__':
    main()
from __future__ import print_function, unicode_literals
import argparse
import logging
import sys
from io import open
from skll.experiments import _write_summary_file
from skll.version import __version__

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Creates an experiment summary TSV file from a list of JSON\
                     files generated by run_experiment.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        conflict_handler='resolve')
    parser.add_argument('summary_file',
                        help='TSV file to store summary of results.')
    parser.add_argument('json_file',
                        help='JSON results file generated by run_experiment.',
                        nargs='+')
    parser.add_argument('-a', '--ablation', action='store_true', default=False,
                        help='The results files are from an ablation run.')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    file_mode = 'w' if sys.version_info >= (3, 0) else 'wb'
    with open(args.summary_file, file_mode) as output_file:
        _write_summary_file(args.json_file, output_file,
                            ablation=int(args.ablation))

if __name__ == '__main__':
    main()
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import re
from os.path import abspath, dirname, join
import numpy as np
from numpy.random import RandomState
from sklearn.datasets.samples_generator import (make_classification,
                                                make_regression)
from sklearn.feature_extraction import FeatureHasher
from skll.data import FeatureSet
from skll.config import _setup_config_parser
_my_dir = abspath(dirname(__file__))

def fill_in_config_paths(config_template_path):
    
    train_dir = join(_my_dir, 'train')
    test_dir = join(_my_dir, 'test')
    output_dir = join(_my_dir, 'output')
    config = _setup_config_parser(config_template_path, validate=False)
    task = config.get("General", "task")
    config.set("Input", "train_directory", train_dir)
    to_fill_in = ['log']
    if task != 'learning_curve':
      to_fill_in.append('predictions')
    if task not in ['cross_validate', 'learning_curve']:
        to_fill_in.append('models')
    if task in ['cross_validate', 'evaluate', 'learning_curve']:
        to_fill_in.append('results')
    for d in to_fill_in:
        config.set("Output", d, join(output_dir))
    if task == 'cross_validate':
        cv_folds_file = config.get("Input", "cv_folds_file")
        if cv_folds_file:
            config.set("Input", "cv_folds_file",
                       join(train_dir, cv_folds_file))
    if task == 'predict' or task == 'evaluate':
        config.set("Input", "test_directory", test_dir)
        custom_learner_path = config.get("Input", "custom_learner_path")
    custom_learner_abs_path = join(_my_dir, custom_learner_path)
    config.set("Input", "custom_learner_path", custom_learner_abs_path)
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}.cfg'.format(config_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def fill_in_config_paths_for_single_file(config_template_path, train_file,
                                         test_file, train_directory='',
                                         test_directory=''):
    
    train_dir = join(_my_dir, 'train')
    test_dir = join(_my_dir, 'test')
    output_dir = join(_my_dir, 'output')
    config = _setup_config_parser(config_template_path, validate=False)
    task = config.get("General", "task")
    config.set("Input", "train_file", join(train_dir, train_file))
    if task == 'predict' or task == 'evaluate':
        config.set("Input", "test_file", join(test_dir, test_file))
    if train_directory:
        config.set("Input", "train_directory", join(train_dir, train_directory))
    if test_directory:
        config.set("Input", "test_directory", join(test_dir, test_directory))
    to_fill_in = ['log', 'predictions']
    if task != 'cross_validate':
        to_fill_in.append('models')
    if task == 'evaluate' or task == 'cross_validate':
        to_fill_in.append('results')
    for d in to_fill_in:
        config.set("Output", d, join(output_dir))
    if task == 'cross_validate':
        cv_folds_file = config.get("Input", "cv_folds_file")
        if cv_folds_file:
            config.set("Input", "cv_folds_file",
                       join(train_dir, cv_folds_file))
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}.cfg'.format(config_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def fill_in_config_options(config_template_path,
                           values_to_fill_dict,
                           sub_prefix):
    
    config = _setup_config_parser(config_template_path, validate=False)
                            to_fill_in = {'General': ['experiment_name', 'task'],
                  'Input': ['train_directory', 'train_file', 'test_directory',
                            'test_file', 'featuresets', 'featureset_names',
                            'feature_hasher', 'hasher_features', 'learners',
                            'sampler', 'shuffle', 'feature_scaling',
                            'learning_curve_cv_folds_list',
                            'learning_curve_train_sizes', 'fixed_parameters',
                            'num_cv_folds', 'bad_option', 'duplicate_option'],
                  'Tuning': ['probability', 'grid_search', 'objective',
                             'param_grids', 'objectives', 'duplicate_option'],
                  'Output': ['results', 'log', 'models',
                             'predictions']}
    for section in to_fill_in:
        for param_name in to_fill_in[section]:
            if param_name in values_to_fill_dict:
                config.set(section, param_name,
                           values_to_fill_dict[param_name])
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}_{}.cfg'.format(config_prefix, sub_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def fill_in_config_paths_for_fancy_output(config_template_path):
    
    train_dir = join(_my_dir, 'train')
    test_dir = join(_my_dir, 'test')
    output_dir = join(_my_dir, 'output')
    config = _setup_config_parser(config_template_path, validate=False)
    config.set("Input", "train_file", join(train_dir, "fancy_train.jsonlines"))
    config.set("Input", "test_file", join(test_dir,
                                          "fancy_test.jsonlines"))
    config.set("Output", "results", output_dir)
    config.set("Output", "log", output_dir)
    config.set("Output", "predictions", output_dir)
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}.cfg'.format(config_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def make_classification_data(num_examples=100, train_test_ratio=0.5,
                             num_features=10, use_feature_hashing=False,
                             feature_bins=4, num_labels=2,
                             empty_labels=False, feature_prefix='f',
                             class_weights=None, non_negative=False,
                             one_string_feature=False, num_string_values=4,
                             random_state=1234567890):
        num_numeric_features = (num_features - 1 if one_string_feature else
                            num_features)
    X, y = make_classification(n_samples=num_examples,
                               n_features=num_numeric_features,
                               n_informative=num_numeric_features,
                               n_redundant=0, n_classes=num_labels,
                               weights=class_weights,
                               random_state=random_state)
            if non_negative:
        X = abs(X)
            ids = ['EXAMPLE_{}'.format(n) for n in range(1, num_examples + 1)]
            if one_string_feature:
        prng = RandomState(random_state)
        random_indices = prng.random_integers(0, num_string_values - 1,
                                              num_examples)
        possible_values = [chr(x) for x in range(97, 97 + num_string_values)]
        string_feature_values = [possible_values[i] for i in random_indices]
        string_feature_column = np.array(string_feature_values,
                                         dtype=object).reshape(100, 1)
        X = np.append(X, string_feature_column, 1)
        feature_names = ['{}{:02d}'.format(feature_prefix, n) for n in
                     range(1, num_features + 1)]
    features = [dict(zip(feature_names, row)) for row in X]
        num_train_examples = int(round(train_test_ratio * num_examples))
    train_features, test_features = (features[:num_train_examples],
                                     features[num_train_examples:])
    train_y, test_y = y[:num_train_examples], y[num_train_examples:]
    train_ids, test_ids = ids[:num_train_examples], ids[num_train_examples:]
        train_labels = None if empty_labels else train_y
    test_labels = None if empty_labels else test_y
            vectorizer = (FeatureHasher(n_features=feature_bins)
                  if use_feature_hashing else None)
    train_fs = FeatureSet('classification_train', train_ids,
                          labels=train_labels, features=train_features,
                          vectorizer=vectorizer)
    if train_test_ratio < 1.0:
        test_fs = FeatureSet('classification_test', test_ids,
                             labels=test_labels, features=test_features,
                             vectorizer=vectorizer)
    else:
        test_fs = None
    return (train_fs, test_fs)

def make_regression_data(num_examples=100, train_test_ratio=0.5,
                         num_features=2, sd_noise=1.0,
                         use_feature_hashing=False,
                         feature_bins=4,
                         start_feature_num=1,
                         random_state=1234567890):
        X, y, weights = make_regression(n_samples=num_examples,
                                    n_features=num_features,
                                    noise=sd_noise, random_state=random_state,
                                    coef=True)
            ids = ['EXAMPLE_{}'.format(n) for n in range(1, num_examples + 1)]
        feature_names = ['f{:02d}'.format(n) for n
                     in range(start_feature_num,
                              start_feature_num + num_features)]
    features = [dict(zip(feature_names, row)) for row in X]
        weightdict = dict(zip(feature_names, weights))
        num_train_examples = int(round(train_test_ratio * num_examples))
    train_features, test_features = (features[:num_train_examples],
                                     features[num_train_examples:])
    train_y, test_y = y[:num_train_examples], y[num_train_examples:]
    train_ids, test_ids = ids[:num_train_examples], ids[num_train_examples:]
            vectorizer = (FeatureHasher(n_features=feature_bins) if
                  use_feature_hashing else None)
    train_fs = FeatureSet('regression_train', train_ids,
                          labels=train_y, features=train_features,
                          vectorizer=vectorizer)
    test_fs = FeatureSet('regression_test', test_ids,
                         labels=test_y, features=test_features,
                         vectorizer=vectorizer)
    return (train_fs, test_fs, weightdict)

def make_sparse_data(use_feature_hashing=False):
            X, y = make_classification(n_samples=500, n_features=3,
                               n_informative=3, n_redundant=0,
                               n_classes=2, random_state=1234567890)
            X = np.abs(X)
        X[np.where(X == 0)] += 1
            ids = ['EXAMPLE_{}'.format(n) for n in range(1, 501)]
            feature_names = ['f{}'.format(n) for n in range(1, 6)]
    features = []
    for row in X:
        row = [0] + row.tolist() + [0]
        features.append(dict(zip(feature_names, row)))
        vectorizer = FeatureHasher(n_features=4) if use_feature_hashing else None
    train_fs = FeatureSet('train_sparse', ids,
                          features=features, labels=y,
                          vectorizer=vectorizer)
        X, y = make_classification(n_samples=100, n_features=4,
                               n_informative=4, n_redundant=0,
                               n_classes=2, random_state=1234567890)
    X = np.abs(X)
    X[np.where(X == 0)] += 1
    ids = ['EXAMPLE_{}'.format(n) for n in range(1, 101)]
            feature_names = ['f{}'.format(n) for n in range(1, 6)]
    features = []
    for row in X:
        row = row.tolist()
        row = row[:3] + [0] + row[3:]
        features.append(dict(zip(feature_names, row)))
    test_fs = FeatureSet('test_sparse', ids,
                         features=features, labels=y,
                         vectorizer=vectorizer)
    return train_fs, test_fs
from sklearn.linear_model import LogisticRegression

class CustomLogisticRegressionWrapper(LogisticRegression):
    pass
from collections import Counter
import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin

class MajorityClassLearner(BaseEstimator, ClassifierMixin):
    def __init__(self):
        self.majority_class = None
    def fit(self, X, y):
        counts = Counter(y)
        max_count = -1
        for label, count in counts.items():
            if count > max_count:
                self.majority_class = label
                max_count = count
    def predict(self, X):
        return np.array([self.majority_class for x in range(X.shape[0])])
from setuptools import setup, find_packages, Extension
from codecs import open
from os import path
try:
    import numpy as np
except ImportError:
    exit('Please install numpy>=1.11.2 first.')
try:
    from Cython.Build import cythonize
    from Cython.Distutils import build_ext
except ImportError:
    USE_CYTHON = False
else:
    USE_CYTHON = True
__version__ = '1.0.2'
here = path.abspath(path.dirname(__file__))
try:
    import pypandoc
    long_description = pypandoc.convert(path.join(here, 'README.md'), 'rst')
except(IOError, ImportError):
    with open(path.join(here, 'README.md'), encoding='utf-8') as f:
        long_description = f.read()
with open(path.join(here, 'requirements.txt'), encoding='utf-8') as f:
    all_reqs = f.read().split('\n')
install_requires = [x.strip() for x in all_reqs if 'git+' not in x]
dependency_links = [x.strip().replace('git+', '') for x in all_reqs if x.startswith('git+')]
cmdclass = {}
ext = '.pyx' if USE_CYTHON else '.c'
extensions = [Extension('surprise.similarities',
                       ['surprise/similarities' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.matrix_factorization',
                        ['surprise/prediction_algorithms/matrix_factorization' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.optimize_baselines',
                        ['surprise/prediction_algorithms/optimize_baselines' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.slope_one',
                        ['surprise/prediction_algorithms/slope_one' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.co_clustering',
                        ['surprise/prediction_algorithms/co_clustering' + ext],
                        include_dirs=[np.get_include()]),
             ]
if USE_CYTHON:
    ext_modules = cythonize(extensions)
    cmdclass.update({'build_ext': build_ext})
else:
	ext_modules = extensions
setup(
    name='scikit-surprise',
    author='Nicolas Hug',
    author_email='contact@nicolas-hug.com',
    description=('An easy-to-use library for recommender systems.'),
    long_description=long_description,
    version=__version__,
    url='http://surpriselib.com',
    license='GPLv3+',
    classifiers=[
      'Development Status :: 5 - Production/Stable',
      'Intended Audience :: Developers',
      'Intended Audience :: Education',
      'Intended Audience :: Science/Research',
      'Topic :: Scientific/Engineering',
      'License :: OSI Approved :: BSD License',
      'Programming Language :: Python :: 3',
      'Programming Language :: Python :: 2.7',
    ],
    keywords='recommender recommendation system',
    packages=find_packages(exclude=['tests*']),
    include_package_data=True,
    ext_modules = ext_modules,
    cmdclass=cmdclass,
    install_requires=install_requires,
    dependency_links=dependency_links,
    entry_points={'console_scripts':
                 ['surprise = surprise.__main__:main']},
)
import sys
import os
import shlex


extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.napoleon',
    'sphinx.ext.coverage',
    'sphinx.ext.mathjax',
    'sphinx.ext.viewcode',
    'sphinx.ext.graphviz',
    'sphinx.ext.inheritance_diagram',
    'sphinx.ext.autosummary',
    'sphinxcontrib.bibtex',
    'sphinxcontrib.spelling',
]
templates_path = ['.templates']
source_suffix = '.rst'

master_doc = 'index'
project = 'Surprise'
copyright = '2015, Nicolas Hug'
author = 'Nicolas Hug'
version = '0'
release = '1'
language = None

exclude_patterns = []

add_function_parentheses = True

pygments_style = 'sphinx'

todo_include_todos = False

import sphinx_rtd_theme
html_theme = "sphinx_rtd_theme"
html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]












htmlhelp_basename = 'Surprisedoc'

latex_elements = {
'pointsize': '12pt',

}
latex_documents = [
  (master_doc, 'Surprise.tex', 'Surprise Documentation',
   'Nicolas Hug', 'manual'),
]




man_pages = [
    (master_doc, 'surprise', 'Surprise Documentation',
     [author], 1)
]


texinfo_documents = [
  (master_doc, 'Surprise', 'Surprise Documentation',
   author, 'Surprise', 'One line description of project.',
   'Miscellaneous'),
]


from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import BaselineOnly
from surprise import KNNBasic
from surprise import Dataset
from surprise import evaluate

data = Dataset.load_builtin('ml-100k')
print('Using ALS')
bsl_options = {'method': 'als',
               'n_epochs': 5,
               'reg_u': 12,
               'reg_i': 5
               }
algo = BaselineOnly(bsl_options=bsl_options)
evaluate(algo, data)
print('Using SGD')
bsl_options = {'method': 'sgd',
               'learning_rate': .00005,
               }
algo = BaselineOnly(bsl_options=bsl_options)
evaluate(algo, data)
print('Using ALS with pearson_baseline similarity')
bsl_options = {'method': 'als',
               'n_epochs': 20,
               }
sim_options = {'name': 'pearson_baseline'}
algo = KNNBasic(bsl_options=bsl_options, sim_options=sim_options)
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import SVD
from surprise import Dataset
from surprise import evaluate, print_perf

data = Dataset.load_builtin('ml-100k')
data.split(n_folds=3)
algo = SVD()
perf = evaluate(algo, data, measures=['RMSE', 'MAE'])
print_perf(perf)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import GridSearch
from surprise import SVD
from surprise import Dataset
param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],
              'reg_all': [0.4, 0.6]}
grid_search = GridSearch(SVD, param_grid, measures=['RMSE', 'FCP'])
data = Dataset.load_builtin('ml-100k')
data.split(n_folds=3)
grid_search.evaluate(data)
print(grid_search.best_score['RMSE'])
print(grid_search.best_params['RMSE'])
print(grid_search.best_score['FCP'])
print(grid_search.best_params['FCP'])
import pandas as pd  
results_df = pd.DataFrame.from_dict(grid_search.cv_results)
print(results_df)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import BaselineOnly
from surprise import Dataset
from surprise import accuracy
data = Dataset.load_builtin('ml-100k')
data.split(n_folds=3)
algo = BaselineOnly()
for trainset, testset in data.folds():
        algo.train(trainset)
    predictions = algo.test(testset)
        rmse = accuracy.rmse(predictions, verbose=True)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import os
from surprise import BaselineOnly
from surprise import Dataset
from surprise import evaluate
from surprise import Reader
file_path = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/u.data')
reader = Reader(line_format='user item rating timestamp', sep='\t')
data = Dataset.load_from_file(file_path, reader=reader)
data.split(n_folds=5)
algo = BaselineOnly()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import os
from surprise import BaselineOnly
from surprise import Dataset
from surprise import evaluate
from surprise import Reader
files_dir = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/')
reader = Reader('ml-100k')
train_file = files_dir + 'u%d.base'
test_file = files_dir + 'u%d.test'
folds_files = [(train_file % i, test_file % i) for i in (1, 2, 3, 4, 5)]
data = Dataset.load_from_folds(folds_files, reader=reader)
algo = BaselineOnly()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import KNNBasic
from surprise import Dataset
from surprise import evaluate
data = Dataset.load_builtin('ml-100k')
trainset = data.build_full_trainset()
algo = KNNBasic()
algo.train(trainset)

uid = str(196)  iid = str(302)  
pred = algo.predict(uid, iid, r_ui=4, verbose=True)

data.split(n_folds=3)
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import KNNBasic
from surprise import Dataset
from surprise import evaluate

data = Dataset.load_builtin('ml-100k')
sim_options = {'name': 'cosine',
               'user_based': False                 }
algo = KNNBasic(sim_options=sim_options)
evaluate(algo, data)
sim_options = {'name': 'pearson_baseline',
               'shrinkage': 0                 }
algo = KNNBasic(sim_options=sim_options)
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate

class MyOwnAlgorithm(AlgoBase):
    def __init__(self):
                AlgoBase.__init__(self)
    def estimate(self, u, i):
        sum_means = self.trainset.global_mean
        div = 1
        if self.trainset.knows_user(u):
            sum_means += np.mean([r for (_, r) in self.trainset.ur[u]])
            div += 1
        if self.trainset.knows_item(i):
            sum_means += np.mean([r for (_, r) in self.trainset.ir[i]])
            div += 1
        return sum_means / div

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate

class MyOwnAlgorithm(AlgoBase):
    def __init__(self):
                AlgoBase.__init__(self)
    def estimate(self, u, i):
        return 3

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate

class MyOwnAlgorithm(AlgoBase):
    def __init__(self):
                AlgoBase.__init__(self)
    def train(self, trainset):
                AlgoBase.train(self, trainset)
                        self.the_mean = np.mean([r for (_, _, r) in
                                 self.trainset.all_ratings()])
    def estimate(self, u, i):
        return self.the_mean

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate
from surprise import PredictionImpossible

class MyOwnAlgorithm(AlgoBase):
    def __init__(self, sim_options={}, bsl_options={}):
        AlgoBase.__init__(self, sim_options=sim_options,
                          bsl_options=bsl_options)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
                self.bu, self.bi = self.compute_baselines()
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
                        neighbors = [(v, self.sim[u, v]) for (v, r) in self.trainset.ir[i]]
                neighbors = sorted(neighbors, key=lambda x: x[1], reverse=True)
        print('The 3 nearest neighbors of user', str(u), 'are:')
        for v, sim_uv in neighbors[:3]:
            print('user {0:} with sim {1:1.2f}'.format(v, sim_uv))
                bsl = self.trainset.global_mean + self.bu[u] + self.bi[i]
        return bsl

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import defaultdict
import numpy as np
from six import iteritems

def rmse(predictions, verbose=True):
    
    if not predictions:
        raise ValueError('Prediction list is empty.')
    mse = np.mean([float((true_r - est)**2)
                   for (_, _, true_r, est, _) in predictions])
    rmse_ = np.sqrt(mse)
    if verbose:
        print('RMSE: {0:1.4f}'.format(rmse_))
    return rmse_

def mae(predictions, verbose=True):
    
    if not predictions:
        raise ValueError('Prediction list is empty.')
    mae_ = np.mean([float(abs(true_r - est))
                    for (_, _, true_r, est, _) in predictions])
    if verbose:
        print('MAE:  {0:1.4f}'.format(mae_))
    return mae_

def fcp(predictions, verbose=True):
    
    if not predictions:
        raise ValueError('Prediction list is empty.')
    predictions_u = defaultdict(list)
    nc_u = defaultdict(int)
    nd_u = defaultdict(int)
    for u0, _, r0, est, _ in predictions:
        predictions_u[u0].append((r0, est))
    for u0, preds in iteritems(predictions_u):
        for r0i, esti in preds:
            for r0j, estj in preds:
                if esti > estj and r0i > r0j:
                    nc_u[u0] += 1
                if esti >= estj and r0i < r0j:
                    nd_u[u0] += 1
    nc = np.mean(list(nc_u.values())) if nc_u else 0
    nd = np.mean(list(nd_u.values())) if nd_u else 0
    try:
        fcp = nc / (nc + nd)
    except ZeroDivisionError:
        raise ValueError('cannot compute fcp on this list of prediction. ' +
                         'Does every user have at least two predictions?')
    if verbose:
        print('FCP:  {0:1.4f}'.format(fcp))
    return fcp

from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import defaultdict
from collections import namedtuple
import sys
import os
import zipfile
import itertools
import random
import numpy as np
from six.moves import input
from six.moves.urllib.request import urlretrieve
from six.moves import range
from six import iteritems

DATASETS_DIR = os.path.expanduser('~') + '/.surprise_data/'
BuiltinDataset = namedtuple('BuiltinDataset', ['url', 'path', 'reader_params'])
BUILTIN_DATASETS = {
    'ml-100k':
        BuiltinDataset(
            url='http://files.grouplens.org/datasets/movielens/ml-100k.zip',
            path=DATASETS_DIR + 'ml-100k/ml-100k/u.data',
            reader_params=dict(line_format='user item rating timestamp',
                               rating_scale=(1, 5),
                               sep='\t')
        ),
    'ml-1m':
        BuiltinDataset(
            url='http://files.grouplens.org/datasets/movielens/ml-1m.zip',
            path=DATASETS_DIR + 'ml-1m/ml-1m/ratings.dat',
            reader_params=dict(line_format='user item rating timestamp',
                               rating_scale=(1, 5),
                               sep='::')
        ),
    'jester':
        BuiltinDataset(
            url='http://eigentaste.berkeley.edu/dataset/jester_dataset_2.zip',
            path=DATASETS_DIR + 'jester/jester_ratings.dat',
            reader_params=dict(line_format='user item rating',
                               rating_scale=(-10, 10))
        )
}

class Dataset:
    
    def __init__(self, reader):
        self.reader = reader
    @classmethod
    def load_builtin(cls, name='ml-100k'):
        
        try:
            dataset = BUILTIN_DATASETS[name]
        except KeyError:
            raise ValueError('unknown dataset ' + name +
                             '. Accepted values are ' +
                             ', '.join(BUILTIN_DATASETS.keys()) + '.')
                if not os.path.isfile(dataset.path):
            answered = False
            while not answered:
                print('Dataset ' + name + ' could not be found. Do you want '
                      'to download it? [Y/n] ', end='')
                choice = input().lower()
                if choice in ['yes', 'y', '', 'omg this is so nice of you!!']:
                    answered = True
                elif choice in ['no', 'n', 'hell no why would i want that?!']:
                    answered = True
                    print("Ok then, I'm out!")
                    sys.exit()
            if not os.path.exists(DATASETS_DIR):
                os.makedirs(DATASETS_DIR)
            print('Trying to download dataset from ' + dataset.url + '...')
            urlretrieve(dataset.url, DATASETS_DIR + 'tmp.zip')
            with zipfile.ZipFile(DATASETS_DIR + 'tmp.zip', 'r') as tmp_zip:
                tmp_zip.extractall(DATASETS_DIR + name)
            os.remove(DATASETS_DIR + 'tmp.zip')
            print('Done! Dataset', name, 'has been saved to', DATASETS_DIR +
                  name)
        reader = Reader(**dataset.reader_params)
        return cls.load_from_file(file_path=dataset.path, reader=reader)
    @classmethod
    def load_from_file(cls, file_path, reader):
        
        return DatasetAutoFolds(ratings_file=file_path, reader=reader)
    @classmethod
    def load_from_folds(cls, folds_files, reader):
        
        return DatasetUserFolds(folds_files=folds_files, reader=reader)
    def read_ratings(self, file_name):
        
        with open(os.path.expanduser(file_name)) as f:
            raw_ratings = [self.reader.parse_line(line) for line in
                           itertools.islice(f, self.reader.skip_lines, None)]
        return raw_ratings
    def folds(self):
        
        for raw_trainset, raw_testset in self.raw_folds():
            trainset = self.construct_trainset(raw_trainset)
            testset = self.construct_testset(raw_testset)
            yield trainset, testset
    def construct_trainset(self, raw_trainset):
        raw2inner_id_users = {}
        raw2inner_id_items = {}
        current_u_index = 0
        current_i_index = 0
        ur = defaultdict(list)
        ir = defaultdict(list)
                for urid, irid, r, timestamp in raw_trainset:
            try:
                uid = raw2inner_id_users[urid]
            except KeyError:
                uid = current_u_index
                raw2inner_id_users[urid] = current_u_index
                current_u_index += 1
            try:
                iid = raw2inner_id_items[irid]
            except KeyError:
                iid = current_i_index
                raw2inner_id_items[irid] = current_i_index
                current_i_index += 1
            ur[uid].append((iid, r))
            ir[iid].append((uid, r))
        n_users = len(ur)          n_items = len(ir)          n_ratings = len(raw_trainset)
        trainset = Trainset(ur,
                            ir,
                            n_users,
                            n_items,
                            n_ratings,
                            self.reader.rating_scale,
                            self.reader.offset,
                            raw2inner_id_users,
                            raw2inner_id_items)
        return trainset
    def construct_testset(self, raw_testset):
        return [(ruid, riid, r_ui_trans)
                for (ruid, riid, r_ui_trans, _) in raw_testset]

class DatasetUserFolds(Dataset):
    
    def __init__(self, folds_files=None, reader=None):
        Dataset.__init__(self, reader)
        self.folds_files = folds_files
                for train_test_files in self.folds_files:
            for f in train_test_files:
                if not os.path.isfile(os.path.expanduser(f)):
                    raise ValueError('File ' + str(f) + ' does not exist.')
    def raw_folds(self):
        for train_file, test_file in self.folds_files:
            raw_train_ratings = self.read_ratings(train_file)
            raw_test_ratings = self.read_ratings(test_file)
            yield raw_train_ratings, raw_test_ratings

class DatasetAutoFolds(Dataset):
    
    def __init__(self, ratings_file=None, reader=None):
        Dataset.__init__(self, reader)
        self.ratings_file = ratings_file
        self.n_folds = 5
        self.shuffle = True
        self.raw_ratings = self.read_ratings(self.ratings_file)
    def build_full_trainset(self):
        
        return self.construct_trainset(self.raw_ratings)
    def raw_folds(self):
        if self.shuffle:
            random.shuffle(self.raw_ratings)
            self.shuffle = False  
        def k_folds(seq, n_folds):
            
            if n_folds > len(seq) or n_folds < 2:
                raise ValueError('Incorrect value for n_folds.')
            start, stop = 0, 0
            for fold_i in range(n_folds):
                start = stop
                stop += len(seq) // n_folds
                if fold_i < len(seq) % n_folds:
                    stop += 1
                yield seq[:start] + seq[stop:], seq[start:stop]
        return k_folds(self.raw_ratings, self.n_folds)
    def split(self, n_folds=5, shuffle=True):
        
        self.n_folds = n_folds
        self.shuffle = shuffle

class Reader():
    
    def __init__(self, name=None, line_format=None, sep=None,
                 rating_scale=(1, 5), skip_lines=0):
        if name:
            try:
                self.__init__(**BUILTIN_DATASETS[name].reader_params)
            except KeyError:
                raise ValueError('unknown reader ' + name +
                                 '. Accepted values are ' +
                                 ', '.join(BUILTIN_DATASETS.keys()) + '.')
        else:
            self.sep = sep
            self.skip_lines = skip_lines
            self.rating_scale = rating_scale
            lower_bound, higher_bound = rating_scale
            self.offset = -lower_bound + 1 if lower_bound <= 0 else 0
            splitted_format = line_format.split()
            entities = ['user', 'item', 'rating']
            if 'timestamp' in splitted_format:
                self.with_timestamp = True
                entities.append('timestamp')
            else:
                self.with_timestamp = False
                        if any(field not in entities for field in splitted_format):
                raise ValueError('line_format parameter is incorrect.')
            self.indexes = [splitted_format.index(entity) for entity in
                            entities]
    def parse_line(self, line):
        '''Parse a line.
        Ratings are translated so that they are all strictly positive.
        Args:
            line(str): The line to parse
        Returns:
            tuple: User id, item id, rating and timestamp. The timestamp is set
            to ``None`` if it does no exist.
            '''
        line = line.split(self.sep)
        try:
            if self.with_timestamp:
                uid, iid, r, timestamp = (line[i].strip()
                                          for i in self.indexes)
            else:
                uid, iid, r = (line[i].strip()
                               for i in self.indexes)
                timestamp = None
        except IndexError:
            raise ValueError(('Impossible to parse line.' +
                              ' Check the line_format  and sep parameters.'))
        return uid, iid, float(r) + self.offset, timestamp

class Trainset:
    
    def __init__(self, ur, ir, n_users, n_items, n_ratings, rating_scale,
                 offset, raw2inner_id_users, raw2inner_id_items):
        self.ur = ur
        self.ir = ir
        self.n_users = n_users
        self.n_items = n_items
        self.n_ratings = n_ratings
        self.rating_scale = rating_scale
        self.offset = offset
        self._raw2inner_id_users = raw2inner_id_users
        self._raw2inner_id_items = raw2inner_id_items
        self._global_mean = None
    def knows_user(self, uid):
        
        return uid in self.ur
    def knows_item(self, iid):
        
        return iid in self.ir
    def to_inner_uid(self, ruid):
        
        try:
            return self._raw2inner_id_users[ruid]
        except KeyError:
            raise ValueError(('User ' + str(ruid) +
                              ' is not part of the trainset.'))
    def to_inner_iid(self, riid):
        
        try:
            return self._raw2inner_id_items[riid]
        except KeyError:
            raise ValueError(('Item ' + str(riid) +
                              ' is not part of the trainset.'))
    def all_ratings(self):
        
        for u, u_ratings in iteritems(self.ur):
            for i, r in u_ratings:
                yield u, i, r
    def all_users(self):
                return range(self.n_users)
    def all_items(self):
                return range(self.n_items)
    @property
    def global_mean(self):
                if self._global_mean is None:
            self._global_mean = np.mean([r for (_, _, r) in
                                         self.all_ratings()])
        return self._global_mean
import pickle

def dump(file_name, predictions, trainset=None, algo=None):
    
    dump_obj = dict()
    dump_obj['predictions'] = predictions
    if trainset is not None:
        dump_obj['trainset'] = trainset
    if algo is not None:
        dump_obj['algo'] = algo.__dict__          dump_obj['algo']['name'] = algo.__class__.__name__
    pickle.dump(dump_obj, open(file_name, 'wb'))
    print('The dump has been saved as file', file_name)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import defaultdict
import time
import os
import numpy as np
from six import iteritems
from six import itervalues
from itertools import product
from . import accuracy
from .dump import dump

def evaluate(algo, data, measures=['rmse', 'mae'], with_dump=False,
             dump_dir=None, verbose=1):
    
    performances = CaseInsensitiveDefaultDict(list)
    print('Evaluating {0} of algorithm {1}.'.format(
          ', '.join((m.upper() for m in measures)),
          algo.__class__.__name__))
    print()
    for fold_i, (trainset, testset) in enumerate(data.folds()):
        if verbose:
            print('-' * 12)
            print('Fold ' + str(fold_i + 1))
                algo.train(trainset)
        predictions = algo.test(testset, verbose=(verbose == 2))
                for measure in measures:
            f = getattr(accuracy, measure.lower())
            performances[measure].append(f(predictions, verbose=verbose))
        if with_dump:
            if dump_dir is None:
                dump_dir = os.path.expanduser('~') + '/.surprise_data/dumps/'
            if not os.path.exists(dump_dir):
                os.makedirs(dump_dir)
            date = time.strftime('%y%m%d-%Hh%Mm%S', time.localtime())
            file_name = date + '-' + algo.__class__.__name__
            file_name += '-fold{0}'.format(fold_i + 1)
            file_name = os.path.join(dump_dir, file_name)
            dump(file_name, predictions, trainset, algo)
    if verbose:
        print('-' * 12)
        print('-' * 12)
        for measure in measures:
            print('Mean {0:4s}: {1:1.4f}'.format(
                  measure.upper(), np.mean(performances[measure])))
        print('-' * 12)
        print('-' * 12)
    return performances

class GridSearch:
    
    def __init__(self, algo_class, param_grid, measures=['rmse', 'mae'],
                 verbose=1):
        self.best_params = CaseInsensitiveDefaultDict(list)
        self.best_index = CaseInsensitiveDefaultDict(list)
        self.best_score = CaseInsensitiveDefaultDict(list)
        self.best_estimator = CaseInsensitiveDefaultDict(list)
        self.cv_results = defaultdict(list)
        self.algo_class = algo_class
        self.param_grid = param_grid
        self.measures = [measure.upper() for measure in measures]
        self.verbose = verbose
        self.param_combinations = [dict(zip(param_grid, v)) for v in
                                   product(*param_grid.values())]
    def evaluate(self, data):
        
        num_of_combinations = len(self.param_combinations)
        params = []
        scores = []
                for combination_index, combination in enumerate(
                self.param_combinations):
            params.append(combination)
            if self.verbose >= 1:
                print('-' * 12)
                print('Parameters combination {} of {}'.
                      format(combination_index + 1, num_of_combinations))
                print('params: ', combination)
                        algo_instance = self.algo_class(**combination)
            evaluate_results = evaluate(algo_instance, data,
                                        measures=self.measures,
                                        verbose=(self.verbose == 2))
                        mean_score = {}
            for measure in self.measures:
                mean_score[measure] = np.mean(evaluate_results[measure])
            scores.append(mean_score)
            if self.verbose == 1:
                print('-' * 12)
                for measure in self.measures:
                    print('Mean {0:4s}: {1:1.4f}'.format(
                        measure, mean_score[measure]))
                print('-' * 12)
                self.cv_results['params'] = params
        self.cv_results['scores'] = scores
                for param, score in zip(params, scores):
            for param_key, score_key in zip(param.keys(), score.keys()):
                self.cv_results[param_key].append(param[param_key])
                self.cv_results[score_key].append(score[score_key])
                for measure in self.measures:
            if measure == 'FCP':
                best_dict = max(self.cv_results['scores'],
                                key=lambda x: x[measure])
            else:
                best_dict = min(self.cv_results['scores'],
                                key=lambda x: x[measure])
            self.best_score[measure] = best_dict[measure]
            self.best_index[measure] = self.cv_results['scores'].index(
                best_dict)
            self.best_params[measure] = self.cv_results['params'][
                self.best_index[measure]]
            self.best_estimator[measure] = self.algo_class(
                **self.best_params[measure])

class CaseInsensitiveDefaultDict(defaultdict):
        def __setitem__(self, key, value):
        super(CaseInsensitiveDefaultDict, self).__setitem__(key.lower(), value)
    def __getitem__(self, key):
        return super(CaseInsensitiveDefaultDict, self).__getitem__(key.lower())

def print_perf(performances):
        n_folds = [len(values) for values in itervalues(performances)][0]
    row_format = '{:<8}' * (n_folds + 2)
    s = row_format.format(
        '',
        *['Fold {0}'.format(i + 1) for i in range(n_folds)] + ['Mean'])
    s += '\n'
    s += '\n'.join(row_format.format(
        key.upper(),
        *['{:1.4f}'.format(v) for v in vals] +
        ['{:1.4f}'.format(np.mean(vals))])
        for (key, vals) in iteritems(performances))
    print(s)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from six import iteritems

def cosine(n_x, yr, min_support):
    
        cdef np.ndarray[np.int_t, ndim=2] prods
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.int_t, ndim=2] sqi
        cdef np.ndarray[np.int_t, ndim=2] sqj
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef int xi, xj, ri, rj
    cdef int min_sprt = min_support
    prods = np.zeros((n_x, n_x), np.int)
    freq = np.zeros((n_x, n_x), np.int)
    sqi = np.zeros((n_x, n_x), np.int)
    sqj = np.zeros((n_x, n_x), np.int)
    sim = np.zeros((n_x, n_x), np.double)
    for y, y_ratings in iteritems(yr):
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                freq[xi, xj] += 1
                prods[xi, xj] += ri * rj
                sqi[xi, xj] += ri**2
                sqj[xi, xj] += rj**2
    for xi in range(n_x):
        sim[xi, xi] = 1
        for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] = 0
            else:
                denum = np.sqrt(sqi[xi, xj] * sqj[xi, xj])
                sim[xi, xj] = prods[xi, xj] / denum
            sim[xj, xi] = sim[xi, xj]
    return sim

def msd(n_x, yr, min_support):
    
        cdef np.ndarray[np.double_t, ndim=2] sq_diff
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef int xi, xj, ri, rj
    cdef int min_sprt = min_support
    sq_diff = np.zeros((n_x, n_x), np.double)
    freq = np.zeros((n_x, n_x), np.int)
    sim = np.zeros((n_x, n_x), np.double)
    for y, y_ratings in iteritems(yr):
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                sq_diff[xi, xj] += (ri - rj)**2
                freq[xi, xj] += 1
    for xi in range(n_x):
        sim[xi, xi] = 1          for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] == 0
            else:
                                sim[xi, xj] = 1 / (sq_diff[xi, xj] / freq[xi, xj] + 1)
            sim[xj, xi] = sim[xi, xj]
    return sim

def pearson(n_x, yr, min_support):
    
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.int_t, ndim=2] prods
        cdef np.ndarray[np.int_t, ndim=2] sqi
        cdef np.ndarray[np.int_t, ndim=2] sqj
        cdef np.ndarray[np.int_t, ndim=2] si
        cdef np.ndarray[np.int_t, ndim=2] sj
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef int xi, xj, ri, rj
    cdef int min_sprt = min_support
    freq = np.zeros((n_x, n_x), np.int)
    prods = np.zeros((n_x, n_x), np.int)
    sqi = np.zeros((n_x, n_x), np.int)
    sqj = np.zeros((n_x, n_x), np.int)
    si = np.zeros((n_x, n_x), np.int)
    sj = np.zeros((n_x, n_x), np.int)
    sim = np.zeros((n_x, n_x), np.double)
    for y, y_ratings in iteritems(yr):
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                prods[xi, xj] += ri * rj
                freq[xi, xj] += 1
                sqi[xi, xj] += ri**2
                sqj[xi, xj] += rj**2
                si[xi, xj] += ri
                sj[xi, xj] += rj
    for xi in range(n_x):
        sim[xi, xi] = 1
        for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] == 0
            else:
                n = freq[xi, xj]
                num = n * prods[xi, xj] - si[xi, xj] * sj[xi, xj]
                denum = np.sqrt((n * sqi[xi, xj] - si[xi, xj]**2) *
                                (n * sqj[xi, xj] - sj[xi, xj]**2))
                if denum == 0:
                    sim[xi, xj] = 0
                else:
                    sim[xi, xj] = num / denum
            sim[xj, xi] = sim[xi, xj]
    return sim

def pearson_baseline(n_x, yr, min_support, global_mean, x_biases, y_biases,
                     shrinkage=100):
    
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.double_t, ndim=2] prods
        cdef np.ndarray[np.double_t, ndim=2] sq_diff_i
        cdef np.ndarray[np.double_t, ndim=2] sq_diff_j
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef np.ndarray[np.double_t, ndim=1] x_biases_
    cdef np.ndarray[np.double_t, ndim=1] y_biases_
    cdef int xi, xj
    cdef double ri, rj, diff_i, diff_j, partial_bias
    cdef int min_sprt = min_support
    cdef double global_mean_ = global_mean
    freq = np.zeros((n_x, n_x), np.int)
    prods = np.zeros((n_x, n_x), np.double)
    sq_diff_i = np.zeros((n_x, n_x), np.double)
    sq_diff_j = np.zeros((n_x, n_x), np.double)
    sim = np.zeros((n_x, n_x), np.double)
    x_biases_ = x_biases
    y_biases_ = y_biases
            min_sprt = max(2, min_sprt)
    for y, y_ratings in iteritems(yr):
        partial_bias = global_mean_ + y_biases_[y]
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                freq[xi, xj] += 1
                diff_i = (ri - (partial_bias + x_biases_[xi]))
                diff_j = (rj - (partial_bias + x_biases_[xj]))
                prods[xi, xj] += diff_i * diff_j
                sq_diff_i[xi, xj] += diff_i**2
                sq_diff_j[xi, xj] += diff_j**2
    for xi in range(n_x):
        sim[xi, xi] = 1
        for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] = 0
            else:
                sim[xi, xj] = prods[xi, xj] / (np.sqrt(sq_diff_i[xi, xj] *
                                                       sq_diff_j[xi, xj]))
                                sim[xi, xj] *= (freq[xi, xj] - 1) / (freq[xi, xj] - 1 +
                                                     shrinkage)
            sim[xj, xi] = sim[xi, xj]
    return sim
from pkg_resources import get_distribution
from .prediction_algorithms import AlgoBase
from .prediction_algorithms import NormalPredictor
from .prediction_algorithms import BaselineOnly
from .prediction_algorithms import KNNBasic
from .prediction_algorithms import KNNWithMeans
from .prediction_algorithms import KNNBaseline
from .prediction_algorithms import SVD
from .prediction_algorithms import SVDpp
from .prediction_algorithms import NMF
from .prediction_algorithms import SlopeOne
from .prediction_algorithms import CoClustering
from .prediction_algorithms import PredictionImpossible
from .prediction_algorithms import Prediction
from .dataset import Dataset
from .dataset import Reader
from .dataset import Trainset
from .evaluate import evaluate
from .evaluate import print_perf
from .evaluate import GridSearch
from .dump import dump
__all__ = ['AlgoBase', 'NormalPredictor', 'BaselineOnly', 'KNNBasic',
           'KNNWithMeans', 'KNNBaseline', 'SVD', 'SVDpp', 'NMF', 'SlopeOne',
           'CoClustering', 'PredictionImpossible', 'Prediction', 'Dataset',
           'Reader', 'Trainset', 'evaluate', 'print_perf', 'GridSearch',
           'dump']
__version__ = get_distribution('scikit-surprise').version
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import random as rd
import sys
import shutil
import argparse
import numpy as np
from surprise.prediction_algorithms import NormalPredictor
from surprise.prediction_algorithms import BaselineOnly
from surprise.prediction_algorithms import KNNBasic
from surprise.prediction_algorithms import KNNBaseline
from surprise.prediction_algorithms import KNNWithMeans
from surprise.prediction_algorithms import SVD
from surprise.prediction_algorithms import SVDpp
from surprise.prediction_algorithms import NMF
from surprise.prediction_algorithms import SlopeOne
from surprise.prediction_algorithms import CoClustering
import surprise.dataset as dataset
from surprise.dataset import Dataset
from surprise.dataset import Reader  from surprise.evaluate import evaluate
from surprise import __version__

def main():
    class MyParser(argparse.ArgumentParser):
        '''A parser which prints the help message when an error occurs. Taken from
        http://stackoverflow.com/questions/4042452/display-help-message-with-python-argparse-when-script-is-called-without-any-argu.'''  
        def error(self, message):
            sys.stderr.write('error: %s\n' % message)
            self.print_help()
            sys.exit(2)
    parser = MyParser(
        description='Evaluate the performance of a rating prediction ' +
        'on a given dataset using cross validation. You can use a built-in ' +
        'or a custom dataset, and you can choose to automatically split the ' +
        'dataset into folds, or manually specify train and test files. ' +
        'Please refer to the documentation page ' +
        '(http://surprise.readthedocs.io/) for more details.',
        epilog=The :mod:`surprise.prediction_algorithms.algo_base` module defines the base
class :class:`AlgoBase` from which every single prediction algorithm has to
inherit.
    def __init__(self, **kwargs):
        self.bsl_options = kwargs.get('bsl_options', {})
        self.sim_options = kwargs.get('sim_options', {})
        if 'user_based' not in self.sim_options:
            self.sim_options['user_based'] = True
    def train(self, trainset):
        
        self.trainset = trainset
                self.bu = self.bi = None
    def predict(self, uid, iid, r_ui, clip=True, verbose=False):
        
                try:
            iuid = self.trainset.to_inner_uid(uid)
        except ValueError:
            iuid = 'UKN__' + str(uid)
        try:
            iiid = self.trainset.to_inner_iid(iid)
        except ValueError:
            iiid = 'UKN__' + str(iid)
        details = {}
        try:
            est = self.estimate(iuid, iiid)
                        if isinstance(est, tuple):
                est, details = est
            details['was_impossible'] = False
        except PredictionImpossible as e:
            est = self.trainset.global_mean
            details['was_impossible'] = True
            details['reason'] = str(e)
                        est -= self.trainset.offset
                if clip:
            lower_bound, higher_bound = self.trainset.rating_scale
            est = min(higher_bound, est)
            est = max(lower_bound, est)
        pred = Prediction(uid, iid, r_ui, est, details)
        if verbose:
            print(pred)
        return pred
    def test(self, testset, verbose=False):
        
                predictions = [self.predict(uid,
                                    iid,
                                    r_ui_trans - self.trainset.offset,
                                    verbose=verbose)
                       for (uid, iid, r_ui_trans) in testset]
        return predictions
    def compute_baselines(self):
        
                                        if self.bu is not None:
            return self.bu, self.bi
        method = dict(als=baseline_als,
                      sgd=baseline_sgd)
        method_name = self.bsl_options.get('method', 'als')
        try:
            print('Estimating biases using', method_name + '...')
            self.bu, self.bi = method[method_name](self)
            return self.bu, self.bi
        except KeyError:
            raise ValueError('Invalid method ' + method_name +
                             ' for baseline computation.' +
                             ' Available methods are als and sgd.')
    def compute_similarities(self):
        
        construction_func = {'cosine': sims.cosine,
                             'msd': sims.msd,
                             'pearson': sims.pearson,
                             'pearson_baseline': sims.pearson_baseline}
        if self.sim_options['user_based']:
            n_x, yr = self.trainset.n_users, self.trainset.ir
        else:
            n_x, yr = self.trainset.n_items, self.trainset.ur
        min_support = self.sim_options.get('min_support', 1)
        args = [n_x, yr, min_support]
        name = self.sim_options.get('name', 'msd').lower()
        if name == 'pearson_baseline':
            shrinkage = self.sim_options.get('shrinkage', 100)
            bu, bi = self.compute_baselines()
            if self.sim_options['user_based']:
                bx, by = bu, bi
            else:
                bx, by = bi, bu
            args += [self.trainset.global_mean, bx, by, shrinkage]
        try:
            print('Computing the {0} similarity matrix...'.format(name))
            sim = construction_func[name](*args)
            print('Done computing similarity matrix.')
            return sim
        except KeyError:
            raise NameError('Wrong sim name ' + name + '. Allowed values ' +
                            'are ' + ', '.join(construction_func.keys()) + '.')
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from .algo_base import AlgoBase

class BaselineOnly(AlgoBase):
    
    def __init__(self, bsl_options={}):
        AlgoBase.__init__(self, bsl_options=bsl_options)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.bu, self.bi = self.compute_baselines()
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        return est
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class CoClustering(AlgoBase):
    
    def __init__(self, n_cltr_u=3, n_cltr_i=3, n_epochs=20, verbose=False):
        AlgoBase.__init__(self)
        self.n_cltr_u = n_cltr_u
        self.n_cltr_i = n_cltr_i
        self.n_epochs = n_epochs
        self.verbose=verbose
    def train(self, trainset):
                
        AlgoBase.train(self, trainset)
                cdef np.ndarray[np.double_t] user_mean
        cdef np.ndarray[np.double_t] item_mean
                cdef np.ndarray[np.int_t] cltr_u
        cdef np.ndarray[np.int_t] cltr_i
                cdef np.ndarray[np.double_t] avg_cltr_u
        cdef np.ndarray[np.double_t] avg_cltr_i
        cdef np.ndarray[np.double_t, ndim=2] avg_cocltr
        cdef np.ndarray[np.double_t] errors
        cdef int u, i, r, uc, ic
        cdef double est
                cltr_u = np.random.randint(self.n_cltr_u, size=trainset.n_users)
        cltr_i = np.random.randint(self.n_cltr_i, size=trainset.n_items)
                user_mean = np.zeros(self.trainset.n_users, np.double)
        item_mean = np.zeros(self.trainset.n_items, np.double)
        for u in trainset.all_users():
            user_mean[u] = np.mean([r for (_, r) in trainset.ur[u]])
        for i in trainset.all_items():
            item_mean[i] = np.mean([r for (_, r) in trainset.ir[i]])
                        for epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(epoch))
                        avg_cltr_u, avg_cltr_i, avg_cocltr = self.compute_averages(cltr_u,
                                                                       cltr_i)
                                    for u in self.trainset.all_users():
                errors = np.zeros(self.n_cltr_u, np.double)
                for uc in range(self.n_cltr_u):
                    for i, r in self.trainset.ur[u]:
                        ic = cltr_i[i]
                        est = (avg_cocltr[uc, ic] +
                               user_mean[u] - avg_cltr_u[uc] +
                               item_mean[i] - avg_cltr_i[ic])
                        errors[uc] += (r - est)**2
                cltr_u[u] = np.argmin(errors)
                                    for i in self.trainset.all_items():
                errors = np.zeros(self.n_cltr_i, np.double)
                for ic in range(self.n_cltr_i):
                    for u, r in self.trainset.ir[i]:
                        uc = cltr_u[u]
                        est = (avg_cocltr[uc, ic] +
                               user_mean[u] - avg_cltr_u[uc] +
                               item_mean[i] - avg_cltr_i[ic])
                        errors[ic] += (r - est)**2
                cltr_i[i] = np.argmin(errors)
                avg_cltr_u, avg_cltr_i, avg_cocltr = self.compute_averages(cltr_u,
                                                                   cltr_i)
                self.cltr_u = cltr_u
        self.cltr_i = cltr_i
        self.user_mean = user_mean
        self.item_mean = item_mean
        self.avg_cltr_u = avg_cltr_u
        self.avg_cltr_i = avg_cltr_i
        self.avg_cocltr = avg_cocltr
    def compute_averages(self, np.ndarray[np.int_t] cltr_u,
                         np.ndarray[np.int_t] cltr_i):
        
                cdef np.ndarray[np.int_t] count_cltr_u
        cdef np.ndarray[np.int_t] count_cltr_i
        cdef np.ndarray[np.int_t, ndim=2] count_cocltr
                cdef np.ndarray[np.int_t] sum_cltr_u
        cdef np.ndarray[np.int_t] sum_cltr_i
        cdef np.ndarray[np.int_t, ndim=2] sum_cocltr
                cdef np.ndarray[np.double_t] avg_cltr_u
        cdef np.ndarray[np.double_t] avg_cltr_i
        cdef np.ndarray[np.double_t, ndim=2] avg_cocltr
        cdef int u, i, r, uc, ic
        cdef double global_mean = self.trainset.global_mean
                count_cltr_u = np.zeros(self.n_cltr_u, np.int)
        count_cltr_i = np.zeros(self.n_cltr_i, np.int)
        count_cocltr = np.zeros((self.n_cltr_u, self.n_cltr_i), np.int)
        sum_cltr_u = np.zeros(self.n_cltr_u, np.int)
        sum_cltr_i = np.zeros(self.n_cltr_i, np.int)
        sum_cocltr = np.zeros((self.n_cltr_u, self.n_cltr_i), np.int)
        avg_cltr_u = np.zeros(self.n_cltr_u, np.double)
        avg_cltr_i = np.zeros(self.n_cltr_i, np.double)
        avg_cocltr = np.zeros((self.n_cltr_u, self.n_cltr_i), np.double)
                for u, i, r in self.trainset.all_ratings():
            uc = cltr_u[u]
            ic = cltr_i[i]
            count_cltr_u[uc] += 1
            count_cltr_i[ic] += 1
            count_cocltr[uc, ic] += 1
            sum_cltr_u[uc] += r
            sum_cltr_i[ic] += r
            sum_cocltr[uc, ic] += r
                for uc in range(self.n_cltr_u):
            if count_cltr_u[uc]:
                avg_cltr_u[uc] = sum_cltr_u[uc] / count_cltr_u[uc]
            else:
                avg_cltr_u[uc] = global_mean
                for ic in range(self.n_cltr_i):
            if count_cltr_i[ic]:
                avg_cltr_i[ic] = sum_cltr_i[ic] / count_cltr_i[ic]
            else:
                avg_cltr_i[ic] = global_mean
                for uc in range(self.n_cltr_u):
            for ic in range(self.n_cltr_i):
                if count_cocltr[uc, ic]:
                    avg_cocltr[uc, ic] = (sum_cocltr[uc, ic] /
                                          count_cocltr[uc, ic])
                else:
                    avg_cocltr[uc, ic] = global_mean
        return avg_cltr_u, avg_cltr_i, avg_cocltr
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            return self.trainset.global_mean
        if not self.trainset.knows_user(u):
            return self.cltr_i[i]
        if not self.trainset.knows_item(i):
            return self.cltr_u[u]
                        cdef int _u = u
        cdef int _i = i
        cdef int uc = self.cltr_u[_u]
        cdef int ic = self.cltr_i[_i]
        cdef double est
        est = (self.avg_cocltr[uc, ic] +
               self.user_mean[_u] - self.avg_cltr_u[uc] +
               self.item_mean[_i] - self.avg_cltr_i[ic])
        return est
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from six import iteritems
from .predictions import PredictionImpossible
from .algo_base import AlgoBase

class SymmetricAlgo(AlgoBase):
    
    def __init__(self, sim_options={}, **kwargs):
        AlgoBase.__init__(self, sim_options=sim_options, **kwargs)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        ub = self.sim_options['user_based']
        self.n_x = self.trainset.n_users if ub else self.trainset.n_items
        self.n_y = self.trainset.n_items if ub else self.trainset.n_users
        self.xr = self.trainset.ur if ub else self.trainset.ir
        self.yr = self.trainset.ir if ub else self.trainset.ur
    def switch(self, u_stuff, i_stuff):
        
        if self.sim_options['user_based']:
            return u_stuff, i_stuff
        else:
            return i_stuff, u_stuff

class KNNBasic(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, **kwargs):
        SymmetricAlgo.__init__(self, sim_options=sim_options, **kwargs)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
        x, y = self.switch(u, i)
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
                sum_sim = sum_ratings = actual_k = 0
        for (_, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                sum_ratings += sim * r
                actual_k += 1
        if actual_k < self.min_k:
            raise PredictionImpossible('Not enough neighbors.')
        est = sum_ratings / sum_sim
        details = {'actual_k': actual_k}
        return est, details

class KNNWithMeans(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, **kwargs):
        SymmetricAlgo.__init__(self, sim_options=sim_options, **kwargs)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.sim = self.compute_similarities()
        self.means = np.zeros(self.n_x)
        for x, ratings in iteritems(self.xr):
            self.means[x] = np.mean([r for (_, r) in ratings])
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
        x, y = self.switch(u, i)
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
        est = self.means[x]
                sum_sim = sum_ratings = actual_k = 0
        for (nb, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                sum_ratings += sim * (r - self.means[nb])
                actual_k += 1
        if actual_k < self.min_k:
            sum_ratings = 0
        try:
            est += sum_ratings / sum_sim
        except ZeroDivisionError:
            pass  
        details = {'actual_k': actual_k}
        return est, details

class KNNBaseline(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, bsl_options={}):
        SymmetricAlgo.__init__(self, sim_options=sim_options,
                               bsl_options=bsl_options)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.bu, self.bi = self.compute_baselines()
        self.bx, self.by = self.switch(self.bu, self.bi)
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        x, y = self.switch(u, i)
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            return est
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
                sum_sim = sum_ratings = actual_k = 0
        for (nb, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                nb_bsl = self.trainset.global_mean + self.bx[nb] + self.by[y]
                sum_ratings += sim * (r - nb_bsl)
                actual_k += 1
        if actual_k < self.min_k:
            sum_ratings = 0
        try:
            est += sum_ratings / sum_sim
        except ZeroDivisionError:
            pass  
        details = {'actual_k': actual_k}
        return est, details
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class SVD(AlgoBase):
    
    def __init__(self, n_factors=100, n_epochs=20, biased=True, lr_all=.005,
                 reg_all=.02, lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None,
                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,
                 verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.biased = biased
        self.lr_bu = lr_bu if lr_bu is not None else lr_all
        self.lr_bi = lr_bi if lr_bi is not None else lr_all
        self.lr_pu = lr_pu if lr_pu is not None else lr_all
        self.lr_qi = lr_qi if lr_qi is not None else lr_all
        self.reg_bu = reg_bu if reg_bu is not None else reg_all
        self.reg_bi = reg_bi if reg_bi is not None else reg_all
        self.reg_pu = reg_pu if reg_pu is not None else reg_all
        self.reg_qi = reg_qi if reg_qi is not None else reg_all
        self.verbose = verbose
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                                                                                                                                                                                                        
                                        
                cdef np.ndarray[np.double_t] bu
                cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] pu
                cdef np.ndarray[np.double_t, ndim=2] qi
        cdef int u, i, f
        cdef double r, err, dot, puf, qif
        cdef double global_mean = self.trainset.global_mean
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double lr_pu = self.lr_pu
        cdef double lr_qi = self.lr_qi
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        pu = np.zeros((trainset.n_users, self.n_factors), np.double) + .1
        qi = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        if not self.biased:
            global_mean = 0
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(current_epoch))
            for u, i, r in trainset.all_ratings():
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * pu[u, f]
                err = r - (global_mean + bu[u] + bi[i] + dot)
                                if self.biased:
                    bu[u] += lr_bu * (err - reg_bu * bu[u])
                    bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    puf = pu[u, f]
                    qif = qi[i, f]
                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)
                    qi[i, f] += lr_qi * (err * puf - reg_qi * qif)
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
    def estimate(self, u, i):
        
        est = self.trainset.global_mean if self.biased else 0
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            est += np.dot(self.qi[i], self.pu[u])
        else:
            raise PredictionImpossible
        return est

class SVDpp(AlgoBase):
    
    def __init__(self, n_factors=20, n_epochs=20, lr_all=.007, reg_all=.02,
                 lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None, lr_yj=None,
                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,
                 reg_yj=None, verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.lr_bu = lr_bu if lr_bu is not None else lr_all
        self.lr_bi = lr_bi if lr_bi is not None else lr_all
        self.lr_pu = lr_pu if lr_pu is not None else lr_all
        self.lr_qi = lr_qi if lr_qi is not None else lr_all
        self.lr_yj = lr_yj if lr_yj is not None else lr_all
        self.reg_bu = reg_bu if reg_bu is not None else reg_all
        self.reg_bi = reg_bi if reg_bi is not None else reg_all
        self.reg_pu = reg_pu if reg_pu is not None else reg_all
        self.reg_qi = reg_qi if reg_qi is not None else reg_all
        self.reg_yj = reg_yj if reg_yj is not None else reg_all
        self.verbose = verbose
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                cdef np.ndarray[np.double_t] bu
                cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] pu
                cdef np.ndarray[np.double_t, ndim=2] qi
                cdef np.ndarray[np.double_t, ndim=2] yj
        cdef int u, i, j, f
        cdef double r, err, dot, puf, qif, sqrt_Iu, _
        cdef double global_mean = self.trainset.global_mean
        cdef np.ndarray[np.double_t] u_impl_fdb
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double lr_pu = self.lr_pu
        cdef double lr_qi = self.lr_qi
        cdef double lr_yj = self.lr_yj
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        cdef double reg_yj = self.reg_yj
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        pu = np.zeros((trainset.n_users, self.n_factors), np.double) + .1
        qi = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        yj = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        u_impl_fdb = np.zeros(self.n_factors, np.double)
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print(" processing epoch {}".format(current_epoch))
            for u, i, r in trainset.all_ratings():
                                Iu = [j for (j, _) in trainset.ur[u]]
                sqrt_Iu = np.sqrt(len(Iu))
                                u_impl_fdb = np.zeros(self.n_factors, np.double)
                for j in Iu:
                    for f in range(self.n_factors):
                        u_impl_fdb[f] += yj[j, f] / sqrt_Iu
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * (pu[u, f] + u_impl_fdb[f])
                err = r - (global_mean + bu[u] + bi[i] + dot)
                                bu[u] += lr_bu * (err - reg_bu * bu[u])
                bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    puf = pu[u, f]
                    qif = qi[i, f]
                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)
                    qi[i, f] += lr_qi * (err * (puf + u_impl_fdb[f]) -
                                         reg_qi * qif)
                    for j in Iu:
                        yj[j, f] += lr_yj * (err * qif / sqrt_Iu -
                                             reg_yj * yj[j, f])
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
        self.yj = yj
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            Iu = len(self.trainset.ur[u])              u_impl_feedback = (sum(self.yj[j] for (j, _)
                               in self.trainset.ur[u]) / np.sqrt(Iu))
            est += np.dot(self.qi[i], self.pu[u] + u_impl_feedback)
        return est

class NMF(AlgoBase):
    
    def __init__(self, n_factors=15, n_epochs=50, biased=False, reg_pu=.06,
                 reg_qi=.06, reg_bu=.02, reg_bi=.02, lr_bu=.005, lr_bi=.005,
                 init_low=0, init_high=1, verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.biased = biased
        self.reg_pu = reg_pu
        self.reg_qi = reg_qi
        self.lr_bu = lr_bu
        self.lr_bi = lr_bi
        self.reg_bu = reg_bu
        self.reg_bi = reg_bi
        self.init_low = init_low
        self.init_high = init_high
        self.verbose = verbose
        if self.init_low < 0:
            raise ValueError('init_low should be greater than zero')
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                cdef np.ndarray[np.double_t, ndim=2] pu
        cdef np.ndarray[np.double_t, ndim=2] qi
                cdef np.ndarray[np.double_t] bu
        cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] user_num
        cdef np.ndarray[np.double_t, ndim=2] user_denom
        cdef np.ndarray[np.double_t, ndim=2] item_num
        cdef np.ndarray[np.double_t, ndim=2] item_denom
        cdef int u, i, f
        cdef double r, est, l, dot, err
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double global_mean = self.trainset.global_mean
                pu = np.random.uniform(self.init_low, self.init_high,
                               size=(trainset.n_users, self.n_factors))
        qi = np.random.uniform(self.init_low, self.init_high,
                               size=(trainset.n_items, self.n_factors))
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        if not self.biased:
            global_mean = 0
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(current_epoch))
                        user_num = np.zeros((trainset.n_users, self.n_factors))
            user_denom = np.zeros((trainset.n_users, self.n_factors))
            item_num = np.zeros((trainset.n_items, self.n_factors))
            item_denom = np.zeros((trainset.n_items, self.n_factors))
                        for u, i, r in trainset.all_ratings():
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * pu[u, f]
                est = global_mean + bu[u] + bi[i] + dot
                err = r - est
                                if self.biased:
                    bu[u] += lr_bu * (err - reg_bu * bu[u])
                    bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    user_num[u, f] += qi[i, f] * r
                    user_denom[u, f] += qi[i, f] * est
                    item_num[i, f] += pu[u, f] * r
                    item_denom[i, f] += pu[u, f] * est
                        for u in trainset.all_users():
                l = len(trainset.ur[u])
                for f in range(self.n_factors):
                    user_denom[u, f] += l * reg_pu * pu[u, f]
                    pu[u, f] *= user_num[u, f] / user_denom[u, f]
                        for i in trainset.all_items():
                l = len(trainset.ir[i])
                for f in range(self.n_factors):
                    item_denom[i, f] += l * reg_qi * qi[i, f]
                    qi[i, f] *= item_num[i, f] / item_denom[i, f]
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
    def estimate(self, u, i):
        est = self.trainset.global_mean if self.biased else 0
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            est += np.dot(self.qi[i], self.pu[u])
        else:
            raise PredictionImpossible
        return est
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range

def baseline_als(self):
    
                
    cdef np.ndarray[np.double_t] bu = np.zeros(self.trainset.n_users)
    cdef np.ndarray[np.double_t] bi = np.zeros(self.trainset.n_items)
    cdef int u, i
    cdef double r, err, dev_i, dev_u
    cdef double global_mean = self.trainset.global_mean
    cdef int n_epochs = self.bsl_options.get('n_epochs', 10)
    cdef double reg_u = self.bsl_options.get('reg_u', 15)
    cdef double reg_i = self.bsl_options.get('reg_i', 10)
    for dummy in range(n_epochs):
        for i in self.trainset.all_items():
            dev_i = 0
            for (u, r) in self.trainset.ir[i]:
                dev_i += r - global_mean - bu[u]
            bi[i] = dev_i / (reg_i + len(self.trainset.ir[i]))
        for u in self.trainset.all_users():
            dev_u = 0
            for (i, r) in self.trainset.ur[u]:
                dev_u += r - global_mean - bi[i]
            bu[u] = dev_u / (reg_u + len(self.trainset.ur[u]))
    return bu, bi

def baseline_sgd(self):
    
    cdef np.ndarray[np.double_t] bu = np.zeros(self.trainset.n_users)
    cdef np.ndarray[np.double_t] bi = np.zeros(self.trainset.n_items)
    cdef int u, i
    cdef double r, err
    cdef double global_mean = self.trainset.global_mean
    cdef int n_epochs = self.bsl_options.get('n_epochs', 20)
    cdef double reg = self.bsl_options.get('reg', 0.02)
    cdef double lr = self.bsl_options.get('learning_rate', 0.005)
    for dummy in range(n_epochs):
        for u, i, r in self.trainset.all_ratings():
            err = (r - (global_mean + bu[u] + bi[i]))
            bu[u] += lr * (err - reg * bu[u])
            bi[i] += lr * (err - reg * bi[i])
    return bu, bi
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import namedtuple

class PredictionImpossible(Exception):
    
    pass

class Prediction(namedtuple('Prediction',
                            ['uid', 'iid', 'r_ui', 'est', 'details'])):
    
    __slots__ = ()  
    def __str__(self):
        s = 'user: {uid:<10} '.format(uid=self.uid)
        s += 'item: {iid:<10} '.format(iid=self.iid)
        s += 'r_ui = {r_ui:1.2f}   '.format(r_ui=self.r_ui)
        s += 'est = {est:1.2f}   '.format(est=self.est)
        s += str(self.details)
        return s
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from .algo_base import AlgoBase

class NormalPredictor(AlgoBase):
    
    def __init__(self):
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        num = sum((r - self.trainset.global_mean)**2
                  for (_, _, r) in self.trainset.all_ratings())
        denum = self.trainset.n_ratings
        self.sigma = np.sqrt(num / denum)
    def estimate(self, *_):
        return np.random.normal(self.trainset.global_mean, self.sigma)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from six import iteritems
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class SlopeOne(AlgoBase):
    
    def __init__(self):
        AlgoBase.__init__(self)
    def train(self, trainset):
        n_items = trainset.n_items
                cdef np.ndarray[np.int_t, ndim=2] freq
                cdef np.ndarray[np.double_t, ndim=2] dev
        cdef int u, i, j, r_ui, r_uj
        AlgoBase.train(self, trainset)
        freq = np.zeros((trainset.n_items, trainset.n_items), np.int)
        dev = np.zeros((trainset.n_items, trainset.n_items), np.double)
                for u, u_ratings in iteritems(trainset.ur):
            for i, r_ui in u_ratings:
                for j, r_uj in u_ratings:
                    freq[i, j] += 1
                    dev[i, j] += r_ui - r_uj
        for i in range(n_items):
            dev[i, i] = 0
            for j in range(i + 1, n_items):
                dev[i, j] /= freq[i, j]
                dev[j, i] = -dev[i, j]
        self.freq = freq
        self.dev = dev
                self.user_mean = [np.mean([r for (_, r) in trainset.ur[u]])
                          for u in trainset.all_users()]
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
                                Ri = [j for (j, _) in self.trainset.ur[u] if self.freq[i, j] > 0]
        est = self.user_mean[u]
        if Ri:
            est += sum(self.dev[i, j] for j in Ri) / len(Ri)
        return est
from .algo_base import AlgoBase
from .random_pred import NormalPredictor
from .baseline_only import BaselineOnly
from .knns import KNNBasic
from .knns import KNNBaseline
from .knns import KNNWithMeans
from .matrix_factorization import SVD
from .matrix_factorization import SVDpp
from .matrix_factorization import NMF
from .slope_one import SlopeOne
from .co_clustering import CoClustering
from .predictions import PredictionImpossible
from .predictions import Prediction
__all__ = ['AlgoBase', 'NormalPredictor', 'BaselineOnly', 'KNNBasic',
           'KNNBaseline', 'KNNWithMeans', 'SVD', 'SVDpp', 'NMF', 'SlopeOne',
           'CoClustering', 'PredictionImpossible', 'Prediction']
from setuptools import setup, find_packages

def calculate_version():
    initpy = open('tpot/_version.py').read().split('\n')
    version = list(filter(lambda x: '__version__' in x, initpy))[0].split('\'')[1]
    return version
package_version = calculate_version()
setup(
    name='TPOT',
    version=package_version,
    author='Randal S. Olson',
    author_email='rso@randalolson.com',
    packages=find_packages(),
    url='https://github.com/rhiever/tpot',
    license='GNU/GPLv3',
    entry_points={'console_scripts': ['tpot=tpot:main', ]},
    description=('Tree-based Pipeline Optimization Tool'),
    long_description='''
A Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.
Contact
=============
If you have any questions or comments about TPOT, please feel free to contact me via:
E-mail: rso@randalolson.com
or Twitter: https://twitter.com/randal_olson
This project is hosted at https://github.com/rhiever/tpot
''',
    zip_safe=True,
    install_requires=['numpy', 'scipy', 'scikit-learn>=0.18.1', 'deap', 'update_checker', 'tqdm', 'pathos'],
    extras_require={'xgboost': ['xgboost']},
    classifiers=[
        'Intended Audience :: Science/Research',
        'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Topic :: Scientific/Engineering :: Artificial Intelligence'
    ],
    keywords=['pipeline optimization', 'hyperparameter optimization', 'data science', 'machine learning', 'genetic programming', 'evolutionary computation'],
)

from __future__ import print_function
import random
import inspect
import warnings
import sys
import time
from functools import partial
from datetime import datetime
from pathos.multiprocessing import ProcessPool

import numpy as np
import deap
from deap import base, creator, tools, gp
from tqdm import tqdm
from sklearn.base import BaseEstimator
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.ensemble import VotingClassifier
from sklearn.metrics.scorer import make_scorer
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from update_checker import update_check
from ._version import __version__
from .operator_utils import TPOTOperatorClassFactory, Operator, ARGType, set_sample_weight
from .export_utils import export_pipeline, expr_to_tree, generate_pipeline_code
from .decorators import _timeout, _pre_test, TimedOutExc
from .built_in_operators import CombineDFs
from .metrics import SCORERS
from .gp_types import Output_Array
from .gp_deap import eaMuPlusLambda, mutNodeReplacement
if sys.platform.startswith('win'):
    import win32api
    try:
        import _thread
    except ImportError:
        import thread as _thread
    def handler(dwCtrlType, hook_sigint=_thread.interrupt_main):
        if dwCtrlType == 0:             hook_sigint()
            return 1         return 0
    win32api.SetConsoleCtrlHandler(handler, 1)

class TPOTBase(BaseEstimator):
    
    def __init__(self, generations=100, population_size=100, offspring_size=None,
                 mutation_rate=0.9, crossover_rate=0.1,
                 scoring=None, cv=5, n_jobs=1,
                 max_time_mins=None, max_eval_time_mins=5,
                 random_state=None, config_dict=None, warm_start=False,
                 verbosity=0, disable_update_check=False):
                if self.__class__.__name__ == 'TPOTBase':
            raise RuntimeError('Do not instantiate the TPOTBase class directly; use TPOTRegressor or TPOTClassifier instead.')
                self.disable_update_check = disable_update_check
        if not self.disable_update_check:
            update_check('tpot', __version__)
        self._pareto_front = None
        self._optimized_pipeline = None
        self._fitted_pipeline = None
        self._pop = None
        self.warm_start = warm_start
        self.population_size = population_size
        self.generations = generations
        self.max_time_mins = max_time_mins
        self.max_eval_time_mins = max_eval_time_mins
                if offspring_size:
            self.offspring_size = offspring_size
        else:
            self.offspring_size = population_size
                if config_dict:
            self.config_dict = config_dict
        else:
            self.config_dict = self.default_config_dict
        self.operators = []
        self.arguments = []
        for key in sorted(self.config_dict.keys()):
            op_class, arg_types = TPOTOperatorClassFactory(key, self.config_dict[key],
            BaseClass=Operator, ArgBaseClass=ARGType)
            if op_class:
                self.operators.append(op_class)
                self.arguments += arg_types
                        if not (max_time_mins is None):
            self.generations = 1000000
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        if self.mutation_rate + self.crossover_rate > 1:
            raise ValueError('The sum of the crossover and mutation probabilities must be <= 1.0.')
        self.verbosity = verbosity
        self.operators_context = {
            'make_pipeline': make_pipeline,
            'make_union': make_union,
            'VotingClassifier': VotingClassifier,
            'FunctionTransformer': FunctionTransformer
        }
        self._pbar = None
                self._evaluated_individuals = {}
        self.random_state = random_state
                if scoring:
            if hasattr(scoring, '__call__'):
                scoring_name = scoring.__name__
                if 'loss' in scoring_name or 'error' in scoring_name:
                    greater_is_better = False
                else:
                    greater_is_better = True
                SCORERS[scoring_name] = make_scorer(scoring, greater_is_better=greater_is_better)
                self.scoring_function = scoring_name
            else:
                if scoring not in SCORERS:
                    raise ValueError('The scoring function {} is not available. '
                                     'Please choose a valid scoring function from the TPOT '
                                     'documentation.'.format(scoring))
                self.scoring_function = scoring
        self.cv = cv
                if sys.platform.startswith('win') and n_jobs > 1:
            print('Warning: Parallelization is not currently supported in TPOT for Windows. ',
                  'Setting n_jobs to 1 during the TPOT optimization process.')
            self.n_jobs = 1
        else:
            self.n_jobs = n_jobs
        self._setup_pset()
        self._setup_toolbox()
    def _setup_pset(self):
        if self.random_state is not None:
            random.seed(self.random_state)
            np.random.seed(self.random_state)
        self._pset = gp.PrimitiveSetTyped('MAIN', [np.ndarray], Output_Array)
                self._pset.renameArguments(ARG0='input_matrix')

                for op in self.operators:
            if op.root:
                                                                p_types = (op.parameter_types()[0], Output_Array)
                self._pset.addPrimitive(op, *p_types)
            self._pset.addPrimitive(op, *op.parameter_types())
                                    for key in sorted(op.import_hash.keys()):
                module_list = ', '.join(sorted(op.import_hash[key]))
                if key.startswith('tpot.'):
                    exec('from {} import {}'.format(key[4:], module_list))
                else:
                    exec('from {} import {}'.format(key, module_list))
                for var in op.import_hash[key]:
                    self.operators_context[var] = eval(var)
        self._pset.addPrimitive(CombineDFs(), [np.ndarray, np.ndarray], np.ndarray)
                for _type in self.arguments:
            type_values = list(_type.values) + ['DEFAULT']
            for val in type_values:
                terminal_name = _type.__name__ + "=" + str(val)
                self._pset.addTerminal(val, _type, name=terminal_name)
        if self.verbosity > 2:
            print('{} operators have been imported by TPOT.'.format(len(self.operators)))

    def _setup_toolbox(self):
        creator.create('FitnessMulti', base.Fitness, weights=(-1.0, 1.0))
        creator.create('Individual', gp.PrimitiveTree, fitness=creator.FitnessMulti)
        self._toolbox = base.Toolbox()
        self._toolbox.register('expr', self._gen_grow_safe, pset=self._pset, min_=1, max_=3)
        self._toolbox.register('individual', tools.initIterate, creator.Individual, self._toolbox.expr)
        self._toolbox.register('population', tools.initRepeat, list, self._toolbox.individual)
        self._toolbox.register('compile', self._compile_to_sklearn)
        self._toolbox.register('select', tools.selNSGA2)
        self._toolbox.register('mate', self._mate_operator)
        self._toolbox.register('expr_mut', self._gen_grow_safe, min_=1, max_=4)
        self._toolbox.register('mutate', self._random_mutation_operator)
    def fit(self, features, classes, sample_weight=None):
                features = features.astype(np.float64)
                if self.classification:
            clf = DecisionTreeClassifier(max_depth=5)
        else:
            clf = DecisionTreeRegressor(max_depth=5)
        try:
            clf = clf.fit(features, classes)
        except:
            raise ValueError('Error: Input data is not in a valid format. '
                             'Please confirm that the input data is scikit-learn compatible. '
                             'For example, the features must be a 2-D array and target labels '
                             'must be a 1-D array.')
                if self.random_state is not None:
            random.seed(self.random_state)             np.random.seed(self.random_state)
        self._start_datetime = datetime.now()
        self._toolbox.register('evaluate', self._evaluate_individuals, features=features, classes=classes, sample_weight=sample_weight)
                if self._pop:
            pop = self._pop
        else:
            pop = self._toolbox.population(n=self.population_size)
        def pareto_eq(ind1, ind2):
                        return np.allclose(ind1.fitness.values, ind2.fitness.values)
                if not self.warm_start or not self._pareto_front:
            self._pareto_front = tools.ParetoFront(similar=pareto_eq)
                if self.max_time_mins:
            total_evals = self.population_size
        else:
            total_evals = self.offspring_size * self.generations + self.population_size
        self._pbar = tqdm(total=total_evals, unit='pipeline', leave=False,
                          disable=not (self.verbosity >= 2), desc='Optimization Progress')
        try:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore')
                pop, _ = eaMuPlusLambda(population=pop, toolbox=self._toolbox,
                    mu=self.population_size, lambda_=self.offspring_size,
                    cxpb=self.crossover_rate, mutpb=self.mutation_rate,
                    ngen=self.generations, pbar=self._pbar, halloffame=self._pareto_front,
                    verbose=self.verbosity, max_time_mins=self.max_time_mins)
                        if self.warm_start:
                self._pop = pop
                except (KeyboardInterrupt, SystemExit):
            if self.verbosity > 0:
                self._pbar.write('')                 self._pbar.write('TPOT closed prematurely. Will use the current best pipeline.')
        finally:
                                    if not isinstance(self._pbar, type(None)):
                self._pbar.close()
                        if self._pareto_front:
                top_score = -float('inf')
                for pipeline, pipeline_scores in zip(self._pareto_front.items, reversed(self._pareto_front.keys)):
                    if pipeline_scores.wvalues[1] > top_score:
                        self._optimized_pipeline = pipeline
                        top_score = pipeline_scores.wvalues[1]
                                                if not self._optimized_pipeline:
                    print('There was an error in the TPOT optimization '
                          'process. This could be because the data was '
                          'not formatted properly, or because data for '
                          'a regression problem was provided to the '
                          'TPOTClassifier object. Please make sure you '
                          'passed the data to TPOT correctly.')
                else:
                    self._fitted_pipeline = self._toolbox.compile(expr=self._optimized_pipeline)
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        self._fitted_pipeline.fit(features, classes)
                    if self.verbosity in [1, 2]:
                                                if self.verbosity >= 2:
                            print('')
                        print('Best pipeline: {}'.format(self._optimized_pipeline))
                                        elif self.verbosity >= 3 and self._pareto_front:
                        self._pareto_front_fitted_pipelines = {}
                        for pipeline in self._pareto_front.items:
                            self._pareto_front_fitted_pipelines[str(pipeline)] = self._toolbox.compile(expr=pipeline)
                            with warnings.catch_warnings():
                                warnings.simplefilter('ignore')
                                self._pareto_front_fitted_pipelines[str(pipeline)].fit(features, classes)
    def predict(self, features):
                if not self._fitted_pipeline:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        return self._fitted_pipeline.predict(features.astype(np.float64))
    def fit_predict(self, features, classes):
                self.fit(features, classes)
        return self.predict(features)
    def score(self, testing_features, testing_classes):
                if self._fitted_pipeline is None:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
                return abs(SCORERS[self.scoring_function](self._fitted_pipeline,
            testing_features.astype(np.float64), testing_classes.astype(np.float64)))
    def predict_proba(self, features):
                if not self._fitted_pipeline:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        else:
            if not(hasattr(self._fitted_pipeline, 'predict_proba')):
                raise RuntimeError('The fitted pipeline does not have the predict_proba() function.')
            return self._fitted_pipeline.predict_proba(features.astype(np.float64))
    def set_params(self, **params):
                self.__init__(**params)
        return self
    def export(self, output_file_name):
                if self._optimized_pipeline is None:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        with open(output_file_name, 'w') as output_file:
            output_file.write(export_pipeline(self._optimized_pipeline, self.operators, self._pset))
    def _compile_to_sklearn(self, expr):
                sklearn_pipeline = generate_pipeline_code(expr_to_tree(expr, self._pset), self.operators)
        return eval(sklearn_pipeline, self.operators_context)
    def _set_param_recursive(self, pipeline_steps, parameter, value):
                for (_, obj) in pipeline_steps:
            recursive_attrs = ['steps', 'transformer_list', 'estimators']
            for attr in recursive_attrs:
                if hasattr(obj, attr):
                    self._set_param_recursive(getattr(obj, attr), parameter, value)
                    break
            else:
                if hasattr(obj, parameter):
                    setattr(obj, parameter, value)
    def _evaluate_individuals(self, individuals, features, classes, sample_weight = None):
                if self.max_time_mins:
            total_mins_elapsed = (datetime.now() - self._start_datetime).total_seconds() / 60.
            if total_mins_elapsed >= self.max_time_mins:
                raise KeyboardInterrupt('{} minutes have elapsed. TPOT will close down.'.format(total_mins_elapsed))
                fitnesses_dict = {}
                eval_individuals_str = []
        sklearn_pipeline_list = []
        operator_count_list = []
        test_idx_list = []
        for indidx, individual in enumerate(individuals):
                                    individual = individuals[indidx]
            individual_str = str(individual)
            if individual_str.count('PolynomialFeatures') > 1:
                if self.verbosity > 2:
                    self._pbar.write('Invalid pipeline encountered. Skipping its evaluation.')
                fitnesses_dict[indidx] = (5000., -float('inf'))
                if not self._pbar.disable:
                    self._pbar.update(1)
                        elif individual_str in self._evaluated_individuals:
                                fitnesses_dict[indidx] = self._evaluated_individuals[individual_str]
                if self.verbosity > 2:
                    self._pbar.write('Pipeline encountered that has previously been evaluated during the '
                                     'optimization process. Using the score from the previous evaluation.')
                if not self._pbar.disable:
                    self._pbar.update(1)
            else:
                try:
                                        sklearn_pipeline = self._toolbox.compile(expr=individual)
                                        self._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
                                        operator_count = 0
                    for i in range(len(individual)):
                        node = individual[i]
                        if ((type(node) is deap.gp.Terminal) or
                             type(node) is deap.gp.Primitive and node.name == 'CombineDFs'):
                            continue
                        operator_count += 1
                except Exception:
                    fitnesses_dict[indidx] = (5000., -float('inf'))
                    if not self._pbar.disable:
                        self._pbar.update(1)
                    continue
                eval_individuals_str.append(individual_str)
                operator_count_list.append(operator_count)
                sklearn_pipeline_list.append(sklearn_pipeline)
                test_idx_list.append(indidx)
        @_timeout(max_eval_time_mins=self.max_eval_time_mins)
        def _wrapped_cross_val_score(sklearn_pipeline, features=features, classes=classes,
                                     cv=self.cv, scoring_function=self.scoring_function,
                                     sample_weight=sample_weight):
            sample_weight_dict = set_sample_weight(sklearn_pipeline.steps, sample_weight)
            from .decorators import TimedOutExc
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    cv_scores = cross_val_score(sklearn_pipeline, features, classes,
                        cv=cv, scoring=scoring_function,
                        n_jobs=1, fit_params=sample_weight_dict)
                resulting_score = np.mean(cv_scores)
            except TimedOutExc:
                resulting_score = 'Timeout'
            except Exception:
                resulting_score = -float('inf')
            return resulting_score
        if not sys.platform.startswith('win'):
            if self.n_jobs == -1:
                pool = ProcessPool()
            else:
                pool = ProcessPool(nodes=self.n_jobs)
            res_imap = pool.imap(_wrapped_cross_val_score, sklearn_pipeline_list)
            if not self._pbar.disable:
                ini_pbar_n = self._pbar.n
                        while not self._pbar.disable:
                tmp_fitness = np.array(res_imap._items)
                num_job_done = len(tmp_fitness)
                if not self._pbar.disable and num_job_done:
                    timeout_index = list(np.where(tmp_fitness[:, 1] == 'Timeout')[0])
                    for idx in timeout_index:
                        if self.verbosity > 2 and self._pbar.n - ini_pbar_n <= idx:
                            self._pbar.write('Skipped pipeline                                              'Continuing to the next pipeline.'.format(ini_pbar_n + idx + 1))
                    self._pbar.update(ini_pbar_n + num_job_done - self._pbar.n)
                if num_job_done >= len(sklearn_pipeline_list):
                    break
                else:
                    time.sleep(0.2)
            resulting_score_list = [-float('inf') if x == 'Timeout' else x for x in list(res_imap)]
        else:
            resulting_score_list = []
            for sklearn_pipeline in sklearn_pipeline_list:
                try:
                    resulting_score = _wrapped_cross_val_score(sklearn_pipeline)
                except TimedOutExc:
                    resulting_score = -float('inf')
                    if self.verbosity > 2 and not self._pbar.disable:
                        self._pbar.write('Skipped pipeline                                          'Continuing to the next pipeline.'.format(self._pbar.n + 1))
                resulting_score_list.append(resulting_score)
                if not self._pbar.disable:
                    self._pbar.update(1)
        for resulting_score, operator_count, individual_str, test_idx in zip(resulting_score_list, operator_count_list, eval_individuals_str, test_idx_list):
            if type(resulting_score) in [float, np.float64, np.float32]:
                self._evaluated_individuals[individual_str] = (max(1, operator_count), resulting_score)
                fitnesses_dict[test_idx] = self._evaluated_individuals[individual_str]
            else:
                raise ValueError('Scoring function does not return a float.')
        fitnesses_ordered = []
        for key in sorted(fitnesses_dict.keys()):
            fitnesses_ordered.append(fitnesses_dict[key])
        return fitnesses_ordered
    @_pre_test
    def _mate_operator(self, ind1, ind2):
        return gp.cxOnePoint(ind1, ind2)
    @_pre_test
    def _random_mutation_operator(self, individual):
                                mutation_techniques = [
            partial(gp.mutInsert, pset=self._pset),
            partial(mutNodeReplacement, pset=self._pset),
            partial(gp.mutShrink)
        ]
        return np.random.choice(mutation_techniques)(individual)
    def _gen_grow_safe(self, pset, min_, max_, type_=None):
                def condition(height, depth, type_):
                        return type_ not in [np.ndarray, Output_Array] or depth == height
        return self._generate(pset, min_, max_, condition, type_)
        @_pre_test
    def _generate(self, pset, min_, max_, condition, type_=None):
                if type_ is None:
            type_ = pset.ret
        expr = []
        height = np.random.randint(min_, max_)
        stack = [(0, type_)]
        while len(stack) != 0:
            depth, type_ = stack.pop()
                        if condition(height, depth, type_):
                try:
                    term = np.random.choice(pset.terminals[type_])
                except IndexError:
                    _, _, traceback = sys.exc_info()
                    raise IndexError("The gp.generate function tried to add "
                                      "a terminal of type '%s', but there is "
                                      "none available." % (type_,)).\
                                      with_traceback(traceback)
                if inspect.isclass(term):
                    term = term()
                expr.append(term)
            else:
                try:
                    prim = np.random.choice(pset.primitives[type_])
                except IndexError:
                    _, _, traceback = sys.exc_info()
                    raise IndexError("The gp.generate function tried to add "
                                      "a primitive of type '%s', but there is "
                                      "none available." % (type_,)).\
                                      with_traceback(traceback)
                expr.append(prim)
                for arg in reversed(prim.args):
                    stack.append((depth+1, arg))
        return expr

import numpy as np
from sklearn.base import BaseEstimator
from sklearn.utils import check_array

class ZeroCount(BaseEstimator):
    
    def __init__(self):
        pass
    def fit(self, X, y=None):
                return self
    def transform(self, X, y=None):
                X = check_array(X)
        n_features = X.shape[1]
        X_transformed = np.copy(X)
        non_zero = np.apply_along_axis(lambda row: np.count_nonzero(row),
                                        axis=1, arr=X_transformed)
        zero_col = np.apply_along_axis(lambda row: (n_features - np.count_nonzero(row)),
                                        axis=1, arr=X_transformed)
        X_transformed = np.insert(X_transformed, n_features, non_zero, axis=1)
        X_transformed = np.insert(X_transformed, n_features + 1, zero_col, axis=1)
        return X_transformed
class CombineDFs(object):
    
    @property
    def __name__(self):
        return self.__class__.__name__
import numpy as np
classifier_config_dict = {
        'sklearn.naive_bayes.GaussianNB': {
    },
    'sklearn.naive_bayes.BernoulliNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
    },
    'sklearn.naive_bayes.MultinomialNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
    },
    'sklearn.tree.DecisionTreeClassifier': {
        'criterion': ["gini", "entropy"],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21)
    },
    'sklearn.ensemble.ExtraTreesClassifier': {
        'criterion': ["gini", "entropy"],
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.RandomForestClassifier': {
        'criterion': ["gini", "entropy"],
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf':  range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.GradientBoostingClassifier': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'subsample': np.arange(0.05, 1.01, 0.05),
        'max_features': np.arange(0, 1.01, 0.05)
    },
    'sklearn.neighbors.KNeighborsClassifier': {
        'n_neighbors': range(1, 101),
        'weights': ["uniform", "distance"],
        'p': [1, 2]
    },
    'sklearn.svm.LinearSVC': {
        'penalty': ["l1", "l2"],
        'loss': ["hinge", "squared_hinge"],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]
    },
    'sklearn.linear_model.LogisticRegression': {
        'penalty': ["l1", "l2"],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'dual': [True, False]
    },
    'xgboost.XGBClassifier': {
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21)
    },
        'sklearn.preprocessing.Binarizer': {
        'threshold': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.decomposition.FastICA': {
        'tol': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.cluster.FeatureAgglomeration': {
        'linkage': ['ward', 'complete', 'average'],
        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed']
    },
    'sklearn.preprocessing.MaxAbsScaler': {
    },
    'sklearn.preprocessing.MinMaxScaler': {
    },
    'sklearn.preprocessing.Normalizer': {
        'norm': ['l1', 'l2', 'max']
    },
    'sklearn.kernel_approximation.Nystroem': {
        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],
        'gamma': np.arange(0.0, 1.01, 0.05),
        'n_components': range(1, 11)
    },
    'sklearn.decomposition.PCA': {
        'svd_solver': ['randomized'],
        'iterated_power': range(1, 11)
    },
    'sklearn.preprocessing.PolynomialFeatures': {
        'degree': [2],
        'include_bias': [False],
        'interaction_only': [False]
    },
    'sklearn.kernel_approximation.RBFSampler': {
        'gamma': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.preprocessing.RobustScaler': {
    },
    'sklearn.preprocessing.StandardScaler': {
    },
    'tpot.built_in_operators.ZeroCount': {
    },
        'sklearn.feature_selection.SelectFwe': {
        'alpha': np.arange(0, 0.05, 0.001),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            } 
    },
    'sklearn.feature_selection.SelectKBest': {
        'k': range(1, 100),         'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.SelectPercentile': {
        'percentile': range(1, 100),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.VarianceThreshold': {
        'threshold': np.arange(0.05, 1.01, 0.05)
    },
    'sklearn.feature_selection.RFE': {
        'step': np.arange(0.05, 1.01, 0.05),
        'estimator': {
            'sklearn.svm.SVC': {
                'kernel': ['linear'],
                'random_state': [42]
                }
        }
    },
   'sklearn.feature_selection.SelectFromModel': {
        'threshold': np.arange(0, 1.01, 0.05),
        'estimator': {
            'sklearn.ensemble.ExtraTreesClassifier': {
                'criterion': ['gini', 'entropy'],
                'max_features': np.arange(0, 1.01, 0.05)
                }
        }
    }
}
import numpy as np
regressor_config_dict = {

    'sklearn.linear_model.ElasticNetCV': {
        'l1_ratio': np.arange(0.0, 1.01, 0.05),
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
    },
    'sklearn.ensemble.ExtraTreesRegressor': {
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.GradientBoostingRegressor': {
        'loss': ["ls", "lad", "huber", "quantile"],
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'subsample': np.arange(0.05, 1.01, 0.05),
        'max_features': np.arange(0, 1.01, 0.05),
        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]
    },
    'sklearn.ensemble.AdaBoostRegressor': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'loss': ["linear", "square", "exponential"],
        'max_depth': range(1, 11)
    },
    'sklearn.tree.DecisionTreeRegressor': {
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21)
    },
    'sklearn.neighbors.KNeighborsRegressor': {
        'n_neighbors': range(1, 101),
        'weights': ["uniform", "distance"],
        'p': [1, 2]
    },
    'sklearn.linear_model.LassoLarsCV': {
        'normalize': [True, False]
    },
    'sklearn.svm.LinearSVR': {
        'loss': ["epsilon_insensitive", "squared_epsilon_insensitive"],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1.]
    },
    'sklearn.ensemble.RandomForestRegressor': {
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.linear_model.RidgeCV': {
    },

    'xgboost.XGBRegressor': {
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21)
    },
        'sklearn.preprocessing.Binarizer': {
        'threshold': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.decomposition.FastICA': {
        'tol': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.cluster.FeatureAgglomeration': {
        'linkage': ['ward', 'complete', 'average'],
        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed']
    },
    'sklearn.preprocessing.MaxAbsScaler': {
    },
    'sklearn.preprocessing.MinMaxScaler': {
    },
    'sklearn.preprocessing.Normalizer': {
        'norm': ['l1', 'l2', 'max']
    },
    'sklearn.kernel_approximation.Nystroem': {
        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],
        'gamma': np.arange(0.0, 1.01, 0.05),
        'n_components': range(1, 11)
    },
    'sklearn.decomposition.PCA': {
        'svd_solver': ['randomized'],
        'iterated_power': range(1, 11)
    },
    'sklearn.preprocessing.PolynomialFeatures': {
        'degree': [2],
        'include_bias': [False],
        'interaction_only': [False]
    },
    'sklearn.kernel_approximation.RBFSampler': {
        'gamma': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.preprocessing.RobustScaler': {
    },
    'sklearn.preprocessing.StandardScaler': {
    },
    'tpot.built_in_operators.ZeroCount': {
    },
        'sklearn.feature_selection.SelectFwe': {
        'alpha': np.arange(0, 0.05, 0.001),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            } 
    },
    'sklearn.feature_selection.SelectKBest': {
        'k': range(1, 100),         'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.SelectPercentile': {
        'percentile': range(1, 100),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.VarianceThreshold': {
        'threshold': np.arange(0.05, 1.01, 0.05)
    },
    'sklearn.feature_selection.SelectFromModel': {
        'threshold': np.arange(0, 1.01, 0.05),
        'estimator': {
                'sklearn.ensemble.ExtraTreesRegressor': {
                    'max_features': np.arange(0, 1.01, 0.05)
                    }
                }
    }
}

from __future__ import print_function
from threading import Thread, current_thread
from functools import wraps
import sys
import warnings
from sklearn.datasets import make_classification, make_regression
from .export_utils import expr_to_tree, generate_pipeline_code
from deap import creator
pretest_X, pretest_y = make_classification(n_samples=50, n_features=10, random_state=42)
pretest_X_reg, pretest_y_reg = make_regression(n_samples=50, n_features=10, random_state=42)

def convert_mins_to_secs(time_minute):
            return max(int(time_minute * 60), 1)

class TimedOutExc(RuntimeError):
    
def timeout_signal_handler(signum, frame):
        raise TimedOutExc("Time Out!")
def _timeout(max_eval_time_mins=5):
        def wrap_func(func):
        if not sys.platform.startswith('win'):
            import signal
            @wraps(func)
            def limitedTime(*args, **kw):
                old_signal_hander = signal.signal(signal.SIGALRM, timeout_signal_handler)
                max_time_seconds = convert_mins_to_secs(max_eval_time_mins)
                signal.alarm(max_time_seconds)
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        ret = func(*args, **kw)
                except:
                    raise TimedOutExc('Time Out!')
                finally:
                    signal.signal(signal.SIGALRM, old_signal_hander)                      signal.alarm(0)                  return ret
        else:
            class InterruptableThread(Thread):
                def __init__(self, args, kwargs):
                    Thread.__init__(self)
                    self.args = args
                    self.kwargs = kwargs
                    self.result = -float('inf')
                    self.daemon = True
                def stop(self):
                    self._stop()
                def run(self):
                                                            current_thread().name = 'MainThread'
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        self.result = func(*self.args, **self.kwargs)
            @wraps(func)
            def limitedTime(*args, **kwargs):
                sys.tracebacklimit = 0
                max_time_seconds = convert_mins_to_secs(max_eval_time_mins)
                                tmp_it = InterruptableThread(args, kwargs)
                tmp_it.start()
                                tmp_it.join(max_time_seconds)
                if tmp_it.isAlive():
                    raise TimedOutExc('Time Out!')
                sys.tracebacklimit = 1000
                tmp_it.stop()
                return tmp_it.result
        return limitedTime
    return wrap_func


def _pre_test(func):
        @wraps(func)
    def check_pipeline(self, *args, **kwargs):
        bad_pipeline = True
        num_test = 0         while bad_pipeline and num_test < 10:                         args = [self._toolbox.clone(arg) if isinstance(arg, creator.Individual) else arg for arg in args]
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    expr = func(self, *args, **kwargs)
                                        expr_tuple = expr if isinstance(expr, tuple) else (expr,)
                    for expr_test in expr_tuple:
                                                sklearn_pipeline = eval(generate_pipeline_code(expr_to_tree(expr_test, self._pset), self.operators), self.operators_context)
                        if self.classification:
                            sklearn_pipeline.fit(pretest_X, pretest_y)
                        else:
                            sklearn_pipeline.fit(pretest_X_reg, pretest_y_reg)
                        bad_pipeline = False
            except BaseException as e:
                if self.verbosity > 2:
                    print_function = print
                                        if not isinstance(self._pbar, type(None)):
                        print_function = self._pbar.write
                    print_function('_pre_test decorator: {fname}: num_test={n} {e}'.format(n=num_test, fname=func.__name__, e=e))
            finally:
                num_test += 1
        return expr
    return check_pipeline

import numpy as np
import argparse
from sklearn.model_selection import train_test_split
from .tpot import TPOTClassifier, TPOTRegressor
from ._version import __version__

def positive_integer(value):
        try:
        value = int(value)
    except Exception:
        raise argparse.ArgumentTypeError('Invalid int value: \'{}\''.format(value))
    if value < 0:
        raise argparse.ArgumentTypeError('Invalid positive int value: \'{}\''.format(value))
    return value

def float_range(value):
        try:
        value = float(value)
    except:
        raise argparse.ArgumentTypeError('Invalid float value: \'{}\''.format(value))
    if value < 0.0 or value > 1.0:
        raise argparse.ArgumentTypeError('Invalid float value: \'{}\''.format(value))
    return value

def main():
        parser = argparse.ArgumentParser(description='A Python tool that '
        'automatically creates and optimizes machine learning pipelines using '
        'genetic programming.', add_help=False)
    parser.add_argument('INPUT_FILE', type=str, help='Data file to use in the TPOT '
        'optimization process. Ensure that the class label column is labeled as "class".')
    parser.add_argument('-h', '--help', action='help',
        help='Show this help message and exit.')
    parser.add_argument('-is', action='store', dest='INPUT_SEPARATOR', default='\t',
        type=str, help='Character used to separate columns in the input file.')
    parser.add_argument('-target', action='store', dest='TARGET_NAME', default='class',
        type=str, help='Name of the target column in the input file.')
    parser.add_argument('-mode', action='store', dest='TPOT_MODE',
        choices=['classification', 'regression'], default='classification', type=str,
        help='Whether TPOT is being used for a supervised classification or regression problem.')
    parser.add_argument('-o', action='store', dest='OUTPUT_FILE', default='',
        type=str, help='File to export the code for the final optimized pipeline.')
    parser.add_argument('-g', action='store', dest='GENERATIONS', default=100,
        type=positive_integer, help='Number of iterations to run the pipeline optimization process.\n'
        'Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline. '
        'TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.')
    parser.add_argument('-p', action='store', dest='POPULATION_SIZE', default=100,
        type=positive_integer, help='Number of individuals to retain in the GP population every generation.\n'
        'Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize the pipeline. '
        'TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.')
    parser.add_argument('-os', action='store', dest='OFFSPRING_SIZE', default=None,
        type=positive_integer, help='Number of offspring to produce in each GP generation. '
        'By default, OFFSPRING_SIZE = POPULATION_SIZE.')
    parser.add_argument('-mr', action='store', dest='MUTATION_RATE', default=0.9,
        type=float_range, help='GP mutation rate in the range [0.0, 1.0]. This tells the '
        'GP algorithm how many pipelines to apply random changes to every generation. '
        'We recommend using the default parameter unless you understand how the mutation '
        'rate affects GP algorithms.')
    parser.add_argument('-xr', action='store', dest='CROSSOVER_RATE', default=0.1,
        type=float_range, help='GP crossover rate in the range [0.0, 1.0]. This tells the '
        'GP algorithm how many pipelines to "breed" every generation. '
        'We recommend using the default parameter unless you understand how the crossover '
        'rate affects GP algorithms.')
    parser.add_argument('-scoring', action='store', dest='SCORING_FN', default=None,
        type=str, help='Function used to evaluate the quality of a given pipeline for '
        'the problem. By default, accuracy is used for classification problems and mean '
        'squared error (mse) is used for regression problems. '
        'TPOT assumes that any function with "error" or "loss" in the name is meant to '
        'be minimized, whereas any other functions will be maximized. '
        'Offers the same options as cross_val_score: '
        '"accuracy", "adjusted_rand_score", "average_precision", "f1", "f1_macro", '
        '"f1_micro", "f1_samples", "f1_weighted", "log_loss", "mean_absolute_error", '
        '"mean_squared_error", "median_absolute_error", "precision", "precision_macro", '
        '"precision_micro", "precision_samples", "precision_weighted", "r2", "recall", '
        '"recall_macro", "recall_micro", "recall_samples", "recall_weighted", "roc_auc"')
    parser.add_argument('-cv', action='store', dest='NUM_CV_FOLDS', default=5,
        type=int, help='Number of folds to evaluate each pipeline over in '
        'k-fold cross-validation during the TPOT optimization process.')
    parser.add_argument('-njobs', action='store', dest='NUM_JOBS', default=1,
        type=int, help='Number of CPUs for evaluating pipelines in parallel '
        ' during the TPOT optimization process. Assigning this to -1 will use as many '
        'cores as available on the computer.')
    parser.add_argument('-maxtime', action='store', dest='MAX_TIME_MINS', default=None,
        type=int, help='How many minutes TPOT has to optimize the pipeline. This '
        'setting will override the GENERATIONS parameter '
        'and allow TPOT to run until it runs out of time.')
    parser.add_argument('-maxeval', action='store', dest='MAX_EVAL_MINS', default=5,
        type=float, help='How many minutes TPOT has to evaluate a single pipeline. '
        'Setting this parameter to higher values will allow TPOT to explore more complex '
        'pipelines but will also allow TPOT to run longer.')
    parser.add_argument('-s', action='store', dest='RANDOM_STATE', default=None,
        type=int, help='Random number generator seed for reproducibility. Set '
        'this seed if you want your TPOT run to be reproducible with the same '
        'seed and data set in the future.')
    parser.add_argument('-config', action='store', dest='CONFIG_FILE', default='',
        type=str, help='Configuration file for customizing the operators and parameters '
        'that TPOT uses in the optimization process.')
    parser.add_argument('-v', action='store', dest='VERBOSITY', default=1,
        choices=[0, 1, 2, 3], type=int, help='How much information TPOT communicates '
        'while it is running: 0 = none, 1 = minimal, 2 = high, 3 = all. '
        'A setting of 2 or higher will add a progress bar during the optimization procedure.')
    parser.add_argument('--no-update-check', action='store_true',
        dest='DISABLE_UPDATE_CHECK', default=False,
        help='Flag indicating whether the TPOT version checker should be disabled.')
    parser.add_argument('--version', action='version',
        version='TPOT {version}'.format(version=__version__),
        help='Show the TPOT version number and exit.')
    args = parser.parse_args()
    if args.VERBOSITY >= 2:
        print('\nTPOT settings:')
        for arg in sorted(args.__dict__):
            arg_val = args.__dict__[arg]
            if arg == 'DISABLE_UPDATE_CHECK':
                continue
            elif arg == 'SCORING_FN' and arg_val is None:
                if args.TPOT_MODE == 'classification':
                    arg_val = 'accuracy'
                else:
                    arg_val = 'mean_squared_error'
            elif arg == 'OFFSPRING_SIZE' and arg_val is None:
                arg_val = args.__dict__['POPULATION_SIZE']
            print('{}\t=\t{}'.format(arg, arg_val))
        print('')
    input_data = np.recfromcsv(args.INPUT_FILE, delimiter=args.INPUT_SEPARATOR, dtype=np.float64, case_sensitive=True)
    if args.TARGET_NAME not in input_data.dtype.names:
        raise ValueError('The provided data file does not seem to have a target column. '
                         'Please make sure to specify the target column using the -target parameter.')
    features = np.delete(input_data.view(np.float64).reshape(input_data.size, -1),
                         input_data.dtype.names.index(args.TARGET_NAME), axis=1)
    training_features, testing_features, training_classes, testing_classes = \
        train_test_split(features, input_data[args.TARGET_NAME], random_state=args.RANDOM_STATE)
    if args.TPOT_MODE == 'classification':
        tpot_type = TPOTClassifier
    else:
        tpot_type = TPOTRegressor
    operator_dict = None
    if args.CONFIG_FILE:
        try:
            with open(args.CONFIG_FILE, 'r') as input_file:
                file_string =  input_file.read()
            operator_dict = eval(file_string[file_string.find('{'):(file_string.rfind('}') + 1)])
        except:
            raise TypeError('The operator configuration file is in a bad format or not available. '
                            'Please check the configuration file before running TPOT.')
    tpot = tpot_type(generations=args.GENERATIONS, population_size=args.POPULATION_SIZE,
                     offspring_size=args.OFFSPRING_SIZE, mutation_rate=args.MUTATION_RATE, crossover_rate=args.CROSSOVER_RATE,
                     cv=args.NUM_CV_FOLDS, n_jobs=args.NUM_JOBS,
                     scoring=args.SCORING_FN,
                     max_time_mins=args.MAX_TIME_MINS, max_eval_time_mins=args.MAX_EVAL_MINS,
                     random_state=args.RANDOM_STATE, config_dict=operator_dict,
                     verbosity=args.VERBOSITY, disable_update_check=args.DISABLE_UPDATE_CHECK)
    print('')
    tpot.fit(training_features, training_classes)
    if args.VERBOSITY in [1, 2] and tpot._optimized_pipeline:
        training_score = max([tpot._pareto_front.keys[x].wvalues[1] for x in range(len(tpot._pareto_front.keys))])
        print('\nTraining score: {}'.format(abs(training_score)))
        print('Holdout score: {}'.format(tpot.score(testing_features, testing_classes)))
    elif args.VERBOSITY >= 3 and tpot._pareto_front:
        print('Final Pareto front testing scores:')
        for pipeline, pipeline_scores in zip(tpot._pareto_front.items, reversed(tpot._pareto_front.keys)):
            tpot._fitted_pipeline = tpot._pareto_front_fitted_pipelines[str(pipeline)]
            print('{}\t{}\t{}'.format(int(abs(pipeline_scores.wvalues[0])),
                                      tpot.score(testing_features, testing_classes),
                                      pipeline))
    if args.OUTPUT_FILE != '':
        tpot.export(args.OUTPUT_FILE)

if __name__ == '__main__':
    main()

import deap
def get_by_name(opname, operators):
        ret_op_classes = [op for op in operators if op.__name__ == opname]
    if len(ret_op_classes) == 0:
        raise TypeError('Cannot found operator {} in operator dictionary'.format(opname))
    elif len(ret_op_classes) > 1:
        print('Found multiple operator {} in operator dictionary'.format(opname),
        'Please check your dictionary file.')
    ret_op_class = ret_op_classes[0]
    return ret_op_class
def export_pipeline(exported_pipeline, operators, pset):
            pipeline_tree = expr_to_tree(exported_pipeline, pset)
        pipeline_text = generate_import_code(exported_pipeline, operators)
        pipeline_text += pipeline_code_wrapper(generate_export_pipeline_code(pipeline_tree, operators))
    return pipeline_text

def expr_to_tree(ind, pset):
        def prim_to_list(prim, args):
        if isinstance(prim, deap.gp.Terminal):
            if prim.name in pset.context:
                 return pset.context[prim.name]
            else:
                 return prim.value
        return [prim.name] + args
    tree = []
    stack = []
    for node in ind:
        stack.append((node, []))
        while len(stack[-1][1]) == stack[-1][0].arity:
            prim, args = stack.pop()
            tree = prim_to_list(prim, args)
            if len(stack) == 0:
                break               stack[-1][1].append(tree)
    return tree

def generate_import_code(pipeline, operators):
            operators_used = [x.name for x in pipeline if isinstance(x, deap.gp.Primitive)]
    pipeline_text = 'import numpy as np\n\n'
        num_op = len(operators_used)
        import_relations = {}
    for op in operators:
        import_relations[op.__name__] = op.import_hash
        num_op_root = 0
    for op in operators_used:
        if op != 'CombineDFs':
            tpot_op = get_by_name(op, operators)
            if tpot_op.root:
                num_op_root += 1
        else:
            num_op_root += 1
        if num_op_root > 1:
        pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline', 'make_union'],
            'sklearn.preprocessing':    ['FunctionTransformer'],
            'sklearn.ensemble':         ['VotingClassifier']
        }
    elif num_op > 1:
        pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline']
        }
    else:         pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split']
        }
        for op in operators_used:
        def merge_imports(old_dict, new_dict):
                        for key in new_dict.keys():
                if key in old_dict.keys():
                                        old_dict[key] = set(old_dict[key]) | set(new_dict[key])
                else:
                    old_dict[key] = set(new_dict[key])
        try:
            operator_import = import_relations[op]
            merge_imports(pipeline_imports, operator_import)
        except KeyError:
            pass  
        for key in sorted(pipeline_imports.keys()):
        module_list = ', '.join(sorted(pipeline_imports[key]))
        pipeline_text += 'from {} import {}\n'.format(key, module_list)
    pipeline_text += 
    return pipeline_text

def pipeline_code_wrapper(pipeline_code):
        return     steps = process_operator(pipeline_tree, operators)
    pipeline_text = "make_pipeline(\n{STEPS}\n)".format(STEPS=_indent(",\n".join(steps), 4))
    return pipeline_text
def generate_export_pipeline_code(pipeline_tree, operators):
        steps = process_operator(pipeline_tree, operators)
        num_step = len(steps)
    if num_step > 1:
        pipeline_text = "make_pipeline(\n{STEPS}\n)".format(STEPS=_indent(",\n".join(steps), 4))
    else:         pipeline_text =  "{STEPS}".format(STEPS=_indent(",\n".join(steps), 0))
    return pipeline_text
def process_operator(operator, operators, depth=0):
    steps = []
    op_name = operator[0]
    if op_name == "CombineDFs":
        steps.append(
            _combine_dfs(operator[1], operator[2], operators)
        )
    else:
        input_name, args = operator[1], operator[2:]
        tpot_op = get_by_name(op_name, operators)
        if input_name != 'input_matrix':
            steps.extend(process_operator(input_name, operators, depth + 1))
                        if tpot_op.root and depth > 0:
            steps.append(
                "make_union(VotingClassifier([(\"est\", {})]), FunctionTransformer(lambda X: X))".
                format(tpot_op.export(*args))
            )
        else:
            steps.append(tpot_op.export(*args))
    return steps

def _indent(text, amount):
        indentation = amount * ' '
    return indentation + ('\n' + indentation).join(text.split('\n'))

def _combine_dfs(left, right, operators):
    def _make_branch(branch):
        if branch == "input_matrix":
            return "FunctionTransformer(lambda X: X)"
        elif branch[0] == "CombineDFs":
            return _combine_dfs(branch[1], branch[2], operators)
        elif branch[1] == "input_matrix":              tpot_op = get_by_name(branch[0], operators)
            if tpot_op.root:
                return Copyright 2015-Present Randal S. Olson
This file is modified based on codes for alogrithms.eaSimple module in DEAP.
This file is part of the TPOT library.
The TPOT library is free software: you can redistribute it and/or
modify it under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your option)
any later version.
The TPOT library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
details. You should have received a copy of the GNU General Public License along
with the TPOT library. If not, see http://www.gnu.org/licenses/.
    offspring = []
    for _ in range(lambda_):
        op_choice = np.random.random()
        if op_choice < cxpb:                        idxs = np.random.randint(0, len(population),size=2)
            ind1, ind2 = toolbox.clone(population[idxs[0]]), toolbox.clone(population[idxs[1]])
            ind_str = str(ind1)
            num_loop = 0
            while ind_str == str(ind1) and num_loop < 50 :                 ind1, ind2 = toolbox.mate(ind1, ind2)
                num_loop += 1
            if ind_str != str(ind1):                 del ind1.fitness.values
            offspring.append(ind1)
        elif op_choice < cxpb + mutpb:              idx = np.random.randint(0, len(population))
            ind = toolbox.clone(population[idx])
            ind_str = str(ind)
            num_loop = 0
            while ind_str == str(ind) and num_loop < 50 :                 ind, = toolbox.mutate(ind)
                num_loop += 1
            if ind_str != str(ind):                 del ind.fitness.values
            offspring.append(ind)
        else:             idx = np.random.randint(0, len(population))
            offspring.append(toolbox.clone(population[idx]))
    return offspring
def eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar,
                   stats=None, halloffame=None, verbose=0, max_time_mins = None):
        logbook = tools.Logbook()
    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])
        invalid_ind = [ind for ind in population if not ind.fitness.valid]
    fitnesses = toolbox.evaluate(invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    if halloffame is not None:
        halloffame.update(population)
    record = stats.compile(population) if stats is not None else {}
    logbook.record(gen=0, nevals=len(invalid_ind), **record)
        for gen in range(1, ngen + 1):
                offspring = varOr(population, toolbox, lambda_, cxpb, mutpb)
                invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
                if not pbar.disable:
            pbar.update(len(offspring)-len(invalid_ind))
            if not (max_time_mins is None) and pbar.n >= pbar.total:
                pbar.total += lambda_
        fitnesses = toolbox.evaluate(invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            ind.fitness.values = fit

                if halloffame is not None:
            halloffame.update(offspring)
                population[:] = toolbox.select(population + offspring, mu)
                if not pbar.disable:
                        if verbose == 2:
                high_score = abs(max([halloffame.keys[x].wvalues[1] for x in range(len(halloffame.keys))]))
                pbar.write('Generation {0} - Current best internal CV score: {1}'.format(gen, high_score))
                        elif verbose == 3:
                pbar.write('Generation {} - Current Pareto front scores:'.format(gen))
                for pipeline, pipeline_scores in zip(halloffame.items, reversed(halloffame.keys)):
                    pbar.write('{}\t{}\t{}'.format(int(abs(pipeline_scores.wvalues[0])),
                                                         abs(pipeline_scores.wvalues[1]),
                                                         pipeline))
                pbar.write('')
                record = stats.compile(population) if stats is not None else {}
        logbook.record(gen=gen, nevals=len(invalid_ind), **record)
    return population, logbook

def mutNodeReplacement(individual, pset):
    
    index = np.random.randint(0, len(individual))
    node = individual[index]
    slice_ = individual.searchSubtree(index)
    if node.arity == 0:          term = np.random.choice(pset.terminals[node.ret])
        if isclass(term):
            term = term()
        individual[index] = term
    else:                   rindex = None
        if index + 1 < len(individual):
            for i, tmpnode in enumerate(individual[index+1:], index+ 1):
                if isinstance(tmpnode, gp.Primitive) and tmpnode.ret in tmpnode.args:
                    rindex = i
                                primitives = pset.primitives[node.ret]
        if len(primitives) != 0:
            new_node = np.random.choice(primitives)
            new_subtree = [None] * len(new_node.args)
            if rindex:
                rnode = individual[rindex]
                rslice = individual.searchSubtree(rindex)
                                position = np.random.choice([i for i, a in enumerate(new_node.args) if a == rnode.ret])
            else:
                position = None
            for i, arg_type in enumerate(new_node.args):
                if i != position:
                    term = np.random.choice(pset.terminals[arg_type])
                    if isclass(term):
                        term = term()
                    new_subtree[i] = term
                        if rindex:
                new_subtree[position:position + 1] = individual[rslice]
                        new_subtree.insert(0, new_node)
            individual[slice_] = new_subtree
    return individual,

class Output_Array(object):
    
    pass

import numpy as np
from sklearn.metrics import make_scorer, SCORERS

def balanced_accuracy(y_true, y_pred):
        all_classes = list(set(np.append(y_true, y_pred)))
    all_class_accuracies = []
    for this_class in all_classes:
        this_class_sensitivity = \
            float(sum((y_pred == this_class) & (y_true == this_class))) /\
            float(sum((y_true == this_class)))
        this_class_specificity = \
            float(sum((y_pred != this_class) & (y_true != this_class))) /\
            float(sum((y_true != this_class)))
        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.
        all_class_accuracies.append(this_class_accuracy)
    return np.mean(all_class_accuracies)
SCORERS['balanced_accuracy'] = make_scorer(balanced_accuracy)

import numpy as np
from sklearn.base import ClassifierMixin
from sklearn.base import RegressorMixin
import inspect

class Operator(object):
        def __init__(self):
        pass
    root = False      import_hash = None
    sklearn_class = None
    arg_types = None
    dep_op_list = {} 
class ARGType(object):
        def __init__(self):
     pass

def source_decode(sourcecode):
        tmp_path = sourcecode.split('.')
    op_str = tmp_path.pop()
    import_str = '.'.join(tmp_path)
    try:
        if sourcecode.startswith('tpot.'):
            exec('from {} import {}'.format(import_str[4:], op_str))
        else:
            exec('from {} import {}'.format(import_str, op_str))
        op_obj = eval(op_str)
    except ImportError:
        print('Warning: {} is not available and will not be used by TPOT.'.format(sourcecode))
        op_obj = None
    return import_str, op_str, op_obj
def set_sample_weight(pipeline_steps, sample_weight=None):
        sample_weight_dict = {}
    if not isinstance(sample_weight, type(None)):
        for (pname, obj) in pipeline_steps:
            if inspect.getargspec(obj.fit).args.count('sample_weight'):
                step_sw = pname + '__sample_weight'
                sample_weight_dict[step_sw] = sample_weight
    if sample_weight_dict:
        return sample_weight_dict
    else:
        return None
def ARGTypeClassFactory(classname, prange, BaseClass=ARGType):
        return type(classname, (BaseClass,), {'values':prange})
def TPOTOperatorClassFactory(opsourse, opdict, BaseClass=Operator, ArgBaseClass=ARGType):
    
    class_profile = {}
    dep_op_list = {}
    import_str, op_str, op_obj = source_decode(opsourse)
    if not op_obj:
        return None, None     else:
                if issubclass(op_obj, ClassifierMixin) or issubclass(op_obj, RegressorMixin):
            class_profile['root'] = True
            optype = "Classifier or Regressor"
        else:
            optype = "Preprocessor or Selector"
        @classmethod
        def op_type(cls):
                        return optype
        class_profile['type'] = op_type
        class_profile['sklearn_class'] = op_obj
        import_hash = {}
        import_hash[import_str] = [op_str]
        arg_types = []
        for pname in sorted(opdict.keys()):
            prange = opdict[pname]
            if not isinstance(prange, dict):
                classname = '{}__{}'.format(op_str, pname)
                arg_types.append(ARGTypeClassFactory(classname, prange))
            else:
                for dkey, dval in prange.items():
                    dep_import_str, dep_op_str, dep_op_obj = source_decode(dkey)
                    if dep_import_str in import_hash:
                        import_hash[import_str].append(dep_op_str)
                    else:
                        import_hash[dep_import_str] = [dep_op_str]
                    dep_op_list[pname]=dep_op_str
                    if dval:
                        for dpname in sorted(dval.keys()):
                            dprange = dval[dpname]
                            classname = '{}__{}__{}'.format(op_str, dep_op_str, dpname)
                            arg_types.append(ARGTypeClassFactory(classname, dprange))
        class_profile['arg_types'] = tuple(arg_types)
        class_profile['import_hash'] = import_hash
        class_profile['dep_op_list'] = dep_op_list
        @classmethod
        def parameter_types(cls):
                        return ([np.ndarray] + arg_types, np.ndarray)

        class_profile['parameter_types'] = parameter_types
        @classmethod
        def export(cls, *args):
            
            op_arguments = []
            if dep_op_list:
                dep_op_arguments = {}
            for arg_class, arg_value in zip(arg_types, args):
                if arg_value == "DEFAULT":
                    continue
                aname_split = arg_class.__name__.split('__')
                if isinstance(arg_value, str):
                    arg_value = '\"{}\"'.format(arg_value)
                if len(aname_split) == 2:                     op_arguments.append("{}={}".format(aname_split[-1], arg_value))
                else:                     if not list(dep_op_list.values()).count(aname_split[1]):
                        raise TypeError('Warning: the operator {} is not in right format in the operator dictionary'.format(aname_split[0]))
                    else:
                        if aname_split[1] not in dep_op_arguments:
                            dep_op_arguments[aname_split[1]] = []
                        dep_op_arguments[aname_split[1]].append("{}={}".format(aname_split[-1], arg_value))
            tmp_op_args = []
            if dep_op_list:
                                for dep_op_pname, dep_op_str in dep_op_list.items():
                    if dep_op_str == 'f_classif':
                        arg_value = dep_op_str
                    else:
                        arg_value = "{}({})".format(dep_op_str, ", ".join(dep_op_arguments[dep_op_str]))
                    tmp_op_args.append("{}={}".format(dep_op_pname, arg_value))
            op_arguments = tmp_op_args + op_arguments
            return "{}({})".format(op_obj.__name__, ", ".join(op_arguments))
        class_profile['export'] = export
        op_classname = 'TPOT_{}'.format(op_str)
        op_class = type(op_classname, (BaseClass,), class_profile)
        op_class.__name__ = op_str
        return op_class, arg_types

from .base import TPOTBase
from .config_classifier import classifier_config_dict
from .config_regressor import regressor_config_dict

class TPOTClassifier(TPOTBase):
    
    scoring_function = 'accuracy'      default_config_dict = classifier_config_dict     classification = True
    regression = False

class TPOTRegressor(TPOTBase):
    
    scoring_function = 'neg_mean_squared_error'      default_config_dict = regressor_config_dict     classification = False
    regression = True

__version__ = '0.7.0'

from ._version import __version__
from .tpot import TPOTClassifier, TPOTRegressor
from .driver import main
import numpy as np
from sklearn.kernel_approximation import RBFSampler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeClassifier
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = make_pipeline(
    RBFSampler(gamma=0.8500000000000001),
    DecisionTreeClassifier(criterion="entropy", max_depth=3, min_samples_leaf=4, min_samples_split=9)
)
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = KNeighborsClassifier(n_neighbors=4, p=2, weights="distance")
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = RandomForestClassifier(bootstrap=False, max_features=0.4, min_samples_leaf=1, min_samples_split=9)
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)
result1 = tpot_data.copy()
gbc1 = GradientBoostingClassifier(learning_rate=0.49, max_features=1.0, min_weight_fraction_leaf=0.09, n_estimators=500, random_state=42)
gbc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['gbc1-classification'] = gbc1.predict(result1.drop('class', axis=1).values)
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import PassiveAggressiveClassifier
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)
result1 = tpot_data.copy()
pagr1 = PassiveAggressiveClassifier(C=0.81, loss="squared_hinge", fit_intercept=True, random_state=42)
pagr1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['pagr1-classification'] = pagr1.predict(result1.drop('class', axis=1).values)
