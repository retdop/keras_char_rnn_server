    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
        x, y = self.switch(u, i)
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
                sum_sim = sum_ratings = actual_k = 0
        for (_, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                sum_ratings += sim * r
                actual_k += 1
        if actual_k < self.min_k:
            raise PredictionImpossible('Not enough neighbors.')
        est = sum_ratings / sum_sim
        details = {'actual_k': actual_k}
        return est, details

class KNNWithMeans(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, **kwargs):
        SymmetricAlgo.__init__(self, sim_options=sim_options, **kwargs)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.sim = self.compute_similarities()
        self.means = np.zeros(self.n_x)
        for x, ratings in iteritems(self.xr):
            self.means[x] = np.mean([r for (_, r) in ratings])
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
        x, y = self.switch(u, i)
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
        est = self.means[x]
                sum_sim = sum_ratings = actual_k = 0
        for (nb, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                sum_ratings += sim * (r - self.means[nb])
                actual_k += 1
        if actual_k < self.min_k:
            sum_ratings = 0
        try:
            est += sum_ratings / sum_sim
        except ZeroDivisionError:
            pass  
        details = {'actual_k': actual_k}
        return est, details

class KNNBaseline(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, bsl_options={}):
        SymmetricAlgo.__init__(self, sim_options=sim_options,
                               bsl_options=bsl_options)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.bu, self.bi = self.compute_baselines()
        self.bx, self.by = self.switch(self.bu, self.bi)
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        x, y = self.switch(u, i)
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            return est
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
                sum_sim = sum_ratings = actual_k = 0
        for (nb, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                nb_bsl = self.trainset.global_mean + self.bx[nb] + self.by[y]
                sum_ratings += sim * (r - nb_bsl)
                actual_k += 1
        if actual_k < self.min_k:
            sum_ratings = 0
        try:
            est += sum_ratings / sum_sim
        except ZeroDivisionError:
            pass  
        details = {'actual_k': actual_k}
        return est, details
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class SVD(AlgoBase):
    
    def __init__(self, n_factors=100, n_epochs=20, biased=True, lr_all=.005,
                 reg_all=.02, lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None,
                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,
                 verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.biased = biased
        self.lr_bu = lr_bu if lr_bu is not None else lr_all
        self.lr_bi = lr_bi if lr_bi is not None else lr_all
        self.lr_pu = lr_pu if lr_pu is not None else lr_all
        self.lr_qi = lr_qi if lr_qi is not None else lr_all
        self.reg_bu = reg_bu if reg_bu is not None else reg_all
        self.reg_bi = reg_bi if reg_bi is not None else reg_all
        self.reg_pu = reg_pu if reg_pu is not None else reg_all
        self.reg_qi = reg_qi if reg_qi is not None else reg_all
        self.verbose = verbose
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                                                                                                                                                                                                        
                                        
                cdef np.ndarray[np.double_t] bu
                cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] pu
                cdef np.ndarray[np.double_t, ndim=2] qi
        cdef int u, i, f
        cdef double r, err, dot, puf, qif
        cdef double global_mean = self.trainset.global_mean
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double lr_pu = self.lr_pu
        cdef double lr_qi = self.lr_qi
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        pu = np.zeros((trainset.n_users, self.n_factors), np.double) + .1
        qi = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        if not self.biased:
            global_mean = 0
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(current_epoch))
            for u, i, r in trainset.all_ratings():
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * pu[u, f]
                err = r - (global_mean + bu[u] + bi[i] + dot)
                                if self.biased:
                    bu[u] += lr_bu * (err - reg_bu * bu[u])
                    bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    puf = pu[u, f]
                    qif = qi[i, f]
                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)
                    qi[i, f] += lr_qi * (err * puf - reg_qi * qif)
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
    def estimate(self, u, i):
        
        est = self.trainset.global_mean if self.biased else 0
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            est += np.dot(self.qi[i], self.pu[u])
        else:
            raise PredictionImpossible
        return est

class SVDpp(AlgoBase):
    
    def __init__(self, n_factors=20, n_epochs=20, lr_all=.007, reg_all=.02,
                 lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None, lr_yj=None,
                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,
                 reg_yj=None, verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.lr_bu = lr_bu if lr_bu is not None else lr_all
        self.lr_bi = lr_bi if lr_bi is not None else lr_all
        self.lr_pu = lr_pu if lr_pu is not None else lr_all
        self.lr_qi = lr_qi if lr_qi is not None else lr_all
        self.lr_yj = lr_yj if lr_yj is not None else lr_all
        self.reg_bu = reg_bu if reg_bu is not None else reg_all
        self.reg_bi = reg_bi if reg_bi is not None else reg_all
        self.reg_pu = reg_pu if reg_pu is not None else reg_all
        self.reg_qi = reg_qi if reg_qi is not None else reg_all
        self.reg_yj = reg_yj if reg_yj is not None else reg_all
        self.verbose = verbose
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                cdef np.ndarray[np.double_t] bu
                cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] pu
                cdef np.ndarray[np.double_t, ndim=2] qi
                cdef np.ndarray[np.double_t, ndim=2] yj
        cdef int u, i, j, f
        cdef double r, err, dot, puf, qif, sqrt_Iu, _
        cdef double global_mean = self.trainset.global_mean
        cdef np.ndarray[np.double_t] u_impl_fdb
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double lr_pu = self.lr_pu
        cdef double lr_qi = self.lr_qi
        cdef double lr_yj = self.lr_yj
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        cdef double reg_yj = self.reg_yj
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        pu = np.zeros((trainset.n_users, self.n_factors), np.double) + .1
        qi = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        yj = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        u_impl_fdb = np.zeros(self.n_factors, np.double)
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print(" processing epoch {}".format(current_epoch))
            for u, i, r in trainset.all_ratings():
                                Iu = [j for (j, _) in trainset.ur[u]]
                sqrt_Iu = np.sqrt(len(Iu))
                                u_impl_fdb = np.zeros(self.n_factors, np.double)
                for j in Iu:
                    for f in range(self.n_factors):
                        u_impl_fdb[f] += yj[j, f] / sqrt_Iu
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * (pu[u, f] + u_impl_fdb[f])
                err = r - (global_mean + bu[u] + bi[i] + dot)
                                bu[u] += lr_bu * (err - reg_bu * bu[u])
                bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    puf = pu[u, f]
                    qif = qi[i, f]
                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)
                    qi[i, f] += lr_qi * (err * (puf + u_impl_fdb[f]) -
                                         reg_qi * qif)
                    for j in Iu:
                        yj[j, f] += lr_yj * (err * qif / sqrt_Iu -
                                             reg_yj * yj[j, f])
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
        self.yj = yj
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            Iu = len(self.trainset.ur[u])              u_impl_feedback = (sum(self.yj[j] for (j, _)
                               in self.trainset.ur[u]) / np.sqrt(Iu))
            est += np.dot(self.qi[i], self.pu[u] + u_impl_feedback)
        return est

class NMF(AlgoBase):
    
    def __init__(self, n_factors=15, n_epochs=50, biased=False, reg_pu=.06,
                 reg_qi=.06, reg_bu=.02, reg_bi=.02, lr_bu=.005, lr_bi=.005,
                 init_low=0, init_high=1, verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.biased = biased
        self.reg_pu = reg_pu
        self.reg_qi = reg_qi
        self.lr_bu = lr_bu
        self.lr_bi = lr_bi
        self.reg_bu = reg_bu
        self.reg_bi = reg_bi
        self.init_low = init_low
        self.init_high = init_high
        self.verbose = verbose
        if self.init_low < 0:
            raise ValueError('init_low should be greater than zero')
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                cdef np.ndarray[np.double_t, ndim=2] pu
        cdef np.ndarray[np.double_t, ndim=2] qi
                cdef np.ndarray[np.double_t] bu
        cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] user_num
        cdef np.ndarray[np.double_t, ndim=2] user_denom
        cdef np.ndarray[np.double_t, ndim=2] item_num
        cdef np.ndarray[np.double_t, ndim=2] item_denom
        cdef int u, i, f
        cdef double r, est, l, dot, err
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double global_mean = self.trainset.global_mean
                pu = np.random.uniform(self.init_low, self.init_high,
                               size=(trainset.n_users, self.n_factors))
        qi = np.random.uniform(self.init_low, self.init_high,
                               size=(trainset.n_items, self.n_factors))
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        if not self.biased:
            global_mean = 0
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(current_epoch))
                        user_num = np.zeros((trainset.n_users, self.n_factors))
            user_denom = np.zeros((trainset.n_users, self.n_factors))
            item_num = np.zeros((trainset.n_items, self.n_factors))
            item_denom = np.zeros((trainset.n_items, self.n_factors))
                        for u, i, r in trainset.all_ratings():
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * pu[u, f]
                est = global_mean + bu[u] + bi[i] + dot
                err = r - est
                                if self.biased:
                    bu[u] += lr_bu * (err - reg_bu * bu[u])
                    bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    user_num[u, f] += qi[i, f] * r
                    user_denom[u, f] += qi[i, f] * est
                    item_num[i, f] += pu[u, f] * r
                    item_denom[i, f] += pu[u, f] * est
                        for u in trainset.all_users():
                l = len(trainset.ur[u])
                for f in range(self.n_factors):
                    user_denom[u, f] += l * reg_pu * pu[u, f]
                    pu[u, f] *= user_num[u, f] / user_denom[u, f]
                        for i in trainset.all_items():
                l = len(trainset.ir[i])
                for f in range(self.n_factors):
                    item_denom[i, f] += l * reg_qi * qi[i, f]
                    qi[i, f] *= item_num[i, f] / item_denom[i, f]
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
    def estimate(self, u, i):
        est = self.trainset.global_mean if self.biased else 0
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            est += np.dot(self.qi[i], self.pu[u])
        else:
            raise PredictionImpossible
        return est
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range

def baseline_als(self):
    
                
    cdef np.ndarray[np.double_t] bu = np.zeros(self.trainset.n_users)
    cdef np.ndarray[np.double_t] bi = np.zeros(self.trainset.n_items)
    cdef int u, i
    cdef double r, err, dev_i, dev_u
    cdef double global_mean = self.trainset.global_mean
    cdef int n_epochs = self.bsl_options.get('n_epochs', 10)
    cdef double reg_u = self.bsl_options.get('reg_u', 15)
    cdef double reg_i = self.bsl_options.get('reg_i', 10)
    for dummy in range(n_epochs):
        for i in self.trainset.all_items():
            dev_i = 0
            for (u, r) in self.trainset.ir[i]:
                dev_i += r - global_mean - bu[u]
            bi[i] = dev_i / (reg_i + len(self.trainset.ir[i]))
        for u in self.trainset.all_users():
            dev_u = 0
            for (i, r) in self.trainset.ur[u]:
                dev_u += r - global_mean - bi[i]
            bu[u] = dev_u / (reg_u + len(self.trainset.ur[u]))
    return bu, bi

def baseline_sgd(self):
    
    cdef np.ndarray[np.double_t] bu = np.zeros(self.trainset.n_users)
    cdef np.ndarray[np.double_t] bi = np.zeros(self.trainset.n_items)
    cdef int u, i
    cdef double r, err
    cdef double global_mean = self.trainset.global_mean
    cdef int n_epochs = self.bsl_options.get('n_epochs', 20)
    cdef double reg = self.bsl_options.get('reg', 0.02)
    cdef double lr = self.bsl_options.get('learning_rate', 0.005)
    for dummy in range(n_epochs):
        for u, i, r in self.trainset.all_ratings():
            err = (r - (global_mean + bu[u] + bi[i]))
            bu[u] += lr * (err - reg * bu[u])
            bi[i] += lr * (err - reg * bi[i])
    return bu, bi
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import namedtuple

class PredictionImpossible(Exception):
    
    pass

class Prediction(namedtuple('Prediction',
                            ['uid', 'iid', 'r_ui', 'est', 'details'])):
    
    __slots__ = ()  
    def __str__(self):
        s = 'user: {uid:<10} '.format(uid=self.uid)
        s += 'item: {iid:<10} '.format(iid=self.iid)
        s += 'r_ui = {r_ui:1.2f}   '.format(r_ui=self.r_ui)
        s += 'est = {est:1.2f}   '.format(est=self.est)
        s += str(self.details)
        return s
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from .algo_base import AlgoBase

class NormalPredictor(AlgoBase):
    
    def __init__(self):
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        num = sum((r - self.trainset.global_mean)**2
                  for (_, _, r) in self.trainset.all_ratings())
        denum = self.trainset.n_ratings
        self.sigma = np.sqrt(num / denum)
    def estimate(self, *_):
        return np.random.normal(self.trainset.global_mean, self.sigma)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from six import iteritems
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class SlopeOne(AlgoBase):
    
    def __init__(self):
        AlgoBase.__init__(self)
    def train(self, trainset):
        n_items = trainset.n_items
                cdef np.ndarray[np.int_t, ndim=2] freq
                cdef np.ndarray[np.double_t, ndim=2] dev
        cdef int u, i, j, r_ui, r_uj
        AlgoBase.train(self, trainset)
        freq = np.zeros((trainset.n_items, trainset.n_items), np.int)
        dev = np.zeros((trainset.n_items, trainset.n_items), np.double)
                for u, u_ratings in iteritems(trainset.ur):
            for i, r_ui in u_ratings:
                for j, r_uj in u_ratings:
                    freq[i, j] += 1
                    dev[i, j] += r_ui - r_uj
        for i in range(n_items):
            dev[i, i] = 0
            for j in range(i + 1, n_items):
                dev[i, j] /= freq[i, j]
                dev[j, i] = -dev[i, j]
        self.freq = freq
        self.dev = dev
                self.user_mean = [np.mean([r for (_, r) in trainset.ur[u]])
                          for u in trainset.all_users()]
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
                                Ri = [j for (j, _) in self.trainset.ur[u] if self.freq[i, j] > 0]
        est = self.user_mean[u]
        if Ri:
            est += sum(self.dev[i, j] for j in Ri) / len(Ri)
        return est
from .algo_base import AlgoBase
from .random_pred import NormalPredictor
from .baseline_only import BaselineOnly
from .knns import KNNBasic
from .knns import KNNBaseline
from .knns import KNNWithMeans
from .matrix_factorization import SVD
from .matrix_factorization import SVDpp
from .matrix_factorization import NMF
from .slope_one import SlopeOne
from .co_clustering import CoClustering
from .predictions import PredictionImpossible
from .predictions import Prediction
__all__ = ['AlgoBase', 'NormalPredictor', 'BaselineOnly', 'KNNBasic',
           'KNNBaseline', 'KNNWithMeans', 'SVD', 'SVDpp', 'NMF', 'SlopeOne',
           'CoClustering', 'PredictionImpossible', 'Prediction']
from setuptools import setup, find_packages

def calculate_version():
    initpy = open('tpot/_version.py').read().split('\n')
    version = list(filter(lambda x: '__version__' in x, initpy))[0].split('\'')[1]
    return version
package_version = calculate_version()
setup(
    name='TPOT',
    version=package_version,
    author='Randal S. Olson',
    author_email='rso@randalolson.com',
    packages=find_packages(),
    url='https://github.com/rhiever/tpot',
    license='GNU/GPLv3',
    entry_points={'console_scripts': ['tpot=tpot:main', ]},
    description=('Tree-based Pipeline Optimization Tool'),
    long_description='''
A Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.
Contact
=============
If you have any questions or comments about TPOT, please feel free to contact me via:
E-mail: rso@randalolson.com
or Twitter: https://twitter.com/randal_olson
This project is hosted at https://github.com/rhiever/tpot
''',
    zip_safe=True,
    install_requires=['numpy', 'scipy', 'scikit-learn>=0.18.1', 'deap', 'update_checker', 'tqdm', 'pathos'],
    extras_require={'xgboost': ['xgboost']},
    classifiers=[
        'Intended Audience :: Science/Research',
        'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Topic :: Scientific/Engineering :: Artificial Intelligence'
    ],
    keywords=['pipeline optimization', 'hyperparameter optimization', 'data science', 'machine learning', 'genetic programming', 'evolutionary computation'],
)

from __future__ import print_function
import random
import inspect
import warnings
import sys
import time
from functools import partial
from datetime import datetime
from pathos.multiprocessing import ProcessPool

import numpy as np
import deap
from deap import base, creator, tools, gp
from tqdm import tqdm
from sklearn.base import BaseEstimator
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.ensemble import VotingClassifier
from sklearn.metrics.scorer import make_scorer
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from update_checker import update_check
from ._version import __version__
from .operator_utils import TPOTOperatorClassFactory, Operator, ARGType, set_sample_weight
from .export_utils import export_pipeline, expr_to_tree, generate_pipeline_code
from .decorators import _timeout, _pre_test, TimedOutExc
from .built_in_operators import CombineDFs
from .metrics import SCORERS
from .gp_types import Output_Array
from .gp_deap import eaMuPlusLambda, mutNodeReplacement
if sys.platform.startswith('win'):
    import win32api
    try:
        import _thread
    except ImportError:
        import thread as _thread
    def handler(dwCtrlType, hook_sigint=_thread.interrupt_main):
        if dwCtrlType == 0:             hook_sigint()
            return 1         return 0
    win32api.SetConsoleCtrlHandler(handler, 1)

class TPOTBase(BaseEstimator):
    
    def __init__(self, generations=100, population_size=100, offspring_size=None,
                 mutation_rate=0.9, crossover_rate=0.1,
                 scoring=None, cv=5, n_jobs=1,
                 max_time_mins=None, max_eval_time_mins=5,
                 random_state=None, config_dict=None, warm_start=False,
                 verbosity=0, disable_update_check=False):
                if self.__class__.__name__ == 'TPOTBase':
            raise RuntimeError('Do not instantiate the TPOTBase class directly; use TPOTRegressor or TPOTClassifier instead.')
                self.disable_update_check = disable_update_check
        if not self.disable_update_check:
            update_check('tpot', __version__)
        self._pareto_front = None
        self._optimized_pipeline = None
        self._fitted_pipeline = None
        self._pop = None
        self.warm_start = warm_start
        self.population_size = population_size
        self.generations = generations
        self.max_time_mins = max_time_mins
        self.max_eval_time_mins = max_eval_time_mins
                if offspring_size:
            self.offspring_size = offspring_size
        else:
            self.offspring_size = population_size
                if config_dict:
            self.config_dict = config_dict
        else:
            self.config_dict = self.default_config_dict
        self.operators = []
        self.arguments = []
        for key in sorted(self.config_dict.keys()):
            op_class, arg_types = TPOTOperatorClassFactory(key, self.config_dict[key],
            BaseClass=Operator, ArgBaseClass=ARGType)
            if op_class:
                self.operators.append(op_class)
                self.arguments += arg_types
                        if not (max_time_mins is None):
            self.generations = 1000000
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        if self.mutation_rate + self.crossover_rate > 1:
            raise ValueError('The sum of the crossover and mutation probabilities must be <= 1.0.')
        self.verbosity = verbosity
        self.operators_context = {
            'make_pipeline': make_pipeline,
            'make_union': make_union,
            'VotingClassifier': VotingClassifier,
            'FunctionTransformer': FunctionTransformer
        }
        self._pbar = None
                self._evaluated_individuals = {}
        self.random_state = random_state
                if scoring:
            if hasattr(scoring, '__call__'):
                scoring_name = scoring.__name__
                if 'loss' in scoring_name or 'error' in scoring_name:
                    greater_is_better = False
                else:
                    greater_is_better = True
                SCORERS[scoring_name] = make_scorer(scoring, greater_is_better=greater_is_better)
                self.scoring_function = scoring_name
            else:
                if scoring not in SCORERS:
                    raise ValueError('The scoring function {} is not available. '
                                     'Please choose a valid scoring function from the TPOT '
                                     'documentation.'.format(scoring))
                self.scoring_function = scoring
        self.cv = cv
                if sys.platform.startswith('win') and n_jobs > 1:
            print('Warning: Parallelization is not currently supported in TPOT for Windows. ',
                  'Setting n_jobs to 1 during the TPOT optimization process.')
            self.n_jobs = 1
        else:
            self.n_jobs = n_jobs
        self._setup_pset()
        self._setup_toolbox()
    def _setup_pset(self):
        if self.random_state is not None:
            random.seed(self.random_state)
            np.random.seed(self.random_state)
        self._pset = gp.PrimitiveSetTyped('MAIN', [np.ndarray], Output_Array)
                self._pset.renameArguments(ARG0='input_matrix')

                for op in self.operators:
            if op.root:
                                                                p_types = (op.parameter_types()[0], Output_Array)
                self._pset.addPrimitive(op, *p_types)
            self._pset.addPrimitive(op, *op.parameter_types())
                                    for key in sorted(op.import_hash.keys()):
                module_list = ', '.join(sorted(op.import_hash[key]))
                if key.startswith('tpot.'):
                    exec('from {} import {}'.format(key[4:], module_list))
                else:
                    exec('from {} import {}'.format(key, module_list))
                for var in op.import_hash[key]:
                    self.operators_context[var] = eval(var)
        self._pset.addPrimitive(CombineDFs(), [np.ndarray, np.ndarray], np.ndarray)
                for _type in self.arguments:
            type_values = list(_type.values) + ['DEFAULT']
            for val in type_values:
                terminal_name = _type.__name__ + "=" + str(val)
                self._pset.addTerminal(val, _type, name=terminal_name)
        if self.verbosity > 2:
            print('{} operators have been imported by TPOT.'.format(len(self.operators)))

    def _setup_toolbox(self):
        creator.create('FitnessMulti', base.Fitness, weights=(-1.0, 1.0))
        creator.create('Individual', gp.PrimitiveTree, fitness=creator.FitnessMulti)
        self._toolbox = base.Toolbox()
        self._toolbox.register('expr', self._gen_grow_safe, pset=self._pset, min_=1, max_=3)
        self._toolbox.register('individual', tools.initIterate, creator.Individual, self._toolbox.expr)
        self._toolbox.register('population', tools.initRepeat, list, self._toolbox.individual)
        self._toolbox.register('compile', self._compile_to_sklearn)
        self._toolbox.register('select', tools.selNSGA2)
        self._toolbox.register('mate', self._mate_operator)
        self._toolbox.register('expr_mut', self._gen_grow_safe, min_=1, max_=4)
        self._toolbox.register('mutate', self._random_mutation_operator)
    def fit(self, features, classes, sample_weight=None):
                features = features.astype(np.float64)
                if self.classification:
            clf = DecisionTreeClassifier(max_depth=5)
        else:
            clf = DecisionTreeRegressor(max_depth=5)
        try:
            clf = clf.fit(features, classes)
        except:
            raise ValueError('Error: Input data is not in a valid format. '
                             'Please confirm that the input data is scikit-learn compatible. '
                             'For example, the features must be a 2-D array and target labels '
                             'must be a 1-D array.')
                if self.random_state is not None:
            random.seed(self.random_state)             np.random.seed(self.random_state)
        self._start_datetime = datetime.now()
        self._toolbox.register('evaluate', self._evaluate_individuals, features=features, classes=classes, sample_weight=sample_weight)
                if self._pop:
            pop = self._pop
        else:
            pop = self._toolbox.population(n=self.population_size)
        def pareto_eq(ind1, ind2):
                        return np.allclose(ind1.fitness.values, ind2.fitness.values)
                if not self.warm_start or not self._pareto_front:
            self._pareto_front = tools.ParetoFront(similar=pareto_eq)
                if self.max_time_mins:
            total_evals = self.population_size
        else:
            total_evals = self.offspring_size * self.generations + self.population_size
        self._pbar = tqdm(total=total_evals, unit='pipeline', leave=False,
                          disable=not (self.verbosity >= 2), desc='Optimization Progress')
        try:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore')
                pop, _ = eaMuPlusLambda(population=pop, toolbox=self._toolbox,
                    mu=self.population_size, lambda_=self.offspring_size,
                    cxpb=self.crossover_rate, mutpb=self.mutation_rate,
                    ngen=self.generations, pbar=self._pbar, halloffame=self._pareto_front,
                    verbose=self.verbosity, max_time_mins=self.max_time_mins)
                        if self.warm_start:
                self._pop = pop
                except (KeyboardInterrupt, SystemExit):
            if self.verbosity > 0:
                self._pbar.write('')                 self._pbar.write('TPOT closed prematurely. Will use the current best pipeline.')
        finally:
                                    if not isinstance(self._pbar, type(None)):
                self._pbar.close()
                        if self._pareto_front:
                top_score = -float('inf')
                for pipeline, pipeline_scores in zip(self._pareto_front.items, reversed(self._pareto_front.keys)):
                    if pipeline_scores.wvalues[1] > top_score:
                        self._optimized_pipeline = pipeline
                        top_score = pipeline_scores.wvalues[1]
                                                if not self._optimized_pipeline:
                    print('There was an error in the TPOT optimization '
                          'process. This could be because the data was '
                          'not formatted properly, or because data for '
                          'a regression problem was provided to the '
                          'TPOTClassifier object. Please make sure you '
                          'passed the data to TPOT correctly.')
                else:
                    self._fitted_pipeline = self._toolbox.compile(expr=self._optimized_pipeline)
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        self._fitted_pipeline.fit(features, classes)
                    if self.verbosity in [1, 2]:
                                                if self.verbosity >= 2:
                            print('')
                        print('Best pipeline: {}'.format(self._optimized_pipeline))
                                        elif self.verbosity >= 3 and self._pareto_front:
                        self._pareto_front_fitted_pipelines = {}
                        for pipeline in self._pareto_front.items:
                            self._pareto_front_fitted_pipelines[str(pipeline)] = self._toolbox.compile(expr=pipeline)
                            with warnings.catch_warnings():
                                warnings.simplefilter('ignore')
                                self._pareto_front_fitted_pipelines[str(pipeline)].fit(features, classes)
    def predict(self, features):
                if not self._fitted_pipeline:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        return self._fitted_pipeline.predict(features.astype(np.float64))
    def fit_predict(self, features, classes):
                self.fit(features, classes)
        return self.predict(features)
    def score(self, testing_features, testing_classes):
                if self._fitted_pipeline is None:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
                return abs(SCORERS[self.scoring_function](self._fitted_pipeline,
            testing_features.astype(np.float64), testing_classes.astype(np.float64)))
    def predict_proba(self, features):
                if not self._fitted_pipeline:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        else:
            if not(hasattr(self._fitted_pipeline, 'predict_proba')):
                raise RuntimeError('The fitted pipeline does not have the predict_proba() function.')
            return self._fitted_pipeline.predict_proba(features.astype(np.float64))
    def set_params(self, **params):
                self.__init__(**params)
        return self
    def export(self, output_file_name):
                if self._optimized_pipeline is None:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        with open(output_file_name, 'w') as output_file:
            output_file.write(export_pipeline(self._optimized_pipeline, self.operators, self._pset))
    def _compile_to_sklearn(self, expr):
                sklearn_pipeline = generate_pipeline_code(expr_to_tree(expr, self._pset), self.operators)
        return eval(sklearn_pipeline, self.operators_context)
    def _set_param_recursive(self, pipeline_steps, parameter, value):
                for (_, obj) in pipeline_steps:
            recursive_attrs = ['steps', 'transformer_list', 'estimators']
            for attr in recursive_attrs:
                if hasattr(obj, attr):
                    self._set_param_recursive(getattr(obj, attr), parameter, value)
                    break
            else:
                if hasattr(obj, parameter):
                    setattr(obj, parameter, value)
    def _evaluate_individuals(self, individuals, features, classes, sample_weight = None):
                if self.max_time_mins:
            total_mins_elapsed = (datetime.now() - self._start_datetime).total_seconds() / 60.
            if total_mins_elapsed >= self.max_time_mins:
                raise KeyboardInterrupt('{} minutes have elapsed. TPOT will close down.'.format(total_mins_elapsed))
                fitnesses_dict = {}
                eval_individuals_str = []
        sklearn_pipeline_list = []
        operator_count_list = []
        test_idx_list = []
        for indidx, individual in enumerate(individuals):
                                    individual = individuals[indidx]
            individual_str = str(individual)
            if individual_str.count('PolynomialFeatures') > 1:
                if self.verbosity > 2:
                    self._pbar.write('Invalid pipeline encountered. Skipping its evaluation.')
                fitnesses_dict[indidx] = (5000., -float('inf'))
                if not self._pbar.disable:
                    self._pbar.update(1)
                        elif individual_str in self._evaluated_individuals:
                                fitnesses_dict[indidx] = self._evaluated_individuals[individual_str]
                if self.verbosity > 2:
                    self._pbar.write('Pipeline encountered that has previously been evaluated during the '
                                     'optimization process. Using the score from the previous evaluation.')
                if not self._pbar.disable:
                    self._pbar.update(1)
            else:
                try:
                                        sklearn_pipeline = self._toolbox.compile(expr=individual)
                                        self._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
                                        operator_count = 0
                    for i in range(len(individual)):
                        node = individual[i]
                        if ((type(node) is deap.gp.Terminal) or
                             type(node) is deap.gp.Primitive and node.name == 'CombineDFs'):
                            continue
                        operator_count += 1
                except Exception:
                    fitnesses_dict[indidx] = (5000., -float('inf'))
                    if not self._pbar.disable:
                        self._pbar.update(1)
                    continue
                eval_individuals_str.append(individual_str)
                operator_count_list.append(operator_count)
                sklearn_pipeline_list.append(sklearn_pipeline)
                test_idx_list.append(indidx)
        @_timeout(max_eval_time_mins=self.max_eval_time_mins)
        def _wrapped_cross_val_score(sklearn_pipeline, features=features, classes=classes,
                                     cv=self.cv, scoring_function=self.scoring_function,
                                     sample_weight=sample_weight):
            sample_weight_dict = set_sample_weight(sklearn_pipeline.steps, sample_weight)
            from .decorators import TimedOutExc
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    cv_scores = cross_val_score(sklearn_pipeline, features, classes,
                        cv=cv, scoring=scoring_function,
                        n_jobs=1, fit_params=sample_weight_dict)
                resulting_score = np.mean(cv_scores)
            except TimedOutExc:
                resulting_score = 'Timeout'
            except Exception:
                resulting_score = -float('inf')
            return resulting_score
        if not sys.platform.startswith('win'):
            if self.n_jobs == -1:
                pool = ProcessPool()
            else:
                pool = ProcessPool(nodes=self.n_jobs)
            res_imap = pool.imap(_wrapped_cross_val_score, sklearn_pipeline_list)
            if not self._pbar.disable:
                ini_pbar_n = self._pbar.n
                        while not self._pbar.disable:
                tmp_fitness = np.array(res_imap._items)
                num_job_done = len(tmp_fitness)
                if not self._pbar.disable and num_job_done:
                    timeout_index = list(np.where(tmp_fitness[:, 1] == 'Timeout')[0])
                    for idx in timeout_index:
                        if self.verbosity > 2 and self._pbar.n - ini_pbar_n <= idx:
                            self._pbar.write('Skipped pipeline                                              'Continuing to the next pipeline.'.format(ini_pbar_n + idx + 1))
                    self._pbar.update(ini_pbar_n + num_job_done - self._pbar.n)
                if num_job_done >= len(sklearn_pipeline_list):
                    break
                else:
                    time.sleep(0.2)
            resulting_score_list = [-float('inf') if x == 'Timeout' else x for x in list(res_imap)]
        else:
            resulting_score_list = []
            for sklearn_pipeline in sklearn_pipeline_list:
                try:
                    resulting_score = _wrapped_cross_val_score(sklearn_pipeline)
                except TimedOutExc:
                    resulting_score = -float('inf')
                    if self.verbosity > 2 and not self._pbar.disable:
                        self._pbar.write('Skipped pipeline                                          'Continuing to the next pipeline.'.format(self._pbar.n + 1))
                resulting_score_list.append(resulting_score)
                if not self._pbar.disable:
                    self._pbar.update(1)
        for resulting_score, operator_count, individual_str, test_idx in zip(resulting_score_list, operator_count_list, eval_individuals_str, test_idx_list):
            if type(resulting_score) in [float, np.float64, np.float32]:
                self._evaluated_individuals[individual_str] = (max(1, operator_count), resulting_score)
                fitnesses_dict[test_idx] = self._evaluated_individuals[individual_str]
            else:
                raise ValueError('Scoring function does not return a float.')
        fitnesses_ordered = []
        for key in sorted(fitnesses_dict.keys()):
            fitnesses_ordered.append(fitnesses_dict[key])
        return fitnesses_ordered
    @_pre_test
    def _mate_operator(self, ind1, ind2):
        return gp.cxOnePoint(ind1, ind2)
    @_pre_test
    def _random_mutation_operator(self, individual):
                                mutation_techniques = [
            partial(gp.mutInsert, pset=self._pset),
            partial(mutNodeReplacement, pset=self._pset),
            partial(gp.mutShrink)
        ]
        return np.random.choice(mutation_techniques)(individual)
    def _gen_grow_safe(self, pset, min_, max_, type_=None):
                def condition(height, depth, type_):
                        return type_ not in [np.ndarray, Output_Array] or depth == height
        return self._generate(pset, min_, max_, condition, type_)
        @_pre_test
    def _generate(self, pset, min_, max_, condition, type_=None):
                if type_ is None:
            type_ = pset.ret
        expr = []
        height = np.random.randint(min_, max_)
        stack = [(0, type_)]
        while len(stack) != 0:
            depth, type_ = stack.pop()
                        if condition(height, depth, type_):
                try:
                    term = np.random.choice(pset.terminals[type_])
                except IndexError:
                    _, _, traceback = sys.exc_info()
                    raise IndexError("The gp.generate function tried to add "
                                      "a terminal of type '%s', but there is "
                                      "none available." % (type_,)).\
                                      with_traceback(traceback)
                if inspect.isclass(term):
                    term = term()
                expr.append(term)
            else:
                try:
                    prim = np.random.choice(pset.primitives[type_])
                except IndexError:
                    _, _, traceback = sys.exc_info()
                    raise IndexError("The gp.generate function tried to add "
                                      "a primitive of type '%s', but there is "
                                      "none available." % (type_,)).\
                                      with_traceback(traceback)
                expr.append(prim)
                for arg in reversed(prim.args):
                    stack.append((depth+1, arg))
        return expr

import numpy as np
from sklearn.base import BaseEstimator
from sklearn.utils import check_array

class ZeroCount(BaseEstimator):
    
    def __init__(self):
        pass
    def fit(self, X, y=None):
                return self
    def transform(self, X, y=None):
                X = check_array(X)
        n_features = X.shape[1]
        X_transformed = np.copy(X)
        non_zero = np.apply_along_axis(lambda row: np.count_nonzero(row),
                                        axis=1, arr=X_transformed)
        zero_col = np.apply_along_axis(lambda row: (n_features - np.count_nonzero(row)),
                                        axis=1, arr=X_transformed)
        X_transformed = np.insert(X_transformed, n_features, non_zero, axis=1)
        X_transformed = np.insert(X_transformed, n_features + 1, zero_col, axis=1)
        return X_transformed
class CombineDFs(object):
    
    @property
    def __name__(self):
        return self.__class__.__name__
import numpy as np
classifier_config_dict = {
        'sklearn.naive_bayes.GaussianNB': {
    },
    'sklearn.naive_bayes.BernoulliNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
    },
    'sklearn.naive_bayes.MultinomialNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
    },
    'sklearn.tree.DecisionTreeClassifier': {
        'criterion': ["gini", "entropy"],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21)
    },
    'sklearn.ensemble.ExtraTreesClassifier': {
        'criterion': ["gini", "entropy"],
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.RandomForestClassifier': {
        'criterion': ["gini", "entropy"],
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf':  range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.GradientBoostingClassifier': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'subsample': np.arange(0.05, 1.01, 0.05),
        'max_features': np.arange(0, 1.01, 0.05)
    },
    'sklearn.neighbors.KNeighborsClassifier': {
        'n_neighbors': range(1, 101),
        'weights': ["uniform", "distance"],
        'p': [1, 2]
    },
    'sklearn.svm.LinearSVC': {
        'penalty': ["l1", "l2"],
        'loss': ["hinge", "squared_hinge"],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]
    },
    'sklearn.linear_model.LogisticRegression': {
        'penalty': ["l1", "l2"],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'dual': [True, False]
    },
    'xgboost.XGBClassifier': {
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21)
    },
        'sklearn.preprocessing.Binarizer': {
        'threshold': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.decomposition.FastICA': {
        'tol': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.cluster.FeatureAgglomeration': {
        'linkage': ['ward', 'complete', 'average'],
        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed']
    },
    'sklearn.preprocessing.MaxAbsScaler': {
    },
    'sklearn.preprocessing.MinMaxScaler': {
    },
    'sklearn.preprocessing.Normalizer': {
        'norm': ['l1', 'l2', 'max']
    },
    'sklearn.kernel_approximation.Nystroem': {
        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],
        'gamma': np.arange(0.0, 1.01, 0.05),
        'n_components': range(1, 11)
    },
    'sklearn.decomposition.PCA': {
        'svd_solver': ['randomized'],
        'iterated_power': range(1, 11)
    },
    'sklearn.preprocessing.PolynomialFeatures': {
        'degree': [2],
        'include_bias': [False],
        'interaction_only': [False]
    },
    'sklearn.kernel_approximation.RBFSampler': {
        'gamma': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.preprocessing.RobustScaler': {
    },
    'sklearn.preprocessing.StandardScaler': {
    },
    'tpot.built_in_operators.ZeroCount': {
    },
        'sklearn.feature_selection.SelectFwe': {
        'alpha': np.arange(0, 0.05, 0.001),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            } 
    },
    'sklearn.feature_selection.SelectKBest': {
        'k': range(1, 100),         'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.SelectPercentile': {
        'percentile': range(1, 100),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.VarianceThreshold': {
        'threshold': np.arange(0.05, 1.01, 0.05)
    },
    'sklearn.feature_selection.RFE': {
        'step': np.arange(0.05, 1.01, 0.05),
        'estimator': {
            'sklearn.svm.SVC': {
                'kernel': ['linear'],
                'random_state': [42]
                }
        }
    },
   'sklearn.feature_selection.SelectFromModel': {
        'threshold': np.arange(0, 1.01, 0.05),
        'estimator': {
            'sklearn.ensemble.ExtraTreesClassifier': {
                'criterion': ['gini', 'entropy'],
                'max_features': np.arange(0, 1.01, 0.05)
                }
        }
    }
}
import numpy as np
regressor_config_dict = {

    'sklearn.linear_model.ElasticNetCV': {
        'l1_ratio': np.arange(0.0, 1.01, 0.05),
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
    },
    'sklearn.ensemble.ExtraTreesRegressor': {
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.GradientBoostingRegressor': {
        'loss': ["ls", "lad", "huber", "quantile"],
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'subsample': np.arange(0.05, 1.01, 0.05),
        'max_features': np.arange(0, 1.01, 0.05),
        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]
    },
    'sklearn.ensemble.AdaBoostRegressor': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'loss': ["linear", "square", "exponential"],
        'max_depth': range(1, 11)
    },
    'sklearn.tree.DecisionTreeRegressor': {
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21)
    },
    'sklearn.neighbors.KNeighborsRegressor': {
        'n_neighbors': range(1, 101),
        'weights': ["uniform", "distance"],
        'p': [1, 2]
    },
    'sklearn.linear_model.LassoLarsCV': {
        'normalize': [True, False]
    },
    'sklearn.svm.LinearSVR': {
        'loss': ["epsilon_insensitive", "squared_epsilon_insensitive"],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1.]
    },
    'sklearn.ensemble.RandomForestRegressor': {
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.linear_model.RidgeCV': {
    },

    'xgboost.XGBRegressor': {
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21)
    },
        'sklearn.preprocessing.Binarizer': {
        'threshold': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.decomposition.FastICA': {
        'tol': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.cluster.FeatureAgglomeration': {
        'linkage': ['ward', 'complete', 'average'],
        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed']
    },
    'sklearn.preprocessing.MaxAbsScaler': {
    },
    'sklearn.preprocessing.MinMaxScaler': {
    },
    'sklearn.preprocessing.Normalizer': {
        'norm': ['l1', 'l2', 'max']
    },
    'sklearn.kernel_approximation.Nystroem': {
        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],
        'gamma': np.arange(0.0, 1.01, 0.05),
        'n_components': range(1, 11)
    },
    'sklearn.decomposition.PCA': {
        'svd_solver': ['randomized'],
        'iterated_power': range(1, 11)
    },
    'sklearn.preprocessing.PolynomialFeatures': {
        'degree': [2],
        'include_bias': [False],
        'interaction_only': [False]
    },
    'sklearn.kernel_approximation.RBFSampler': {
        'gamma': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.preprocessing.RobustScaler': {
    },
    'sklearn.preprocessing.StandardScaler': {
    },
    'tpot.built_in_operators.ZeroCount': {
    },
        'sklearn.feature_selection.SelectFwe': {
        'alpha': np.arange(0, 0.05, 0.001),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            } 
    },
    'sklearn.feature_selection.SelectKBest': {
        'k': range(1, 100),         'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.SelectPercentile': {
        'percentile': range(1, 100),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.VarianceThreshold': {
        'threshold': np.arange(0.05, 1.01, 0.05)
    },
    'sklearn.feature_selection.SelectFromModel': {
        'threshold': np.arange(0, 1.01, 0.05),
        'estimator': {
                'sklearn.ensemble.ExtraTreesRegressor': {
                    'max_features': np.arange(0, 1.01, 0.05)
                    }
                }
    }
}

from __future__ import print_function
from threading import Thread, current_thread
from functools import wraps
import sys
import warnings
from sklearn.datasets import make_classification, make_regression
from .export_utils import expr_to_tree, generate_pipeline_code
from deap import creator
pretest_X, pretest_y = make_classification(n_samples=50, n_features=10, random_state=42)
pretest_X_reg, pretest_y_reg = make_regression(n_samples=50, n_features=10, random_state=42)

def convert_mins_to_secs(time_minute):
            return max(int(time_minute * 60), 1)

class TimedOutExc(RuntimeError):
    
def timeout_signal_handler(signum, frame):
        raise TimedOutExc("Time Out!")
def _timeout(max_eval_time_mins=5):
        def wrap_func(func):
        if not sys.platform.startswith('win'):
            import signal
            @wraps(func)
            def limitedTime(*args, **kw):
                old_signal_hander = signal.signal(signal.SIGALRM, timeout_signal_handler)
                max_time_seconds = convert_mins_to_secs(max_eval_time_mins)
                signal.alarm(max_time_seconds)
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        ret = func(*args, **kw)
                except:
                    raise TimedOutExc('Time Out!')
                finally:
                    signal.signal(signal.SIGALRM, old_signal_hander)                      signal.alarm(0)                  return ret
        else:
            class InterruptableThread(Thread):
                def __init__(self, args, kwargs):
                    Thread.__init__(self)
                    self.args = args
                    self.kwargs = kwargs
                    self.result = -float('inf')
                    self.daemon = True
                def stop(self):
                    self._stop()
                def run(self):
                                                            current_thread().name = 'MainThread'
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        self.result = func(*self.args, **self.kwargs)
            @wraps(func)
            def limitedTime(*args, **kwargs):
                sys.tracebacklimit = 0
                max_time_seconds = convert_mins_to_secs(max_eval_time_mins)
                                tmp_it = InterruptableThread(args, kwargs)
                tmp_it.start()
                                tmp_it.join(max_time_seconds)
                if tmp_it.isAlive():
                    raise TimedOutExc('Time Out!')
                sys.tracebacklimit = 1000
                tmp_it.stop()
                return tmp_it.result
        return limitedTime
    return wrap_func


def _pre_test(func):
        @wraps(func)
    def check_pipeline(self, *args, **kwargs):
        bad_pipeline = True
        num_test = 0         while bad_pipeline and num_test < 10:                         args = [self._toolbox.clone(arg) if isinstance(arg, creator.Individual) else arg for arg in args]
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    expr = func(self, *args, **kwargs)
                                        expr_tuple = expr if isinstance(expr, tuple) else (expr,)
                    for expr_test in expr_tuple:
                                                sklearn_pipeline = eval(generate_pipeline_code(expr_to_tree(expr_test, self._pset), self.operators), self.operators_context)
                        if self.classification:
                            sklearn_pipeline.fit(pretest_X, pretest_y)
                        else:
                            sklearn_pipeline.fit(pretest_X_reg, pretest_y_reg)
                        bad_pipeline = False
            except BaseException as e:
                if self.verbosity > 2:
                    print_function = print
                                        if not isinstance(self._pbar, type(None)):
                        print_function = self._pbar.write
                    print_function('_pre_test decorator: {fname}: num_test={n} {e}'.format(n=num_test, fname=func.__name__, e=e))
            finally:
                num_test += 1
        return expr
    return check_pipeline

import numpy as np
import argparse
from sklearn.model_selection import train_test_split
from .tpot import TPOTClassifier, TPOTRegressor
from ._version import __version__

def positive_integer(value):
        try:
        value = int(value)
    except Exception:
        raise argparse.ArgumentTypeError('Invalid int value: \'{}\''.format(value))
    if value < 0:
        raise argparse.ArgumentTypeError('Invalid positive int value: \'{}\''.format(value))
    return value

def float_range(value):
        try:
        value = float(value)
    except:
        raise argparse.ArgumentTypeError('Invalid float value: \'{}\''.format(value))
    if value < 0.0 or value > 1.0:
        raise argparse.ArgumentTypeError('Invalid float value: \'{}\''.format(value))
    return value

def main():
        parser = argparse.ArgumentParser(description='A Python tool that '
        'automatically creates and optimizes machine learning pipelines using '
        'genetic programming.', add_help=False)
    parser.add_argument('INPUT_FILE', type=str, help='Data file to use in the TPOT '
        'optimization process. Ensure that the class label column is labeled as "class".')
    parser.add_argument('-h', '--help', action='help',
        help='Show this help message and exit.')
    parser.add_argument('-is', action='store', dest='INPUT_SEPARATOR', default='\t',
        type=str, help='Character used to separate columns in the input file.')
    parser.add_argument('-target', action='store', dest='TARGET_NAME', default='class',
        type=str, help='Name of the target column in the input file.')
    parser.add_argument('-mode', action='store', dest='TPOT_MODE',
        choices=['classification', 'regression'], default='classification', type=str,
        help='Whether TPOT is being used for a supervised classification or regression problem.')
    parser.add_argument('-o', action='store', dest='OUTPUT_FILE', default='',
        type=str, help='File to export the code for the final optimized pipeline.')
    parser.add_argument('-g', action='store', dest='GENERATIONS', default=100,
        type=positive_integer, help='Number of iterations to run the pipeline optimization process.\n'
        'Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline. '
        'TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.')
    parser.add_argument('-p', action='store', dest='POPULATION_SIZE', default=100,
        type=positive_integer, help='Number of individuals to retain in the GP population every generation.\n'
        'Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize the pipeline. '
        'TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.')
    parser.add_argument('-os', action='store', dest='OFFSPRING_SIZE', default=None,
        type=positive_integer, help='Number of offspring to produce in each GP generation. '
        'By default, OFFSPRING_SIZE = POPULATION_SIZE.')
    parser.add_argument('-mr', action='store', dest='MUTATION_RATE', default=0.9,
        type=float_range, help='GP mutation rate in the range [0.0, 1.0]. This tells the '
        'GP algorithm how many pipelines to apply random changes to every generation. '
        'We recommend using the default parameter unless you understand how the mutation '
        'rate affects GP algorithms.')
    parser.add_argument('-xr', action='store', dest='CROSSOVER_RATE', default=0.1,
        type=float_range, help='GP crossover rate in the range [0.0, 1.0]. This tells the '
        'GP algorithm how many pipelines to "breed" every generation. '
        'We recommend using the default parameter unless you understand how the crossover '
        'rate affects GP algorithms.')
    parser.add_argument('-scoring', action='store', dest='SCORING_FN', default=None,
        type=str, help='Function used to evaluate the quality of a given pipeline for '
        'the problem. By default, accuracy is used for classification problems and mean '
        'squared error (mse) is used for regression problems. '
        'TPOT assumes that any function with "error" or "loss" in the name is meant to '
        'be minimized, whereas any other functions will be maximized. '
        'Offers the same options as cross_val_score: '
        '"accuracy", "adjusted_rand_score", "average_precision", "f1", "f1_macro", '
        '"f1_micro", "f1_samples", "f1_weighted", "log_loss", "mean_absolute_error", '
        '"mean_squared_error", "median_absolute_error", "precision", "precision_macro", '
        '"precision_micro", "precision_samples", "precision_weighted", "r2", "recall", '
        '"recall_macro", "recall_micro", "recall_samples", "recall_weighted", "roc_auc"')
    parser.add_argument('-cv', action='store', dest='NUM_CV_FOLDS', default=5,
        type=int, help='Number of folds to evaluate each pipeline over in '
        'k-fold cross-validation during the TPOT optimization process.')
    parser.add_argument('-njobs', action='store', dest='NUM_JOBS', default=1,
        type=int, help='Number of CPUs for evaluating pipelines in parallel '
        ' during the TPOT optimization process. Assigning this to -1 will use as many '
        'cores as available on the computer.')
    parser.add_argument('-maxtime', action='store', dest='MAX_TIME_MINS', default=None,
        type=int, help='How many minutes TPOT has to optimize the pipeline. This '
        'setting will override the GENERATIONS parameter '
        'and allow TPOT to run until it runs out of time.')
    parser.add_argument('-maxeval', action='store', dest='MAX_EVAL_MINS', default=5,
        type=float, help='How many minutes TPOT has to evaluate a single pipeline. '
        'Setting this parameter to higher values will allow TPOT to explore more complex '
        'pipelines but will also allow TPOT to run longer.')
    parser.add_argument('-s', action='store', dest='RANDOM_STATE', default=None,
        type=int, help='Random number generator seed for reproducibility. Set '
        'this seed if you want your TPOT run to be reproducible with the same '
        'seed and data set in the future.')
    parser.add_argument('-config', action='store', dest='CONFIG_FILE', default='',
        type=str, help='Configuration file for customizing the operators and parameters '
        'that TPOT uses in the optimization process.')
    parser.add_argument('-v', action='store', dest='VERBOSITY', default=1,
        choices=[0, 1, 2, 3], type=int, help='How much information TPOT communicates '
        'while it is running: 0 = none, 1 = minimal, 2 = high, 3 = all. '
        'A setting of 2 or higher will add a progress bar during the optimization procedure.')
    parser.add_argument('--no-update-check', action='store_true',
        dest='DISABLE_UPDATE_CHECK', default=False,
        help='Flag indicating whether the TPOT version checker should be disabled.')
    parser.add_argument('--version', action='version',
        version='TPOT {version}'.format(version=__version__),
        help='Show the TPOT version number and exit.')
    args = parser.parse_args()
    if args.VERBOSITY >= 2:
        print('\nTPOT settings:')
        for arg in sorted(args.__dict__):
            arg_val = args.__dict__[arg]
            if arg == 'DISABLE_UPDATE_CHECK':
                continue
            elif arg == 'SCORING_FN' and arg_val is None:
                if args.TPOT_MODE == 'classification':
                    arg_val = 'accuracy'
                else:
                    arg_val = 'mean_squared_error'
            elif arg == 'OFFSPRING_SIZE' and arg_val is None:
                arg_val = args.__dict__['POPULATION_SIZE']
            print('{}\t=\t{}'.format(arg, arg_val))
        print('')
    input_data = np.recfromcsv(args.INPUT_FILE, delimiter=args.INPUT_SEPARATOR, dtype=np.float64, case_sensitive=True)
    if args.TARGET_NAME not in input_data.dtype.names:
        raise ValueError('The provided data file does not seem to have a target column. '
                         'Please make sure to specify the target column using the -target parameter.')
    features = np.delete(input_data.view(np.float64).reshape(input_data.size, -1),
                         input_data.dtype.names.index(args.TARGET_NAME), axis=1)
    training_features, testing_features, training_classes, testing_classes = \
        train_test_split(features, input_data[args.TARGET_NAME], random_state=args.RANDOM_STATE)
    if args.TPOT_MODE == 'classification':
        tpot_type = TPOTClassifier
    else:
        tpot_type = TPOTRegressor
    operator_dict = None
    if args.CONFIG_FILE:
        try:
            with open(args.CONFIG_FILE, 'r') as input_file:
                file_string =  input_file.read()
            operator_dict = eval(file_string[file_string.find('{'):(file_string.rfind('}') + 1)])
        except:
            raise TypeError('The operator configuration file is in a bad format or not available. '
                            'Please check the configuration file before running TPOT.')
    tpot = tpot_type(generations=args.GENERATIONS, population_size=args.POPULATION_SIZE,
                     offspring_size=args.OFFSPRING_SIZE, mutation_rate=args.MUTATION_RATE, crossover_rate=args.CROSSOVER_RATE,
                     cv=args.NUM_CV_FOLDS, n_jobs=args.NUM_JOBS,
                     scoring=args.SCORING_FN,
                     max_time_mins=args.MAX_TIME_MINS, max_eval_time_mins=args.MAX_EVAL_MINS,
                     random_state=args.RANDOM_STATE, config_dict=operator_dict,
                     verbosity=args.VERBOSITY, disable_update_check=args.DISABLE_UPDATE_CHECK)
    print('')
    tpot.fit(training_features, training_classes)
    if args.VERBOSITY in [1, 2] and tpot._optimized_pipeline:
        training_score = max([tpot._pareto_front.keys[x].wvalues[1] for x in range(len(tpot._pareto_front.keys))])
        print('\nTraining score: {}'.format(abs(training_score)))
        print('Holdout score: {}'.format(tpot.score(testing_features, testing_classes)))
    elif args.VERBOSITY >= 3 and tpot._pareto_front:
        print('Final Pareto front testing scores:')
        for pipeline, pipeline_scores in zip(tpot._pareto_front.items, reversed(tpot._pareto_front.keys)):
            tpot._fitted_pipeline = tpot._pareto_front_fitted_pipelines[str(pipeline)]
            print('{}\t{}\t{}'.format(int(abs(pipeline_scores.wvalues[0])),
                                      tpot.score(testing_features, testing_classes),
                                      pipeline))
    if args.OUTPUT_FILE != '':
        tpot.export(args.OUTPUT_FILE)

if __name__ == '__main__':
    main()

import deap
def get_by_name(opname, operators):
        ret_op_classes = [op for op in operators if op.__name__ == opname]
    if len(ret_op_classes) == 0:
        raise TypeError('Cannot found operator {} in operator dictionary'.format(opname))
    elif len(ret_op_classes) > 1:
        print('Found multiple operator {} in operator dictionary'.format(opname),
        'Please check your dictionary file.')
    ret_op_class = ret_op_classes[0]
    return ret_op_class
def export_pipeline(exported_pipeline, operators, pset):
            pipeline_tree = expr_to_tree(exported_pipeline, pset)
        pipeline_text = generate_import_code(exported_pipeline, operators)
        pipeline_text += pipeline_code_wrapper(generate_export_pipeline_code(pipeline_tree, operators))
    return pipeline_text

def expr_to_tree(ind, pset):
        def prim_to_list(prim, args):
        if isinstance(prim, deap.gp.Terminal):
            if prim.name in pset.context:
                 return pset.context[prim.name]
            else:
                 return prim.value
        return [prim.name] + args
    tree = []
    stack = []
    for node in ind:
        stack.append((node, []))
        while len(stack[-1][1]) == stack[-1][0].arity:
            prim, args = stack.pop()
            tree = prim_to_list(prim, args)
            if len(stack) == 0:
                break               stack[-1][1].append(tree)
    return tree

def generate_import_code(pipeline, operators):
            operators_used = [x.name for x in pipeline if isinstance(x, deap.gp.Primitive)]
    pipeline_text = 'import numpy as np\n\n'
        num_op = len(operators_used)
        import_relations = {}
    for op in operators:
        import_relations[op.__name__] = op.import_hash
        num_op_root = 0
    for op in operators_used:
        if op != 'CombineDFs':
            tpot_op = get_by_name(op, operators)
            if tpot_op.root:
                num_op_root += 1
        else:
            num_op_root += 1
        if num_op_root > 1:
        pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline', 'make_union'],
            'sklearn.preprocessing':    ['FunctionTransformer'],
            'sklearn.ensemble':         ['VotingClassifier']
        }
    elif num_op > 1:
        pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline']
        }
    else:         pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split']
        }
        for op in operators_used:
        def merge_imports(old_dict, new_dict):
                        for key in new_dict.keys():
                if key in old_dict.keys():
                                        old_dict[key] = set(old_dict[key]) | set(new_dict[key])
                else:
                    old_dict[key] = set(new_dict[key])
        try:
            operator_import = import_relations[op]
            merge_imports(pipeline_imports, operator_import)
        except KeyError:
            pass  
        for key in sorted(pipeline_imports.keys()):
        module_list = ', '.join(sorted(pipeline_imports[key]))
        pipeline_text += 'from {} import {}\n'.format(key, module_list)
    pipeline_text += 
    return pipeline_text

def pipeline_code_wrapper(pipeline_code):
        return     steps = process_operator(pipeline_tree, operators)
    pipeline_text = "make_pipeline(\n{STEPS}\n)".format(STEPS=_indent(",\n".join(steps), 4))
    return pipeline_text
def generate_export_pipeline_code(pipeline_tree, operators):
        steps = process_operator(pipeline_tree, operators)
        num_step = len(steps)
    if num_step > 1:
        pipeline_text = "make_pipeline(\n{STEPS}\n)".format(STEPS=_indent(",\n".join(steps), 4))
    else:         pipeline_text =  "{STEPS}".format(STEPS=_indent(",\n".join(steps), 0))
    return pipeline_text
def process_operator(operator, operators, depth=0):
    steps = []
    op_name = operator[0]
    if op_name == "CombineDFs":
        steps.append(
            _combine_dfs(operator[1], operator[2], operators)
        )
    else:
        input_name, args = operator[1], operator[2:]
        tpot_op = get_by_name(op_name, operators)
        if input_name != 'input_matrix':
            steps.extend(process_operator(input_name, operators, depth + 1))
                        if tpot_op.root and depth > 0:
            steps.append(
                "make_union(VotingClassifier([(\"est\", {})]), FunctionTransformer(lambda X: X))".
                format(tpot_op.export(*args))
            )
        else:
            steps.append(tpot_op.export(*args))
    return steps

def _indent(text, amount):
        indentation = amount * ' '
    return indentation + ('\n' + indentation).join(text.split('\n'))

def _combine_dfs(left, right, operators):
    def _make_branch(branch):
        if branch == "input_matrix":
            return "FunctionTransformer(lambda X: X)"
        elif branch[0] == "CombineDFs":
            return _combine_dfs(branch[1], branch[2], operators)
        elif branch[1] == "input_matrix":              tpot_op = get_by_name(branch[0], operators)
            if tpot_op.root:
                return Copyright 2015-Present Randal S. Olson
This file is modified based on codes for alogrithms.eaSimple module in DEAP.
This file is part of the TPOT library.
The TPOT library is free software: you can redistribute it and/or
modify it under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your option)
any later version.
The TPOT library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
details. You should have received a copy of the GNU General Public License along
with the TPOT library. If not, see http://www.gnu.org/licenses/.
    offspring = []
    for _ in range(lambda_):
        op_choice = np.random.random()
        if op_choice < cxpb:                        idxs = np.random.randint(0, len(population),size=2)
            ind1, ind2 = toolbox.clone(population[idxs[0]]), toolbox.clone(population[idxs[1]])
            ind_str = str(ind1)
            num_loop = 0
            while ind_str == str(ind1) and num_loop < 50 :                 ind1, ind2 = toolbox.mate(ind1, ind2)
                num_loop += 1
            if ind_str != str(ind1):                 del ind1.fitness.values
            offspring.append(ind1)
        elif op_choice < cxpb + mutpb:              idx = np.random.randint(0, len(population))
            ind = toolbox.clone(population[idx])
            ind_str = str(ind)
            num_loop = 0
            while ind_str == str(ind) and num_loop < 50 :                 ind, = toolbox.mutate(ind)
                num_loop += 1
            if ind_str != str(ind):                 del ind.fitness.values
            offspring.append(ind)
        else:             idx = np.random.randint(0, len(population))
            offspring.append(toolbox.clone(population[idx]))
    return offspring
def eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar,
                   stats=None, halloffame=None, verbose=0, max_time_mins = None):
        logbook = tools.Logbook()
    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])
        invalid_ind = [ind for ind in population if not ind.fitness.valid]
    fitnesses = toolbox.evaluate(invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    if halloffame is not None:
        halloffame.update(population)
    record = stats.compile(population) if stats is not None else {}
    logbook.record(gen=0, nevals=len(invalid_ind), **record)
        for gen in range(1, ngen + 1):
                offspring = varOr(population, toolbox, lambda_, cxpb, mutpb)
                invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
                if not pbar.disable:
            pbar.update(len(offspring)-len(invalid_ind))
            if not (max_time_mins is None) and pbar.n >= pbar.total:
                pbar.total += lambda_
        fitnesses = toolbox.evaluate(invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            ind.fitness.values = fit

                if halloffame is not None:
            halloffame.update(offspring)
                population[:] = toolbox.select(population + offspring, mu)
                if not pbar.disable:
                        if verbose == 2:
                high_score = abs(max([halloffame.keys[x].wvalues[1] for x in range(len(halloffame.keys))]))
                pbar.write('Generation {0} - Current best internal CV score: {1}'.format(gen, high_score))
                        elif verbose == 3:
                pbar.write('Generation {} - Current Pareto front scores:'.format(gen))
                for pipeline, pipeline_scores in zip(halloffame.items, reversed(halloffame.keys)):
                    pbar.write('{}\t{}\t{}'.format(int(abs(pipeline_scores.wvalues[0])),
                                                         abs(pipeline_scores.wvalues[1]),
                                                         pipeline))
                pbar.write('')
                record = stats.compile(population) if stats is not None else {}
        logbook.record(gen=gen, nevals=len(invalid_ind), **record)
    return population, logbook

def mutNodeReplacement(individual, pset):
    
    index = np.random.randint(0, len(individual))
    node = individual[index]
    slice_ = individual.searchSubtree(index)
    if node.arity == 0:          term = np.random.choice(pset.terminals[node.ret])
        if isclass(term):
            term = term()
        individual[index] = term
    else:                   rindex = None
        if index + 1 < len(individual):
            for i, tmpnode in enumerate(individual[index+1:], index+ 1):
                if isinstance(tmpnode, gp.Primitive) and tmpnode.ret in tmpnode.args:
                    rindex = i
                                primitives = pset.primitives[node.ret]
        if len(primitives) != 0:
            new_node = np.random.choice(primitives)
            new_subtree = [None] * len(new_node.args)
            if rindex:
                rnode = individual[rindex]
                rslice = individual.searchSubtree(rindex)
                                position = np.random.choice([i for i, a in enumerate(new_node.args) if a == rnode.ret])
            else:
                position = None
            for i, arg_type in enumerate(new_node.args):
                if i != position:
                    term = np.random.choice(pset.terminals[arg_type])
                    if isclass(term):
                        term = term()
                    new_subtree[i] = term
                        if rindex:
                new_subtree[position:position + 1] = individual[rslice]
                        new_subtree.insert(0, new_node)
            individual[slice_] = new_subtree
    return individual,

class Output_Array(object):
    
    pass

import numpy as np
from sklearn.metrics import make_scorer, SCORERS

def balanced_accuracy(y_true, y_pred):
        all_classes = list(set(np.append(y_true, y_pred)))
    all_class_accuracies = []
    for this_class in all_classes:
        this_class_sensitivity = \
            float(sum((y_pred == this_class) & (y_true == this_class))) /\
            float(sum((y_true == this_class)))
        this_class_specificity = \
            float(sum((y_pred != this_class) & (y_true != this_class))) /\
            float(sum((y_true != this_class)))
        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.
        all_class_accuracies.append(this_class_accuracy)
    return np.mean(all_class_accuracies)
SCORERS['balanced_accuracy'] = make_scorer(balanced_accuracy)

import numpy as np
from sklearn.base import ClassifierMixin
from sklearn.base import RegressorMixin
import inspect

class Operator(object):
        def __init__(self):
        pass
    root = False      import_hash = None
    sklearn_class = None
    arg_types = None
    dep_op_list = {} 
class ARGType(object):
        def __init__(self):
     pass

def source_decode(sourcecode):
        tmp_path = sourcecode.split('.')
    op_str = tmp_path.pop()
    import_str = '.'.join(tmp_path)
    try:
        if sourcecode.startswith('tpot.'):
            exec('from {} import {}'.format(import_str[4:], op_str))
        else:
            exec('from {} import {}'.format(import_str, op_str))
        op_obj = eval(op_str)
    except ImportError:
        print('Warning: {} is not available and will not be used by TPOT.'.format(sourcecode))
        op_obj = None
    return import_str, op_str, op_obj
def set_sample_weight(pipeline_steps, sample_weight=None):
        sample_weight_dict = {}
    if not isinstance(sample_weight, type(None)):
        for (pname, obj) in pipeline_steps:
            if inspect.getargspec(obj.fit).args.count('sample_weight'):
                step_sw = pname + '__sample_weight'
                sample_weight_dict[step_sw] = sample_weight
    if sample_weight_dict:
        return sample_weight_dict
    else:
        return None
def ARGTypeClassFactory(classname, prange, BaseClass=ARGType):
        return type(classname, (BaseClass,), {'values':prange})
def TPOTOperatorClassFactory(opsourse, opdict, BaseClass=Operator, ArgBaseClass=ARGType):
    
    class_profile = {}
    dep_op_list = {}
    import_str, op_str, op_obj = source_decode(opsourse)
    if not op_obj:
        return None, None     else:
                if issubclass(op_obj, ClassifierMixin) or issubclass(op_obj, RegressorMixin):
            class_profile['root'] = True
            optype = "Classifier or Regressor"
        else:
            optype = "Preprocessor or Selector"
        @classmethod
        def op_type(cls):
                        return optype
        class_profile['type'] = op_type
        class_profile['sklearn_class'] = op_obj
        import_hash = {}
        import_hash[import_str] = [op_str]
        arg_types = []
        for pname in sorted(opdict.keys()):
            prange = opdict[pname]
            if not isinstance(prange, dict):
                classname = '{}__{}'.format(op_str, pname)
                arg_types.append(ARGTypeClassFactory(classname, prange))
            else:
                for dkey, dval in prange.items():
                    dep_import_str, dep_op_str, dep_op_obj = source_decode(dkey)
                    if dep_import_str in import_hash:
                        import_hash[import_str].append(dep_op_str)
                    else:
                        import_hash[dep_import_str] = [dep_op_str]
                    dep_op_list[pname]=dep_op_str
                    if dval:
                        for dpname in sorted(dval.keys()):
                            dprange = dval[dpname]
                            classname = '{}__{}__{}'.format(op_str, dep_op_str, dpname)
                            arg_types.append(ARGTypeClassFactory(classname, dprange))
        class_profile['arg_types'] = tuple(arg_types)
        class_profile['import_hash'] = import_hash
        class_profile['dep_op_list'] = dep_op_list
        @classmethod
        def parameter_types(cls):
                        return ([np.ndarray] + arg_types, np.ndarray)

        class_profile['parameter_types'] = parameter_types
        @classmethod
        def export(cls, *args):
            
            op_arguments = []
            if dep_op_list:
                dep_op_arguments = {}
            for arg_class, arg_value in zip(arg_types, args):
                if arg_value == "DEFAULT":
                    continue
                aname_split = arg_class.__name__.split('__')
                if isinstance(arg_value, str):
                    arg_value = '\"{}\"'.format(arg_value)
                if len(aname_split) == 2:                     op_arguments.append("{}={}".format(aname_split[-1], arg_value))
                else:                     if not list(dep_op_list.values()).count(aname_split[1]):
                        raise TypeError('Warning: the operator {} is not in right format in the operator dictionary'.format(aname_split[0]))
                    else:
                        if aname_split[1] not in dep_op_arguments:
                            dep_op_arguments[aname_split[1]] = []
                        dep_op_arguments[aname_split[1]].append("{}={}".format(aname_split[-1], arg_value))
            tmp_op_args = []
            if dep_op_list:
                                for dep_op_pname, dep_op_str in dep_op_list.items():
                    if dep_op_str == 'f_classif':
                        arg_value = dep_op_str
                    else:
                        arg_value = "{}({})".format(dep_op_str, ", ".join(dep_op_arguments[dep_op_str]))
                    tmp_op_args.append("{}={}".format(dep_op_pname, arg_value))
            op_arguments = tmp_op_args + op_arguments
            return "{}({})".format(op_obj.__name__, ", ".join(op_arguments))
        class_profile['export'] = export
        op_classname = 'TPOT_{}'.format(op_str)
        op_class = type(op_classname, (BaseClass,), class_profile)
        op_class.__name__ = op_str
        return op_class, arg_types

from .base import TPOTBase
from .config_classifier import classifier_config_dict
from .config_regressor import regressor_config_dict

class TPOTClassifier(TPOTBase):
    
    scoring_function = 'accuracy'      default_config_dict = classifier_config_dict     classification = True
    regression = False

class TPOTRegressor(TPOTBase):
    
    scoring_function = 'neg_mean_squared_error'      default_config_dict = regressor_config_dict     classification = False
    regression = True

__version__ = '0.7.0'

from ._version import __version__
from .tpot import TPOTClassifier, TPOTRegressor
from .driver import main
import numpy as np
from sklearn.kernel_approximation import RBFSampler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeClassifier
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = make_pipeline(
    RBFSampler(gamma=0.8500000000000001),
    DecisionTreeClassifier(criterion="entropy", max_depth=3, min_samples_leaf=4, min_samples_split=9)
)
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = KNeighborsClassifier(n_neighbors=4, p=2, weights="distance")
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = RandomForestClassifier(bootstrap=False, max_features=0.4, min_samples_leaf=1, min_samples_split=9)
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)
result1 = tpot_data.copy()
gbc1 = GradientBoostingClassifier(learning_rate=0.49, max_features=1.0, min_weight_fraction_leaf=0.09, n_estimators=500, random_state=42)
gbc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['gbc1-classification'] = gbc1.predict(result1.drop('class', axis=1).values)
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import PassiveAggressiveClassifier
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)
result1 = tpot_data.copy()
pagr1 = PassiveAggressiveClassifier(C=0.81, loss="squared_hinge", fit_intercept=True, random_state=42)
pagr1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['pagr1-classification'] = pagr1.predict(result1.drop('class', axis=1).values)
